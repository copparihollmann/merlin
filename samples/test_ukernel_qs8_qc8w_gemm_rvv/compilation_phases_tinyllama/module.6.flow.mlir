#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "znver3", cpu_features = "+prfchw,-cldemote,+avx,+aes,+sahf,+pclmul,-xop,+crc32,-amx-fp8,+xsaves,-avx512fp16,-usermsr,-sm4,-egpr,+sse4.1,-avx512ifma,+xsave,+sse4.2,-tsxldtrk,-sm3,-ptwrite,-widekl,-movrs,+invpcid,+64bit,+xsavec,-avx10.1-512,-avx512vpopcntdq,+cmov,-avx512vp2intersect,-avx512cd,+movbe,-avxvnniint8,-ccmp,-amx-int8,-kl,-avx10.1-256,-sha512,-avxvnni,-rtm,+adx,+avx2,-hreset,-movdiri,-serialize,+vpclmulqdq,-avx512vl,-uintr,-cf,+clflushopt,-raoint,-cmpccxadd,+bmi,-amx-tile,+sse,-avx10.2-256,-gfni,-avxvnniint16,-amx-fp16,-zu,-ndd,+xsaveopt,+rdrnd,-avx512f,-amx-bf16,-avx512bf16,-avx512vnni,-push2pop2,+cx8,-avx512bw,+sse3,+pku,-nf,-amx-tf32,-amx-avx512,+fsgsbase,+clzero,+mwaitx,-lwp,+lzcnt,+sha,-movdir64b,-ppx,+wbnoinvd,-enqcmd,-amx-transpose,-avx10.2-512,-avxneconvert,-tbm,-pconfig,-amx-complex,+ssse3,+cx16,+bmi2,+fma,+popcnt,-avxifma,+f16c,-avx512bitalg,+rdpru,+clwb,+mmx,+sse2,+rdseed,-avx512vbmi2,-prefetchi,-amx-movrs,+rdpid,-fma4,-avx512vbmi,+shstk,+vaes,-waitpkg,-sgx,+fxsr,-avx512dq,+sse4a", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 32 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0)>
#map1 = affine_map<(d0, d1) -> (d0, d1)>
#map2 = affine_map<(d0) -> (d0)>
#map3 = affine_map<(d0, d1) -> (d1)>
#map4 = affine_map<(d0, d1, d2) -> (d0, d1)>
#map5 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map6 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map7 = affine_map<(d0, d1, d2, d3) -> (d1, d2, d3)>
#map8 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#map9 = affine_map<(d0, d1, d2, d3, d4) -> (d0, d1, d2, d4)>
#map10 = affine_map<(d0, d1, d2, d3, d4) -> (d1, d4)>
#map11 = affine_map<(d0, d1, d2, d3, d4) -> (d0, d1, d2, d3, d4)>
#map12 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map13 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d1, d3)>
#map14 = affine_map<(d0, d1, d2, d3, d4) -> (d0, d2, d1, d4)>
#map15 = affine_map<(d0, d1, d2, d3, d4) -> (d2, d4)>
#map16 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3, d5)>
#map17 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d6, d1, d5)>
#map18 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d6, d1, d4)>
#map19 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> ()>
#map20 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3, d6)>
#map21 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3, d4)>
#map22 = affine_map<(d0, d1, d2, d3, d4) -> (d0, d3, d1, d2, d4)>
#map23 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map24 = affine_map<(d0, d1, d2, d3) -> (d0, d3)>
#map25 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d3, d1, d4, d5)>
#map26 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2, d3, d4, d5)>
#map27 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d4, d1, d5, d3)>
#map28 = affine_map<(d0, d1, d2, d3) -> (d0, d2)>
#map29 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map30 = affine_map<(d0, d1, d2, d3) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module @module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  flow.executable private @prefill_bs4$async_dispatch_0 {
    flow.executable.export public @prefill_bs4$async_dispatch_0_slow_memcpy workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_0_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<32000x64x1x17xi16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<32000x64x1xi16>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0, 0], sizes = [32000, 64, 1, 1], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<32000x64x1x17xi16>> -> tensor<32000x64x1xi16>
        iree_tensor_ext.dispatch.tensor.store %0, %arg1, offsets = [0, 0, 0], sizes = [32000, 64, 1], strides = [1, 1, 1] : tensor<32000x64x1xi16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<32000x64x1xi16>>
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_1 {
    flow.executable.export public @prefill_bs4$async_dispatch_1_slow_memcpy workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_1_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<32000x64x17xi16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<32000x64x16xi16>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 1], sizes = [32000, 64, 16], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<32000x64x17xi16>> -> tensor<32000x64x16xi16>
        iree_tensor_ext.dispatch.tensor.store %0, %arg1, offsets = [0, 0, 0], sizes = [32000, 64, 16], strides = [1, 1, 1] : tensor<32000x64x16xi16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<32000x64x16xi16>>
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_2 {
    flow.executable.export public @prefill_bs4$async_dispatch_2_elementwise_2048000x32_f16xi8xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_2_elementwise_2048000x32_f16xi8xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048000xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048000x32xi8>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<2048000x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0], sizes = [2048000], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048000xf16>> -> tensor<2048000xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [2048000, 32], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048000x32xi8>> -> tensor<2048000x32xi8>
        %2 = tensor.empty() : tensor<2048000x32xf16>
        %3 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<2048000xf16>, tensor<2048000x32xi8>) outs(%2 : tensor<2048000x32xf16>) {
        ^bb0(%in: f16, %in_0: i8, %out: f16):
          %4 = arith.sitofp %in_0 : i8 to f16
          %5 = arith.mulf %in, %4 : f16
          linalg.yield %5 : f16
        } -> tensor<2048000x32xf16>
        iree_tensor_ext.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [2048000, 32], strides = [1, 1] : tensor<2048000x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<2048000x32xf16>>
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_3 {
    flow.executable.export public @prefill_bs4$async_dispatch_3_elementwise_broadcast_Dx2048_i64xf16 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_3_elementwise_broadcast_Dx2048_i64xf16(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<32000x2048xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x2048xf16>>) {
        %c32 = arith.constant 32 : index
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x2048xf16>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 2048], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<32000x2048xf16>> -> tensor<32000x2048xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %5 = tensor.empty(%0) : tensor<?x2048xf16>
        %6 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%4 : tensor<?xi64>) outs(%5 : tensor<?x2048xf16>) {
        ^bb0(%in: i64, %out: f16):
          %7 = linalg.index 1 : index
          %8 = arith.remsi %7, %c32 : index
          %9 = arith.divsi %7, %c32 : index
          %10 = arith.index_cast %in : i64 to index
          %11 = arith.muli %9, %c32 overflow<nsw> : index
          %12 = arith.addi %8, %11 : index
          %extracted = tensor.extract %3[%10, %12] : tensor<32000x2048xf16>
          linalg.yield %extracted : f16
        } -> tensor<?x2048xf16>
        iree_tensor_ext.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [%0, 2048], strides = [1, 1] : tensor<?x2048xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x2048xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_4 {
    flow.executable.export public @prefill_bs4$async_dispatch_4_elementwise_D_f16xf32 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_4_elementwise_D_f16xf32(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xf32>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xf16>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xf32>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xf16>>{%0} -> tensor<?xf16>
        %4 = tensor.empty(%0) : tensor<?xf32>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xf16>) outs(%4 : tensor<?xf32>) {
        ^bb0(%in: f16, %out: f32):
          %6 = arith.extf %in : f16 to f32
          linalg.yield %6 : f32
        } -> tensor<?xf32>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xf32>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_5 {
    flow.executable.export public @prefill_bs4$async_dispatch_5_reduction_Dx2048_f32 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_5_reduction_Dx2048_f32(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x2048xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xf32>>) {
        %c2_i64 = arith.constant 2 : i64
        %cst = arith.constant 0.000000e+00 : f32
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x2048xf16>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xf32>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [%0, 2048], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x2048xf16>>{%0} -> tensor<?x2048xf16>
        %4 = tensor.empty(%0) : tensor<?xf32>
        %5 = linalg.fill ins(%cst : f32) outs(%4 : tensor<?xf32>) -> tensor<?xf32>
        %6 = tensor.empty(%0) : tensor<?x2048xf32>
        %7 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%3 : tensor<?x2048xf16>) outs(%6 : tensor<?x2048xf32>) {
        ^bb0(%in: f16, %out: f32):
          %9 = arith.extf %in : f16 to f32
          linalg.yield %9 : f32
        } -> tensor<?x2048xf32>
        %8 = linalg.generic {indexing_maps = [#map1, #map], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<?x2048xf32>) outs(%5 : tensor<?xf32>) {
        ^bb0(%in: f32, %out: f32):
          %9 = math.fpowi %in, %c2_i64 : f32, i64
          %10 = arith.addf %9, %out : f32
          linalg.yield %10 : f32
        } -> tensor<?xf32>
        iree_tensor_ext.dispatch.tensor.store %8, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xf32>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_6 {
    flow.executable.export public @prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x2048xf32>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xf32>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x2048xf16>>) {
        %cst = arith.constant 9.99999974E-6 : f32
        %cst_0 = arith.constant 2.048000e+03 : f32
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x2048xf32>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xf32>>{%0}
        %3 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x2048xf16>>{%0}
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [%0, 2048], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x2048xf32>>{%0} -> tensor<?x2048xf32>
        %5 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xf32>>{%0} -> tensor<?xf32>
        %6 = tensor.empty(%0) : tensor<?x2048xf16>
        %7 = linalg.generic {indexing_maps = [#map1, #map, #map1], iterator_types = ["parallel", "parallel"]} ins(%4, %5 : tensor<?x2048xf32>, tensor<?xf32>) outs(%6 : tensor<?x2048xf16>) {
        ^bb0(%in: f32, %in_1: f32, %out: f16):
          %8 = arith.divf %in_1, %cst_0 : f32
          %9 = arith.addf %8, %cst : f32
          %10 = math.rsqrt %9 : f32
          %11 = arith.mulf %in, %10 : f32
          %12 = arith.truncf %11 : f32 to f16
          linalg.yield %12 : f16
        } -> tensor<?x2048xf16>
        iree_tensor_ext.dispatch.tensor.store %7, %3, offsets = [0, 0], sizes = [%0, 2048], strides = [1, 1] : tensor<?x2048xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x2048xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_7 {
    flow.executable.export public @prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048xf32>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x2048xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x2048xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x2048xf16>>{%0}
        %2 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x2048xf16>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [2048], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048xf32>> -> tensor<2048xf32>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [%0, 2048], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x2048xf16>>{%0} -> tensor<?x2048xf16>
        %5 = tensor.empty(%0) : tensor<?x2048xf16>
        %6 = linalg.generic {indexing_maps = [#map3, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%3, %4 : tensor<2048xf32>, tensor<?x2048xf16>) outs(%5 : tensor<?x2048xf16>) {
        ^bb0(%in: f32, %in_0: f16, %out: f16):
          %7 = arith.extf %in_0 : f16 to f32
          %8 = arith.mulf %in, %7 : f32
          %9 = arith.truncf %8 : f32 to f16
          linalg.yield %9 : f16
        } -> tensor<?x2048xf16>
        iree_tensor_ext.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [%0, 2048], strides = [1, 1] : tensor<?x2048xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x2048xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_8 {
    flow.executable.export public @prefill_bs4$async_dispatch_8_slow_memcpy workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_8_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64x1x17xi16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<2048x64x1xi16>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0, 0], sizes = [2048, 64, 1, 1], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64x1x17xi16>> -> tensor<2048x64x1xi16>
        iree_tensor_ext.dispatch.tensor.store %0, %arg1, offsets = [0, 0, 0], sizes = [2048, 64, 1], strides = [1, 1, 1] : tensor<2048x64x1xi16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<2048x64x1xi16>>
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_9 {
    flow.executable.export public @prefill_bs4$async_dispatch_9_slow_memcpy workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_9_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64x17xi16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<2048x64x16xi16>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 1], sizes = [2048, 64, 16], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64x17xi16>> -> tensor<2048x64x16xi16>
        iree_tensor_ext.dispatch.tensor.store %0, %arg1, offsets = [0, 0, 0], sizes = [2048, 64, 16], strides = [1, 1, 1] : tensor<2048x64x16xi16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<2048x64x16xi16>>
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_10 {
    flow.executable.export public @prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64x32xi8>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x64x32xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x2048xf16>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x64x32xf16>>{%0}
        %2 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x2048xf16>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [2048, 64], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64xf16>> -> tensor<2048x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [2048, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64x32xi8>> -> tensor<2048x64x32xi8>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [%0, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x64x32xf16>>{%0} -> tensor<?x64x32xf16>
        %6 = tensor.empty() : tensor<2048x64x32xf16>
        %7 = linalg.generic {indexing_maps = [#map4, #map5, #map5], iterator_types = ["parallel", "parallel", "parallel"]} ins(%3, %4 : tensor<2048x64xf16>, tensor<2048x64x32xi8>) outs(%6 : tensor<2048x64x32xf16>) {
        ^bb0(%in: f16, %in_0: i8, %out: f16):
          %13 = arith.extsi %in_0 : i8 to i32
          %14 = arith.sitofp %13 : i32 to f16
          %15 = arith.mulf %14, %in : f16
          linalg.yield %15 : f16
        } -> tensor<2048x64x32xf16>
        %8 = tensor.empty(%0) : tensor<?x2048xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%8 : tensor<?x2048xf32>) -> tensor<?x2048xf32>
        %10 = linalg.generic {indexing_maps = [#map6, #map7, #map8], iterator_types = ["parallel", "parallel", "reduction", "reduction"]} ins(%5, %7 : tensor<?x64x32xf16>, tensor<2048x64x32xf16>) outs(%9 : tensor<?x2048xf32>) {
        ^bb0(%in: f16, %in_0: f16, %out: f32):
          %13 = arith.mulf %in, %in_0 : f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<?x2048xf32>
        %11 = tensor.empty(%0) : tensor<?x2048xf16>
        %12 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%10 : tensor<?x2048xf32>) outs(%11 : tensor<?x2048xf16>) {
        ^bb0(%in: f32, %out: f16):
          %13 = arith.truncf %in : f32 to f16
          linalg.yield %13 : f16
        } -> tensor<?x2048xf16>
        iree_tensor_ext.dispatch.tensor.store %12, %2, offsets = [0, 0], sizes = [%0, 2048], strides = [1, 1] : tensor<?x2048xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x2048xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_11 {
    flow.executable.export public @prefill_bs4$async_dispatch_11_slow_memcpy workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_11_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<256x64x1x17xi16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<256x64x1xi16>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0, 0], sizes = [256, 64, 1, 1], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<256x64x1x17xi16>> -> tensor<256x64x1xi16>
        iree_tensor_ext.dispatch.tensor.store %0, %arg1, offsets = [0, 0, 0], sizes = [256, 64, 1], strides = [1, 1, 1] : tensor<256x64x1xi16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<256x64x1xi16>>
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_12 {
    flow.executable.export public @prefill_bs4$async_dispatch_12_slow_memcpy workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_12_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<256x64x17xi16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<256x64x16xi16>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 1], sizes = [256, 64, 16], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<256x64x17xi16>> -> tensor<256x64x16xi16>
        iree_tensor_ext.dispatch.tensor.store %0, %arg1, offsets = [0, 0, 0], sizes = [256, 64, 16], strides = [1, 1, 1] : tensor<256x64x16xi16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<256x64x16xi16>>
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_13 {
    flow.executable.export public @prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<256x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<256x64x32xi8>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x64x32xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x256xf16>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x64x32xf16>>{%0}
        %2 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x256xf16>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [256, 64], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<256x64xf16>> -> tensor<256x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [256, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<256x64x32xi8>> -> tensor<256x64x32xi8>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [%0, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x64x32xf16>>{%0} -> tensor<?x64x32xf16>
        %6 = tensor.empty() : tensor<256x64x32xf16>
        %7 = linalg.generic {indexing_maps = [#map4, #map5, #map5], iterator_types = ["parallel", "parallel", "parallel"]} ins(%3, %4 : tensor<256x64xf16>, tensor<256x64x32xi8>) outs(%6 : tensor<256x64x32xf16>) {
        ^bb0(%in: f16, %in_0: i8, %out: f16):
          %13 = arith.extsi %in_0 : i8 to i32
          %14 = arith.sitofp %13 : i32 to f16
          %15 = arith.mulf %14, %in : f16
          linalg.yield %15 : f16
        } -> tensor<256x64x32xf16>
        %8 = tensor.empty(%0) : tensor<?x256xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%8 : tensor<?x256xf32>) -> tensor<?x256xf32>
        %10 = linalg.generic {indexing_maps = [#map6, #map7, #map8], iterator_types = ["parallel", "parallel", "reduction", "reduction"]} ins(%5, %7 : tensor<?x64x32xf16>, tensor<256x64x32xf16>) outs(%9 : tensor<?x256xf32>) {
        ^bb0(%in: f16, %in_0: f16, %out: f32):
          %13 = arith.mulf %in, %in_0 : f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<?x256xf32>
        %11 = tensor.empty(%0) : tensor<?x256xf16>
        %12 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%10 : tensor<?x256xf32>) outs(%11 : tensor<?x256xf16>) {
        ^bb0(%in: f32, %out: f16):
          %13 = arith.truncf %in : f32 to f16
          linalg.yield %13 : f16
        } -> tensor<?x256xf16>
        iree_tensor_ext.dispatch.tensor.store %12, %2, offsets = [0, 0], sizes = [%0, 256], strides = [1, 1] : tensor<?x256xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x256xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_17 {
    flow.executable.export public @prefill_bs4$async_dispatch_17_elementwise_broadcast_1x1xD_f32_pack workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_17_elementwise_broadcast_1x1xD_f32_pack(%arg0: index, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<1x?x1x8x1xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 1 : index
        %1 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<1x?x1x8x1xf32>>{%0}
        %2 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %3 = tensor.empty(%0) : tensor<1x?x1x8x1xf32>
        %4 = tensor.empty(%2) : tensor<1x1x?xf32>
        %5 = linalg.generic {indexing_maps = [#map5], iterator_types = ["parallel", "parallel", "parallel"]} outs(%4 : tensor<1x1x?xf32>) {
        ^bb0(%out: f32):
          %6 = linalg.index 2 : index
          %7 = arith.index_cast %6 : index to i64
          %8 = arith.sitofp %7 : i64 to f32
          linalg.yield %8 : f32
        } -> tensor<1x1x?xf32>
        %pack = linalg.pack %5 padding_value(%cst : f32) outer_dims_perm = [0, 2, 1] inner_dims_pos = [2, 1] inner_tiles = [8, 1] into %3 : tensor<1x1x?xf32> -> tensor<1x?x1x8x1xf32>
        iree_tensor_ext.dispatch.tensor.store %pack, %1, offsets = [0, 0, 0, 0, 0], sizes = [1, %0, 1, 8, 1], strides = [1, 1, 1, 1, 1] : tensor<1x?x1x8x1xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<1x?x1x8x1xf32>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_18 {
    flow.executable.export public @prefill_bs4$async_dispatch_18_batch_mmt4d_1x4xDx1x8x8x1_f32 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_18_batch_mmt4d_1x4xDx1x8x8x1_f32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x?x1x8x1xf32>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<1x4x?x8x8xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant dense<[[[[[1.000000e+00], [0.749894261], [0.562341332], [0.421696514], [0.316227764], [0.237137362], [0.177827939], [0.133352146]]], [[[1.000000e-01], [0.0749894157], [0.056234125], [0.0421696492], [0.0316227749], [0.023713734], [0.0177827943], [0.0133352149]]], [[[0.00999999884], [0.00749894138], [0.00562341232], [0.00421696389], [0.00316227647], [0.00237137382], [0.00177827943], [0.00133352145]]], [[[9.99999931E-4], [7.49894068E-4], [5.62341185E-4], [4.21696401E-4], [3.16227786E-4], [2.37137268E-4], [1.7782794E-4], [1.33352063E-4]]]]]> : tensor<1x4x1x8x1xf32>
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x?x1x8x1xf32>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<1x4x?x8x8xf32>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0, 0, 0], sizes = [1, %0, 1, 8, 1], strides = [1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x?x1x8x1xf32>>{%0} -> tensor<1x?x1x8x1xf32>
        %4 = tensor.empty(%0) : tensor<1x4x?x8x8xf32>
        %5 = linalg.fill ins(%cst : f32) outs(%4 : tensor<1x4x?x8x8xf32>) -> tensor<1x4x?x8x8xf32>
        %6 = linalg.batch_mmt4d ins(%cst_0, %3 : tensor<1x4x1x8x1xf32>, tensor<1x?x1x8x1xf32>) outs(%5 : tensor<1x4x?x8x8xf32>) -> tensor<1x4x?x8x8xf32>
        iree_tensor_ext.dispatch.tensor.store %6, %2, offsets = [0, 0, 0, 0, 0], sizes = [1, 4, %0, 8, 8], strides = [1, 1, 1, 1, 1] : tensor<1x4x?x8x8xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<1x4x?x8x8xf32>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_19 {
    flow.executable.export public @prefill_bs4$async_dispatch_19_unpack_f32 workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_19_unpack_f32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x4x?x8x8xf32>>, %arg1: index, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x1x32xf32>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x4x?x8x8xf32>>{%0}
        %3 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x1x32xf32>>{%1}
        %4 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 0, 0, 0, 0], sizes = [1, 4, %0, 8, 8], strides = [1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x4x?x8x8xf32>>{%0} -> tensor<1x4x?x8x8xf32>
        %5 = tensor.empty(%1) : tensor<?x1x32xf32>
        %unpack = linalg.unpack %4 outer_dims_perm = [1, 2, 0] inner_dims_pos = [2, 0] inner_tiles = [8, 8] into %5 : tensor<1x4x?x8x8xf32> -> tensor<?x1x32xf32>
        iree_tensor_ext.dispatch.tensor.store %unpack, %3, offsets = [0, 0, 0], sizes = [%1, 1, 32], strides = [1, 1, 1] : tensor<?x1x32xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x1x32xf32>>{%1}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_20 {
    flow.executable.export public @prefill_bs4$async_dispatch_20_elementwise_D_f32xf16 workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_20_elementwise_D_f32xf16(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xf32>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 1 : index
        %2 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xf32>>{%1}
        %3 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xf16>>{%0}
        %4 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0], sizes = [%1], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xf32>>{%1} -> tensor<?xf32>
        %5 = tensor.empty(%0) : tensor<?xf16>
        %6 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%4 : tensor<?xf32>) outs(%5 : tensor<?xf16>) {
        ^bb0(%in: f32, %out: f16):
          %7 = math.cos %in : f32
          %8 = arith.truncf %7 : f32 to f16
          linalg.yield %8 : f16
        } -> tensor<?xf16>
        iree_tensor_ext.dispatch.tensor.store %6, %3, offsets = [0], sizes = [%0], strides = [1] : tensor<?xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_21 {
    flow.executable.export public @prefill_bs4$async_dispatch_21_elementwise_D_f32xf16 workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_21_elementwise_D_f32xf16(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xf32>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 1 : index
        %2 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xf32>>{%1}
        %3 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xf16>>{%0}
        %4 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0], sizes = [%1], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xf32>>{%1} -> tensor<?xf32>
        %5 = tensor.empty(%0) : tensor<?xf16>
        %6 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%4 : tensor<?xf32>) outs(%5 : tensor<?xf16>) {
        ^bb0(%in: f32, %out: f16):
          %7 = math.sin %in : f32
          %8 = arith.truncf %7 : f32 to f16
          linalg.yield %8 : f16
        } -> tensor<?xf16>
        iree_tensor_ext.dispatch.tensor.store %6, %3, offsets = [0], sizes = [%0], strides = [1] : tensor<?xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_22 {
    flow.executable.export public @prefill_bs4$async_dispatch_22_slow_memcpy workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_22_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x32x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x32x64xf16>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x32x32xf16>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0, 0], sizes = [4, %0, 32, 32], strides = [1, 1, 1, 2] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x32x64xf16>>{%0} -> tensor<4x?x32x32xf16>
        iree_tensor_ext.dispatch.tensor.store %3, %2, offsets = [0, 0, 0, 0], sizes = [4, %0, 32, 32], strides = [1, 1, 1, 1] : tensor<4x?x32x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x32x32xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_23 {
    flow.executable.export public @prefill_bs4$async_dispatch_23_slow_memcpy workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_23_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x32x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x32x64xf16>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x32x32xf16>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0, 1], sizes = [4, %0, 32, 32], strides = [1, 1, 1, 2] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x32x64xf16>>{%0} -> tensor<4x?x32x32xf16>
        iree_tensor_ext.dispatch.tensor.store %3, %2, offsets = [0, 0, 0, 0], sizes = [4, %0, 32, 32], strides = [1, 1, 1, 1] : tensor<4x?x32x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x32x32xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_24 {
    flow.executable.export public @prefill_bs4$async_dispatch_24_slow_memcpy workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_24_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x4x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x64xf16>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x4x32xf16>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0, 0], sizes = [4, %0, 4, 32], strides = [1, 1, 1, 2] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x64xf16>>{%0} -> tensor<4x?x4x32xf16>
        iree_tensor_ext.dispatch.tensor.store %3, %2, offsets = [0, 0, 0, 0], sizes = [4, %0, 4, 32], strides = [1, 1, 1, 1] : tensor<4x?x4x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x4x32xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_25 {
    flow.executable.export public @prefill_bs4$async_dispatch_25_slow_memcpy workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_25_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x4x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x64xf16>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x4x32xf16>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0, 1], sizes = [4, %0, 4, 32], strides = [1, 1, 1, 2] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x64xf16>>{%0} -> tensor<4x?x4x32xf16>
        iree_tensor_ext.dispatch.tensor.store %3, %2, offsets = [0, 0, 0, 0], sizes = [4, %0, 4, 32], strides = [1, 1, 1, 1] : tensor<4x?x4x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x4x32xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_26 {
    flow.executable.export public @prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16 workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x32xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x32xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32xf16>>, %arg4: index, %arg5: index, %arg6: index, %arg7: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x4x2x32xf16>>) {
        %c0 = arith.constant 0 : index
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg5, 1 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x32xf16>>{%arg6}
        %2 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x32xf16>>{%arg6}
        %4 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32xf16>>{%0}
        %5 = flow.dispatch.tie_shape %arg7 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x4x2x32xf16>>{%0}
        %6 = iree_tensor_ext.dispatch.workload.ordinal %arg4, 0 : index
        %7 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0, 0], sizes = [4, %6, 4, 32], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x32xf16>>{%arg6} -> tensor<4x?x4x32xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 0], sizes = [%0, 32], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32xf16>>{%0} -> tensor<?x32xf16>
        %9 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0, 0, 0], sizes = [4, %6, 4, 32], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x32xf16>>{%arg6} -> tensor<4x?x4x32xf16>
        %10 = iree_tensor_ext.dispatch.tensor.load %4, offsets = [0, 0], sizes = [%0, 32], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32xf16>>{%0} -> tensor<?x32xf16>
        %11 = tensor.empty(%0) : tensor<4x?x4x2x32xf16>
        %12 = linalg.generic {indexing_maps = [#map9, #map10, #map9, #map10, #map11], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%7, %8, %9, %10 : tensor<4x?x4x32xf16>, tensor<?x32xf16>, tensor<4x?x4x32xf16>, tensor<?x32xf16>) outs(%11 : tensor<4x?x4x2x32xf16>) {
        ^bb0(%in: f16, %in_0: f16, %in_1: f16, %in_2: f16, %out: f16):
          %13 = linalg.index 3 : index
          %14 = arith.mulf %in, %in_2 : f16
          %15 = arith.mulf %in_1, %in_0 : f16
          %16 = arith.mulf %in_1, %in_2 : f16
          %17 = arith.mulf %in, %in_0 : f16
          %18 = arith.addf %15, %14 : f16
          %19 = arith.subf %17, %16 : f16
          %20 = arith.cmpi eq, %13, %c0 : index
          %21 = arith.select %20, %19, %18 : f16
          linalg.yield %21 : f16
        } -> tensor<4x?x4x2x32xf16>
        iree_tensor_ext.dispatch.tensor.store %12, %5, offsets = [0, 0, 0, 0, 0], sizes = [4, %0, 4, 2, 32], strides = [1, 1, 1, 1, 1] : tensor<4x?x4x2x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x4x2x32xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_27 {
    flow.executable.export public @prefill_bs4$async_dispatch_27_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_27_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c44_i64 = arith.constant 44 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %5 = tensor.empty(%0) : tensor<?xi64>
        %6:2 = linalg.generic {indexing_maps = [#map2, #map2, #map2], iterator_types = ["parallel"]} ins(%4 : tensor<?xi64>) outs(%5, %5 : tensor<?xi64>, tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64, %out_0: i64):
          %7 = arith.muli %in, %c22_i64 : i64
          %8 = arith.muli %in, %c44_i64 : i64
          linalg.yield %7, %8 : i64, i64
        } -> (tensor<?xi64>, tensor<?xi64>)
        iree_tensor_ext.dispatch.tensor.store %6#0, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        iree_tensor_ext.dispatch.tensor.store %6#1, %3, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_28 {
    flow.executable.export public @prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32x4x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x4x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32x4x64xf16>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x4x32x64xf16>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0, 0], sizes = [%0, 32, 4, 64], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32x4x64xf16>>{%0} -> tensor<?x32x4x64xf16>
        %4 = tensor.empty(%0) : tensor<?x4x32x64xf16>
        %5 = linalg.generic {indexing_maps = [#map12, #map13], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%3 : tensor<?x32x4x64xf16>) outs(%4 : tensor<?x4x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<?x4x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0, 0, 0, 0], sizes = [%0, 4, 32, 64], strides = [1, 1, 1, 1] : tensor<?x4x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x4x32x64xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_29 {
    flow.executable.export public @prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1, %arg2)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x32x2x1x32xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<readwrite:tensor<?x4x32x2x1x32xf16>>, %arg3: index, %arg4: index, %arg5: index) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg4, 1 : index
        %2 = iree_tensor_ext.dispatch.workload.ordinal %arg5, 2 : index
        %3 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x32x2x1x32xf16>>{%0}
        %4 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %5 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readwrite:tensor<?x4x32x2x1x32xf16>>{%2}
        %6 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0, 0, 0, 0, 0, 0], sizes = [4, %0, 4, 32, 2, 1, 32], strides = [1, 1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x32x2x1x32xf16>>{%0} -> tensor<4x?x4x32x2x1x32xf16>
        %7 = iree_tensor_ext.dispatch.tensor.load %4, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %8 = iree_tensor_ext.dispatch.tensor.load %5, offsets = [0, 0, 0, 0, 0, 0], sizes = [%2, 4, 32, 2, 1, 32], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readwrite:tensor<?x4x32x2x1x32xf16>>{%2} -> tensor<?x4x32x2x1x32xf16>
        %9 = iree_linalg_ext.scatter dimension_map = [0] unique_indices(true) ins(%6, %7 : tensor<4x?x4x32x2x1x32xf16>, tensor<4x?xi64>) outs(%8 : tensor<?x4x32x2x1x32xf16>) {
        ^bb0(%arg6: f16, %arg7: f16):
          iree_linalg_ext.yield %arg6 : f16
        } -> tensor<?x4x32x2x1x32xf16>
        iree_tensor_ext.dispatch.tensor.store %9, %5, offsets = [0, 0, 0, 0, 0, 0], sizes = [%2, 4, 32, 2, 1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<?x4x32x2x1x32xf16> -> !iree_tensor_ext.dispatch.tensor<readwrite:tensor<?x4x32x2x1x32xf16>>{%2}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_30 {
    flow.executable.export public @prefill_bs4$async_dispatch_30_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_30_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c1_i64 = arith.constant 1 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c1_i64 : i64
          linalg.yield %6 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_33 {
    flow.executable.export public @prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16 workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x32x32xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x32x32xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32xf16>>, %arg4: index, %arg5: index, %arg6: index, %arg7: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x32x?x2x32xf16>>) {
        %c0 = arith.constant 0 : index
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg5, 1 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x32x32xf16>>{%arg6}
        %2 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x32x32xf16>>{%arg6}
        %4 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32xf16>>{%0}
        %5 = flow.dispatch.tie_shape %arg7 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x32x?x2x32xf16>>{%0}
        %6 = iree_tensor_ext.dispatch.workload.ordinal %arg4, 0 : index
        %7 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0, 0], sizes = [4, %6, 32, 32], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x32x32xf16>>{%arg6} -> tensor<4x?x32x32xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 0], sizes = [%0, 32], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32xf16>>{%0} -> tensor<?x32xf16>
        %9 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0, 0, 0], sizes = [4, %6, 32, 32], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x32x32xf16>>{%arg6} -> tensor<4x?x32x32xf16>
        %10 = iree_tensor_ext.dispatch.tensor.load %4, offsets = [0, 0], sizes = [%0, 32], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32xf16>>{%0} -> tensor<?x32xf16>
        %11 = tensor.empty(%0) : tensor<4x32x?x2x32xf16>
        %12 = linalg.generic {indexing_maps = [#map14, #map15, #map14, #map15, #map11], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%7, %8, %9, %10 : tensor<4x?x32x32xf16>, tensor<?x32xf16>, tensor<4x?x32x32xf16>, tensor<?x32xf16>) outs(%11 : tensor<4x32x?x2x32xf16>) {
        ^bb0(%in: f16, %in_0: f16, %in_1: f16, %in_2: f16, %out: f16):
          %13 = linalg.index 3 : index
          %14 = arith.mulf %in, %in_2 : f16
          %15 = arith.mulf %in_1, %in_0 : f16
          %16 = arith.mulf %in_1, %in_2 : f16
          %17 = arith.mulf %in, %in_0 : f16
          %18 = arith.addf %15, %14 : f16
          %19 = arith.subf %17, %16 : f16
          %20 = arith.cmpi eq, %13, %c0 : index
          %21 = arith.select %20, %19, %18 : f16
          linalg.yield %21 : f16
        } -> tensor<4x32x?x2x32xf16>
        iree_tensor_ext.dispatch.tensor.store %12, %5, offsets = [0, 0, 0, 0, 0], sizes = [4, 32, %0, 2, 32], strides = [1, 1, 1, 1, 1] : tensor<4x32x?x2x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x32x?x2x32xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_34 {
    flow.executable.export public @prefill_bs4$async_dispatch_34_elementwise_broadcast_4xD_i64xf16 workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_34_elementwise_broadcast_4xD_i64xf16(%arg0: index, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?xf16>>) {
        %cst = arith.constant 0xFC00 : f16
        %cst_0 = arith.constant 0.000000e+00 : f16
        %c32 = arith.constant 32 : index
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?xf16>>{%0}
        %2 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 1 : index
        %3 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty(%0) : tensor<4x?xf16>
        %5 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4x?xf16>) {
        ^bb0(%in: i64, %out: f16):
          %6 = linalg.index 1 : index
          %7 = arith.remsi %6, %c32 : index
          %8 = arith.divsi %6, %c32 : index
          %9 = arith.remsi %8, %2 : index
          %10 = arith.divsi %8, %2 : index
          %11 = arith.remsi %10, %c32 : index
          %12 = arith.divsi %10, %c32 : index
          %13 = arith.remsi %12, %2 : index
          %14 = arith.muli %9, %c32 overflow<nsw> : index
          %15 = arith.addi %7, %14 : index
          %16 = arith.index_cast %15 : index to i64
          %17 = arith.cmpi sge, %16, %in : i64
          %18 = arith.muli %13, %c32 overflow<nsw> : index
          %19 = arith.addi %11, %18 : index
          %20 = arith.index_cast %19 : index to i64
          %21 = arith.cmpi sgt, %16, %20 : i64
          %22 = arith.ori %21, %17 : i1
          %23 = arith.select %22, %cst, %cst_0 : f16
          linalg.yield %23 : f16
        } -> tensor<4x?xf16>
        iree_tensor_ext.dispatch.tensor.store %5, %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : tensor<4x?xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_35 {
    flow.executable.export public @prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x4x8x?x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x64xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x64xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x4x8x?x?xf16>>, %arg5: index, %arg6: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x4x8x64xf16>>) {
        %cst = arith.constant 1.250000e-01 : f16
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg5, 1 : index
        %2 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x4x8x?x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x64xf16>>{%0}
        %4 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x64xf16>>{%1}
        %5 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x4x8x?x?xf16>>{%0, %0}
        %6 = flow.dispatch.tie_shape %arg6 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x4x8x64xf16>>{%0}
        %7 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 0, 0, 0, 0], sizes = [4, 4, 8, %0, 64], strides = [1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x4x8x?x64xf16>>{%0} -> tensor<4x4x8x?x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0, 0, 0], sizes = [4, %0, 4, 64], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x64xf16>>{%0} -> tensor<4x?x4x64xf16>
        %9 = iree_tensor_ext.dispatch.tensor.load %4, offsets = [0, 0, 0, 0], sizes = [4, %1, 4, 64], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x64xf16>>{%1} -> tensor<4x?x4x64xf16>
        %10 = iree_tensor_ext.dispatch.tensor.load %5, offsets = [0, 0, 0, 0, 0], sizes = [4, 4, 8, %0, %0], strides = [1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x4x8x?x?xf16>>{%0, %0} -> tensor<4x4x8x?x?xf16>
        %11 = tensor.empty(%0) : tensor<4x4x8x?x64xf16>
        %12 = iree_linalg_ext.attention {indexing_maps = [#map16, #map17, #map18, #map19, #map20, #map21]} ins(%7, %8, %9, %cst, %10 : tensor<4x4x8x?x64xf16>, tensor<4x?x4x64xf16>, tensor<4x?x4x64xf16>, f16, tensor<4x4x8x?x?xf16>) outs(%11 : tensor<4x4x8x?x64xf16>) {
        ^bb0(%arg7: f32):
          iree_linalg_ext.yield %arg7 : f32
        } -> tensor<4x4x8x?x64xf16>
        %13 = tensor.empty(%0) : tensor<4x?x4x8x64xf16>
        %14 = linalg.generic {indexing_maps = [#map11, #map22], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%12 : tensor<4x4x8x?x64xf16>) outs(%13 : tensor<4x?x4x8x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x?x4x8x64xf16>
        iree_tensor_ext.dispatch.tensor.store %14, %6, offsets = [0, 0, 0, 0, 0], sizes = [4, %0, 4, 8, 64], strides = [1, 1, 1, 1, 1] : tensor<4x?x4x8x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x4x8x64xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_38 {
    flow.executable.export public @prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64x32xi8>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x64x32xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x2048xf16>>, %arg5: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x2048xf16>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x64x32xf16>>{%0}
        %2 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x2048xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg5 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x2048xf16>>{%0}
        %4 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [2048, 64], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64xf16>> -> tensor<2048x64xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [2048, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64x32xi8>> -> tensor<2048x64x32xi8>
        %6 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [%0, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x64x32xf16>>{%0} -> tensor<?x64x32xf16>
        %7 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 0], sizes = [%0, 2048], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x2048xf16>>{%0} -> tensor<?x2048xf16>
        %8 = tensor.empty() : tensor<2048x64x32xf16>
        %9 = linalg.generic {indexing_maps = [#map4, #map5, #map5], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4, %5 : tensor<2048x64xf16>, tensor<2048x64x32xi8>) outs(%8 : tensor<2048x64x32xf16>) {
        ^bb0(%in: f16, %in_0: i8, %out: f16):
          %15 = arith.extsi %in_0 : i8 to i32
          %16 = arith.sitofp %15 : i32 to f16
          %17 = arith.mulf %16, %in : f16
          linalg.yield %17 : f16
        } -> tensor<2048x64x32xf16>
        %10 = tensor.empty(%0) : tensor<?x2048xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%10 : tensor<?x2048xf32>) -> tensor<?x2048xf32>
        %12 = linalg.generic {indexing_maps = [#map6, #map7, #map8], iterator_types = ["parallel", "parallel", "reduction", "reduction"]} ins(%6, %9 : tensor<?x64x32xf16>, tensor<2048x64x32xf16>) outs(%11 : tensor<?x2048xf32>) {
        ^bb0(%in: f16, %in_0: f16, %out: f32):
          %15 = arith.mulf %in, %in_0 : f16
          %16 = arith.extf %15 : f16 to f32
          %17 = arith.addf %16, %out : f32
          linalg.yield %17 : f32
        } -> tensor<?x2048xf32>
        %13 = tensor.empty(%0) : tensor<?x2048xf16>
        %14 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%7, %12 : tensor<?x2048xf16>, tensor<?x2048xf32>) outs(%13 : tensor<?x2048xf16>) {
        ^bb0(%in: f16, %in_0: f32, %out: f16):
          %15 = arith.truncf %in_0 : f32 to f16
          %16 = arith.addf %in, %15 : f16
          linalg.yield %16 : f16
        } -> tensor<?x2048xf16>
        iree_tensor_ext.dispatch.tensor.store %14, %3, offsets = [0, 0], sizes = [%0, 2048], strides = [1, 1] : tensor<?x2048xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x2048xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_43 {
    flow.executable.export public @prefill_bs4$async_dispatch_43_slow_memcpy workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_43_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64x1x17xi16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<5632x64x1xi16>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0, 0], sizes = [5632, 64, 1, 1], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64x1x17xi16>> -> tensor<5632x64x1xi16>
        iree_tensor_ext.dispatch.tensor.store %0, %arg1, offsets = [0, 0, 0], sizes = [5632, 64, 1], strides = [1, 1, 1] : tensor<5632x64x1xi16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<5632x64x1xi16>>
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_44 {
    flow.executable.export public @prefill_bs4$async_dispatch_44_slow_memcpy workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_44_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64x17xi16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<5632x64x16xi16>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 1], sizes = [5632, 64, 16], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64x17xi16>> -> tensor<5632x64x16xi16>
        iree_tensor_ext.dispatch.tensor.store %0, %arg1, offsets = [0, 0, 0], sizes = [5632, 64, 16], strides = [1, 1, 1] : tensor<5632x64x16xi16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<5632x64x16xi16>>
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_47 {
    flow.executable.export public @prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64x32xi8>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x64x32xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x5632xf16>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x64x32xf16>>{%0}
        %2 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x5632xf16>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [5632, 64], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64xf16>> -> tensor<5632x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [5632, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64x32xi8>> -> tensor<5632x64x32xi8>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [%0, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x64x32xf16>>{%0} -> tensor<?x64x32xf16>
        %6 = tensor.empty() : tensor<5632x64x32xf16>
        %7 = linalg.generic {indexing_maps = [#map4, #map5, #map5], iterator_types = ["parallel", "parallel", "parallel"]} ins(%3, %4 : tensor<5632x64xf16>, tensor<5632x64x32xi8>) outs(%6 : tensor<5632x64x32xf16>) {
        ^bb0(%in: f16, %in_0: i8, %out: f16):
          %13 = arith.extsi %in_0 : i8 to i32
          %14 = arith.sitofp %13 : i32 to f16
          %15 = arith.mulf %14, %in : f16
          linalg.yield %15 : f16
        } -> tensor<5632x64x32xf16>
        %8 = tensor.empty(%0) : tensor<?x5632xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%8 : tensor<?x5632xf32>) -> tensor<?x5632xf32>
        %10 = linalg.generic {indexing_maps = [#map6, #map7, #map8], iterator_types = ["parallel", "parallel", "reduction", "reduction"]} ins(%5, %7 : tensor<?x64x32xf16>, tensor<5632x64x32xf16>) outs(%9 : tensor<?x5632xf32>) {
        ^bb0(%in: f16, %in_0: f16, %out: f32):
          %13 = arith.mulf %in, %in_0 : f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<?x5632xf32>
        %11 = tensor.empty(%0) : tensor<?x5632xf16>
        %12 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%10 : tensor<?x5632xf32>) outs(%11 : tensor<?x5632xf16>) {
        ^bb0(%in: f32, %out: f16):
          %13 = arith.truncf %in : f32 to f16
          linalg.yield %13 : f16
        } -> tensor<?x5632xf16>
        iree_tensor_ext.dispatch.tensor.store %12, %2, offsets = [0, 0], sizes = [%0, 5632], strides = [1, 1] : tensor<?x5632xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x5632xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_48 {
    flow.executable.export public @prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64x32xi8>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x64x32xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x5632xf16>>, %arg5: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x5632xf16>>) {
        %cst = arith.constant 1.000000e+00 : f16
        %cst_0 = arith.constant 0.000000e+00 : f32
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x64x32xf16>>{%0}
        %2 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x5632xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg5 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x5632xf16>>{%0}
        %4 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [5632, 64], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64xf16>> -> tensor<5632x64xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [5632, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64x32xi8>> -> tensor<5632x64x32xi8>
        %6 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [%0, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x64x32xf16>>{%0} -> tensor<?x64x32xf16>
        %7 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 0], sizes = [%0, 5632], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x5632xf16>>{%0} -> tensor<?x5632xf16>
        %8 = tensor.empty() : tensor<5632x64x32xf16>
        %9 = linalg.generic {indexing_maps = [#map4, #map5, #map5], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4, %5 : tensor<5632x64xf16>, tensor<5632x64x32xi8>) outs(%8 : tensor<5632x64x32xf16>) {
        ^bb0(%in: f16, %in_1: i8, %out: f16):
          %15 = arith.extsi %in_1 : i8 to i32
          %16 = arith.sitofp %15 : i32 to f16
          %17 = arith.mulf %16, %in : f16
          linalg.yield %17 : f16
        } -> tensor<5632x64x32xf16>
        %10 = tensor.empty(%0) : tensor<?x5632xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<?x5632xf32>) -> tensor<?x5632xf32>
        %12 = linalg.generic {indexing_maps = [#map6, #map7, #map8], iterator_types = ["parallel", "parallel", "reduction", "reduction"]} ins(%6, %9 : tensor<?x64x32xf16>, tensor<5632x64x32xf16>) outs(%11 : tensor<?x5632xf32>) {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.mulf %in, %in_1 : f16
          %16 = arith.extf %15 : f16 to f32
          %17 = arith.addf %16, %out : f32
          linalg.yield %17 : f32
        } -> tensor<?x5632xf32>
        %13 = tensor.empty(%0) : tensor<?x5632xf16>
        %14 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%12, %7 : tensor<?x5632xf32>, tensor<?x5632xf16>) outs(%13 : tensor<?x5632xf16>) {
        ^bb0(%in: f32, %in_1: f16, %out: f16):
          %15 = arith.truncf %in : f32 to f16
          %16 = arith.negf %15 : f16
          %17 = math.exp %16 : f16
          %18 = arith.addf %17, %cst : f16
          %19 = arith.divf %cst, %18 : f16
          %20 = arith.mulf %19, %15 : f16
          %21 = arith.mulf %20, %in_1 : f16
          linalg.yield %21 : f16
        } -> tensor<?x5632xf16>
        iree_tensor_ext.dispatch.tensor.store %14, %3, offsets = [0, 0], sizes = [%0, 5632], strides = [1, 1] : tensor<?x5632xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x5632xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_49 {
    flow.executable.export public @prefill_bs4$async_dispatch_49_slow_memcpy workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_49_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x176x1x17xi16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<2048x176x1xi16>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0, 0], sizes = [2048, 176, 1, 1], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x176x1x17xi16>> -> tensor<2048x176x1xi16>
        iree_tensor_ext.dispatch.tensor.store %0, %arg1, offsets = [0, 0, 0], sizes = [2048, 176, 1], strides = [1, 1, 1] : tensor<2048x176x1xi16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<2048x176x1xi16>>
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_50 {
    flow.executable.export public @prefill_bs4$async_dispatch_50_slow_memcpy workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_50_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x176x17xi16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<2048x176x16xi16>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 1], sizes = [2048, 176, 16], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x176x17xi16>> -> tensor<2048x176x16xi16>
        iree_tensor_ext.dispatch.tensor.store %0, %arg1, offsets = [0, 0, 0], sizes = [2048, 176, 16], strides = [1, 1, 1] : tensor<2048x176x16xi16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<2048x176x16xi16>>
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_51 {
    flow.executable.export public @prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x176xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x176x32xi8>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x176x32xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x2048xf16>>, %arg5: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x2048xf16>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x176x32xf16>>{%0}
        %2 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x2048xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg5 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x2048xf16>>{%0}
        %4 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [2048, 176], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x176xf16>> -> tensor<2048x176xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [2048, 176, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x176x32xi8>> -> tensor<2048x176x32xi8>
        %6 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [%0, 176, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x176x32xf16>>{%0} -> tensor<?x176x32xf16>
        %7 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 0], sizes = [%0, 2048], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x2048xf16>>{%0} -> tensor<?x2048xf16>
        %8 = tensor.empty() : tensor<2048x176x32xf16>
        %9 = linalg.generic {indexing_maps = [#map4, #map5, #map5], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4, %5 : tensor<2048x176xf16>, tensor<2048x176x32xi8>) outs(%8 : tensor<2048x176x32xf16>) {
        ^bb0(%in: f16, %in_0: i8, %out: f16):
          %15 = arith.extsi %in_0 : i8 to i32
          %16 = arith.sitofp %15 : i32 to f16
          %17 = arith.mulf %16, %in : f16
          linalg.yield %17 : f16
        } -> tensor<2048x176x32xf16>
        %10 = tensor.empty(%0) : tensor<?x2048xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%10 : tensor<?x2048xf32>) -> tensor<?x2048xf32>
        %12 = linalg.generic {indexing_maps = [#map6, #map7, #map8], iterator_types = ["parallel", "parallel", "reduction", "reduction"]} ins(%6, %9 : tensor<?x176x32xf16>, tensor<2048x176x32xf16>) outs(%11 : tensor<?x2048xf32>) {
        ^bb0(%in: f16, %in_0: f16, %out: f32):
          %15 = arith.mulf %in, %in_0 : f16
          %16 = arith.extf %15 : f16 to f32
          %17 = arith.addf %16, %out : f32
          linalg.yield %17 : f32
        } -> tensor<?x2048xf32>
        %13 = tensor.empty(%0) : tensor<?x2048xf16>
        %14 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%7, %12 : tensor<?x2048xf16>, tensor<?x2048xf32>) outs(%13 : tensor<?x2048xf16>) {
        ^bb0(%in: f16, %in_0: f32, %out: f16):
          %15 = arith.truncf %in_0 : f32 to f16
          %16 = arith.addf %in, %15 : f16
          linalg.yield %16 : f16
        } -> tensor<?x2048xf16>
        iree_tensor_ext.dispatch.tensor.store %14, %3, offsets = [0, 0], sizes = [%0, 2048], strides = [1, 1] : tensor<?x2048xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x2048xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_70 {
    flow.executable.export public @prefill_bs4$async_dispatch_70_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_70_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c1_i64 = arith.constant 1 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c1_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_112 {
    flow.executable.export public @prefill_bs4$async_dispatch_112_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_112_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c2_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_154 {
    flow.executable.export public @prefill_bs4$async_dispatch_154_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_154_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c3_i64 = arith.constant 3 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c3_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_196 {
    flow.executable.export public @prefill_bs4$async_dispatch_196_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_196_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c4_i64 = arith.constant 4 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c4_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_238 {
    flow.executable.export public @prefill_bs4$async_dispatch_238_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_238_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c5_i64 = arith.constant 5 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c5_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_280 {
    flow.executable.export public @prefill_bs4$async_dispatch_280_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_280_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c6_i64 = arith.constant 6 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c6_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_322 {
    flow.executable.export public @prefill_bs4$async_dispatch_322_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_322_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c7_i64 = arith.constant 7 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c7_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_364 {
    flow.executable.export public @prefill_bs4$async_dispatch_364_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_364_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c8_i64 = arith.constant 8 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c8_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_406 {
    flow.executable.export public @prefill_bs4$async_dispatch_406_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_406_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c9_i64 = arith.constant 9 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c9_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_448 {
    flow.executable.export public @prefill_bs4$async_dispatch_448_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_448_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c10_i64 = arith.constant 10 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c10_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_490 {
    flow.executable.export public @prefill_bs4$async_dispatch_490_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_490_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c11_i64 = arith.constant 11 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c11_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_532 {
    flow.executable.export public @prefill_bs4$async_dispatch_532_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_532_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c12_i64 = arith.constant 12 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c12_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_574 {
    flow.executable.export public @prefill_bs4$async_dispatch_574_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_574_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c13_i64 = arith.constant 13 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c13_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_616 {
    flow.executable.export public @prefill_bs4$async_dispatch_616_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_616_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c14_i64 = arith.constant 14 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c14_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_658 {
    flow.executable.export public @prefill_bs4$async_dispatch_658_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_658_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c15_i64 = arith.constant 15 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c15_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_700 {
    flow.executable.export public @prefill_bs4$async_dispatch_700_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_700_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c16_i64 = arith.constant 16 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c16_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_742 {
    flow.executable.export public @prefill_bs4$async_dispatch_742_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_742_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c17_i64 = arith.constant 17 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c17_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_784 {
    flow.executable.export public @prefill_bs4$async_dispatch_784_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_784_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c18_i64 = arith.constant 18 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c18_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_826 {
    flow.executable.export public @prefill_bs4$async_dispatch_826_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_826_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c19_i64 = arith.constant 19 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c19_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_868 {
    flow.executable.export public @prefill_bs4$async_dispatch_868_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_868_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c20_i64 = arith.constant 20 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c20_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_908 {
    flow.executable.export public @prefill_bs4$async_dispatch_908_elementwise_D_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_908_elementwise_D_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c21_i64 = arith.constant 21 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [%0], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?xi64>>{%0} -> tensor<?xi64>
        %4 = tensor.empty(%0) : tensor<?xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<?xi64>) outs(%4 : tensor<?xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = arith.addi %in, %c21_i64 : i64
          %7 = arith.muli %6, %c2_i64 : i64
          linalg.yield %7 : i64
        } -> tensor<?xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %2, offsets = [0], sizes = [%0], strides = [1] : tensor<?xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?xi64>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_914 {
    flow.executable.export public @prefill_bs4$async_dispatch_914_slow_memcpy workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_914_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x8x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x4x8x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x8x64xf16>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x4x8x32xf16>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0, 0, 0], sizes = [4, %0, 4, 8, 32], strides = [1, 1, 1, 1, 2] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x8x64xf16>>{%0} -> tensor<4x?x4x8x32xf16>
        iree_tensor_ext.dispatch.tensor.store %3, %2, offsets = [0, 0, 0, 0, 0], sizes = [4, %0, 4, 8, 32], strides = [1, 1, 1, 1, 1] : tensor<4x?x4x8x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x4x8x32xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_915 {
    flow.executable.export public @prefill_bs4$async_dispatch_915_slow_memcpy workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_915_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x8x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x4x8x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x8x64xf16>>{%0}
        %2 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x4x8x32xf16>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0, 0, 1], sizes = [4, %0, 4, 8, 32], strides = [1, 1, 1, 1, 2] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x4x8x64xf16>>{%0} -> tensor<4x?x4x8x32xf16>
        iree_tensor_ext.dispatch.tensor.store %3, %2, offsets = [0, 0, 0, 0, 0], sizes = [4, %0, 4, 8, 32], strides = [1, 1, 1, 1, 1] : tensor<4x?x4x8x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?x4x8x32xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_916 {
    flow.executable.export public @prefill_bs4$async_dispatch_916_elementwise_broadcast_4x32xDx2x32_f16 workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_916_elementwise_broadcast_4x32xDx2x32_f16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x32x32xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x32x32xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32xf16>>, %arg4: index, %arg5: index, %arg6: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x32x?x2x32xf16>>) {
        %c0 = arith.constant 0 : index
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg4, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg5, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x32x32xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32xf16>>{%1}
        %4 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x32x32xf16>>{%0}
        %5 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32xf16>>{%1}
        %6 = flow.dispatch.tie_shape %arg6 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x32x?x2x32xf16>>{%1}
        %7 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 0, 0, 0], sizes = [4, %0, 32, 32], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x32x32xf16>>{%0} -> tensor<4x?x32x32xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [%1, 32], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32xf16>>{%1} -> tensor<?x32xf16>
        %9 = iree_tensor_ext.dispatch.tensor.load %4, offsets = [0, 0, 0, 0], sizes = [4, %0, 32, 32], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?x32x32xf16>>{%0} -> tensor<4x?x32x32xf16>
        %10 = iree_tensor_ext.dispatch.tensor.load %5, offsets = [0, 0], sizes = [%1, 32], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x32xf16>>{%1} -> tensor<?x32xf16>
        %11 = tensor.empty(%1) : tensor<4x32x?x2x32xf16>
        %12 = linalg.generic {indexing_maps = [#map14, #map15, #map14, #map15, #map11], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%7, %8, %9, %10 : tensor<4x?x32x32xf16>, tensor<?x32xf16>, tensor<4x?x32x32xf16>, tensor<?x32xf16>) outs(%11 : tensor<4x32x?x2x32xf16>) {
        ^bb0(%in: f16, %in_0: f16, %in_1: f16, %in_2: f16, %out: f16):
          %13 = linalg.index 3 : index
          %14 = arith.mulf %in, %in_2 : f16
          %15 = arith.mulf %in_1, %in_0 : f16
          %16 = arith.mulf %in_1, %in_2 : f16
          %17 = arith.mulf %in, %in_0 : f16
          %18 = arith.addf %15, %14 : f16
          %19 = arith.subf %17, %16 : f16
          %20 = arith.cmpi eq, %13, %c0 : index
          %21 = arith.select %20, %19, %18 : f16
          linalg.yield %21 : f16
        } -> tensor<4x32x?x2x32xf16>
        iree_tensor_ext.dispatch.tensor.store %12, %6, offsets = [0, 0, 0, 0, 0], sizes = [4, 32, %1, 2, 32], strides = [1, 1, 1, 1, 1] : tensor<4x32x?x2x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x32x?x2x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @prefill_bs4$async_dispatch_940 {
    flow.executable.export public @prefill_bs4$async_dispatch_940_matmul_like_Dx32000x64x32_f16xf16xf32 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @prefill_bs4$async_dispatch_940_matmul_like_Dx32000x64x32_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<32000x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<32000x64x32xi8>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x64x32xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x32000xf16>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x64x32xf16>>{%0}
        %2 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x32000xf16>>{%0}
        %3 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [32000, 64], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<32000x64xf16>> -> tensor<32000x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [32000, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<32000x64x32xi8>> -> tensor<32000x64x32xi8>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [%0, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x64x32xf16>>{%0} -> tensor<?x64x32xf16>
        %6 = tensor.empty() : tensor<32000x64x32xf16>
        %7 = linalg.generic {indexing_maps = [#map4, #map5, #map5], iterator_types = ["parallel", "parallel", "parallel"]} ins(%3, %4 : tensor<32000x64xf16>, tensor<32000x64x32xi8>) outs(%6 : tensor<32000x64x32xf16>) {
        ^bb0(%in: f16, %in_0: i8, %out: f16):
          %13 = arith.extsi %in_0 : i8 to i32
          %14 = arith.sitofp %13 : i32 to f16
          %15 = arith.mulf %14, %in : f16
          linalg.yield %15 : f16
        } -> tensor<32000x64x32xf16>
        %8 = tensor.empty(%0) : tensor<?x32000xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%8 : tensor<?x32000xf32>) -> tensor<?x32000xf32>
        %10 = linalg.generic {indexing_maps = [#map6, #map7, #map8], iterator_types = ["parallel", "parallel", "reduction", "reduction"]} ins(%5, %7 : tensor<?x64x32xf16>, tensor<32000x64x32xf16>) outs(%9 : tensor<?x32000xf32>) {
        ^bb0(%in: f16, %in_0: f16, %out: f32):
          %13 = arith.mulf %in, %in_0 : f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<?x32000xf32>
        %11 = tensor.empty(%0) : tensor<?x32000xf16>
        %12 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%10 : tensor<?x32000xf32>) outs(%11 : tensor<?x32000xf16>) {
        ^bb0(%in: f32, %out: f16):
          %13 = arith.truncf %in : f32 to f16
          linalg.yield %13 : f16
        } -> tensor<?x32000xf16>
        iree_tensor_ext.dispatch.tensor.store %12, %2, offsets = [0, 0], sizes = [%0, 32000], strides = [1, 1] : tensor<?x32000xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<?x32000xf16>>{%0}
        return
      }
    }
  }
  util.global private @__parameter_model_output_weight_tensor_32000x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"output.weight"> : tensor<32000x2176xi8>
  util.global private @__parameter_model_output_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"output_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_21_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.21.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_21_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.21.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_21_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.21.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_21_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.21.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_21_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.21.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_21_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.21.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_21_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.21.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_21_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.21.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_21_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.21.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_20_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.20.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_20_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.20.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_20_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.20.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_20_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.20.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_20_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.20.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_20_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.20.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_20_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.20.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_20_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.20.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_20_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.20.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_19_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.19.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_19_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.19.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_19_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.19.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_19_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.19.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_19_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.19.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_19_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.19.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_19_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.19.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_19_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.19.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_19_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.19.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_18_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.18.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_18_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.18.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_18_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.18.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_18_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.18.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_18_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.18.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_18_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.18.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_18_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.18.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_18_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.18.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_18_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.18.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_17_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.17.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_17_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.17.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_17_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.17.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_17_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.17.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_17_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.17.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_17_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.17.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_17_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.17.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_17_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.17.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_17_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.17.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_16_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.16.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_16_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.16.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_16_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.16.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_16_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.16.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_16_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.16.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_16_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.16.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_16_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.16.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_16_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.16.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_16_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.16.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_15_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.15.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_15_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.15.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_15_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.15.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_15_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.15.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_15_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.15.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_15_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.15.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_15_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.15.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_15_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.15.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_15_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.15.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_14_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.14.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_14_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.14.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_14_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.14.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_14_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.14.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_14_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.14.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_14_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.14.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_14_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.14.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_14_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.14.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_14_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.14.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_13_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.13.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_13_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.13.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_13_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.13.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_13_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.13.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_13_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.13.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_13_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.13.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_13_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.13.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_13_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.13.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_13_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.13.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_12_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.12.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_12_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.12.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_12_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.12.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_12_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.12.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_12_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.12.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_12_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.12.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_12_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.12.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_12_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.12.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_12_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.12.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_11_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.11.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_11_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.11.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_11_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.11.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_11_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.11.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_11_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.11.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_11_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.11.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_11_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.11.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_11_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.11.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_11_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.11.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_10_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.10.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_10_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.10.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_10_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.10.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_10_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.10.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_10_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.10.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_10_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.10.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_10_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.10.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_10_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.10.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_10_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.10.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_9_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.9.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_9_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.9.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_9_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.9.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_9_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.9.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_9_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.9.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_9_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.9.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_9_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.9.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_9_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.9.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_9_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.9.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_8_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.8.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_8_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.8.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_8_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.8.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_8_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.8.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_8_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.8.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_8_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.8.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_8_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.8.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_8_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.8.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_8_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.8.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_7_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.7.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_7_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.7.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_7_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.7.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_7_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.7.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_7_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.7.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_7_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.7.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_7_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.7.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_7_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.7.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_7_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.7.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_6_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.6.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_6_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.6.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_6_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.6.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_6_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.6.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_6_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.6.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_6_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.6.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_6_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.6.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_6_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.6.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_6_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.6.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_5_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.5.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_5_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.5.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_5_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.5.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_5_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.5.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_5_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.5.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_5_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.5.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_5_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.5.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_5_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.5.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_5_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.5.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_4_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.4.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_4_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.4.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_4_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.4.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_4_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.4.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_4_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.4.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_4_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.4.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_4_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.4.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_4_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.4.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_4_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.4.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_3_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.3.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_3_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.3.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_3_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.3.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_3_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.3.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_3_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.3.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_3_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.3.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_3_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.3.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_3_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.3.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_3_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.3.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_2_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.2.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_2_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.2.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_2_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.2.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_2_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.2.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_2_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.2.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_2_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.2.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_2_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.2.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_2_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.2.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_2_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.2.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_1_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.1.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_1_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.1.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_1_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.1.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_1_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.1.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_1_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.1.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_1_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.1.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_1_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.1.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_1_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.1.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_1_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.1.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_0_ffn_down_weight_tensor_2048x5984xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.0.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__parameter_model_blk_0_ffn_up_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.0.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_0_ffn_gate_weight_tensor_5632x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.0.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__parameter_model_blk_0_ffn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.0.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_blk_0_attn_output_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.0.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_0_attn_v_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.0.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_0_attn_k_weight_tensor_256x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.0.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__parameter_model_blk_0_attn_q_weight_tensor_2048x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.0.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__parameter_model_blk_0_attn_norm_weight_tensor_2048xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"blk.0.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__parameter_model_token_embd_weight_tensor_32000x2176xi8 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = #flow.parameter.named<"model"::"token_embd.weight"> : tensor<32000x2176xi8>
  util.func public @prefill_bs4$async(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view, %arg3: !hal.buffer_view, %arg4: !hal.fence, %arg5: !hal.fence) -> !hal.buffer_view attributes {inlining_policy = #util.inline.never, iree.abi.model = "coarse-fences", iree.abi.stub} {
    %c44 = arith.constant 44 : index
    %c4 = arith.constant 4 : index
    %c8192 = arith.constant 8192 : index
    %c32768 = arith.constant 32768 : index
    %c1024 = arith.constant 1024 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c1 = arith.constant 1 : index
    %__parameter_model_output_weight_tensor_32000x2176xi8 = util.global.load immutable @__parameter_model_output_weight_tensor_32000x2176xi8 : tensor<32000x2176xi8>
    %__parameter_model_output_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_output_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_21_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_21_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_21_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_21_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_21_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_21_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_21_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_21_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_21_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_21_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_21_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_21_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_21_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_21_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_21_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_21_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_21_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_21_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_20_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_20_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_20_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_20_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_20_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_20_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_20_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_20_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_20_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_20_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_20_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_20_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_20_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_20_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_20_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_20_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_20_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_20_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_19_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_19_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_19_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_19_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_19_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_19_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_19_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_19_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_19_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_19_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_19_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_19_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_19_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_19_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_19_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_19_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_19_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_19_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_18_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_18_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_18_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_18_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_18_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_18_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_18_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_18_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_18_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_18_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_18_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_18_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_18_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_18_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_18_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_18_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_18_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_18_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_17_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_17_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_17_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_17_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_17_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_17_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_17_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_17_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_17_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_17_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_17_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_17_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_17_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_17_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_17_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_17_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_17_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_17_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_16_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_16_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_16_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_16_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_16_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_16_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_16_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_16_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_16_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_16_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_16_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_16_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_16_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_16_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_16_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_16_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_16_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_16_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_15_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_15_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_15_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_15_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_15_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_15_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_15_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_15_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_15_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_15_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_15_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_15_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_15_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_15_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_15_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_15_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_15_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_15_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_14_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_14_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_14_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_14_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_14_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_14_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_14_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_14_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_14_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_14_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_14_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_14_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_14_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_14_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_14_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_14_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_14_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_14_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_13_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_13_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_13_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_13_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_13_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_13_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_13_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_13_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_13_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_13_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_13_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_13_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_13_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_13_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_13_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_13_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_13_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_13_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_12_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_12_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_12_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_12_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_12_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_12_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_12_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_12_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_12_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_12_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_12_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_12_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_12_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_12_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_12_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_12_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_12_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_12_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_11_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_11_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_11_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_11_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_11_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_11_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_11_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_11_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_11_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_11_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_11_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_11_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_11_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_11_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_11_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_11_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_11_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_11_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_10_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_10_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_10_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_10_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_10_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_10_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_10_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_10_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_10_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_10_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_10_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_10_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_10_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_10_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_10_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_10_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_10_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_10_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_9_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_9_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_9_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_9_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_9_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_9_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_9_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_9_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_9_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_9_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_9_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_9_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_9_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_9_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_9_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_9_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_9_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_9_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_8_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_8_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_8_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_8_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_8_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_8_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_8_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_8_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_8_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_8_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_8_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_8_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_8_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_8_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_8_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_8_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_8_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_8_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_7_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_7_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_7_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_7_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_7_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_7_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_7_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_7_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_7_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_7_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_7_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_7_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_7_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_7_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_7_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_7_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_7_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_7_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_6_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_6_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_6_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_6_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_6_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_6_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_6_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_6_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_6_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_6_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_6_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_6_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_6_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_6_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_6_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_6_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_6_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_6_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_5_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_5_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_5_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_5_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_5_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_5_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_5_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_5_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_5_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_5_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_5_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_5_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_5_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_5_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_5_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_5_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_5_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_5_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_4_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_4_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_4_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_4_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_4_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_4_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_4_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_4_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_4_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_4_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_4_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_4_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_4_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_4_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_4_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_4_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_4_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_4_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_3_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_3_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_3_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_3_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_3_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_3_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_3_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_3_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_3_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_3_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_3_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_3_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_3_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_3_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_3_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_3_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_3_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_3_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_2_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_2_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_2_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_2_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_2_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_2_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_2_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_2_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_2_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_2_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_2_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_2_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_2_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_2_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_2_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_2_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_2_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_2_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_1_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_1_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_1_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_1_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_1_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_1_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_1_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_1_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_1_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_1_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_1_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_1_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_1_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_1_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_1_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_1_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_1_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_1_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_0_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_0_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_0_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_0_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_0_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_0_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_0_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_0_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_0_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_0_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_0_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_0_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_0_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_0_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_0_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_0_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_0_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_0_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_token_embd_weight_tensor_32000x2176xi8 = util.global.load immutable @__parameter_model_token_embd_weight_tensor_32000x2176xi8 : tensor<32000x2176xi8>
    %0 = hal.buffer_view.dim<%arg0 : !hal.buffer_view>[1] : index
    %1 = hal.tensor.import on(#hal.device.affinity<@__device_0>) wait(%arg4) => %arg0 : !hal.buffer_view -> tensor<4x?xi64>{%0}
    %2 = hal.tensor.import on(#hal.device.affinity<@__device_0>) wait(%arg4) => %arg1 : !hal.buffer_view -> tensor<4xi64>
    %3 = hal.buffer_view.dim<%arg2 : !hal.buffer_view>[1] : index
    %4 = hal.tensor.import on(#hal.device.affinity<@__device_0>) wait(%arg4) => %arg2 : !hal.buffer_view -> tensor<4x?xi64>{%3}
    %5 = hal.buffer_view.dim<%arg3 : !hal.buffer_view>[0] : index
    %6 = hal.tensor.import on(#hal.device.affinity<@__device_0>) wait(%arg4) => %arg3 : !hal.buffer_view -> tensor<?x360448xf16>{%5}
    %7 = arith.muli %3, %c32 : index
    %8 = util.assume.int %7<umin = 32, umax = 2016, udiv = 32> : index
    %9 = util.assume.int %3<umin = 1, umax = 63> : index
    %10 = util.assume.int %5<umin = 1, umax = 9007199254740991> : index
    %11 = flow.tensor.bitcast %__parameter_model_token_embd_weight_tensor_32000x2176xi8 : tensor<32000x2176xi8> -> tensor<32000x64x17xi16>
    %12 = flow.tensor.bitcast %__parameter_model_token_embd_weight_tensor_32000x2176xi8 : tensor<32000x2176xi8> -> tensor<32000x64x1x17xi16>
    %13 = flow.dispatch @prefill_bs4$async_dispatch_0::@prefill_bs4$async_dispatch_0_slow_memcpy(%12) : (tensor<32000x64x1x17xi16>) -> tensor<32000x64x1xi16>
    %14 = flow.dispatch @prefill_bs4$async_dispatch_1::@prefill_bs4$async_dispatch_1_slow_memcpy(%11) : (tensor<32000x64x17xi16>) -> tensor<32000x64x16xi16>
    %15 = flow.tensor.bitcast %13 : tensor<32000x64x1xi16> -> tensor<2048000xf16>
    %16 = flow.tensor.bitcast %14 : tensor<32000x64x16xi16> -> tensor<2048000x32xi8>
    %17 = flow.dispatch @prefill_bs4$async_dispatch_2::@prefill_bs4$async_dispatch_2_elementwise_2048000x32_f16xi8xf16(%15, %16) : (tensor<2048000xf16>, tensor<2048000x32xi8>) -> tensor<2048000x32xf16>
    %18 = flow.tensor.reshape %17 : tensor<2048000x32xf16> -> tensor<32000x2048xf16>
    %19 = arith.muli %8, %c4 overflow<nsw> : index
    %20 = flow.tensor.reshape %1 : tensor<4x?xi64>{%8} -> tensor<?xi64>{%19}
    %21 = flow.dispatch @prefill_bs4$async_dispatch_3::@prefill_bs4$async_dispatch_3_elementwise_broadcast_Dx2048_i64xf16[%19](%19, %18, %20) : (index, tensor<32000x2048xf16>, tensor<?xi64>{%19}) -> tensor<?x2048xf16>{%19}
    %22 = arith.muli %8, %c8192 overflow<nsw> : index
    %23 = flow.tensor.reshape %21 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %24 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %23) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %25 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %21) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %26 = flow.tensor.reshape %24 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %27 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %26, %25) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %28 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_0_attn_norm_weight_tensor_2048xf32, %27) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %29 = flow.tensor.bitcast %__parameter_model_blk_0_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %30 = flow.tensor.bitcast %__parameter_model_blk_0_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %31 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%30) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %32 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%29) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %33 = flow.tensor.bitcast %32 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %34 = flow.tensor.bitcast %31 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %35 = flow.tensor.reshape %28 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %36 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%34, %33, %19, %35) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %37 = flow.tensor.reshape %36 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %38 = flow.tensor.bitcast %__parameter_model_blk_0_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %39 = flow.tensor.bitcast %__parameter_model_blk_0_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %40 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%39) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %41 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%38) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %42 = flow.tensor.bitcast %41 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %43 = flow.tensor.bitcast %40 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %44 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%43, %42, %19, %35) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %45 = flow.tensor.reshape %44 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %46 = flow.tensor.bitcast %__parameter_model_blk_0_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %47 = flow.tensor.bitcast %__parameter_model_blk_0_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %48 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%47) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %49 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%46) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %50 = flow.tensor.bitcast %49 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %51 = flow.tensor.bitcast %48 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %52 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%51, %50, %19, %35) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %53 = arith.divui %8, %c32 : index
    %54 = arith.subi %8, %c1 : index
    %55 = arith.divui %54, %c8 : index
    %56 = arith.addi %55, %c1 : index
    %57 = flow.dispatch @prefill_bs4$async_dispatch_17::@prefill_bs4$async_dispatch_17_elementwise_broadcast_1x1xD_f32_pack[%8, %56](%8, %56) : (index, index) -> tensor<1x?x1x8x1xf32>{%56}
    %58 = flow.dispatch @prefill_bs4$async_dispatch_18::@prefill_bs4$async_dispatch_18_batch_mmt4d_1x4xDx1x8x8x1_f32[%56](%57, %56) : (tensor<1x?x1x8x1xf32>{%56}, index) -> tensor<1x4x?x8x8xf32>{%56}
    %59 = flow.dispatch @prefill_bs4$async_dispatch_19::@prefill_bs4$async_dispatch_19_unpack_f32[%56, %8](%58, %56, %8) : (tensor<1x4x?x8x8xf32>{%56}, index, index) -> tensor<?x1x32xf32>{%8}
    %60 = arith.muli %8, %c32 overflow<nsw> : index
    %61 = flow.tensor.reshape %59 : tensor<?x1x32xf32>{%8} -> tensor<?xf32>{%60}
    %62 = arith.muli %53, %c1024 overflow<nsw> : index
    %63 = flow.dispatch @prefill_bs4$async_dispatch_20::@prefill_bs4$async_dispatch_20_elementwise_D_f32xf16[%62, %60](%62, %61, %60) : (index, tensor<?xf32>{%60}, index) -> tensor<?xf16>{%62}
    %64 = flow.dispatch @prefill_bs4$async_dispatch_21::@prefill_bs4$async_dispatch_21_elementwise_D_f32xf16[%62, %60](%62, %61, %60) : (index, tensor<?xf32>{%60}, index) -> tensor<?xf16>{%62}
    %65 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%37, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %66 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%37, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %67 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%45, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %68 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%45, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %69 = flow.tensor.reshape %63 : tensor<?xf16>{%62} -> tensor<?x32xf16>{%8}
    %70 = flow.tensor.reshape %64 : tensor<?xf16>{%62} -> tensor<?x32xf16>{%8}
    %71 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%67, %69, %68, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %72 = arith.muli %9, %c4 overflow<nsw> : index
    %73 = flow.tensor.reshape %4 : tensor<4x?xi64>{%9} -> tensor<?xi64>{%72}
    %74:2 = flow.dispatch @prefill_bs4$async_dispatch_27::@prefill_bs4$async_dispatch_27_elementwise_D_i64[%72](%72, %73) : (index, tensor<?xi64>{%72}) -> (tensor<?xi64>{%72}, tensor<?xi64>{%72})
    %75 = flow.tensor.reshape %74#1 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %76 = arith.muli %53, %c4 overflow<nsw> : index
    %77 = flow.tensor.reshape %71 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %78 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %77) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %79 = flow.tensor.reshape %78 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %80 = arith.muli %10, %c44 overflow<nsw> : index
    %81 = flow.tensor.reshape %6 : tensor<?x360448xf16>{%10} -> tensor<?x4x32x2x1x32xf16>{%80}
    %82 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%79, %75, %81, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %81{%80}
    %83 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %74#1) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %84 = flow.tensor.reshape %83 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %85 = flow.tensor.reshape %52 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %86 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %85) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %87 = flow.tensor.reshape %86 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %88 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%87, %84, %82, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %82{%80}
    %89 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%65, %69, %66, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %90 = arith.muli %53, %53 overflow<nsw> : index
    %91 = arith.muli %90, %c32768 overflow<nsw> : index
    %92 = flow.dispatch @prefill_bs4$async_dispatch_34::@prefill_bs4$async_dispatch_34_elementwise_broadcast_4xD_i64xf16[%91, %53](%91, %53, %2) : (index, index, tensor<4xi64>) -> tensor<4x?xf16>{%91}
    %93 = flow.tensor.reshape %89 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %94 = flow.tensor.reshape %71 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %95 = flow.tensor.reshape %52 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %96 = flow.tensor.reshape %92 : tensor<4x?xf16>{%91} -> tensor<4x4x8x?x?xf16>{%8, %8}
    %97 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %93, %94, %95, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %98 = flow.tensor.bitcast %__parameter_model_blk_0_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %99 = flow.tensor.bitcast %__parameter_model_blk_0_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %100 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%99) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %101 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%98) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %102 = flow.tensor.bitcast %101 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %103 = flow.tensor.bitcast %100 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %104 = flow.tensor.reshape %97 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %105 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%103, %102, %19, %104, %21) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %106 = flow.tensor.reshape %105 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %107 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %106) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %108 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %105) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %109 = flow.tensor.reshape %107 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %110 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %109, %108) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %111 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_0_ffn_norm_weight_tensor_2048xf32, %110) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %112 = flow.tensor.bitcast %__parameter_model_blk_0_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %113 = flow.tensor.bitcast %__parameter_model_blk_0_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %114 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%113) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %115 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%112) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %116 = flow.tensor.bitcast %115 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %117 = flow.tensor.bitcast %114 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %118 = flow.tensor.bitcast %__parameter_model_blk_0_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %119 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%118) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %120 = flow.tensor.bitcast %119 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %121 = flow.tensor.bitcast %__parameter_model_blk_0_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %122 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%121) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %123 = flow.tensor.bitcast %122 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %124 = flow.tensor.reshape %111 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %125 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%120, %123, %19, %124) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %126 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%117, %116, %19, %124, %125) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %127 = flow.tensor.bitcast %__parameter_model_blk_0_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %128 = flow.tensor.bitcast %__parameter_model_blk_0_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %129 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%128) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %130 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%127) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %131 = flow.tensor.bitcast %130 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %132 = flow.tensor.bitcast %129 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %133 = flow.tensor.reshape %126 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %134 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%132, %131, %19, %133, %105) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %135 = flow.tensor.reshape %134 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %136 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %135) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %137 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %134) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %138 = flow.tensor.reshape %136 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %139 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %138, %137) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %140 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_1_attn_norm_weight_tensor_2048xf32, %139) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %141 = flow.tensor.bitcast %__parameter_model_blk_1_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %142 = flow.tensor.bitcast %__parameter_model_blk_1_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %143 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%142) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %144 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%141) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %145 = flow.tensor.bitcast %144 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %146 = flow.tensor.bitcast %143 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %147 = flow.tensor.reshape %140 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %148 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%146, %145, %19, %147) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %149 = flow.tensor.reshape %148 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %150 = flow.tensor.bitcast %__parameter_model_blk_1_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %151 = flow.tensor.bitcast %__parameter_model_blk_1_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %152 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%151) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %153 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%150) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %154 = flow.tensor.bitcast %153 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %155 = flow.tensor.bitcast %152 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %156 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%155, %154, %19, %147) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %157 = flow.tensor.reshape %156 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %158 = flow.tensor.bitcast %__parameter_model_blk_1_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %159 = flow.tensor.bitcast %__parameter_model_blk_1_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %160 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%159) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %161 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%158) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %162 = flow.tensor.bitcast %161 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %163 = flow.tensor.bitcast %160 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %164 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%163, %162, %19, %147) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %165 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%149, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %166 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%149, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %167 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%157, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %168 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%157, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %169 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%167, %69, %168, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %170 = flow.dispatch @prefill_bs4$async_dispatch_70::@prefill_bs4$async_dispatch_70_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %171 = flow.tensor.reshape %170 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %172 = flow.tensor.reshape %169 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %173 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %172) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %174 = flow.tensor.reshape %173 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %175 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%174, %171, %88, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %88{%80}
    %176 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %170) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %177 = flow.tensor.reshape %176 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %178 = flow.tensor.reshape %164 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %179 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %178) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %180 = flow.tensor.reshape %179 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %181 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%180, %177, %175, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %175{%80}
    %182 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%165, %69, %166, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %183 = flow.tensor.reshape %182 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %184 = flow.tensor.reshape %169 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %185 = flow.tensor.reshape %164 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %186 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %183, %184, %185, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %187 = flow.tensor.bitcast %__parameter_model_blk_1_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %188 = flow.tensor.bitcast %__parameter_model_blk_1_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %189 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%188) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %190 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%187) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %191 = flow.tensor.bitcast %190 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %192 = flow.tensor.bitcast %189 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %193 = flow.tensor.reshape %186 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %194 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%192, %191, %19, %193, %134) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %195 = flow.tensor.reshape %194 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %196 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %195) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %197 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %194) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %198 = flow.tensor.reshape %196 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %199 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %198, %197) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %200 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_1_ffn_norm_weight_tensor_2048xf32, %199) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %201 = flow.tensor.bitcast %__parameter_model_blk_1_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %202 = flow.tensor.bitcast %__parameter_model_blk_1_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %203 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%202) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %204 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%201) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %205 = flow.tensor.bitcast %204 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %206 = flow.tensor.bitcast %203 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %207 = flow.tensor.bitcast %__parameter_model_blk_1_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %208 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%207) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %209 = flow.tensor.bitcast %208 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %210 = flow.tensor.bitcast %__parameter_model_blk_1_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %211 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%210) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %212 = flow.tensor.bitcast %211 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %213 = flow.tensor.reshape %200 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %214 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%209, %212, %19, %213) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %215 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%206, %205, %19, %213, %214) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %216 = flow.tensor.bitcast %__parameter_model_blk_1_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %217 = flow.tensor.bitcast %__parameter_model_blk_1_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %218 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%217) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %219 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%216) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %220 = flow.tensor.bitcast %219 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %221 = flow.tensor.bitcast %218 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %222 = flow.tensor.reshape %215 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %223 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%221, %220, %19, %222, %194) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %224 = flow.tensor.reshape %223 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %225 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %224) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %226 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %223) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %227 = flow.tensor.reshape %225 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %228 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %227, %226) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %229 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_2_attn_norm_weight_tensor_2048xf32, %228) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %230 = flow.tensor.bitcast %__parameter_model_blk_2_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %231 = flow.tensor.bitcast %__parameter_model_blk_2_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %232 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%231) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %233 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%230) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %234 = flow.tensor.bitcast %233 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %235 = flow.tensor.bitcast %232 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %236 = flow.tensor.reshape %229 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %237 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%235, %234, %19, %236) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %238 = flow.tensor.reshape %237 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %239 = flow.tensor.bitcast %__parameter_model_blk_2_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %240 = flow.tensor.bitcast %__parameter_model_blk_2_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %241 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%240) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %242 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%239) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %243 = flow.tensor.bitcast %242 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %244 = flow.tensor.bitcast %241 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %245 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%244, %243, %19, %236) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %246 = flow.tensor.reshape %245 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %247 = flow.tensor.bitcast %__parameter_model_blk_2_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %248 = flow.tensor.bitcast %__parameter_model_blk_2_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %249 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%248) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %250 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%247) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %251 = flow.tensor.bitcast %250 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %252 = flow.tensor.bitcast %249 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %253 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%252, %251, %19, %236) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %254 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%238, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %255 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%238, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %256 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%246, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %257 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%246, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %258 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%256, %69, %257, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %259 = flow.dispatch @prefill_bs4$async_dispatch_112::@prefill_bs4$async_dispatch_112_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %260 = flow.tensor.reshape %259 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %261 = flow.tensor.reshape %258 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %262 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %261) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %263 = flow.tensor.reshape %262 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %264 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%263, %260, %181, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %181{%80}
    %265 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %259) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %266 = flow.tensor.reshape %265 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %267 = flow.tensor.reshape %253 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %268 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %267) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %269 = flow.tensor.reshape %268 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %270 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%269, %266, %264, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %264{%80}
    %271 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%254, %69, %255, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %272 = flow.tensor.reshape %271 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %273 = flow.tensor.reshape %258 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %274 = flow.tensor.reshape %253 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %275 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %272, %273, %274, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %276 = flow.tensor.bitcast %__parameter_model_blk_2_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %277 = flow.tensor.bitcast %__parameter_model_blk_2_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %278 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%277) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %279 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%276) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %280 = flow.tensor.bitcast %279 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %281 = flow.tensor.bitcast %278 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %282 = flow.tensor.reshape %275 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %283 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%281, %280, %19, %282, %223) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %284 = flow.tensor.reshape %283 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %285 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %284) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %286 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %283) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %287 = flow.tensor.reshape %285 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %288 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %287, %286) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %289 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_2_ffn_norm_weight_tensor_2048xf32, %288) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %290 = flow.tensor.bitcast %__parameter_model_blk_2_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %291 = flow.tensor.bitcast %__parameter_model_blk_2_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %292 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%291) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %293 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%290) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %294 = flow.tensor.bitcast %293 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %295 = flow.tensor.bitcast %292 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %296 = flow.tensor.bitcast %__parameter_model_blk_2_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %297 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%296) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %298 = flow.tensor.bitcast %297 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %299 = flow.tensor.bitcast %__parameter_model_blk_2_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %300 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%299) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %301 = flow.tensor.bitcast %300 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %302 = flow.tensor.reshape %289 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %303 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%298, %301, %19, %302) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %304 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%295, %294, %19, %302, %303) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %305 = flow.tensor.bitcast %__parameter_model_blk_2_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %306 = flow.tensor.bitcast %__parameter_model_blk_2_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %307 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%306) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %308 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%305) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %309 = flow.tensor.bitcast %308 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %310 = flow.tensor.bitcast %307 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %311 = flow.tensor.reshape %304 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %312 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%310, %309, %19, %311, %283) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %313 = flow.tensor.reshape %312 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %314 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %313) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %315 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %312) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %316 = flow.tensor.reshape %314 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %317 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %316, %315) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %318 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_3_attn_norm_weight_tensor_2048xf32, %317) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %319 = flow.tensor.bitcast %__parameter_model_blk_3_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %320 = flow.tensor.bitcast %__parameter_model_blk_3_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %321 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%320) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %322 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%319) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %323 = flow.tensor.bitcast %322 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %324 = flow.tensor.bitcast %321 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %325 = flow.tensor.reshape %318 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %326 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%324, %323, %19, %325) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %327 = flow.tensor.reshape %326 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %328 = flow.tensor.bitcast %__parameter_model_blk_3_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %329 = flow.tensor.bitcast %__parameter_model_blk_3_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %330 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%329) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %331 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%328) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %332 = flow.tensor.bitcast %331 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %333 = flow.tensor.bitcast %330 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %334 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%333, %332, %19, %325) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %335 = flow.tensor.reshape %334 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %336 = flow.tensor.bitcast %__parameter_model_blk_3_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %337 = flow.tensor.bitcast %__parameter_model_blk_3_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %338 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%337) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %339 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%336) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %340 = flow.tensor.bitcast %339 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %341 = flow.tensor.bitcast %338 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %342 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%341, %340, %19, %325) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %343 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%327, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %344 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%327, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %345 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%335, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %346 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%335, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %347 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%345, %69, %346, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %348 = flow.dispatch @prefill_bs4$async_dispatch_154::@prefill_bs4$async_dispatch_154_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %349 = flow.tensor.reshape %348 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %350 = flow.tensor.reshape %347 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %351 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %350) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %352 = flow.tensor.reshape %351 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %353 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%352, %349, %270, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %270{%80}
    %354 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %348) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %355 = flow.tensor.reshape %354 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %356 = flow.tensor.reshape %342 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %357 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %356) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %358 = flow.tensor.reshape %357 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %359 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%358, %355, %353, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %353{%80}
    %360 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%343, %69, %344, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %361 = flow.tensor.reshape %360 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %362 = flow.tensor.reshape %347 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %363 = flow.tensor.reshape %342 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %364 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %361, %362, %363, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %365 = flow.tensor.bitcast %__parameter_model_blk_3_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %366 = flow.tensor.bitcast %__parameter_model_blk_3_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %367 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%366) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %368 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%365) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %369 = flow.tensor.bitcast %368 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %370 = flow.tensor.bitcast %367 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %371 = flow.tensor.reshape %364 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %372 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%370, %369, %19, %371, %312) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %373 = flow.tensor.reshape %372 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %374 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %373) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %375 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %372) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %376 = flow.tensor.reshape %374 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %377 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %376, %375) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %378 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_3_ffn_norm_weight_tensor_2048xf32, %377) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %379 = flow.tensor.bitcast %__parameter_model_blk_3_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %380 = flow.tensor.bitcast %__parameter_model_blk_3_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %381 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%380) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %382 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%379) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %383 = flow.tensor.bitcast %382 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %384 = flow.tensor.bitcast %381 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %385 = flow.tensor.bitcast %__parameter_model_blk_3_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %386 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%385) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %387 = flow.tensor.bitcast %386 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %388 = flow.tensor.bitcast %__parameter_model_blk_3_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %389 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%388) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %390 = flow.tensor.bitcast %389 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %391 = flow.tensor.reshape %378 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %392 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%387, %390, %19, %391) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %393 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%384, %383, %19, %391, %392) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %394 = flow.tensor.bitcast %__parameter_model_blk_3_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %395 = flow.tensor.bitcast %__parameter_model_blk_3_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %396 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%395) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %397 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%394) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %398 = flow.tensor.bitcast %397 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %399 = flow.tensor.bitcast %396 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %400 = flow.tensor.reshape %393 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %401 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%399, %398, %19, %400, %372) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %402 = flow.tensor.reshape %401 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %403 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %402) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %404 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %401) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %405 = flow.tensor.reshape %403 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %406 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %405, %404) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %407 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_4_attn_norm_weight_tensor_2048xf32, %406) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %408 = flow.tensor.bitcast %__parameter_model_blk_4_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %409 = flow.tensor.bitcast %__parameter_model_blk_4_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %410 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%409) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %411 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%408) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %412 = flow.tensor.bitcast %411 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %413 = flow.tensor.bitcast %410 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %414 = flow.tensor.reshape %407 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %415 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%413, %412, %19, %414) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %416 = flow.tensor.reshape %415 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %417 = flow.tensor.bitcast %__parameter_model_blk_4_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %418 = flow.tensor.bitcast %__parameter_model_blk_4_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %419 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%418) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %420 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%417) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %421 = flow.tensor.bitcast %420 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %422 = flow.tensor.bitcast %419 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %423 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%422, %421, %19, %414) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %424 = flow.tensor.reshape %423 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %425 = flow.tensor.bitcast %__parameter_model_blk_4_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %426 = flow.tensor.bitcast %__parameter_model_blk_4_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %427 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%426) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %428 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%425) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %429 = flow.tensor.bitcast %428 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %430 = flow.tensor.bitcast %427 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %431 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%430, %429, %19, %414) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %432 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%416, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %433 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%416, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %434 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%424, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %435 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%424, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %436 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%434, %69, %435, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %437 = flow.dispatch @prefill_bs4$async_dispatch_196::@prefill_bs4$async_dispatch_196_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %438 = flow.tensor.reshape %437 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %439 = flow.tensor.reshape %436 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %440 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %439) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %441 = flow.tensor.reshape %440 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %442 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%441, %438, %359, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %359{%80}
    %443 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %437) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %444 = flow.tensor.reshape %443 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %445 = flow.tensor.reshape %431 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %446 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %445) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %447 = flow.tensor.reshape %446 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %448 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%447, %444, %442, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %442{%80}
    %449 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%432, %69, %433, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %450 = flow.tensor.reshape %449 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %451 = flow.tensor.reshape %436 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %452 = flow.tensor.reshape %431 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %453 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %450, %451, %452, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %454 = flow.tensor.bitcast %__parameter_model_blk_4_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %455 = flow.tensor.bitcast %__parameter_model_blk_4_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %456 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%455) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %457 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%454) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %458 = flow.tensor.bitcast %457 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %459 = flow.tensor.bitcast %456 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %460 = flow.tensor.reshape %453 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %461 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%459, %458, %19, %460, %401) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %462 = flow.tensor.reshape %461 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %463 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %462) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %464 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %461) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %465 = flow.tensor.reshape %463 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %466 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %465, %464) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %467 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_4_ffn_norm_weight_tensor_2048xf32, %466) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %468 = flow.tensor.bitcast %__parameter_model_blk_4_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %469 = flow.tensor.bitcast %__parameter_model_blk_4_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %470 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%469) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %471 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%468) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %472 = flow.tensor.bitcast %471 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %473 = flow.tensor.bitcast %470 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %474 = flow.tensor.bitcast %__parameter_model_blk_4_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %475 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%474) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %476 = flow.tensor.bitcast %475 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %477 = flow.tensor.bitcast %__parameter_model_blk_4_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %478 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%477) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %479 = flow.tensor.bitcast %478 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %480 = flow.tensor.reshape %467 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %481 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%476, %479, %19, %480) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %482 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%473, %472, %19, %480, %481) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %483 = flow.tensor.bitcast %__parameter_model_blk_4_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %484 = flow.tensor.bitcast %__parameter_model_blk_4_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %485 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%484) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %486 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%483) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %487 = flow.tensor.bitcast %486 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %488 = flow.tensor.bitcast %485 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %489 = flow.tensor.reshape %482 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %490 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%488, %487, %19, %489, %461) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %491 = flow.tensor.reshape %490 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %492 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %491) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %493 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %490) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %494 = flow.tensor.reshape %492 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %495 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %494, %493) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %496 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_5_attn_norm_weight_tensor_2048xf32, %495) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %497 = flow.tensor.bitcast %__parameter_model_blk_5_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %498 = flow.tensor.bitcast %__parameter_model_blk_5_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %499 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%498) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %500 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%497) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %501 = flow.tensor.bitcast %500 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %502 = flow.tensor.bitcast %499 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %503 = flow.tensor.reshape %496 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %504 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%502, %501, %19, %503) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %505 = flow.tensor.reshape %504 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %506 = flow.tensor.bitcast %__parameter_model_blk_5_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %507 = flow.tensor.bitcast %__parameter_model_blk_5_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %508 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%507) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %509 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%506) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %510 = flow.tensor.bitcast %509 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %511 = flow.tensor.bitcast %508 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %512 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%511, %510, %19, %503) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %513 = flow.tensor.reshape %512 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %514 = flow.tensor.bitcast %__parameter_model_blk_5_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %515 = flow.tensor.bitcast %__parameter_model_blk_5_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %516 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%515) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %517 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%514) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %518 = flow.tensor.bitcast %517 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %519 = flow.tensor.bitcast %516 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %520 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%519, %518, %19, %503) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %521 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%505, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %522 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%505, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %523 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%513, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %524 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%513, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %525 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%523, %69, %524, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %526 = flow.dispatch @prefill_bs4$async_dispatch_238::@prefill_bs4$async_dispatch_238_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %527 = flow.tensor.reshape %526 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %528 = flow.tensor.reshape %525 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %529 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %528) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %530 = flow.tensor.reshape %529 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %531 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%530, %527, %448, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %448{%80}
    %532 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %526) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %533 = flow.tensor.reshape %532 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %534 = flow.tensor.reshape %520 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %535 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %534) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %536 = flow.tensor.reshape %535 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %537 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%536, %533, %531, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %531{%80}
    %538 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%521, %69, %522, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %539 = flow.tensor.reshape %538 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %540 = flow.tensor.reshape %525 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %541 = flow.tensor.reshape %520 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %542 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %539, %540, %541, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %543 = flow.tensor.bitcast %__parameter_model_blk_5_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %544 = flow.tensor.bitcast %__parameter_model_blk_5_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %545 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%544) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %546 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%543) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %547 = flow.tensor.bitcast %546 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %548 = flow.tensor.bitcast %545 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %549 = flow.tensor.reshape %542 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %550 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%548, %547, %19, %549, %490) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %551 = flow.tensor.reshape %550 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %552 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %551) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %553 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %550) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %554 = flow.tensor.reshape %552 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %555 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %554, %553) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %556 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_5_ffn_norm_weight_tensor_2048xf32, %555) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %557 = flow.tensor.bitcast %__parameter_model_blk_5_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %558 = flow.tensor.bitcast %__parameter_model_blk_5_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %559 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%558) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %560 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%557) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %561 = flow.tensor.bitcast %560 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %562 = flow.tensor.bitcast %559 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %563 = flow.tensor.bitcast %__parameter_model_blk_5_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %564 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%563) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %565 = flow.tensor.bitcast %564 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %566 = flow.tensor.bitcast %__parameter_model_blk_5_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %567 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%566) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %568 = flow.tensor.bitcast %567 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %569 = flow.tensor.reshape %556 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %570 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%565, %568, %19, %569) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %571 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%562, %561, %19, %569, %570) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %572 = flow.tensor.bitcast %__parameter_model_blk_5_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %573 = flow.tensor.bitcast %__parameter_model_blk_5_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %574 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%573) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %575 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%572) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %576 = flow.tensor.bitcast %575 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %577 = flow.tensor.bitcast %574 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %578 = flow.tensor.reshape %571 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %579 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%577, %576, %19, %578, %550) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %580 = flow.tensor.reshape %579 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %581 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %580) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %582 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %579) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %583 = flow.tensor.reshape %581 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %584 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %583, %582) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %585 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_6_attn_norm_weight_tensor_2048xf32, %584) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %586 = flow.tensor.bitcast %__parameter_model_blk_6_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %587 = flow.tensor.bitcast %__parameter_model_blk_6_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %588 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%587) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %589 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%586) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %590 = flow.tensor.bitcast %589 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %591 = flow.tensor.bitcast %588 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %592 = flow.tensor.reshape %585 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %593 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%591, %590, %19, %592) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %594 = flow.tensor.reshape %593 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %595 = flow.tensor.bitcast %__parameter_model_blk_6_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %596 = flow.tensor.bitcast %__parameter_model_blk_6_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %597 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%596) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %598 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%595) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %599 = flow.tensor.bitcast %598 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %600 = flow.tensor.bitcast %597 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %601 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%600, %599, %19, %592) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %602 = flow.tensor.reshape %601 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %603 = flow.tensor.bitcast %__parameter_model_blk_6_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %604 = flow.tensor.bitcast %__parameter_model_blk_6_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %605 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%604) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %606 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%603) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %607 = flow.tensor.bitcast %606 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %608 = flow.tensor.bitcast %605 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %609 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%608, %607, %19, %592) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %610 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%594, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %611 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%594, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %612 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%602, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %613 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%602, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %614 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%612, %69, %613, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %615 = flow.dispatch @prefill_bs4$async_dispatch_280::@prefill_bs4$async_dispatch_280_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %616 = flow.tensor.reshape %615 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %617 = flow.tensor.reshape %614 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %618 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %617) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %619 = flow.tensor.reshape %618 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %620 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%619, %616, %537, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %537{%80}
    %621 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %615) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %622 = flow.tensor.reshape %621 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %623 = flow.tensor.reshape %609 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %624 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %623) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %625 = flow.tensor.reshape %624 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %626 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%625, %622, %620, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %620{%80}
    %627 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%610, %69, %611, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %628 = flow.tensor.reshape %627 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %629 = flow.tensor.reshape %614 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %630 = flow.tensor.reshape %609 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %631 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %628, %629, %630, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %632 = flow.tensor.bitcast %__parameter_model_blk_6_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %633 = flow.tensor.bitcast %__parameter_model_blk_6_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %634 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%633) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %635 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%632) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %636 = flow.tensor.bitcast %635 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %637 = flow.tensor.bitcast %634 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %638 = flow.tensor.reshape %631 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %639 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%637, %636, %19, %638, %579) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %640 = flow.tensor.reshape %639 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %641 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %640) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %642 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %639) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %643 = flow.tensor.reshape %641 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %644 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %643, %642) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %645 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_6_ffn_norm_weight_tensor_2048xf32, %644) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %646 = flow.tensor.bitcast %__parameter_model_blk_6_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %647 = flow.tensor.bitcast %__parameter_model_blk_6_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %648 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%647) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %649 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%646) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %650 = flow.tensor.bitcast %649 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %651 = flow.tensor.bitcast %648 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %652 = flow.tensor.bitcast %__parameter_model_blk_6_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %653 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%652) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %654 = flow.tensor.bitcast %653 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %655 = flow.tensor.bitcast %__parameter_model_blk_6_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %656 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%655) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %657 = flow.tensor.bitcast %656 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %658 = flow.tensor.reshape %645 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %659 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%654, %657, %19, %658) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %660 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%651, %650, %19, %658, %659) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %661 = flow.tensor.bitcast %__parameter_model_blk_6_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %662 = flow.tensor.bitcast %__parameter_model_blk_6_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %663 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%662) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %664 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%661) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %665 = flow.tensor.bitcast %664 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %666 = flow.tensor.bitcast %663 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %667 = flow.tensor.reshape %660 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %668 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%666, %665, %19, %667, %639) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %669 = flow.tensor.reshape %668 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %670 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %669) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %671 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %668) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %672 = flow.tensor.reshape %670 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %673 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %672, %671) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %674 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_7_attn_norm_weight_tensor_2048xf32, %673) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %675 = flow.tensor.bitcast %__parameter_model_blk_7_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %676 = flow.tensor.bitcast %__parameter_model_blk_7_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %677 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%676) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %678 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%675) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %679 = flow.tensor.bitcast %678 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %680 = flow.tensor.bitcast %677 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %681 = flow.tensor.reshape %674 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %682 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%680, %679, %19, %681) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %683 = flow.tensor.reshape %682 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %684 = flow.tensor.bitcast %__parameter_model_blk_7_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %685 = flow.tensor.bitcast %__parameter_model_blk_7_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %686 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%685) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %687 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%684) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %688 = flow.tensor.bitcast %687 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %689 = flow.tensor.bitcast %686 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %690 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%689, %688, %19, %681) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %691 = flow.tensor.reshape %690 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %692 = flow.tensor.bitcast %__parameter_model_blk_7_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %693 = flow.tensor.bitcast %__parameter_model_blk_7_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %694 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%693) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %695 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%692) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %696 = flow.tensor.bitcast %695 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %697 = flow.tensor.bitcast %694 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %698 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%697, %696, %19, %681) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %699 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%683, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %700 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%683, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %701 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%691, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %702 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%691, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %703 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%701, %69, %702, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %704 = flow.dispatch @prefill_bs4$async_dispatch_322::@prefill_bs4$async_dispatch_322_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %705 = flow.tensor.reshape %704 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %706 = flow.tensor.reshape %703 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %707 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %706) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %708 = flow.tensor.reshape %707 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %709 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%708, %705, %626, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %626{%80}
    %710 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %704) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %711 = flow.tensor.reshape %710 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %712 = flow.tensor.reshape %698 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %713 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %712) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %714 = flow.tensor.reshape %713 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %715 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%714, %711, %709, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %709{%80}
    %716 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%699, %69, %700, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %717 = flow.tensor.reshape %716 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %718 = flow.tensor.reshape %703 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %719 = flow.tensor.reshape %698 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %720 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %717, %718, %719, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %721 = flow.tensor.bitcast %__parameter_model_blk_7_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %722 = flow.tensor.bitcast %__parameter_model_blk_7_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %723 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%722) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %724 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%721) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %725 = flow.tensor.bitcast %724 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %726 = flow.tensor.bitcast %723 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %727 = flow.tensor.reshape %720 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %728 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%726, %725, %19, %727, %668) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %729 = flow.tensor.reshape %728 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %730 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %729) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %731 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %728) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %732 = flow.tensor.reshape %730 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %733 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %732, %731) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %734 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_7_ffn_norm_weight_tensor_2048xf32, %733) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %735 = flow.tensor.bitcast %__parameter_model_blk_7_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %736 = flow.tensor.bitcast %__parameter_model_blk_7_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %737 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%736) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %738 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%735) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %739 = flow.tensor.bitcast %738 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %740 = flow.tensor.bitcast %737 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %741 = flow.tensor.bitcast %__parameter_model_blk_7_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %742 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%741) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %743 = flow.tensor.bitcast %742 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %744 = flow.tensor.bitcast %__parameter_model_blk_7_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %745 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%744) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %746 = flow.tensor.bitcast %745 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %747 = flow.tensor.reshape %734 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %748 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%743, %746, %19, %747) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %749 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%740, %739, %19, %747, %748) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %750 = flow.tensor.bitcast %__parameter_model_blk_7_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %751 = flow.tensor.bitcast %__parameter_model_blk_7_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %752 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%751) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %753 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%750) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %754 = flow.tensor.bitcast %753 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %755 = flow.tensor.bitcast %752 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %756 = flow.tensor.reshape %749 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %757 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%755, %754, %19, %756, %728) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %758 = flow.tensor.reshape %757 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %759 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %758) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %760 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %757) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %761 = flow.tensor.reshape %759 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %762 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %761, %760) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %763 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_8_attn_norm_weight_tensor_2048xf32, %762) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %764 = flow.tensor.bitcast %__parameter_model_blk_8_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %765 = flow.tensor.bitcast %__parameter_model_blk_8_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %766 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%765) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %767 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%764) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %768 = flow.tensor.bitcast %767 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %769 = flow.tensor.bitcast %766 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %770 = flow.tensor.reshape %763 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %771 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%769, %768, %19, %770) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %772 = flow.tensor.reshape %771 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %773 = flow.tensor.bitcast %__parameter_model_blk_8_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %774 = flow.tensor.bitcast %__parameter_model_blk_8_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %775 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%774) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %776 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%773) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %777 = flow.tensor.bitcast %776 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %778 = flow.tensor.bitcast %775 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %779 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%778, %777, %19, %770) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %780 = flow.tensor.reshape %779 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %781 = flow.tensor.bitcast %__parameter_model_blk_8_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %782 = flow.tensor.bitcast %__parameter_model_blk_8_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %783 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%782) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %784 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%781) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %785 = flow.tensor.bitcast %784 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %786 = flow.tensor.bitcast %783 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %787 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%786, %785, %19, %770) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %788 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%772, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %789 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%772, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %790 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%780, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %791 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%780, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %792 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%790, %69, %791, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %793 = flow.dispatch @prefill_bs4$async_dispatch_364::@prefill_bs4$async_dispatch_364_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %794 = flow.tensor.reshape %793 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %795 = flow.tensor.reshape %792 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %796 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %795) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %797 = flow.tensor.reshape %796 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %798 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%797, %794, %715, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %715{%80}
    %799 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %793) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %800 = flow.tensor.reshape %799 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %801 = flow.tensor.reshape %787 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %802 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %801) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %803 = flow.tensor.reshape %802 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %804 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%803, %800, %798, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %798{%80}
    %805 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%788, %69, %789, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %806 = flow.tensor.reshape %805 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %807 = flow.tensor.reshape %792 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %808 = flow.tensor.reshape %787 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %809 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %806, %807, %808, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %810 = flow.tensor.bitcast %__parameter_model_blk_8_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %811 = flow.tensor.bitcast %__parameter_model_blk_8_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %812 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%811) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %813 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%810) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %814 = flow.tensor.bitcast %813 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %815 = flow.tensor.bitcast %812 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %816 = flow.tensor.reshape %809 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %817 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%815, %814, %19, %816, %757) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %818 = flow.tensor.reshape %817 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %819 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %818) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %820 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %817) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %821 = flow.tensor.reshape %819 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %822 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %821, %820) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %823 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_8_ffn_norm_weight_tensor_2048xf32, %822) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %824 = flow.tensor.bitcast %__parameter_model_blk_8_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %825 = flow.tensor.bitcast %__parameter_model_blk_8_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %826 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%825) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %827 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%824) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %828 = flow.tensor.bitcast %827 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %829 = flow.tensor.bitcast %826 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %830 = flow.tensor.bitcast %__parameter_model_blk_8_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %831 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%830) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %832 = flow.tensor.bitcast %831 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %833 = flow.tensor.bitcast %__parameter_model_blk_8_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %834 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%833) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %835 = flow.tensor.bitcast %834 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %836 = flow.tensor.reshape %823 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %837 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%832, %835, %19, %836) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %838 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%829, %828, %19, %836, %837) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %839 = flow.tensor.bitcast %__parameter_model_blk_8_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %840 = flow.tensor.bitcast %__parameter_model_blk_8_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %841 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%840) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %842 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%839) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %843 = flow.tensor.bitcast %842 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %844 = flow.tensor.bitcast %841 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %845 = flow.tensor.reshape %838 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %846 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%844, %843, %19, %845, %817) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %847 = flow.tensor.reshape %846 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %848 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %847) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %849 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %846) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %850 = flow.tensor.reshape %848 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %851 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %850, %849) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %852 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_9_attn_norm_weight_tensor_2048xf32, %851) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %853 = flow.tensor.bitcast %__parameter_model_blk_9_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %854 = flow.tensor.bitcast %__parameter_model_blk_9_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %855 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%854) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %856 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%853) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %857 = flow.tensor.bitcast %856 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %858 = flow.tensor.bitcast %855 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %859 = flow.tensor.reshape %852 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %860 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%858, %857, %19, %859) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %861 = flow.tensor.reshape %860 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %862 = flow.tensor.bitcast %__parameter_model_blk_9_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %863 = flow.tensor.bitcast %__parameter_model_blk_9_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %864 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%863) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %865 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%862) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %866 = flow.tensor.bitcast %865 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %867 = flow.tensor.bitcast %864 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %868 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%867, %866, %19, %859) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %869 = flow.tensor.reshape %868 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %870 = flow.tensor.bitcast %__parameter_model_blk_9_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %871 = flow.tensor.bitcast %__parameter_model_blk_9_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %872 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%871) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %873 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%870) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %874 = flow.tensor.bitcast %873 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %875 = flow.tensor.bitcast %872 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %876 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%875, %874, %19, %859) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %877 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%861, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %878 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%861, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %879 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%869, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %880 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%869, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %881 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%879, %69, %880, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %882 = flow.dispatch @prefill_bs4$async_dispatch_406::@prefill_bs4$async_dispatch_406_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %883 = flow.tensor.reshape %882 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %884 = flow.tensor.reshape %881 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %885 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %884) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %886 = flow.tensor.reshape %885 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %887 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%886, %883, %804, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %804{%80}
    %888 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %882) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %889 = flow.tensor.reshape %888 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %890 = flow.tensor.reshape %876 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %891 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %890) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %892 = flow.tensor.reshape %891 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %893 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%892, %889, %887, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %887{%80}
    %894 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%877, %69, %878, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %895 = flow.tensor.reshape %894 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %896 = flow.tensor.reshape %881 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %897 = flow.tensor.reshape %876 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %898 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %895, %896, %897, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %899 = flow.tensor.bitcast %__parameter_model_blk_9_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %900 = flow.tensor.bitcast %__parameter_model_blk_9_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %901 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%900) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %902 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%899) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %903 = flow.tensor.bitcast %902 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %904 = flow.tensor.bitcast %901 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %905 = flow.tensor.reshape %898 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %906 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%904, %903, %19, %905, %846) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %907 = flow.tensor.reshape %906 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %908 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %907) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %909 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %906) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %910 = flow.tensor.reshape %908 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %911 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %910, %909) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %912 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_9_ffn_norm_weight_tensor_2048xf32, %911) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %913 = flow.tensor.bitcast %__parameter_model_blk_9_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %914 = flow.tensor.bitcast %__parameter_model_blk_9_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %915 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%914) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %916 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%913) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %917 = flow.tensor.bitcast %916 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %918 = flow.tensor.bitcast %915 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %919 = flow.tensor.bitcast %__parameter_model_blk_9_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %920 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%919) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %921 = flow.tensor.bitcast %920 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %922 = flow.tensor.bitcast %__parameter_model_blk_9_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %923 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%922) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %924 = flow.tensor.bitcast %923 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %925 = flow.tensor.reshape %912 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %926 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%921, %924, %19, %925) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %927 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%918, %917, %19, %925, %926) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %928 = flow.tensor.bitcast %__parameter_model_blk_9_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %929 = flow.tensor.bitcast %__parameter_model_blk_9_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %930 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%929) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %931 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%928) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %932 = flow.tensor.bitcast %931 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %933 = flow.tensor.bitcast %930 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %934 = flow.tensor.reshape %927 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %935 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%933, %932, %19, %934, %906) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %936 = flow.tensor.reshape %935 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %937 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %936) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %938 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %935) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %939 = flow.tensor.reshape %937 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %940 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %939, %938) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %941 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_10_attn_norm_weight_tensor_2048xf32, %940) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %942 = flow.tensor.bitcast %__parameter_model_blk_10_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %943 = flow.tensor.bitcast %__parameter_model_blk_10_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %944 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%943) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %945 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%942) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %946 = flow.tensor.bitcast %945 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %947 = flow.tensor.bitcast %944 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %948 = flow.tensor.reshape %941 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %949 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%947, %946, %19, %948) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %950 = flow.tensor.reshape %949 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %951 = flow.tensor.bitcast %__parameter_model_blk_10_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %952 = flow.tensor.bitcast %__parameter_model_blk_10_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %953 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%952) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %954 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%951) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %955 = flow.tensor.bitcast %954 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %956 = flow.tensor.bitcast %953 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %957 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%956, %955, %19, %948) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %958 = flow.tensor.reshape %957 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %959 = flow.tensor.bitcast %__parameter_model_blk_10_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %960 = flow.tensor.bitcast %__parameter_model_blk_10_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %961 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%960) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %962 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%959) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %963 = flow.tensor.bitcast %962 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %964 = flow.tensor.bitcast %961 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %965 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%964, %963, %19, %948) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %966 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%950, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %967 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%950, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %968 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%958, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %969 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%958, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %970 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%968, %69, %969, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %971 = flow.dispatch @prefill_bs4$async_dispatch_448::@prefill_bs4$async_dispatch_448_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %972 = flow.tensor.reshape %971 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %973 = flow.tensor.reshape %970 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %974 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %973) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %975 = flow.tensor.reshape %974 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %976 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%975, %972, %893, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %893{%80}
    %977 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %971) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %978 = flow.tensor.reshape %977 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %979 = flow.tensor.reshape %965 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %980 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %979) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %981 = flow.tensor.reshape %980 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %982 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%981, %978, %976, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %976{%80}
    %983 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%966, %69, %967, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %984 = flow.tensor.reshape %983 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %985 = flow.tensor.reshape %970 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %986 = flow.tensor.reshape %965 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %987 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %984, %985, %986, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %988 = flow.tensor.bitcast %__parameter_model_blk_10_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %989 = flow.tensor.bitcast %__parameter_model_blk_10_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %990 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%989) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %991 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%988) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %992 = flow.tensor.bitcast %991 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %993 = flow.tensor.bitcast %990 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %994 = flow.tensor.reshape %987 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %995 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%993, %992, %19, %994, %935) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %996 = flow.tensor.reshape %995 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %997 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %996) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %998 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %995) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %999 = flow.tensor.reshape %997 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1000 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %999, %998) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1001 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_10_ffn_norm_weight_tensor_2048xf32, %1000) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1002 = flow.tensor.bitcast %__parameter_model_blk_10_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1003 = flow.tensor.bitcast %__parameter_model_blk_10_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1004 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1003) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1005 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1002) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1006 = flow.tensor.bitcast %1005 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1007 = flow.tensor.bitcast %1004 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1008 = flow.tensor.bitcast %__parameter_model_blk_10_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1009 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1008) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1010 = flow.tensor.bitcast %1009 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1011 = flow.tensor.bitcast %__parameter_model_blk_10_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1012 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1011) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1013 = flow.tensor.bitcast %1012 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1014 = flow.tensor.reshape %1001 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1015 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1010, %1013, %19, %1014) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1016 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1007, %1006, %19, %1014, %1015) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1017 = flow.tensor.bitcast %__parameter_model_blk_10_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1018 = flow.tensor.bitcast %__parameter_model_blk_10_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1019 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1018) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1020 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1017) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1021 = flow.tensor.bitcast %1020 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1022 = flow.tensor.bitcast %1019 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1023 = flow.tensor.reshape %1016 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %1024 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%1022, %1021, %19, %1023, %995) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1025 = flow.tensor.reshape %1024 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1026 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1025) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1027 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1024) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1028 = flow.tensor.reshape %1026 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1029 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1028, %1027) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1030 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_11_attn_norm_weight_tensor_2048xf32, %1029) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1031 = flow.tensor.bitcast %__parameter_model_blk_11_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1032 = flow.tensor.bitcast %__parameter_model_blk_11_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1033 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1032) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1034 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1031) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1035 = flow.tensor.bitcast %1034 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1036 = flow.tensor.bitcast %1033 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1037 = flow.tensor.reshape %1030 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1038 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1036, %1035, %19, %1037) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1039 = flow.tensor.reshape %1038 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %1040 = flow.tensor.bitcast %__parameter_model_blk_11_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1041 = flow.tensor.bitcast %__parameter_model_blk_11_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1042 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1041) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1043 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1040) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1044 = flow.tensor.bitcast %1043 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1045 = flow.tensor.bitcast %1042 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1046 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1045, %1044, %19, %1037) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1047 = flow.tensor.reshape %1046 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1048 = flow.tensor.bitcast %__parameter_model_blk_11_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1049 = flow.tensor.bitcast %__parameter_model_blk_11_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1050 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1049) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1051 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1048) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1052 = flow.tensor.bitcast %1051 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1053 = flow.tensor.bitcast %1050 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1054 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1053, %1052, %19, %1037) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1055 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%1039, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1056 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%1039, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1057 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%1047, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1058 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%1047, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1059 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%1057, %69, %1058, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %1060 = flow.dispatch @prefill_bs4$async_dispatch_490::@prefill_bs4$async_dispatch_490_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1061 = flow.tensor.reshape %1060 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1062 = flow.tensor.reshape %1059 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %1063 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1062) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1064 = flow.tensor.reshape %1063 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1065 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1064, %1061, %982, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %982{%80}
    %1066 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %1060) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1067 = flow.tensor.reshape %1066 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1068 = flow.tensor.reshape %1054 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %1069 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1068) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1070 = flow.tensor.reshape %1069 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1071 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1070, %1067, %1065, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1065{%80}
    %1072 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%1055, %69, %1056, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %1073 = flow.tensor.reshape %1072 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %1074 = flow.tensor.reshape %1059 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %1075 = flow.tensor.reshape %1054 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1076 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %1073, %1074, %1075, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %1077 = flow.tensor.bitcast %__parameter_model_blk_11_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1078 = flow.tensor.bitcast %__parameter_model_blk_11_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1079 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1078) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1080 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1077) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1081 = flow.tensor.bitcast %1080 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1082 = flow.tensor.bitcast %1079 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1083 = flow.tensor.reshape %1076 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %1084 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1082, %1081, %19, %1083, %1024) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1085 = flow.tensor.reshape %1084 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1086 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1085) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1087 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1084) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1088 = flow.tensor.reshape %1086 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1089 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1088, %1087) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1090 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_11_ffn_norm_weight_tensor_2048xf32, %1089) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1091 = flow.tensor.bitcast %__parameter_model_blk_11_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1092 = flow.tensor.bitcast %__parameter_model_blk_11_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1093 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1092) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1094 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1091) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1095 = flow.tensor.bitcast %1094 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1096 = flow.tensor.bitcast %1093 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1097 = flow.tensor.bitcast %__parameter_model_blk_11_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1098 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1097) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1099 = flow.tensor.bitcast %1098 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1100 = flow.tensor.bitcast %__parameter_model_blk_11_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1101 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1100) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1102 = flow.tensor.bitcast %1101 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1103 = flow.tensor.reshape %1090 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1104 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1099, %1102, %19, %1103) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1105 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1096, %1095, %19, %1103, %1104) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1106 = flow.tensor.bitcast %__parameter_model_blk_11_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1107 = flow.tensor.bitcast %__parameter_model_blk_11_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1108 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1107) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1109 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1106) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1110 = flow.tensor.bitcast %1109 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1111 = flow.tensor.bitcast %1108 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1112 = flow.tensor.reshape %1105 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %1113 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%1111, %1110, %19, %1112, %1084) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1114 = flow.tensor.reshape %1113 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1115 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1114) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1116 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1113) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1117 = flow.tensor.reshape %1115 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1118 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1117, %1116) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1119 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_12_attn_norm_weight_tensor_2048xf32, %1118) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1120 = flow.tensor.bitcast %__parameter_model_blk_12_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1121 = flow.tensor.bitcast %__parameter_model_blk_12_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1122 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1121) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1123 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1120) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1124 = flow.tensor.bitcast %1123 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1125 = flow.tensor.bitcast %1122 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1126 = flow.tensor.reshape %1119 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1127 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1125, %1124, %19, %1126) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1128 = flow.tensor.reshape %1127 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %1129 = flow.tensor.bitcast %__parameter_model_blk_12_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1130 = flow.tensor.bitcast %__parameter_model_blk_12_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1131 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1130) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1132 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1129) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1133 = flow.tensor.bitcast %1132 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1134 = flow.tensor.bitcast %1131 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1135 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1134, %1133, %19, %1126) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1136 = flow.tensor.reshape %1135 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1137 = flow.tensor.bitcast %__parameter_model_blk_12_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1138 = flow.tensor.bitcast %__parameter_model_blk_12_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1139 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1138) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1140 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1137) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1141 = flow.tensor.bitcast %1140 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1142 = flow.tensor.bitcast %1139 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1143 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1142, %1141, %19, %1126) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1144 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%1128, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1145 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%1128, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1146 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%1136, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1147 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%1136, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1148 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%1146, %69, %1147, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %1149 = flow.dispatch @prefill_bs4$async_dispatch_532::@prefill_bs4$async_dispatch_532_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1150 = flow.tensor.reshape %1149 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1151 = flow.tensor.reshape %1148 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %1152 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1151) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1153 = flow.tensor.reshape %1152 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1154 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1153, %1150, %1071, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1071{%80}
    %1155 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %1149) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1156 = flow.tensor.reshape %1155 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1157 = flow.tensor.reshape %1143 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %1158 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1157) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1159 = flow.tensor.reshape %1158 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1160 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1159, %1156, %1154, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1154{%80}
    %1161 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%1144, %69, %1145, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %1162 = flow.tensor.reshape %1161 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %1163 = flow.tensor.reshape %1148 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %1164 = flow.tensor.reshape %1143 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1165 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %1162, %1163, %1164, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %1166 = flow.tensor.bitcast %__parameter_model_blk_12_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1167 = flow.tensor.bitcast %__parameter_model_blk_12_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1168 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1167) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1169 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1166) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1170 = flow.tensor.bitcast %1169 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1171 = flow.tensor.bitcast %1168 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1172 = flow.tensor.reshape %1165 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %1173 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1171, %1170, %19, %1172, %1113) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1174 = flow.tensor.reshape %1173 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1175 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1174) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1176 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1173) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1177 = flow.tensor.reshape %1175 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1178 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1177, %1176) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1179 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_12_ffn_norm_weight_tensor_2048xf32, %1178) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1180 = flow.tensor.bitcast %__parameter_model_blk_12_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1181 = flow.tensor.bitcast %__parameter_model_blk_12_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1182 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1181) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1183 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1180) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1184 = flow.tensor.bitcast %1183 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1185 = flow.tensor.bitcast %1182 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1186 = flow.tensor.bitcast %__parameter_model_blk_12_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1187 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1186) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1188 = flow.tensor.bitcast %1187 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1189 = flow.tensor.bitcast %__parameter_model_blk_12_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1190 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1189) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1191 = flow.tensor.bitcast %1190 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1192 = flow.tensor.reshape %1179 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1193 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1188, %1191, %19, %1192) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1194 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1185, %1184, %19, %1192, %1193) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1195 = flow.tensor.bitcast %__parameter_model_blk_12_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1196 = flow.tensor.bitcast %__parameter_model_blk_12_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1197 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1196) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1198 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1195) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1199 = flow.tensor.bitcast %1198 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1200 = flow.tensor.bitcast %1197 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1201 = flow.tensor.reshape %1194 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %1202 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%1200, %1199, %19, %1201, %1173) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1203 = flow.tensor.reshape %1202 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1204 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1203) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1205 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1202) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1206 = flow.tensor.reshape %1204 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1207 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1206, %1205) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1208 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_13_attn_norm_weight_tensor_2048xf32, %1207) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1209 = flow.tensor.bitcast %__parameter_model_blk_13_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1210 = flow.tensor.bitcast %__parameter_model_blk_13_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1211 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1210) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1212 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1209) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1213 = flow.tensor.bitcast %1212 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1214 = flow.tensor.bitcast %1211 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1215 = flow.tensor.reshape %1208 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1216 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1214, %1213, %19, %1215) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1217 = flow.tensor.reshape %1216 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %1218 = flow.tensor.bitcast %__parameter_model_blk_13_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1219 = flow.tensor.bitcast %__parameter_model_blk_13_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1220 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1219) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1221 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1218) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1222 = flow.tensor.bitcast %1221 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1223 = flow.tensor.bitcast %1220 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1224 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1223, %1222, %19, %1215) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1225 = flow.tensor.reshape %1224 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1226 = flow.tensor.bitcast %__parameter_model_blk_13_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1227 = flow.tensor.bitcast %__parameter_model_blk_13_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1228 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1227) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1229 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1226) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1230 = flow.tensor.bitcast %1229 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1231 = flow.tensor.bitcast %1228 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1232 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1231, %1230, %19, %1215) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1233 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%1217, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1234 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%1217, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1235 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%1225, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1236 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%1225, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1237 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%1235, %69, %1236, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %1238 = flow.dispatch @prefill_bs4$async_dispatch_574::@prefill_bs4$async_dispatch_574_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1239 = flow.tensor.reshape %1238 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1240 = flow.tensor.reshape %1237 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %1241 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1240) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1242 = flow.tensor.reshape %1241 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1243 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1242, %1239, %1160, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1160{%80}
    %1244 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %1238) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1245 = flow.tensor.reshape %1244 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1246 = flow.tensor.reshape %1232 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %1247 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1246) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1248 = flow.tensor.reshape %1247 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1249 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1248, %1245, %1243, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1243{%80}
    %1250 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%1233, %69, %1234, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %1251 = flow.tensor.reshape %1250 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %1252 = flow.tensor.reshape %1237 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %1253 = flow.tensor.reshape %1232 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1254 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %1251, %1252, %1253, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %1255 = flow.tensor.bitcast %__parameter_model_blk_13_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1256 = flow.tensor.bitcast %__parameter_model_blk_13_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1257 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1256) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1258 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1255) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1259 = flow.tensor.bitcast %1258 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1260 = flow.tensor.bitcast %1257 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1261 = flow.tensor.reshape %1254 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %1262 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1260, %1259, %19, %1261, %1202) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1263 = flow.tensor.reshape %1262 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1264 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1263) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1265 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1262) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1266 = flow.tensor.reshape %1264 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1267 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1266, %1265) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1268 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_13_ffn_norm_weight_tensor_2048xf32, %1267) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1269 = flow.tensor.bitcast %__parameter_model_blk_13_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1270 = flow.tensor.bitcast %__parameter_model_blk_13_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1271 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1270) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1272 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1269) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1273 = flow.tensor.bitcast %1272 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1274 = flow.tensor.bitcast %1271 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1275 = flow.tensor.bitcast %__parameter_model_blk_13_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1276 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1275) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1277 = flow.tensor.bitcast %1276 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1278 = flow.tensor.bitcast %__parameter_model_blk_13_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1279 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1278) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1280 = flow.tensor.bitcast %1279 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1281 = flow.tensor.reshape %1268 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1282 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1277, %1280, %19, %1281) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1283 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1274, %1273, %19, %1281, %1282) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1284 = flow.tensor.bitcast %__parameter_model_blk_13_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1285 = flow.tensor.bitcast %__parameter_model_blk_13_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1286 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1285) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1287 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1284) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1288 = flow.tensor.bitcast %1287 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1289 = flow.tensor.bitcast %1286 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1290 = flow.tensor.reshape %1283 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %1291 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%1289, %1288, %19, %1290, %1262) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1292 = flow.tensor.reshape %1291 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1293 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1292) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1294 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1291) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1295 = flow.tensor.reshape %1293 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1296 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1295, %1294) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1297 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_14_attn_norm_weight_tensor_2048xf32, %1296) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1298 = flow.tensor.bitcast %__parameter_model_blk_14_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1299 = flow.tensor.bitcast %__parameter_model_blk_14_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1300 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1299) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1301 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1298) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1302 = flow.tensor.bitcast %1301 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1303 = flow.tensor.bitcast %1300 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1304 = flow.tensor.reshape %1297 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1305 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1303, %1302, %19, %1304) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1306 = flow.tensor.reshape %1305 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %1307 = flow.tensor.bitcast %__parameter_model_blk_14_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1308 = flow.tensor.bitcast %__parameter_model_blk_14_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1309 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1308) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1310 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1307) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1311 = flow.tensor.bitcast %1310 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1312 = flow.tensor.bitcast %1309 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1313 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1312, %1311, %19, %1304) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1314 = flow.tensor.reshape %1313 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1315 = flow.tensor.bitcast %__parameter_model_blk_14_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1316 = flow.tensor.bitcast %__parameter_model_blk_14_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1317 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1316) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1318 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1315) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1319 = flow.tensor.bitcast %1318 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1320 = flow.tensor.bitcast %1317 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1321 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1320, %1319, %19, %1304) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1322 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%1306, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1323 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%1306, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1324 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%1314, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1325 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%1314, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1326 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%1324, %69, %1325, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %1327 = flow.dispatch @prefill_bs4$async_dispatch_616::@prefill_bs4$async_dispatch_616_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1328 = flow.tensor.reshape %1327 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1329 = flow.tensor.reshape %1326 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %1330 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1329) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1331 = flow.tensor.reshape %1330 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1332 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1331, %1328, %1249, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1249{%80}
    %1333 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %1327) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1334 = flow.tensor.reshape %1333 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1335 = flow.tensor.reshape %1321 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %1336 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1335) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1337 = flow.tensor.reshape %1336 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1338 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1337, %1334, %1332, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1332{%80}
    %1339 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%1322, %69, %1323, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %1340 = flow.tensor.reshape %1339 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %1341 = flow.tensor.reshape %1326 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %1342 = flow.tensor.reshape %1321 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1343 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %1340, %1341, %1342, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %1344 = flow.tensor.bitcast %__parameter_model_blk_14_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1345 = flow.tensor.bitcast %__parameter_model_blk_14_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1346 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1345) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1347 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1344) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1348 = flow.tensor.bitcast %1347 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1349 = flow.tensor.bitcast %1346 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1350 = flow.tensor.reshape %1343 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %1351 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1349, %1348, %19, %1350, %1291) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1352 = flow.tensor.reshape %1351 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1353 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1352) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1354 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1351) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1355 = flow.tensor.reshape %1353 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1356 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1355, %1354) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1357 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_14_ffn_norm_weight_tensor_2048xf32, %1356) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1358 = flow.tensor.bitcast %__parameter_model_blk_14_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1359 = flow.tensor.bitcast %__parameter_model_blk_14_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1360 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1359) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1361 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1358) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1362 = flow.tensor.bitcast %1361 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1363 = flow.tensor.bitcast %1360 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1364 = flow.tensor.bitcast %__parameter_model_blk_14_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1365 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1364) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1366 = flow.tensor.bitcast %1365 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1367 = flow.tensor.bitcast %__parameter_model_blk_14_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1368 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1367) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1369 = flow.tensor.bitcast %1368 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1370 = flow.tensor.reshape %1357 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1371 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1366, %1369, %19, %1370) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1372 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1363, %1362, %19, %1370, %1371) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1373 = flow.tensor.bitcast %__parameter_model_blk_14_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1374 = flow.tensor.bitcast %__parameter_model_blk_14_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1375 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1374) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1376 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1373) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1377 = flow.tensor.bitcast %1376 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1378 = flow.tensor.bitcast %1375 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1379 = flow.tensor.reshape %1372 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %1380 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%1378, %1377, %19, %1379, %1351) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1381 = flow.tensor.reshape %1380 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1382 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1381) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1383 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1380) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1384 = flow.tensor.reshape %1382 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1385 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1384, %1383) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1386 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_15_attn_norm_weight_tensor_2048xf32, %1385) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1387 = flow.tensor.bitcast %__parameter_model_blk_15_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1388 = flow.tensor.bitcast %__parameter_model_blk_15_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1389 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1388) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1390 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1387) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1391 = flow.tensor.bitcast %1390 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1392 = flow.tensor.bitcast %1389 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1393 = flow.tensor.reshape %1386 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1394 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1392, %1391, %19, %1393) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1395 = flow.tensor.reshape %1394 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %1396 = flow.tensor.bitcast %__parameter_model_blk_15_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1397 = flow.tensor.bitcast %__parameter_model_blk_15_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1398 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1397) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1399 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1396) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1400 = flow.tensor.bitcast %1399 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1401 = flow.tensor.bitcast %1398 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1402 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1401, %1400, %19, %1393) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1403 = flow.tensor.reshape %1402 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1404 = flow.tensor.bitcast %__parameter_model_blk_15_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1405 = flow.tensor.bitcast %__parameter_model_blk_15_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1406 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1405) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1407 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1404) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1408 = flow.tensor.bitcast %1407 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1409 = flow.tensor.bitcast %1406 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1410 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1409, %1408, %19, %1393) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1411 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%1395, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1412 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%1395, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1413 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%1403, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1414 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%1403, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1415 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%1413, %69, %1414, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %1416 = flow.dispatch @prefill_bs4$async_dispatch_658::@prefill_bs4$async_dispatch_658_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1417 = flow.tensor.reshape %1416 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1418 = flow.tensor.reshape %1415 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %1419 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1418) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1420 = flow.tensor.reshape %1419 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1421 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1420, %1417, %1338, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1338{%80}
    %1422 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %1416) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1423 = flow.tensor.reshape %1422 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1424 = flow.tensor.reshape %1410 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %1425 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1424) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1426 = flow.tensor.reshape %1425 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1427 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1426, %1423, %1421, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1421{%80}
    %1428 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%1411, %69, %1412, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %1429 = flow.tensor.reshape %1428 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %1430 = flow.tensor.reshape %1415 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %1431 = flow.tensor.reshape %1410 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1432 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %1429, %1430, %1431, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %1433 = flow.tensor.bitcast %__parameter_model_blk_15_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1434 = flow.tensor.bitcast %__parameter_model_blk_15_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1435 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1434) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1436 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1433) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1437 = flow.tensor.bitcast %1436 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1438 = flow.tensor.bitcast %1435 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1439 = flow.tensor.reshape %1432 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %1440 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1438, %1437, %19, %1439, %1380) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1441 = flow.tensor.reshape %1440 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1442 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1441) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1443 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1440) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1444 = flow.tensor.reshape %1442 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1445 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1444, %1443) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1446 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_15_ffn_norm_weight_tensor_2048xf32, %1445) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1447 = flow.tensor.bitcast %__parameter_model_blk_15_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1448 = flow.tensor.bitcast %__parameter_model_blk_15_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1449 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1448) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1450 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1447) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1451 = flow.tensor.bitcast %1450 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1452 = flow.tensor.bitcast %1449 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1453 = flow.tensor.bitcast %__parameter_model_blk_15_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1454 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1453) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1455 = flow.tensor.bitcast %1454 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1456 = flow.tensor.bitcast %__parameter_model_blk_15_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1457 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1456) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1458 = flow.tensor.bitcast %1457 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1459 = flow.tensor.reshape %1446 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1460 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1455, %1458, %19, %1459) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1461 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1452, %1451, %19, %1459, %1460) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1462 = flow.tensor.bitcast %__parameter_model_blk_15_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1463 = flow.tensor.bitcast %__parameter_model_blk_15_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1464 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1463) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1465 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1462) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1466 = flow.tensor.bitcast %1465 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1467 = flow.tensor.bitcast %1464 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1468 = flow.tensor.reshape %1461 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %1469 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%1467, %1466, %19, %1468, %1440) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1470 = flow.tensor.reshape %1469 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1471 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1470) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1472 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1469) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1473 = flow.tensor.reshape %1471 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1474 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1473, %1472) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1475 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_16_attn_norm_weight_tensor_2048xf32, %1474) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1476 = flow.tensor.bitcast %__parameter_model_blk_16_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1477 = flow.tensor.bitcast %__parameter_model_blk_16_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1478 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1477) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1479 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1476) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1480 = flow.tensor.bitcast %1479 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1481 = flow.tensor.bitcast %1478 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1482 = flow.tensor.reshape %1475 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1483 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1481, %1480, %19, %1482) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1484 = flow.tensor.reshape %1483 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %1485 = flow.tensor.bitcast %__parameter_model_blk_16_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1486 = flow.tensor.bitcast %__parameter_model_blk_16_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1487 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1486) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1488 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1485) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1489 = flow.tensor.bitcast %1488 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1490 = flow.tensor.bitcast %1487 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1491 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1490, %1489, %19, %1482) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1492 = flow.tensor.reshape %1491 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1493 = flow.tensor.bitcast %__parameter_model_blk_16_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1494 = flow.tensor.bitcast %__parameter_model_blk_16_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1495 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1494) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1496 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1493) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1497 = flow.tensor.bitcast %1496 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1498 = flow.tensor.bitcast %1495 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1499 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1498, %1497, %19, %1482) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1500 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%1484, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1501 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%1484, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1502 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%1492, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1503 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%1492, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1504 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%1502, %69, %1503, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %1505 = flow.dispatch @prefill_bs4$async_dispatch_700::@prefill_bs4$async_dispatch_700_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1506 = flow.tensor.reshape %1505 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1507 = flow.tensor.reshape %1504 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %1508 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1507) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1509 = flow.tensor.reshape %1508 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1510 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1509, %1506, %1427, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1427{%80}
    %1511 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %1505) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1512 = flow.tensor.reshape %1511 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1513 = flow.tensor.reshape %1499 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %1514 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1513) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1515 = flow.tensor.reshape %1514 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1516 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1515, %1512, %1510, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1510{%80}
    %1517 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%1500, %69, %1501, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %1518 = flow.tensor.reshape %1517 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %1519 = flow.tensor.reshape %1504 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %1520 = flow.tensor.reshape %1499 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1521 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %1518, %1519, %1520, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %1522 = flow.tensor.bitcast %__parameter_model_blk_16_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1523 = flow.tensor.bitcast %__parameter_model_blk_16_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1524 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1523) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1525 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1522) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1526 = flow.tensor.bitcast %1525 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1527 = flow.tensor.bitcast %1524 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1528 = flow.tensor.reshape %1521 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %1529 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1527, %1526, %19, %1528, %1469) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1530 = flow.tensor.reshape %1529 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1531 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1530) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1532 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1529) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1533 = flow.tensor.reshape %1531 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1534 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1533, %1532) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1535 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_16_ffn_norm_weight_tensor_2048xf32, %1534) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1536 = flow.tensor.bitcast %__parameter_model_blk_16_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1537 = flow.tensor.bitcast %__parameter_model_blk_16_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1538 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1537) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1539 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1536) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1540 = flow.tensor.bitcast %1539 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1541 = flow.tensor.bitcast %1538 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1542 = flow.tensor.bitcast %__parameter_model_blk_16_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1543 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1542) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1544 = flow.tensor.bitcast %1543 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1545 = flow.tensor.bitcast %__parameter_model_blk_16_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1546 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1545) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1547 = flow.tensor.bitcast %1546 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1548 = flow.tensor.reshape %1535 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1549 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1544, %1547, %19, %1548) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1550 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1541, %1540, %19, %1548, %1549) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1551 = flow.tensor.bitcast %__parameter_model_blk_16_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1552 = flow.tensor.bitcast %__parameter_model_blk_16_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1553 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1552) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1554 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1551) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1555 = flow.tensor.bitcast %1554 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1556 = flow.tensor.bitcast %1553 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1557 = flow.tensor.reshape %1550 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %1558 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%1556, %1555, %19, %1557, %1529) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1559 = flow.tensor.reshape %1558 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1560 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1559) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1561 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1558) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1562 = flow.tensor.reshape %1560 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1563 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1562, %1561) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1564 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_17_attn_norm_weight_tensor_2048xf32, %1563) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1565 = flow.tensor.bitcast %__parameter_model_blk_17_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1566 = flow.tensor.bitcast %__parameter_model_blk_17_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1567 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1566) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1568 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1565) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1569 = flow.tensor.bitcast %1568 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1570 = flow.tensor.bitcast %1567 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1571 = flow.tensor.reshape %1564 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1572 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1570, %1569, %19, %1571) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1573 = flow.tensor.reshape %1572 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %1574 = flow.tensor.bitcast %__parameter_model_blk_17_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1575 = flow.tensor.bitcast %__parameter_model_blk_17_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1576 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1575) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1577 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1574) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1578 = flow.tensor.bitcast %1577 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1579 = flow.tensor.bitcast %1576 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1580 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1579, %1578, %19, %1571) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1581 = flow.tensor.reshape %1580 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1582 = flow.tensor.bitcast %__parameter_model_blk_17_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1583 = flow.tensor.bitcast %__parameter_model_blk_17_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1584 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1583) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1585 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1582) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1586 = flow.tensor.bitcast %1585 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1587 = flow.tensor.bitcast %1584 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1588 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1587, %1586, %19, %1571) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1589 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%1573, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1590 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%1573, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1591 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%1581, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1592 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%1581, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1593 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%1591, %69, %1592, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %1594 = flow.dispatch @prefill_bs4$async_dispatch_742::@prefill_bs4$async_dispatch_742_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1595 = flow.tensor.reshape %1594 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1596 = flow.tensor.reshape %1593 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %1597 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1596) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1598 = flow.tensor.reshape %1597 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1599 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1598, %1595, %1516, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1516{%80}
    %1600 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %1594) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1601 = flow.tensor.reshape %1600 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1602 = flow.tensor.reshape %1588 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %1603 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1602) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1604 = flow.tensor.reshape %1603 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1605 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1604, %1601, %1599, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1599{%80}
    %1606 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%1589, %69, %1590, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %1607 = flow.tensor.reshape %1606 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %1608 = flow.tensor.reshape %1593 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %1609 = flow.tensor.reshape %1588 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1610 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %1607, %1608, %1609, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %1611 = flow.tensor.bitcast %__parameter_model_blk_17_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1612 = flow.tensor.bitcast %__parameter_model_blk_17_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1613 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1612) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1614 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1611) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1615 = flow.tensor.bitcast %1614 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1616 = flow.tensor.bitcast %1613 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1617 = flow.tensor.reshape %1610 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %1618 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1616, %1615, %19, %1617, %1558) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1619 = flow.tensor.reshape %1618 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1620 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1619) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1621 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1618) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1622 = flow.tensor.reshape %1620 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1623 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1622, %1621) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1624 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_17_ffn_norm_weight_tensor_2048xf32, %1623) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1625 = flow.tensor.bitcast %__parameter_model_blk_17_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1626 = flow.tensor.bitcast %__parameter_model_blk_17_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1627 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1626) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1628 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1625) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1629 = flow.tensor.bitcast %1628 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1630 = flow.tensor.bitcast %1627 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1631 = flow.tensor.bitcast %__parameter_model_blk_17_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1632 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1631) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1633 = flow.tensor.bitcast %1632 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1634 = flow.tensor.bitcast %__parameter_model_blk_17_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1635 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1634) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1636 = flow.tensor.bitcast %1635 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1637 = flow.tensor.reshape %1624 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1638 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1633, %1636, %19, %1637) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1639 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1630, %1629, %19, %1637, %1638) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1640 = flow.tensor.bitcast %__parameter_model_blk_17_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1641 = flow.tensor.bitcast %__parameter_model_blk_17_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1642 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1641) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1643 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1640) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1644 = flow.tensor.bitcast %1643 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1645 = flow.tensor.bitcast %1642 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1646 = flow.tensor.reshape %1639 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %1647 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%1645, %1644, %19, %1646, %1618) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1648 = flow.tensor.reshape %1647 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1649 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1648) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1650 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1647) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1651 = flow.tensor.reshape %1649 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1652 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1651, %1650) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1653 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_18_attn_norm_weight_tensor_2048xf32, %1652) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1654 = flow.tensor.bitcast %__parameter_model_blk_18_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1655 = flow.tensor.bitcast %__parameter_model_blk_18_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1656 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1655) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1657 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1654) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1658 = flow.tensor.bitcast %1657 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1659 = flow.tensor.bitcast %1656 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1660 = flow.tensor.reshape %1653 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1661 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1659, %1658, %19, %1660) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1662 = flow.tensor.reshape %1661 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %1663 = flow.tensor.bitcast %__parameter_model_blk_18_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1664 = flow.tensor.bitcast %__parameter_model_blk_18_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1665 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1664) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1666 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1663) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1667 = flow.tensor.bitcast %1666 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1668 = flow.tensor.bitcast %1665 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1669 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1668, %1667, %19, %1660) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1670 = flow.tensor.reshape %1669 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1671 = flow.tensor.bitcast %__parameter_model_blk_18_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1672 = flow.tensor.bitcast %__parameter_model_blk_18_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1673 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1672) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1674 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1671) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1675 = flow.tensor.bitcast %1674 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1676 = flow.tensor.bitcast %1673 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1677 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1676, %1675, %19, %1660) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1678 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%1662, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1679 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%1662, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1680 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%1670, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1681 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%1670, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1682 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%1680, %69, %1681, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %1683 = flow.dispatch @prefill_bs4$async_dispatch_784::@prefill_bs4$async_dispatch_784_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1684 = flow.tensor.reshape %1683 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1685 = flow.tensor.reshape %1682 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %1686 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1685) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1687 = flow.tensor.reshape %1686 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1688 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1687, %1684, %1605, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1605{%80}
    %1689 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %1683) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1690 = flow.tensor.reshape %1689 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1691 = flow.tensor.reshape %1677 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %1692 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1691) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1693 = flow.tensor.reshape %1692 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1694 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1693, %1690, %1688, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1688{%80}
    %1695 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%1678, %69, %1679, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %1696 = flow.tensor.reshape %1695 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %1697 = flow.tensor.reshape %1682 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %1698 = flow.tensor.reshape %1677 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1699 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %1696, %1697, %1698, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %1700 = flow.tensor.bitcast %__parameter_model_blk_18_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1701 = flow.tensor.bitcast %__parameter_model_blk_18_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1702 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1701) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1703 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1700) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1704 = flow.tensor.bitcast %1703 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1705 = flow.tensor.bitcast %1702 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1706 = flow.tensor.reshape %1699 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %1707 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1705, %1704, %19, %1706, %1647) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1708 = flow.tensor.reshape %1707 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1709 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1708) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1710 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1707) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1711 = flow.tensor.reshape %1709 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1712 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1711, %1710) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1713 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_18_ffn_norm_weight_tensor_2048xf32, %1712) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1714 = flow.tensor.bitcast %__parameter_model_blk_18_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1715 = flow.tensor.bitcast %__parameter_model_blk_18_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1716 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1715) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1717 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1714) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1718 = flow.tensor.bitcast %1717 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1719 = flow.tensor.bitcast %1716 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1720 = flow.tensor.bitcast %__parameter_model_blk_18_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1721 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1720) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1722 = flow.tensor.bitcast %1721 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1723 = flow.tensor.bitcast %__parameter_model_blk_18_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1724 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1723) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1725 = flow.tensor.bitcast %1724 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1726 = flow.tensor.reshape %1713 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1727 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1722, %1725, %19, %1726) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1728 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1719, %1718, %19, %1726, %1727) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1729 = flow.tensor.bitcast %__parameter_model_blk_18_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1730 = flow.tensor.bitcast %__parameter_model_blk_18_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1731 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1730) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1732 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1729) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1733 = flow.tensor.bitcast %1732 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1734 = flow.tensor.bitcast %1731 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1735 = flow.tensor.reshape %1728 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %1736 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%1734, %1733, %19, %1735, %1707) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1737 = flow.tensor.reshape %1736 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1738 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1737) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1739 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1736) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1740 = flow.tensor.reshape %1738 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1741 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1740, %1739) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1742 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_19_attn_norm_weight_tensor_2048xf32, %1741) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1743 = flow.tensor.bitcast %__parameter_model_blk_19_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1744 = flow.tensor.bitcast %__parameter_model_blk_19_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1745 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1744) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1746 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1743) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1747 = flow.tensor.bitcast %1746 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1748 = flow.tensor.bitcast %1745 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1749 = flow.tensor.reshape %1742 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1750 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1748, %1747, %19, %1749) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1751 = flow.tensor.reshape %1750 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %1752 = flow.tensor.bitcast %__parameter_model_blk_19_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1753 = flow.tensor.bitcast %__parameter_model_blk_19_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1754 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1753) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1755 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1752) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1756 = flow.tensor.bitcast %1755 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1757 = flow.tensor.bitcast %1754 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1758 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1757, %1756, %19, %1749) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1759 = flow.tensor.reshape %1758 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1760 = flow.tensor.bitcast %__parameter_model_blk_19_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1761 = flow.tensor.bitcast %__parameter_model_blk_19_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1762 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1761) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1763 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1760) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1764 = flow.tensor.bitcast %1763 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1765 = flow.tensor.bitcast %1762 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1766 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1765, %1764, %19, %1749) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1767 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%1751, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1768 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%1751, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1769 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%1759, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1770 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%1759, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1771 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%1769, %69, %1770, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %1772 = flow.dispatch @prefill_bs4$async_dispatch_826::@prefill_bs4$async_dispatch_826_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1773 = flow.tensor.reshape %1772 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1774 = flow.tensor.reshape %1771 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %1775 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1774) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1776 = flow.tensor.reshape %1775 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1777 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1776, %1773, %1694, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1694{%80}
    %1778 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %1772) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1779 = flow.tensor.reshape %1778 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1780 = flow.tensor.reshape %1766 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %1781 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1780) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1782 = flow.tensor.reshape %1781 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1783 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1782, %1779, %1777, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1777{%80}
    %1784 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%1767, %69, %1768, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %1785 = flow.tensor.reshape %1784 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %1786 = flow.tensor.reshape %1771 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %1787 = flow.tensor.reshape %1766 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1788 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %1785, %1786, %1787, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %1789 = flow.tensor.bitcast %__parameter_model_blk_19_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1790 = flow.tensor.bitcast %__parameter_model_blk_19_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1791 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1790) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1792 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1789) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1793 = flow.tensor.bitcast %1792 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1794 = flow.tensor.bitcast %1791 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1795 = flow.tensor.reshape %1788 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %1796 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1794, %1793, %19, %1795, %1736) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1797 = flow.tensor.reshape %1796 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1798 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1797) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1799 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1796) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1800 = flow.tensor.reshape %1798 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1801 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1800, %1799) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1802 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_19_ffn_norm_weight_tensor_2048xf32, %1801) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1803 = flow.tensor.bitcast %__parameter_model_blk_19_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1804 = flow.tensor.bitcast %__parameter_model_blk_19_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1805 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1804) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1806 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1803) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1807 = flow.tensor.bitcast %1806 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1808 = flow.tensor.bitcast %1805 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1809 = flow.tensor.bitcast %__parameter_model_blk_19_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1810 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1809) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1811 = flow.tensor.bitcast %1810 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1812 = flow.tensor.bitcast %__parameter_model_blk_19_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1813 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1812) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1814 = flow.tensor.bitcast %1813 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1815 = flow.tensor.reshape %1802 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1816 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1811, %1814, %19, %1815) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1817 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1808, %1807, %19, %1815, %1816) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1818 = flow.tensor.bitcast %__parameter_model_blk_19_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1819 = flow.tensor.bitcast %__parameter_model_blk_19_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1820 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1819) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1821 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1818) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1822 = flow.tensor.bitcast %1821 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1823 = flow.tensor.bitcast %1820 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1824 = flow.tensor.reshape %1817 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %1825 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%1823, %1822, %19, %1824, %1796) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1826 = flow.tensor.reshape %1825 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1827 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1826) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1828 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1825) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1829 = flow.tensor.reshape %1827 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1830 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1829, %1828) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1831 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_20_attn_norm_weight_tensor_2048xf32, %1830) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1832 = flow.tensor.bitcast %__parameter_model_blk_20_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1833 = flow.tensor.bitcast %__parameter_model_blk_20_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1834 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1833) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1835 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1832) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1836 = flow.tensor.bitcast %1835 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1837 = flow.tensor.bitcast %1834 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1838 = flow.tensor.reshape %1831 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1839 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1837, %1836, %19, %1838) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1840 = flow.tensor.reshape %1839 : tensor<?x2048xf16>{%19} -> tensor<4x?x32x64xf16>{%8}
    %1841 = flow.tensor.bitcast %__parameter_model_blk_20_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1842 = flow.tensor.bitcast %__parameter_model_blk_20_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1843 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1842) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1844 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1841) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1845 = flow.tensor.bitcast %1844 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1846 = flow.tensor.bitcast %1843 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1847 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1846, %1845, %19, %1838) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1848 = flow.tensor.reshape %1847 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1849 = flow.tensor.bitcast %__parameter_model_blk_20_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1850 = flow.tensor.bitcast %__parameter_model_blk_20_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1851 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1850) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1852 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1849) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1853 = flow.tensor.bitcast %1852 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1854 = flow.tensor.bitcast %1851 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1855 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1854, %1853, %19, %1838) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1856 = flow.dispatch @prefill_bs4$async_dispatch_22::@prefill_bs4$async_dispatch_22_slow_memcpy[%8](%1840, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1857 = flow.dispatch @prefill_bs4$async_dispatch_23::@prefill_bs4$async_dispatch_23_slow_memcpy[%8](%1840, %8) : (tensor<4x?x32x64xf16>{%8}, index) -> tensor<4x?x32x32xf16>{%8}
    %1858 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%1848, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1859 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%1848, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1860 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%1858, %69, %1859, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %1861 = flow.dispatch @prefill_bs4$async_dispatch_868::@prefill_bs4$async_dispatch_868_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1862 = flow.tensor.reshape %1861 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1863 = flow.tensor.reshape %1860 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %1864 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1863) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1865 = flow.tensor.reshape %1864 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1866 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1865, %1862, %1783, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1783{%80}
    %1867 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %1861) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1868 = flow.tensor.reshape %1867 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1869 = flow.tensor.reshape %1855 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %1870 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1869) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1871 = flow.tensor.reshape %1870 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1872 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1871, %1868, %1866, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1866{%80}
    %1873 = flow.dispatch @prefill_bs4$async_dispatch_33::@prefill_bs4$async_dispatch_33_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%1856, %69, %1857, %70, %8, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %1874 = flow.tensor.reshape %1873 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %1875 = flow.tensor.reshape %1860 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %1876 = flow.tensor.reshape %1855 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1877 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %1874, %1875, %1876, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %1878 = flow.tensor.bitcast %__parameter_model_blk_20_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1879 = flow.tensor.bitcast %__parameter_model_blk_20_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1880 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1879) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1881 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1878) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1882 = flow.tensor.bitcast %1881 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1883 = flow.tensor.bitcast %1880 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1884 = flow.tensor.reshape %1877 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %1885 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1883, %1882, %19, %1884, %1825) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1886 = flow.tensor.reshape %1885 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1887 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1886) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1888 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1885) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1889 = flow.tensor.reshape %1887 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1890 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1889, %1888) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1891 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_20_ffn_norm_weight_tensor_2048xf32, %1890) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1892 = flow.tensor.bitcast %__parameter_model_blk_20_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1893 = flow.tensor.bitcast %__parameter_model_blk_20_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1894 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1893) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1895 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1892) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1896 = flow.tensor.bitcast %1895 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1897 = flow.tensor.bitcast %1894 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1898 = flow.tensor.bitcast %__parameter_model_blk_20_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1899 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1898) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1900 = flow.tensor.bitcast %1899 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1901 = flow.tensor.bitcast %__parameter_model_blk_20_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1902 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1901) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1903 = flow.tensor.bitcast %1902 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1904 = flow.tensor.reshape %1891 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1905 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1900, %1903, %19, %1904) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1906 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1897, %1896, %19, %1904, %1905) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1907 = flow.tensor.bitcast %__parameter_model_blk_20_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1908 = flow.tensor.bitcast %__parameter_model_blk_20_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1909 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1908) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1910 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1907) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1911 = flow.tensor.bitcast %1910 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1912 = flow.tensor.bitcast %1909 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1913 = flow.tensor.reshape %1906 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %1914 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%1912, %1911, %19, %1913, %1885) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1915 = flow.tensor.reshape %1914 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1916 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1915) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1917 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1914) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1918 = flow.tensor.reshape %1916 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1919 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1918, %1917) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1920 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_21_attn_norm_weight_tensor_2048xf32, %1919) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1921 = flow.tensor.bitcast %__parameter_model_blk_21_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1922 = flow.tensor.bitcast %__parameter_model_blk_21_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1923 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1922) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1924 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1921) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1925 = flow.tensor.bitcast %1924 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1926 = flow.tensor.bitcast %1923 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1927 = flow.tensor.reshape %1920 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1928 = flow.dispatch @prefill_bs4$async_dispatch_10::@prefill_bs4$async_dispatch_10_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1926, %1925, %19, %1927) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1929 = flow.tensor.reshape %1928 : tensor<?x2048xf16>{%19} -> tensor<4x?x4x8x64xf16>{%8}
    %1930 = flow.tensor.bitcast %__parameter_model_blk_21_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1931 = flow.tensor.bitcast %__parameter_model_blk_21_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1932 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1931) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1933 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1930) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1934 = flow.tensor.bitcast %1933 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1935 = flow.tensor.bitcast %1932 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1936 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1935, %1934, %19, %1927) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1937 = flow.tensor.reshape %1936 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1938 = flow.tensor.bitcast %__parameter_model_blk_21_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1939 = flow.tensor.bitcast %__parameter_model_blk_21_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1940 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1939) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1941 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1938) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1942 = flow.tensor.bitcast %1941 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1943 = flow.tensor.bitcast %1940 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1944 = flow.dispatch @prefill_bs4$async_dispatch_13::@prefill_bs4$async_dispatch_13_matmul_like_Dx256x64x32_f16xf16xf32[%19](%1943, %1942, %19, %1927) : (tensor<256x64xf16>, tensor<256x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x256xf16>{%19}
    %1945 = flow.dispatch @prefill_bs4$async_dispatch_24::@prefill_bs4$async_dispatch_24_slow_memcpy[%8](%1937, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1946 = flow.dispatch @prefill_bs4$async_dispatch_25::@prefill_bs4$async_dispatch_25_slow_memcpy[%8](%1937, %8) : (tensor<4x?x4x64xf16>{%8}, index) -> tensor<4x?x4x32xf16>{%8}
    %1947 = flow.dispatch @prefill_bs4$async_dispatch_26::@prefill_bs4$async_dispatch_26_elementwise_broadcast_4xDx4x2x32_f16[%8, %8](%1945, %69, %1946, %70, %8, %8, %8) : (tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x4x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index, index) -> tensor<4x?x4x2x32xf16>{%8}
    %1948 = flow.dispatch @prefill_bs4$async_dispatch_908::@prefill_bs4$async_dispatch_908_elementwise_D_i64[%72](%72, %74#0) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1949 = flow.tensor.reshape %1948 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1950 = flow.tensor.reshape %1947 : tensor<4x?x4x2x32xf16>{%8} -> tensor<?x32x4x64xf16>{%76}
    %1951 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1950) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1952 = flow.tensor.reshape %1951 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1953 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1952, %1949, %1872, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1872{%80}
    %1954 = flow.dispatch @prefill_bs4$async_dispatch_30::@prefill_bs4$async_dispatch_30_elementwise_D_i64[%72](%72, %1948) : (index, tensor<?xi64>{%72}) -> tensor<?xi64>{%72}
    %1955 = flow.tensor.reshape %1954 : tensor<?xi64>{%72} -> tensor<4x?xi64>{%9}
    %1956 = flow.tensor.reshape %1944 : tensor<?x256xf16>{%19} -> tensor<?x32x4x64xf16>{%76}
    %1957 = flow.dispatch @prefill_bs4$async_dispatch_28::@prefill_bs4$async_dispatch_28_transpose_Dx32x4x64_f16[%76](%76, %1956) : (index, tensor<?x32x4x64xf16>{%76}) -> tensor<?x4x32x64xf16>{%76}
    %1958 = flow.tensor.reshape %1957 : tensor<?x4x32x64xf16>{%76} -> tensor<4x?x4x32x2x1x32xf16>{%53}
    %1959 = flow.dispatch @prefill_bs4$async_dispatch_29::@prefill_bs4$async_dispatch_29_scatter_4xDx4x32x2x1x32xf16_dispatch_tensor_store[%53, %9, %80](%1958, %1955, %1953, %53, %9, %80) : (tensor<4x?x4x32x2x1x32xf16>{%53}, tensor<4x?xi64>{%9}, tensor<?x4x32x2x1x32xf16>{%80}, index, index, index) -> %1953{%80}
    %1960 = flow.tensor.reshape %1959 : tensor<?x4x32x2x1x32xf16>{%80} -> tensor<?x360448xf16>{%10}
    %1961 = flow.dispatch @prefill_bs4$async_dispatch_914::@prefill_bs4$async_dispatch_914_slow_memcpy[%8](%1929, %8) : (tensor<4x?x4x8x64xf16>{%8}, index) -> tensor<4x?x4x8x32xf16>{%8}
    %1962 = flow.dispatch @prefill_bs4$async_dispatch_915::@prefill_bs4$async_dispatch_915_slow_memcpy[%8](%1929, %8) : (tensor<4x?x4x8x64xf16>{%8}, index) -> tensor<4x?x4x8x32xf16>{%8}
    %1963 = flow.tensor.reshape %1961 : tensor<4x?x4x8x32xf16>{%8} -> tensor<4x?x32x32xf16>{%8}
    %1964 = flow.tensor.reshape %1962 : tensor<4x?x4x8x32xf16>{%8} -> tensor<4x?x32x32xf16>{%8}
    %1965 = flow.dispatch @prefill_bs4$async_dispatch_916::@prefill_bs4$async_dispatch_916_elementwise_broadcast_4x32xDx2x32_f16[%8, %8](%1963, %69, %1964, %70, %8, %8) : (tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, tensor<4x?x32x32xf16>{%8}, tensor<?x32xf16>{%8}, index, index) -> tensor<4x32x?x2x32xf16>{%8}
    %1966 = flow.tensor.reshape %1965 : tensor<4x32x?x2x32xf16>{%8} -> tensor<4x4x8x?x64xf16>{%8}
    %1967 = flow.tensor.reshape %1947 : tensor<4x?x4x2x32xf16>{%8} -> tensor<4x?x4x64xf16>{%8}
    %1968 = flow.tensor.reshape %1944 : tensor<?x256xf16>{%19} -> tensor<4x?x4x64xf16>{%8}
    %1969 = flow.dispatch @prefill_bs4$async_dispatch_35::@prefill_bs4$async_dispatch_35_attention_4x4x8xDxDxf16_generic[%8, %8](%8, %1966, %1967, %1968, %96, %8) : (index, tensor<4x4x8x?x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x?x4x64xf16>{%8}, tensor<4x4x8x?x?xf16>{%8, %8}, index) -> tensor<4x?x4x8x64xf16>{%8}
    %1970 = flow.tensor.bitcast %__parameter_model_blk_21_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1971 = flow.tensor.bitcast %__parameter_model_blk_21_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1972 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1971) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1973 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1970) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1974 = flow.tensor.bitcast %1973 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1975 = flow.tensor.bitcast %1972 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1976 = flow.tensor.reshape %1969 : tensor<4x?x4x8x64xf16>{%8} -> tensor<?x64x32xf16>{%19}
    %1977 = flow.dispatch @prefill_bs4$async_dispatch_38::@prefill_bs4$async_dispatch_38_matmul_like_Dx2048x64x32_f16xf16xf32[%19](%1975, %1974, %19, %1976, %1914) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1978 = flow.tensor.reshape %1977 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %1979 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %1978) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %1980 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %1977) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %1981 = flow.tensor.reshape %1979 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %1982 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %1981, %1980) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %1983 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_blk_21_ffn_norm_weight_tensor_2048xf32, %1982) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %1984 = flow.tensor.bitcast %__parameter_model_blk_21_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1985 = flow.tensor.bitcast %__parameter_model_blk_21_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1986 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1985) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1987 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1984) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1988 = flow.tensor.bitcast %1987 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1989 = flow.tensor.bitcast %1986 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1990 = flow.tensor.bitcast %__parameter_model_blk_21_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1991 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1990) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1992 = flow.tensor.bitcast %1991 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1993 = flow.tensor.bitcast %__parameter_model_blk_21_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1994 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1993) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1995 = flow.tensor.bitcast %1994 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1996 = flow.tensor.reshape %1983 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %1997 = flow.dispatch @prefill_bs4$async_dispatch_47::@prefill_bs4$async_dispatch_47_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1992, %1995, %19, %1996) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1998 = flow.dispatch @prefill_bs4$async_dispatch_48::@prefill_bs4$async_dispatch_48_matmul_like_Dx5632x64x32_f16xf16xf32[%19](%1989, %1988, %19, %1996, %1997) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, index, tensor<?x64x32xf16>{%19}, tensor<?x5632xf16>{%19}) -> tensor<?x5632xf16>{%19}
    %1999 = flow.tensor.bitcast %__parameter_model_blk_21_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %2000 = flow.tensor.bitcast %__parameter_model_blk_21_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %2001 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%2000) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %2002 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1999) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %2003 = flow.tensor.bitcast %2002 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %2004 = flow.tensor.bitcast %2001 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %2005 = flow.tensor.reshape %1998 : tensor<?x5632xf16>{%19} -> tensor<?x176x32xf16>{%19}
    %2006 = flow.dispatch @prefill_bs4$async_dispatch_51::@prefill_bs4$async_dispatch_51_matmul_like_Dx2048x176x32_f16xf16xf32[%19](%2004, %2003, %19, %2005, %1977) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, index, tensor<?x176x32xf16>{%19}, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %2007 = flow.tensor.reshape %2006 : tensor<?x2048xf16>{%19} -> tensor<?xf16>{%22}
    %2008 = flow.dispatch @prefill_bs4$async_dispatch_4::@prefill_bs4$async_dispatch_4_elementwise_D_f16xf32[%22](%22, %2007) : (index, tensor<?xf16>{%22}) -> tensor<?xf32>{%22}
    %2009 = flow.dispatch @prefill_bs4$async_dispatch_5::@prefill_bs4$async_dispatch_5_reduction_Dx2048_f32[%19](%19, %2006) : (index, tensor<?x2048xf16>{%19}) -> tensor<?xf32>{%19}
    %2010 = flow.tensor.reshape %2008 : tensor<?xf32>{%22} -> tensor<?x2048xf32>{%19}
    %2011 = flow.dispatch @prefill_bs4$async_dispatch_6::@prefill_bs4$async_dispatch_6_elementwise_Dx2048_f32xf32xf16[%19](%19, %2010, %2009) : (index, tensor<?x2048xf32>{%19}, tensor<?xf32>{%19}) -> tensor<?x2048xf16>{%19}
    %2012 = flow.dispatch @prefill_bs4$async_dispatch_7::@prefill_bs4$async_dispatch_7_elementwise_Dx2048_f32xf16xf16[%19](%19, %__parameter_model_output_norm_weight_tensor_2048xf32, %2011) : (index, tensor<2048xf32>, tensor<?x2048xf16>{%19}) -> tensor<?x2048xf16>{%19}
    %2013 = flow.tensor.bitcast %__parameter_model_output_weight_tensor_32000x2176xi8 : tensor<32000x2176xi8> -> tensor<32000x64x17xi16>
    %2014 = flow.tensor.bitcast %__parameter_model_output_weight_tensor_32000x2176xi8 : tensor<32000x2176xi8> -> tensor<32000x64x1x17xi16>
    %2015 = flow.dispatch @prefill_bs4$async_dispatch_0::@prefill_bs4$async_dispatch_0_slow_memcpy(%2014) : (tensor<32000x64x1x17xi16>) -> tensor<32000x64x1xi16>
    %2016 = flow.dispatch @prefill_bs4$async_dispatch_1::@prefill_bs4$async_dispatch_1_slow_memcpy(%2013) : (tensor<32000x64x17xi16>) -> tensor<32000x64x16xi16>
    %2017 = flow.tensor.bitcast %2016 : tensor<32000x64x16xi16> -> tensor<32000x64x32xi8>
    %2018 = flow.tensor.bitcast %2015 : tensor<32000x64x1xi16> -> tensor<32000x64xf16>
    %2019 = flow.tensor.reshape %2012 : tensor<?x2048xf16>{%19} -> tensor<?x64x32xf16>{%19}
    %2020 = flow.dispatch @prefill_bs4$async_dispatch_940::@prefill_bs4$async_dispatch_940_matmul_like_Dx32000x64x32_f16xf16xf32[%19](%2018, %2017, %19, %2019) : (tensor<32000x64xf16>, tensor<32000x64x32xi8>, index, tensor<?x64x32xf16>{%19}) -> tensor<?x32000xf16>{%19}
    %2021 = flow.tensor.reshape %2020 : tensor<?x32000xf16>{%19} -> tensor<4x?x32000xf16>{%8}
    %2022 = hal.tensor.alias on(#hal.device.affinity<@__device_0>) wait(%arg4) => %1960 : tensor<?x360448xf16>{%10} to %arg3 : !hal.buffer_view
    %2023:2 = hal.tensor.barrier join(%2022, %2021 : tensor<?x360448xf16>, tensor<4x?x32000xf16>) => %arg5 : !hal.fence
    %2024 = hal.tensor.export %2023#1 : tensor<4x?x32000xf16>{%8} -> !hal.buffer_view
    util.return %2024 : !hal.buffer_view
  }
  util.func public @prefill_bs4(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view, %arg3: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %1 = util.call @prefill_bs4$async(%arg0, %arg1, %arg2, %arg3, %0, %fence) : (!hal.buffer_view, !hal.buffer_view, !hal.buffer_view, !hal.buffer_view, !hal.fence, !hal.fence) -> !hal.buffer_view
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) flags("None") : i32
    util.return %1 : !hal.buffer_view
  }
  flow.executable private @decode_bs4$async_dispatch_3 {
    flow.executable.export public @decode_bs4$async_dispatch_3_elementwise_broadcast_4x2048_i64xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_3_elementwise_broadcast_4x2048_i64xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<32000x2048xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x2048xf16>>) {
        %c32 = arith.constant 32 : index
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [32000, 2048], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<32000x2048xf16>> -> tensor<32000x2048xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %2 = tensor.empty() : tensor<4x2048xf16>
        %3 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%1 : tensor<4xi64>) outs(%2 : tensor<4x2048xf16>) {
        ^bb0(%in: i64, %out: f16):
          %4 = linalg.index 1 : index
          %5 = arith.remsi %4, %c32 : index
          %6 = arith.divsi %4, %c32 : index
          %7 = arith.index_cast %in : i64 to index
          %8 = arith.muli %6, %c32 overflow<nsw> : index
          %9 = arith.addi %5, %8 : index
          %extracted = tensor.extract %0[%7, %9] : tensor<32000x2048xf16>
          linalg.yield %extracted : f16
        } -> tensor<4x2048xf16>
        iree_tensor_ext.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [4, 2048], strides = [1, 1] : tensor<4x2048xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x2048xf16>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_4 {
    flow.executable.export public @decode_bs4$async_dispatch_4_elementwise_8192_f16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<8192xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<8192xf32>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0], sizes = [8192], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<8192xf16>> -> tensor<8192xf16>
        %1 = tensor.empty() : tensor<8192xf32>
        %2 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%0 : tensor<8192xf16>) outs(%1 : tensor<8192xf32>) {
        ^bb0(%in: f16, %out: f32):
          %3 = arith.extf %in : f16 to f32
          linalg.yield %3 : f32
        } -> tensor<8192xf32>
        iree_tensor_ext.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [8192], strides = [1] : tensor<8192xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<8192xf32>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_5 {
    flow.executable.export public @decode_bs4$async_dispatch_5_reduction_4x2048_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_5_reduction_4x2048_f32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x2048xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %c2_i64 = arith.constant 2 : i64
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [4, 2048], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x2048xf16>> -> tensor<4x2048xf16>
        %1 = tensor.empty() : tensor<4xf32>
        %2 = tensor.empty() : tensor<4x2048xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%1 : tensor<4xf32>) -> tensor<4xf32>
        %4 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<4x2048xf16>) outs(%2 : tensor<4x2048xf32>) {
        ^bb0(%in: f16, %out: f32):
          %6 = arith.extf %in : f16 to f32
          linalg.yield %6 : f32
        } -> tensor<4x2048xf32>
        %5 = linalg.generic {indexing_maps = [#map1, #map], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<4x2048xf32>) outs(%3 : tensor<4xf32>) {
        ^bb0(%in: f32, %out: f32):
          %6 = math.fpowi %in, %c2_i64 : f32, i64
          %7 = arith.addf %6, %out : f32
          linalg.yield %7 : f32
        } -> tensor<4xf32>
        iree_tensor_ext.dispatch.tensor.store %5, %arg1, offsets = [0], sizes = [4], strides = [1] : tensor<4xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xf32>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_6 {
    flow.executable.export public @decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x2048xf32>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xf32>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x2048xf16>>) {
        %cst = arith.constant 2.048000e+03 : f32
        %cst_0 = arith.constant 9.99999974E-6 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [4, 2048], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x2048xf32>> -> tensor<4x2048xf32>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xf32>> -> tensor<4xf32>
        %2 = tensor.empty() : tensor<4x2048xf16>
        %3 = linalg.generic {indexing_maps = [#map1, #map, #map1], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<4x2048xf32>, tensor<4xf32>) outs(%2 : tensor<4x2048xf16>) {
        ^bb0(%in: f32, %in_1: f32, %out: f16):
          %4 = arith.divf %in_1, %cst : f32
          %5 = arith.addf %4, %cst_0 : f32
          %6 = math.rsqrt %5 : f32
          %7 = arith.mulf %in, %6 : f32
          %8 = arith.truncf %7 : f32 to f16
          linalg.yield %8 : f16
        } -> tensor<4x2048xf16>
        iree_tensor_ext.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [4, 2048], strides = [1, 1] : tensor<4x2048xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x2048xf16>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_7 {
    flow.executable.export public @decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048xf32>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x2048xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x2048xf16>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0], sizes = [2048], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048xf32>> -> tensor<2048xf32>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 2048], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x2048xf16>> -> tensor<4x2048xf16>
        %2 = tensor.empty() : tensor<4x2048xf16>
        %3 = linalg.generic {indexing_maps = [#map3, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<2048xf32>, tensor<4x2048xf16>) outs(%2 : tensor<4x2048xf16>) {
        ^bb0(%in: f32, %in_0: f16, %out: f16):
          %4 = arith.extf %in_0 : f16 to f32
          %5 = arith.mulf %in, %4 : f32
          %6 = arith.truncf %5 : f32 to f16
          linalg.yield %6 : f16
        } -> tensor<4x2048xf16>
        iree_tensor_ext.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [4, 2048], strides = [1, 1] : tensor<4x2048xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x2048xf16>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_10 {
    flow.executable.export public @decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64x32xi8>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x64x32xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x2048xf16>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [2048, 64], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64xf16>> -> tensor<2048x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [2048, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64x32xi8>> -> tensor<2048x64x32xi8>
        %2 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x64x32xf16>> -> tensor<4x64x32xf16>
        %3 = tensor.empty() : tensor<4x2048xf16>
        %4 = tensor.empty() : tensor<4x2048xf32>
        %5 = tensor.empty() : tensor<2048x64x32xf16>
        %6 = linalg.generic {indexing_maps = [#map4, #map5, #map5], iterator_types = ["parallel", "parallel", "parallel"]} ins(%0, %1 : tensor<2048x64xf16>, tensor<2048x64x32xi8>) outs(%5 : tensor<2048x64x32xf16>) {
        ^bb0(%in: f16, %in_0: i8, %out: f16):
          %10 = arith.extsi %in_0 : i8 to i32
          %11 = arith.sitofp %10 : i32 to f16
          %12 = arith.mulf %11, %in : f16
          linalg.yield %12 : f16
        } -> tensor<2048x64x32xf16>
        %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<4x2048xf32>) -> tensor<4x2048xf32>
        %8 = linalg.generic {indexing_maps = [#map6, #map7, #map8], iterator_types = ["parallel", "parallel", "reduction", "reduction"]} ins(%2, %6 : tensor<4x64x32xf16>, tensor<2048x64x32xf16>) outs(%7 : tensor<4x2048xf32>) {
        ^bb0(%in: f16, %in_0: f16, %out: f32):
          %10 = arith.mulf %in, %in_0 : f16
          %11 = arith.extf %10 : f16 to f32
          %12 = arith.addf %11, %out : f32
          linalg.yield %12 : f32
        } -> tensor<4x2048xf32>
        %9 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<4x2048xf32>) outs(%3 : tensor<4x2048xf16>) {
        ^bb0(%in: f32, %out: f16):
          %10 = arith.truncf %in : f32 to f16
          linalg.yield %10 : f16
        } -> tensor<4x2048xf16>
        iree_tensor_ext.dispatch.tensor.store %9, %arg3, offsets = [0, 0], sizes = [4, 2048], strides = [1, 1] : tensor<4x2048xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x2048xf16>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_13 {
    flow.executable.export public @decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<256x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<256x64x32xi8>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x64x32xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x256xf16>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [256, 64], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<256x64xf16>> -> tensor<256x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [256, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<256x64x32xi8>> -> tensor<256x64x32xi8>
        %2 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x64x32xf16>> -> tensor<4x64x32xf16>
        %3 = tensor.empty() : tensor<4x256xf16>
        %4 = tensor.empty() : tensor<4x256xf32>
        %5 = tensor.empty() : tensor<256x64x32xf16>
        %6 = linalg.generic {indexing_maps = [#map4, #map5, #map5], iterator_types = ["parallel", "parallel", "parallel"]} ins(%0, %1 : tensor<256x64xf16>, tensor<256x64x32xi8>) outs(%5 : tensor<256x64x32xf16>) {
        ^bb0(%in: f16, %in_0: i8, %out: f16):
          %10 = arith.extsi %in_0 : i8 to i32
          %11 = arith.sitofp %10 : i32 to f16
          %12 = arith.mulf %11, %in : f16
          linalg.yield %12 : f16
        } -> tensor<256x64x32xf16>
        %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<4x256xf32>) -> tensor<4x256xf32>
        %8 = linalg.generic {indexing_maps = [#map6, #map7, #map8], iterator_types = ["parallel", "parallel", "reduction", "reduction"]} ins(%2, %6 : tensor<4x64x32xf16>, tensor<256x64x32xf16>) outs(%7 : tensor<4x256xf32>) {
        ^bb0(%in: f16, %in_0: f16, %out: f32):
          %10 = arith.mulf %in, %in_0 : f16
          %11 = arith.extf %10 : f16 to f32
          %12 = arith.addf %11, %out : f32
          linalg.yield %12 : f32
        } -> tensor<4x256xf32>
        %9 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<4x256xf32>) outs(%3 : tensor<4x256xf16>) {
        ^bb0(%in: f32, %out: f16):
          %10 = arith.truncf %in : f32 to f16
          linalg.yield %10 : f16
        } -> tensor<4x256xf16>
        iree_tensor_ext.dispatch.tensor.store %9, %arg3, offsets = [0, 0], sizes = [4, 256], strides = [1, 1] : tensor<4x256xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x256xf16>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_17 {
    flow.executable.export public @decode_bs4$async_dispatch_17_elementwise_4_i64xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_17_elementwise_4_i64xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xf32>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %1 = tensor.empty() : tensor<4xf32>
        %2 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%0 : tensor<4xi64>) outs(%1 : tensor<4xf32>) {
        ^bb0(%in: i64, %out: f32):
          %3 = arith.sitofp %in : i64 to f32
          linalg.yield %3 : f32
        } -> tensor<4xf32>
        iree_tensor_ext.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [4], strides = [1] : tensor<4xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xf32>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_18 {
    flow.executable.export public @decode_bs4$async_dispatch_18_batch_mmt4d_4x1x4x1x1x8x1_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_18_batch_mmt4d_4x1x4x1x1x8x1_f32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x1x1x1x1xf32>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x4x1x8x1xf32>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x1x4x1x8xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0, 0, 0], sizes = [4, 1, 1, 1, 1], strides = [1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x1x1x1x1xf32>> -> tensor<4x1x1x1x1xf32>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0, 0, 0], sizes = [4, 4, 1, 8, 1], strides = [1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x4x1x8x1xf32>> -> tensor<4x4x1x8x1xf32>
        %2 = tensor.empty() : tensor<4x1x4x1x8xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<4x1x4x1x8xf32>) -> tensor<4x1x4x1x8xf32>
        %4 = linalg.batch_mmt4d ins(%0, %1 : tensor<4x1x1x1x1xf32>, tensor<4x4x1x8x1xf32>) outs(%3 : tensor<4x1x4x1x8xf32>) -> tensor<4x1x4x1x8xf32>
        iree_tensor_ext.dispatch.tensor.store %4, %arg2, offsets = [0, 0, 0, 0, 0], sizes = [4, 1, 4, 1, 8], strides = [1, 1, 1, 1, 1] : tensor<4x1x4x1x8xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x1x4x1x8xf32>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_19 {
    flow.executable.export public @decode_bs4$async_dispatch_19_unpack_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_19_unpack_f32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x1x4x1x8xf32>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x32x1xf32>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0, 0, 0], sizes = [4, 1, 4, 1, 8], strides = [1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x1x4x1x8xf32>> -> tensor<4x1x4x1x8xf32>
        %1 = tensor.empty() : tensor<4x32x1xf32>
        %unpack = linalg.unpack %0 outer_dims_perm = [0, 2, 1] inner_dims_pos = [2, 1] inner_tiles = [1, 8] into %1 : tensor<4x1x4x1x8xf32> -> tensor<4x32x1xf32>
        iree_tensor_ext.dispatch.tensor.store %unpack, %arg1, offsets = [0, 0, 0], sizes = [4, 32, 1], strides = [1, 1, 1] : tensor<4x32x1xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x32x1xf32>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_20 {
    flow.executable.export public @decode_bs4$async_dispatch_20_elementwise_128_f32xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_20_elementwise_128_f32xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<128xf32>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<128xf16>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0], sizes = [128], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<128xf32>> -> tensor<128xf32>
        %1 = tensor.empty() : tensor<128xf16>
        %2 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%0 : tensor<128xf32>) outs(%1 : tensor<128xf16>) {
        ^bb0(%in: f32, %out: f16):
          %3 = math.cos %in : f32
          %4 = arith.truncf %3 : f32 to f16
          linalg.yield %4 : f16
        } -> tensor<128xf16>
        iree_tensor_ext.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [128], strides = [1] : tensor<128xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<128xf16>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_21 {
    flow.executable.export public @decode_bs4$async_dispatch_21_elementwise_128_f32xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_21_elementwise_128_f32xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<128xf32>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<128xf16>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0], sizes = [128], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<128xf32>> -> tensor<128xf32>
        %1 = tensor.empty() : tensor<128xf16>
        %2 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%0 : tensor<128xf32>) outs(%1 : tensor<128xf16>) {
        ^bb0(%in: f32, %out: f16):
          %3 = math.sin %in : f32
          %4 = arith.truncf %3 : f32 to f16
          linalg.yield %4 : f16
        } -> tensor<128xf16>
        iree_tensor_ext.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [128], strides = [1] : tensor<128xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<128xf16>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_22 {
    flow.executable.export public @decode_bs4$async_dispatch_22_slow_memcpy workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_22_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x1x32x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x32x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0, 0], sizes = [4, 1, 32, 32], strides = [1, 1, 1, 2] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x1x32x64xf16>> -> tensor<4x32x32xf16>
        iree_tensor_ext.dispatch.tensor.store %0, %arg1, offsets = [0, 0, 0], sizes = [4, 32, 32], strides = [1, 1, 1] : tensor<4x32x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x32x32xf16>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_23 {
    flow.executable.export public @decode_bs4$async_dispatch_23_slow_memcpy workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_23_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x1x32x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x32x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0, 1], sizes = [4, 1, 32, 32], strides = [1, 1, 1, 2] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x1x32x64xf16>> -> tensor<4x32x32xf16>
        iree_tensor_ext.dispatch.tensor.store %0, %arg1, offsets = [0, 0, 0], sizes = [4, 32, 32], strides = [1, 1, 1] : tensor<4x32x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x32x32xf16>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_24 {
    flow.executable.export public @decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x32x32xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x32xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x32x32xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x32xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x32x2x32xf16>>) {
        %c0 = arith.constant 0 : index
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [4, 32, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x32x32xf16>> -> tensor<4x32x32xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 32], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x32xf16>> -> tensor<4x32xf16>
        %2 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 32, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x32x32xf16>> -> tensor<4x32x32xf16>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [4, 32], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x32xf16>> -> tensor<4x32xf16>
        %4 = tensor.empty() : tensor<4x32x2x32xf16>
        %5 = linalg.generic {indexing_maps = [#map23, #map24, #map23, #map24, #map12], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0, %1, %2, %3 : tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) outs(%4 : tensor<4x32x2x32xf16>) {
        ^bb0(%in: f16, %in_0: f16, %in_1: f16, %in_2: f16, %out: f16):
          %6 = linalg.index 2 : index
          %7 = arith.mulf %in, %in_2 : f16
          %8 = arith.mulf %in_1, %in_0 : f16
          %9 = arith.mulf %in_1, %in_2 : f16
          %10 = arith.mulf %in, %in_0 : f16
          %11 = arith.addf %8, %7 : f16
          %12 = arith.subf %10, %9 : f16
          %13 = arith.cmpi eq, %6, %c0 : index
          %14 = arith.select %13, %12, %11 : f16
          linalg.yield %14 : f16
        } -> tensor<4x32x2x32xf16>
        iree_tensor_ext.dispatch.tensor.store %5, %arg4, offsets = [0, 0, 0, 0], sizes = [4, 32, 2, 32], strides = [1, 1, 1, 1] : tensor<4x32x2x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x32x2x32xf16>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_25 {
    flow.executable.export public @decode_bs4$async_dispatch_25_slow_memcpy workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_25_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x1x4x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0, 0], sizes = [4, 1, 4, 32], strides = [1, 1, 1, 2] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x1x4x64xf16>> -> tensor<4x4x32xf16>
        iree_tensor_ext.dispatch.tensor.store %0, %arg1, offsets = [0, 0, 0], sizes = [4, 4, 32], strides = [1, 1, 1] : tensor<4x4x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x32xf16>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_26 {
    flow.executable.export public @decode_bs4$async_dispatch_26_slow_memcpy workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_26_slow_memcpy(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x1x4x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0, 1], sizes = [4, 1, 4, 32], strides = [1, 1, 1, 2] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x1x4x64xf16>> -> tensor<4x4x32xf16>
        iree_tensor_ext.dispatch.tensor.store %0, %arg1, offsets = [0, 0, 0], sizes = [4, 4, 32], strides = [1, 1, 1] : tensor<4x4x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x32xf16>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_27 {
    flow.executable.export public @decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x4x32xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x32xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x4x32xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x32xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x2x32xf16>>) {
        %c0 = arith.constant 0 : index
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [4, 4, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x4x32xf16>> -> tensor<4x4x32xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 32], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x32xf16>> -> tensor<4x32xf16>
        %2 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 4, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x4x32xf16>> -> tensor<4x4x32xf16>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [4, 32], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x32xf16>> -> tensor<4x32xf16>
        %4 = tensor.empty() : tensor<4x4x2x32xf16>
        %5 = linalg.generic {indexing_maps = [#map23, #map24, #map23, #map24, #map12], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%0, %1, %2, %3 : tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) outs(%4 : tensor<4x4x2x32xf16>) {
        ^bb0(%in: f16, %in_0: f16, %in_1: f16, %in_2: f16, %out: f16):
          %6 = linalg.index 2 : index
          %7 = arith.mulf %in, %in_2 : f16
          %8 = arith.mulf %in_1, %in_0 : f16
          %9 = arith.mulf %in_1, %in_2 : f16
          %10 = arith.mulf %in, %in_0 : f16
          %11 = arith.addf %8, %7 : f16
          %12 = arith.subf %10, %9 : f16
          %13 = arith.cmpi eq, %6, %c0 : index
          %14 = arith.select %13, %12, %11 : f16
          linalg.yield %14 : f16
        } -> tensor<4x4x2x32xf16>
        iree_tensor_ext.dispatch.tensor.store %5, %arg4, offsets = [0, 0, 0, 0], sizes = [4, 4, 2, 32], strides = [1, 1, 1, 1] : tensor<4x4x2x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x2x32xf16>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_28 {
    flow.executable.export public @decode_bs4$async_dispatch_28_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_28_elementwise_4_i64(%arg0: index, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %cst = arith.constant 3.200000e+01 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %1 = tensor.empty() : tensor<4xi64>
        %2 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%0 : tensor<4xi64>) outs(%1 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %3 = arith.sitofp %in : i64 to f32
          %4 = arith.divf %3, %cst : f32
          %5 = math.floor %4 : f32
          %6 = arith.fptosi %5 : f32 to i64
          linalg.yield %6 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %2, %arg2, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_29 {
    flow.executable.export public @decode_bs4$async_dispatch_29_elementwise_4_i64 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_29_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c32_i64 = arith.constant 32 : i64
        %c0_i64 = arith.constant 0 : i64
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %1 = tensor.empty() : tensor<4xi64>
        %2 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%0 : tensor<4xi64>) outs(%1 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %3 = arith.remsi %in, %c32_i64 : i64
          %4 = arith.cmpi ne, %3, %c0_i64 : i64
          %5 = arith.cmpi slt, %3, %c0_i64 : i64
          %6 = arith.andi %4, %5 : i1
          %7 = arith.addi %3, %c32_i64 : i64
          %8 = arith.select %6, %7, %3 : i64
          linalg.yield %8 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_30 {
    flow.executable.export public @decode_bs4$async_dispatch_30_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_30_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c44_i64 = arith.constant 44 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c44_i64 : i64
          linalg.yield %8 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_31 {
    flow.executable.export public @decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4xi64>>) {
        %c4_i64 = arith.constant 4 : i64
        %c32_i64 = arith.constant 32 : i64
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %2 = tensor.empty() : tensor<4x4xi64>
        %3 = linalg.generic {indexing_maps = [#map, #map, #map1], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<4xi64>, tensor<4xi64>) outs(%2 : tensor<4x4xi64>) {
        ^bb0(%in: i64, %in_0: i64, %out: i64):
          %4 = linalg.index 1 : index
          %5 = arith.index_cast %4 : index to i64
          %6 = arith.muli %in, %c4_i64 : i64
          %7 = arith.addi %6, %5 : i64
          %8 = arith.muli %7, %c32_i64 : i64
          %9 = arith.addi %8, %in_0 : i64
          linalg.yield %9 : i64
        } -> tensor<4x4xi64>
        iree_tensor_ext.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [4, 4], strides = [1, 1] : tensor<4x4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_32 {
    flow.executable.export public @decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x4x2x32x1xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x4xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<readwrite:tensor<?x2x32x1xf16>>, %arg3: index) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 0 : index
        %1 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readwrite:tensor<?x2x32x1xf16>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0, 0, 0], sizes = [4, 4, 2, 32, 1], strides = [1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x4x2x32x1xf16>> -> tensor<4x4x2x32x1xf16>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [4, 4], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x4xi64>> -> tensor<4x4xi64>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0, 0], sizes = [%0, 2, 32, 1], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readwrite:tensor<?x2x32x1xf16>>{%0} -> tensor<?x2x32x1xf16>
        %5 = iree_linalg_ext.scatter dimension_map = [0] unique_indices(true) ins(%2, %3 : tensor<4x4x2x32x1xf16>, tensor<4x4xi64>) outs(%4 : tensor<?x2x32x1xf16>) {
        ^bb0(%arg4: f16, %arg5: f16):
          iree_linalg_ext.yield %arg4 : f16
        } -> tensor<?x2x32x1xf16>
        iree_tensor_ext.dispatch.tensor.store %5, %1, offsets = [0, 0, 0, 0], sizes = [%0, 2, 32, 1], strides = [1, 1, 1, 1] : tensor<?x2x32x1xf16> -> !iree_tensor_ext.dispatch.tensor<readwrite:tensor<?x2x32x1xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_33 {
    flow.executable.export public @decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4xi64>>) {
        %c1_i64 = arith.constant 1 : i64
        %c4_i64 = arith.constant 4 : i64
        %c32_i64 = arith.constant 32 : i64
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %2 = tensor.empty() : tensor<4x4xi64>
        %3 = linalg.generic {indexing_maps = [#map, #map, #map1], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<4xi64>, tensor<4xi64>) outs(%2 : tensor<4x4xi64>) {
        ^bb0(%in: i64, %in_0: i64, %out: i64):
          %4 = arith.addi %in, %c1_i64 : i64
          %5 = linalg.index 1 : index
          %6 = arith.index_cast %5 : index to i64
          %7 = arith.muli %4, %c4_i64 : i64
          %8 = arith.addi %7, %6 : i64
          %9 = arith.muli %8, %c32_i64 : i64
          %10 = arith.addi %9, %in_0 : i64
          linalg.yield %10 : i64
        } -> tensor<4x4xi64>
        iree_tensor_ext.dispatch.tensor.store %3, %arg2, offsets = [0, 0], sizes = [4, 4], strides = [1, 1] : tensor<4x4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_35 {
    flow.executable.export public @decode_bs4$async_dispatch_35_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_35_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 0, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_36 {
    flow.executable.export public @decode_bs4$async_dispatch_36_elementwise_broadcast_4xD_i64xf16 workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_36_elementwise_broadcast_4xD_i64xf16(%arg0: index, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?xf16>>) {
        %cst = arith.constant 0xFC00 : f16
        %cst_0 = arith.constant 0.000000e+00 : f16
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg0, 0 : index
        %1 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?xf16>>{%0}
        %2 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 1 : index
        %3 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty(%0) : tensor<4x?xf16>
        %5 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4x?xf16>) {
        ^bb0(%in: i64, %out: f16):
          %6 = linalg.index 1 : index
          %7 = arith.remsi %6, %2 : index
          %8 = arith.index_cast %7 : index to i64
          %9 = arith.cmpi sge, %8, %in : i64
          %10 = arith.select %9, %cst, %cst_0 : f16
          linalg.yield %10 : f16
        } -> tensor<4x?xf16>
        iree_tensor_ext.dispatch.tensor.store %5, %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : tensor<4x?xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x?xf16>>{%0}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_37 {
    flow.executable.export public @decode_bs4$async_dispatch_37_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_37_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 0, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_38 {
    flow.executable.export public @decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<128x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<128x?x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<128x64x?xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<128x?xf16>>, %arg4: index, %arg5: index, %arg6: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<128x64xf16>>) {
        %cst = arith.constant 1.250000e-01 : f16
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg4, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg5, 1 : index
        %2 = flow.dispatch.tie_shape %arg1 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<128x?x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<128x64x?xf16>>{%0}
        %4 = flow.dispatch.tie_shape %arg3 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<128x?xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [128, 64], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<128x64xf16>> -> tensor<128x64xf16>
        %6 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 0, 0], sizes = [128, %0, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<128x?x64xf16>>{%0} -> tensor<128x?x64xf16>
        %7 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0, 0], sizes = [128, 64, %0], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<128x64x?xf16>>{%0} -> tensor<128x64x?xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %4, offsets = [0, 0], sizes = [128, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<128x?xf16>>{%1} -> tensor<128x?xf16>
        %9 = tensor.empty() : tensor<128x64xf16>
        %10 = iree_linalg_ext.attention {indexing_maps = [#map28, #map29, #map23, #map30, #map24, #map8]} ins(%5, %6, %7, %cst, %8 : tensor<128x64xf16>, tensor<128x?x64xf16>, tensor<128x64x?xf16>, f16, tensor<128x?xf16>) outs(%9 : tensor<128x64xf16>) {
        ^bb0(%arg7: f32):
          iree_linalg_ext.yield %arg7 : f32
        } -> tensor<128x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %arg6, offsets = [0, 0], sizes = [128, 64], strides = [1, 1] : tensor<128x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<128x64xf16>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_41 {
    flow.executable.export public @decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64x32xi8>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x64x32xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x2048xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x2048xf16>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [2048, 64], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64xf16>> -> tensor<2048x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [2048, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x64x32xi8>> -> tensor<2048x64x32xi8>
        %2 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x64x32xf16>> -> tensor<4x64x32xf16>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [4, 2048], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x2048xf16>> -> tensor<4x2048xf16>
        %4 = tensor.empty() : tensor<4x2048xf16>
        %5 = tensor.empty() : tensor<4x2048xf32>
        %6 = tensor.empty() : tensor<2048x64x32xf16>
        %7 = linalg.generic {indexing_maps = [#map4, #map5, #map5], iterator_types = ["parallel", "parallel", "parallel"]} ins(%0, %1 : tensor<2048x64xf16>, tensor<2048x64x32xi8>) outs(%6 : tensor<2048x64x32xf16>) {
        ^bb0(%in: f16, %in_0: i8, %out: f16):
          %11 = arith.extsi %in_0 : i8 to i32
          %12 = arith.sitofp %11 : i32 to f16
          %13 = arith.mulf %12, %in : f16
          linalg.yield %13 : f16
        } -> tensor<2048x64x32xf16>
        %8 = linalg.fill ins(%cst : f32) outs(%5 : tensor<4x2048xf32>) -> tensor<4x2048xf32>
        %9 = linalg.generic {indexing_maps = [#map6, #map7, #map8], iterator_types = ["parallel", "parallel", "reduction", "reduction"]} ins(%2, %7 : tensor<4x64x32xf16>, tensor<2048x64x32xf16>) outs(%8 : tensor<4x2048xf32>) {
        ^bb0(%in: f16, %in_0: f16, %out: f32):
          %11 = arith.mulf %in, %in_0 : f16
          %12 = arith.extf %11 : f16 to f32
          %13 = arith.addf %12, %out : f32
          linalg.yield %13 : f32
        } -> tensor<4x2048xf32>
        %10 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%3, %9 : tensor<4x2048xf16>, tensor<4x2048xf32>) outs(%4 : tensor<4x2048xf16>) {
        ^bb0(%in: f16, %in_0: f32, %out: f16):
          %11 = arith.truncf %in_0 : f32 to f16
          %12 = arith.addf %in, %11 : f16
          linalg.yield %12 : f16
        } -> tensor<4x2048xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %arg4, offsets = [0, 0], sizes = [4, 2048], strides = [1, 1] : tensor<4x2048xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x2048xf16>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_50 {
    flow.executable.export public @decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64x32xi8>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x64x32xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x5632xf16>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [5632, 64], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64xf16>> -> tensor<5632x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [5632, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64x32xi8>> -> tensor<5632x64x32xi8>
        %2 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x64x32xf16>> -> tensor<4x64x32xf16>
        %3 = tensor.empty() : tensor<4x5632xf16>
        %4 = tensor.empty() : tensor<4x5632xf32>
        %5 = tensor.empty() : tensor<5632x64x32xf16>
        %6 = linalg.generic {indexing_maps = [#map4, #map5, #map5], iterator_types = ["parallel", "parallel", "parallel"]} ins(%0, %1 : tensor<5632x64xf16>, tensor<5632x64x32xi8>) outs(%5 : tensor<5632x64x32xf16>) {
        ^bb0(%in: f16, %in_0: i8, %out: f16):
          %10 = arith.extsi %in_0 : i8 to i32
          %11 = arith.sitofp %10 : i32 to f16
          %12 = arith.mulf %11, %in : f16
          linalg.yield %12 : f16
        } -> tensor<5632x64x32xf16>
        %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<4x5632xf32>) -> tensor<4x5632xf32>
        %8 = linalg.generic {indexing_maps = [#map6, #map7, #map8], iterator_types = ["parallel", "parallel", "reduction", "reduction"]} ins(%2, %6 : tensor<4x64x32xf16>, tensor<5632x64x32xf16>) outs(%7 : tensor<4x5632xf32>) {
        ^bb0(%in: f16, %in_0: f16, %out: f32):
          %10 = arith.mulf %in, %in_0 : f16
          %11 = arith.extf %10 : f16 to f32
          %12 = arith.addf %11, %out : f32
          linalg.yield %12 : f32
        } -> tensor<4x5632xf32>
        %9 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<4x5632xf32>) outs(%3 : tensor<4x5632xf16>) {
        ^bb0(%in: f32, %out: f16):
          %10 = arith.truncf %in : f32 to f16
          linalg.yield %10 : f16
        } -> tensor<4x5632xf16>
        iree_tensor_ext.dispatch.tensor.store %9, %arg3, offsets = [0, 0], sizes = [4, 5632], strides = [1, 1] : tensor<4x5632xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x5632xf16>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_51 {
    flow.executable.export public @decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64x32xi8>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x64x32xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x5632xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x5632xf16>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.000000e+00 : f16
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [5632, 64], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64xf16>> -> tensor<5632x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [5632, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<5632x64x32xi8>> -> tensor<5632x64x32xi8>
        %2 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x64x32xf16>> -> tensor<4x64x32xf16>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [4, 5632], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x5632xf16>> -> tensor<4x5632xf16>
        %4 = tensor.empty() : tensor<4x5632xf16>
        %5 = tensor.empty() : tensor<4x5632xf32>
        %6 = tensor.empty() : tensor<5632x64x32xf16>
        %7 = linalg.generic {indexing_maps = [#map4, #map5, #map5], iterator_types = ["parallel", "parallel", "parallel"]} ins(%0, %1 : tensor<5632x64xf16>, tensor<5632x64x32xi8>) outs(%6 : tensor<5632x64x32xf16>) {
        ^bb0(%in: f16, %in_1: i8, %out: f16):
          %11 = arith.extsi %in_1 : i8 to i32
          %12 = arith.sitofp %11 : i32 to f16
          %13 = arith.mulf %12, %in : f16
          linalg.yield %13 : f16
        } -> tensor<5632x64x32xf16>
        %8 = linalg.fill ins(%cst : f32) outs(%5 : tensor<4x5632xf32>) -> tensor<4x5632xf32>
        %9 = linalg.generic {indexing_maps = [#map6, #map7, #map8], iterator_types = ["parallel", "parallel", "reduction", "reduction"]} ins(%2, %7 : tensor<4x64x32xf16>, tensor<5632x64x32xf16>) outs(%8 : tensor<4x5632xf32>) {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %11 = arith.mulf %in, %in_1 : f16
          %12 = arith.extf %11 : f16 to f32
          %13 = arith.addf %12, %out : f32
          linalg.yield %13 : f32
        } -> tensor<4x5632xf32>
        %10 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%9, %3 : tensor<4x5632xf32>, tensor<4x5632xf16>) outs(%4 : tensor<4x5632xf16>) {
        ^bb0(%in: f32, %in_1: f16, %out: f16):
          %11 = arith.truncf %in : f32 to f16
          %12 = arith.negf %11 : f16
          %13 = math.exp %12 : f16
          %14 = arith.addf %13, %cst_0 : f16
          %15 = arith.divf %cst_0, %14 : f16
          %16 = arith.mulf %15, %11 : f16
          %17 = arith.mulf %16, %in_1 : f16
          linalg.yield %17 : f16
        } -> tensor<4x5632xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %arg4, offsets = [0, 0], sizes = [4, 5632], strides = [1, 1] : tensor<4x5632xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x5632xf16>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_54 {
    flow.executable.export public @decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x176xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x176x32xi8>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x176x32xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x2048xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x2048xf16>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [2048, 176], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x176xf16>> -> tensor<2048x176xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [2048, 176, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2048x176x32xi8>> -> tensor<2048x176x32xi8>
        %2 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 176, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x176x32xf16>> -> tensor<4x176x32xf16>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [4, 2048], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x2048xf16>> -> tensor<4x2048xf16>
        %4 = tensor.empty() : tensor<4x2048xf16>
        %5 = tensor.empty() : tensor<4x2048xf32>
        %6 = tensor.empty() : tensor<2048x176x32xf16>
        %7 = linalg.generic {indexing_maps = [#map4, #map5, #map5], iterator_types = ["parallel", "parallel", "parallel"]} ins(%0, %1 : tensor<2048x176xf16>, tensor<2048x176x32xi8>) outs(%6 : tensor<2048x176x32xf16>) {
        ^bb0(%in: f16, %in_0: i8, %out: f16):
          %11 = arith.extsi %in_0 : i8 to i32
          %12 = arith.sitofp %11 : i32 to f16
          %13 = arith.mulf %12, %in : f16
          linalg.yield %13 : f16
        } -> tensor<2048x176x32xf16>
        %8 = linalg.fill ins(%cst : f32) outs(%5 : tensor<4x2048xf32>) -> tensor<4x2048xf32>
        %9 = linalg.generic {indexing_maps = [#map6, #map7, #map8], iterator_types = ["parallel", "parallel", "reduction", "reduction"]} ins(%2, %7 : tensor<4x176x32xf16>, tensor<2048x176x32xf16>) outs(%8 : tensor<4x2048xf32>) {
        ^bb0(%in: f16, %in_0: f16, %out: f32):
          %11 = arith.mulf %in, %in_0 : f16
          %12 = arith.extf %11 : f16 to f32
          %13 = arith.addf %12, %out : f32
          linalg.yield %13 : f32
        } -> tensor<4x2048xf32>
        %10 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%3, %9 : tensor<4x2048xf16>, tensor<4x2048xf32>) outs(%4 : tensor<4x2048xf16>) {
        ^bb0(%in: f16, %in_0: f32, %out: f16):
          %11 = arith.truncf %in_0 : f32 to f16
          %12 = arith.addf %in, %11 : f16
          linalg.yield %12 : f16
        } -> tensor<4x2048xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %arg4, offsets = [0, 0], sizes = [4, 2048], strides = [1, 1] : tensor<4x2048xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x2048xf16>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_74 {
    flow.executable.export public @decode_bs4$async_dispatch_74_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_74_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c1_i64 = arith.constant 1 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c1_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_79 {
    flow.executable.export public @decode_bs4$async_dispatch_79_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_79_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 1, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_80 {
    flow.executable.export public @decode_bs4$async_dispatch_80_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_80_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 1, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_117 {
    flow.executable.export public @decode_bs4$async_dispatch_117_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_117_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c2_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_122 {
    flow.executable.export public @decode_bs4$async_dispatch_122_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_122_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 2, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_123 {
    flow.executable.export public @decode_bs4$async_dispatch_123_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_123_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 2, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_160 {
    flow.executable.export public @decode_bs4$async_dispatch_160_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_160_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c3_i64 = arith.constant 3 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c3_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_165 {
    flow.executable.export public @decode_bs4$async_dispatch_165_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_165_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 3, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_166 {
    flow.executable.export public @decode_bs4$async_dispatch_166_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_166_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 3, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_203 {
    flow.executable.export public @decode_bs4$async_dispatch_203_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_203_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c4_i64 = arith.constant 4 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c4_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_208 {
    flow.executable.export public @decode_bs4$async_dispatch_208_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_208_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 4, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_209 {
    flow.executable.export public @decode_bs4$async_dispatch_209_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_209_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 4, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_246 {
    flow.executable.export public @decode_bs4$async_dispatch_246_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_246_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c5_i64 = arith.constant 5 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c5_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_251 {
    flow.executable.export public @decode_bs4$async_dispatch_251_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_251_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 5, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_252 {
    flow.executable.export public @decode_bs4$async_dispatch_252_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_252_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 5, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_289 {
    flow.executable.export public @decode_bs4$async_dispatch_289_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_289_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c6_i64 = arith.constant 6 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c6_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_294 {
    flow.executable.export public @decode_bs4$async_dispatch_294_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_294_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 6, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_295 {
    flow.executable.export public @decode_bs4$async_dispatch_295_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_295_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 6, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_332 {
    flow.executable.export public @decode_bs4$async_dispatch_332_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_332_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c7_i64 = arith.constant 7 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c7_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_337 {
    flow.executable.export public @decode_bs4$async_dispatch_337_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_337_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 7, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_338 {
    flow.executable.export public @decode_bs4$async_dispatch_338_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_338_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 7, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_375 {
    flow.executable.export public @decode_bs4$async_dispatch_375_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_375_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c8_i64 = arith.constant 8 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c8_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_380 {
    flow.executable.export public @decode_bs4$async_dispatch_380_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_380_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 8, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_381 {
    flow.executable.export public @decode_bs4$async_dispatch_381_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_381_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 8, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_418 {
    flow.executable.export public @decode_bs4$async_dispatch_418_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_418_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c9_i64 = arith.constant 9 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c9_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_423 {
    flow.executable.export public @decode_bs4$async_dispatch_423_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_423_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 9, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_424 {
    flow.executable.export public @decode_bs4$async_dispatch_424_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_424_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 9, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_461 {
    flow.executable.export public @decode_bs4$async_dispatch_461_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_461_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c10_i64 = arith.constant 10 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c10_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_466 {
    flow.executable.export public @decode_bs4$async_dispatch_466_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_466_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 10, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_467 {
    flow.executable.export public @decode_bs4$async_dispatch_467_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_467_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 10, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_504 {
    flow.executable.export public @decode_bs4$async_dispatch_504_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_504_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c11_i64 = arith.constant 11 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c11_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_509 {
    flow.executable.export public @decode_bs4$async_dispatch_509_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_509_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 11, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_510 {
    flow.executable.export public @decode_bs4$async_dispatch_510_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_510_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 11, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_547 {
    flow.executable.export public @decode_bs4$async_dispatch_547_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_547_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c12_i64 = arith.constant 12 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c12_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_552 {
    flow.executable.export public @decode_bs4$async_dispatch_552_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_552_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 12, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_553 {
    flow.executable.export public @decode_bs4$async_dispatch_553_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_553_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 12, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_590 {
    flow.executable.export public @decode_bs4$async_dispatch_590_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_590_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c13_i64 = arith.constant 13 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c13_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_595 {
    flow.executable.export public @decode_bs4$async_dispatch_595_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_595_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 13, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_596 {
    flow.executable.export public @decode_bs4$async_dispatch_596_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_596_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 13, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_633 {
    flow.executable.export public @decode_bs4$async_dispatch_633_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_633_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c14_i64 = arith.constant 14 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c14_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_638 {
    flow.executable.export public @decode_bs4$async_dispatch_638_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_638_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 14, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_639 {
    flow.executable.export public @decode_bs4$async_dispatch_639_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_639_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 14, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_676 {
    flow.executable.export public @decode_bs4$async_dispatch_676_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_676_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c15_i64 = arith.constant 15 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c15_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_681 {
    flow.executable.export public @decode_bs4$async_dispatch_681_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_681_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 15, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_682 {
    flow.executable.export public @decode_bs4$async_dispatch_682_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_682_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 15, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_719 {
    flow.executable.export public @decode_bs4$async_dispatch_719_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_719_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c16_i64 = arith.constant 16 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c16_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_724 {
    flow.executable.export public @decode_bs4$async_dispatch_724_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_724_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 16, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_725 {
    flow.executable.export public @decode_bs4$async_dispatch_725_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_725_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 16, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_762 {
    flow.executable.export public @decode_bs4$async_dispatch_762_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_762_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c17_i64 = arith.constant 17 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c17_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_767 {
    flow.executable.export public @decode_bs4$async_dispatch_767_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_767_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 17, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_768 {
    flow.executable.export public @decode_bs4$async_dispatch_768_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_768_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 17, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_805 {
    flow.executable.export public @decode_bs4$async_dispatch_805_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_805_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c18_i64 = arith.constant 18 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c18_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_810 {
    flow.executable.export public @decode_bs4$async_dispatch_810_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_810_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 18, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_811 {
    flow.executable.export public @decode_bs4$async_dispatch_811_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_811_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 18, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_848 {
    flow.executable.export public @decode_bs4$async_dispatch_848_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_848_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c19_i64 = arith.constant 19 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c19_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_853 {
    flow.executable.export public @decode_bs4$async_dispatch_853_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_853_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 19, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_854 {
    flow.executable.export public @decode_bs4$async_dispatch_854_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_854_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 19, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_891 {
    flow.executable.export public @decode_bs4$async_dispatch_891_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_891_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c20_i64 = arith.constant 20 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c20_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_896 {
    flow.executable.export public @decode_bs4$async_dispatch_896_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_896_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 20, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_897 {
    flow.executable.export public @decode_bs4$async_dispatch_897_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_897_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 20, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_934 {
    flow.executable.export public @decode_bs4$async_dispatch_934_elementwise_4_i64 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_934_elementwise_4_i64(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>>, %arg2: index, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>) {
        %c2_i64 = arith.constant 2 : i64
        %c21_i64 = arith.constant 21 : i64
        %c22_i64 = arith.constant 22 : i64
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg2, 0 : index
        %1 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0}
        %2 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [4, %0], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%0} -> tensor<4x?xi64>
        %3 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0], sizes = [4], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4xi64>> -> tensor<4xi64>
        %4 = tensor.empty() : tensor<4xi64>
        %5 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel"]} ins(%3 : tensor<4xi64>) outs(%4 : tensor<4xi64>) {
        ^bb0(%in: i64, %out: i64):
          %6 = linalg.index 0 : index
          %7 = arith.index_cast %in : i64 to index
          %extracted = tensor.extract %2[%6, %7] : tensor<4x?xi64>
          %8 = arith.muli %extracted, %c22_i64 : i64
          %9 = arith.addi %8, %c21_i64 : i64
          %10 = arith.muli %9, %c2_i64 : i64
          linalg.yield %10 : i64
        } -> tensor<4xi64>
        iree_tensor_ext.dispatch.tensor.store %5, %arg3, offsets = [0], sizes = [4], strides = [1] : tensor<4xi64> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4xi64>>
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_939 {
    flow.executable.export public @decode_bs4$async_dispatch_939_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_939_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x?x32x64xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 21, 0, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map25, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x?x32x64xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x?x32x64xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, %1, 32, 64], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x?x32x64xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x?x32x64xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_940 {
    flow.executable.export public @decode_bs4$async_dispatch_940_gather_4xDx4x32x64xf16_generic workgroups(%arg0: index, %arg1: index) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice(%arg0, %arg1)
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_940_gather_4xDx4x32x64xf16_generic(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>, %arg1: index, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>, %arg3: index, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>) {
        %0 = iree_tensor_ext.dispatch.workload.ordinal %arg1, 0 : index
        %1 = iree_tensor_ext.dispatch.workload.ordinal %arg3, 1 : index
        %2 = flow.dispatch.tie_shape %arg0 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0}
        %3 = flow.dispatch.tie_shape %arg2 : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1}
        %4 = flow.dispatch.tie_shape %arg4 : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        %5 = iree_tensor_ext.dispatch.tensor.load %3, offsets = [0, 0], sizes = [4, %1], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x?xi64>>{%1} -> tensor<4x?xi64>
        %6 = tensor.empty(%1) : tensor<4x4x8x64x?x32xf16>
        %7 = tensor.empty(%1) : tensor<4x?x4x32x64xf16>
        %8 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 21, 1, 0, 0, 0], sizes = [%0, 1, 1, 4, 32, 64], strides = [1, 1, 1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<?x22x2x4x32x64xf16>>{%0} -> tensor<?x4x32x64xf16>
        %9 = iree_linalg_ext.gather dimension_map = [0] ins(%8, %5 : tensor<?x4x32x64xf16>, tensor<4x?xi64>) outs(%7 : tensor<4x?x4x32x64xf16>) -> tensor<4x?x4x32x64xf16>
        %10 = linalg.generic {indexing_maps = [#map27, #map26], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<4x?x4x32x64xf16>) outs(%6 : tensor<4x4x8x64x?x32xf16>) {
        ^bb0(%in: f16, %out: f16):
          linalg.yield %in : f16
        } -> tensor<4x4x8x64x?x32xf16>
        iree_tensor_ext.dispatch.tensor.store %10, %4, offsets = [0, 0, 0, 0, 0, 0], sizes = [4, 4, 8, 64, %1, 32], strides = [1, 1, 1, 1, 1, 1] : tensor<4x4x8x64x?x32xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x4x8x64x?x32xf16>>{%1}
        return
      }
    }
  }
  flow.executable private @decode_bs4$async_dispatch_964 {
    flow.executable.export public @decode_bs4$async_dispatch_964_matmul_like_4x32000x64x32_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @decode_bs4$async_dispatch_964_matmul_like_4x32000x64x32_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<32000x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<32000x64x32xi8>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x64x32xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x32000xf16>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [32000, 64], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<32000x64xf16>> -> tensor<32000x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [32000, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<32000x64x32xi8>> -> tensor<32000x64x32xi8>
        %2 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 64, 32], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x64x32xf16>> -> tensor<4x64x32xf16>
        %3 = tensor.empty() : tensor<4x32000xf16>
        %4 = tensor.empty() : tensor<4x32000xf32>
        %5 = tensor.empty() : tensor<32000x64x32xf16>
        %6 = linalg.generic {indexing_maps = [#map4, #map5, #map5], iterator_types = ["parallel", "parallel", "parallel"]} ins(%0, %1 : tensor<32000x64xf16>, tensor<32000x64x32xi8>) outs(%5 : tensor<32000x64x32xf16>) {
        ^bb0(%in: f16, %in_0: i8, %out: f16):
          %10 = arith.extsi %in_0 : i8 to i32
          %11 = arith.sitofp %10 : i32 to f16
          %12 = arith.mulf %11, %in : f16
          linalg.yield %12 : f16
        } -> tensor<32000x64x32xf16>
        %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<4x32000xf32>) -> tensor<4x32000xf32>
        %8 = linalg.generic {indexing_maps = [#map6, #map7, #map8], iterator_types = ["parallel", "parallel", "reduction", "reduction"]} ins(%2, %6 : tensor<4x64x32xf16>, tensor<32000x64x32xf16>) outs(%7 : tensor<4x32000xf32>) {
        ^bb0(%in: f16, %in_0: f16, %out: f32):
          %10 = arith.mulf %in, %in_0 : f16
          %11 = arith.extf %10 : f16 to f32
          %12 = arith.addf %11, %out : f32
          linalg.yield %12 : f32
        } -> tensor<4x32000xf32>
        %9 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<4x32000xf32>) outs(%3 : tensor<4x32000xf16>) {
        ^bb0(%in: f32, %out: f16):
          %10 = arith.truncf %in : f32 to f16
          linalg.yield %10 : f16
        } -> tensor<4x32000xf16>
        iree_tensor_ext.dispatch.tensor.store %9, %arg3, offsets = [0, 0], sizes = [4, 32000], strides = [1, 1] : tensor<4x32000xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x32000xf16>>
        return
      }
    }
  }
  util.global private @__constant_tensor_4x4x1x8x1xf32 {inlining_policy = #util.inline.never, stream.affinity.default = #hal.device.affinity<@__device_0>} = dense<"0x0000803F12F93F3F9AF50F3F9BE8D73E9BE8A13E23D4723E8718363E778D083ECDCCCC3D0D94993DC155663D15BA2C3DE286013D4E43C23C39AD913CF27B5A3C09D7233CAEB9F53B9A44B83B752E8A3B323E4F3B0D691B3B2815E93A8EC9AE3A6E12833A8A94443A146A133A2217DD3960CBA5390DA8783953773A396CD40B390000803F12F93F3F9AF50F3F9BE8D73E9BE8A13E23D4723E8718363E778D083ECDCCCC3D0D94993DC155663D15BA2C3DE286013D4E43C23C39AD913CF27B5A3C09D7233CAEB9F53B9A44B83B752E8A3B323E4F3B0D691B3B2815E93A8EC9AE3A6E12833A8A94443A146A133A2217DD3960CBA5390DA8783953773A396CD40B390000803F12F93F3F9AF50F3F9BE8D73E9BE8A13E23D4723E8718363E778D083ECDCCCC3D0D94993DC155663D15BA2C3DE286013D4E43C23C39AD913CF27B5A3C09D7233CAEB9F53B9A44B83B752E8A3B323E4F3B0D691B3B2815E93A8EC9AE3A6E12833A8A94443A146A133A2217DD3960CBA5390DA8783953773A396CD40B390000803F12F93F3F9AF50F3F9BE8D73E9BE8A13E23D4723E8718363E778D083ECDCCCC3D0D94993DC155663D15BA2C3DE286013D4E43C23C39AD913CF27B5A3C09D7233CAEB9F53B9A44B83B752E8A3B323E4F3B0D691B3B2815E93A8EC9AE3A6E12833A8A94443A146A133A2217DD3960CBA5390DA8783953773A396CD40B39"> : tensor<4x4x1x8x1xf32>
  util.func public @decode_bs4$async(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view, %arg3: !hal.buffer_view, %arg4: !hal.buffer_view, %arg5: !hal.fence, %arg6: !hal.fence) -> !hal.buffer_view attributes {inlining_policy = #util.inline.never, iree.abi.model = "coarse-fences", iree.abi.stub} {
    %c5632 = arith.constant 5632 : index
    %c1024 = arith.constant 1024 : index
    %c32 = arith.constant 32 : index
    %__parameter_model_output_weight_tensor_32000x2176xi8 = util.global.load immutable @__parameter_model_output_weight_tensor_32000x2176xi8 : tensor<32000x2176xi8>
    %__parameter_model_output_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_output_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_21_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_21_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_21_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_21_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_21_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_21_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_21_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_21_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_21_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_21_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_21_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_21_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_21_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_21_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_21_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_21_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_21_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_21_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_20_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_20_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_20_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_20_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_20_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_20_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_20_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_20_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_20_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_20_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_20_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_20_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_20_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_20_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_20_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_20_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_20_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_20_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_19_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_19_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_19_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_19_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_19_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_19_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_19_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_19_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_19_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_19_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_19_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_19_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_19_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_19_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_19_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_19_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_19_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_19_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_18_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_18_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_18_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_18_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_18_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_18_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_18_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_18_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_18_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_18_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_18_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_18_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_18_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_18_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_18_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_18_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_18_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_18_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_17_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_17_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_17_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_17_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_17_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_17_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_17_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_17_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_17_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_17_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_17_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_17_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_17_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_17_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_17_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_17_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_17_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_17_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_16_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_16_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_16_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_16_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_16_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_16_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_16_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_16_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_16_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_16_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_16_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_16_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_16_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_16_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_16_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_16_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_16_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_16_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_15_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_15_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_15_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_15_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_15_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_15_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_15_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_15_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_15_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_15_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_15_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_15_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_15_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_15_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_15_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_15_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_15_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_15_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_14_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_14_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_14_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_14_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_14_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_14_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_14_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_14_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_14_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_14_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_14_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_14_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_14_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_14_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_14_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_14_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_14_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_14_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_13_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_13_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_13_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_13_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_13_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_13_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_13_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_13_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_13_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_13_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_13_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_13_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_13_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_13_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_13_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_13_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_13_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_13_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_12_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_12_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_12_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_12_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_12_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_12_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_12_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_12_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_12_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_12_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_12_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_12_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_12_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_12_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_12_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_12_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_12_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_12_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_11_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_11_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_11_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_11_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_11_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_11_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_11_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_11_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_11_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_11_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_11_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_11_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_11_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_11_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_11_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_11_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_11_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_11_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_10_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_10_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_10_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_10_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_10_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_10_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_10_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_10_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_10_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_10_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_10_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_10_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_10_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_10_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_10_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_10_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_10_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_10_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_9_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_9_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_9_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_9_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_9_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_9_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_9_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_9_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_9_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_9_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_9_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_9_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_9_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_9_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_9_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_9_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_9_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_9_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_8_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_8_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_8_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_8_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_8_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_8_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_8_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_8_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_8_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_8_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_8_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_8_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_8_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_8_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_8_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_8_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_8_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_8_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_7_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_7_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_7_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_7_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_7_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_7_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_7_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_7_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_7_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_7_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_7_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_7_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_7_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_7_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_7_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_7_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_7_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_7_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_6_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_6_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_6_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_6_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_6_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_6_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_6_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_6_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_6_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_6_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_6_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_6_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_6_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_6_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_6_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_6_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_6_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_6_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_5_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_5_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_5_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_5_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_5_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_5_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_5_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_5_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_5_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_5_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_5_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_5_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_5_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_5_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_5_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_5_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_5_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_5_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_4_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_4_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_4_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_4_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_4_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_4_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_4_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_4_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_4_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_4_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_4_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_4_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_4_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_4_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_4_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_4_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_4_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_4_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_3_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_3_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_3_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_3_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_3_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_3_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_3_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_3_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_3_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_3_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_3_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_3_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_3_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_3_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_3_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_3_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_3_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_3_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_2_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_2_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_2_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_2_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_2_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_2_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_2_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_2_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_2_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_2_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_2_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_2_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_2_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_2_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_2_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_2_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_2_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_2_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_1_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_1_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_1_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_1_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_1_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_1_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_1_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_1_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_1_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_1_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_1_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_1_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_1_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_1_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_1_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_1_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_1_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_1_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_0_ffn_down_weight_tensor_2048x5984xi8 = util.global.load immutable @__parameter_model_blk_0_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8>
    %__parameter_model_blk_0_ffn_up_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_0_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_0_ffn_gate_weight_tensor_5632x2176xi8 = util.global.load immutable @__parameter_model_blk_0_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8>
    %__parameter_model_blk_0_ffn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_0_ffn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_blk_0_attn_output_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_0_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_0_attn_v_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_0_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_0_attn_k_weight_tensor_256x2176xi8 = util.global.load immutable @__parameter_model_blk_0_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8>
    %__parameter_model_blk_0_attn_q_weight_tensor_2048x2176xi8 = util.global.load immutable @__parameter_model_blk_0_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8>
    %__parameter_model_blk_0_attn_norm_weight_tensor_2048xf32 = util.global.load immutable @__parameter_model_blk_0_attn_norm_weight_tensor_2048xf32 : tensor<2048xf32>
    %__parameter_model_token_embd_weight_tensor_32000x2176xi8 = util.global.load immutable @__parameter_model_token_embd_weight_tensor_32000x2176xi8 : tensor<32000x2176xi8>
    %__constant_tensor_4x4x1x8x1xf32 = util.global.load immutable @__constant_tensor_4x4x1x8x1xf32 : tensor<4x4x1x8x1xf32>
    %0 = hal.tensor.import on(#hal.device.affinity<@__device_0>) wait(%arg5) => %arg0 : !hal.buffer_view -> tensor<4x1xi64>
    %1 = hal.tensor.import on(#hal.device.affinity<@__device_0>) wait(%arg5) => %arg1 : !hal.buffer_view -> tensor<4xi64>
    %2 = hal.tensor.import on(#hal.device.affinity<@__device_0>) wait(%arg5) => %arg2 : !hal.buffer_view -> tensor<4xi64>
    %3 = hal.buffer_view.dim<%arg3 : !hal.buffer_view>[1] : index
    %4 = hal.tensor.import on(#hal.device.affinity<@__device_0>) wait(%arg5) => %arg3 : !hal.buffer_view -> tensor<4x?xi64>{%3}
    %5 = hal.buffer_view.dim<%arg4 : !hal.buffer_view>[0] : index
    %6 = hal.tensor.import on(#hal.device.affinity<@__device_0>) wait(%arg5) => %arg4 : !hal.buffer_view -> tensor<?x360448xf16>{%5}
    %7 = util.assume.int %3<umin = 1, umax = 63> : index
    %8 = util.assume.int %5<umin = 1, umax = 9007199254740991> : index
    %9 = flow.tensor.bitcast %__parameter_model_token_embd_weight_tensor_32000x2176xi8 : tensor<32000x2176xi8> -> tensor<32000x64x17xi16>
    %10 = flow.tensor.bitcast %__parameter_model_token_embd_weight_tensor_32000x2176xi8 : tensor<32000x2176xi8> -> tensor<32000x64x1x17xi16>
    %11 = flow.dispatch @prefill_bs4$async_dispatch_0::@prefill_bs4$async_dispatch_0_slow_memcpy(%10) : (tensor<32000x64x1x17xi16>) -> tensor<32000x64x1xi16>
    %12 = flow.dispatch @prefill_bs4$async_dispatch_1::@prefill_bs4$async_dispatch_1_slow_memcpy(%9) : (tensor<32000x64x17xi16>) -> tensor<32000x64x16xi16>
    %13 = flow.tensor.bitcast %11 : tensor<32000x64x1xi16> -> tensor<2048000xf16>
    %14 = flow.tensor.bitcast %12 : tensor<32000x64x16xi16> -> tensor<2048000x32xi8>
    %15 = flow.dispatch @prefill_bs4$async_dispatch_2::@prefill_bs4$async_dispatch_2_elementwise_2048000x32_f16xi8xf16(%13, %14) : (tensor<2048000xf16>, tensor<2048000x32xi8>) -> tensor<2048000x32xf16>
    %16 = flow.tensor.reshape %15 : tensor<2048000x32xf16> -> tensor<32000x2048xf16>
    %17 = flow.tensor.reshape %0 : tensor<4x1xi64> -> tensor<4xi64>
    %18 = flow.dispatch @decode_bs4$async_dispatch_3::@decode_bs4$async_dispatch_3_elementwise_broadcast_4x2048_i64xf16(%16, %17) : (tensor<32000x2048xf16>, tensor<4xi64>) -> tensor<4x2048xf16>
    %19 = flow.tensor.reshape %18 : tensor<4x2048xf16> -> tensor<8192xf16>
    %20 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%19) : (tensor<8192xf16>) -> tensor<8192xf32>
    %21 = flow.tensor.reshape %20 : tensor<8192xf32> -> tensor<4x2048xf32>
    %22 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%18) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %23 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%21, %22) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %24 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_0_attn_norm_weight_tensor_2048xf32, %23) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %25 = flow.tensor.reshape %24 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %26 = flow.tensor.bitcast %__parameter_model_blk_0_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %27 = flow.tensor.bitcast %__parameter_model_blk_0_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %28 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%27) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %29 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%26) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %30 = flow.tensor.bitcast %29 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %31 = flow.tensor.bitcast %28 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %32 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%31, %30, %25) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %33 = flow.tensor.reshape %32 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %34 = flow.tensor.bitcast %__parameter_model_blk_0_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %35 = flow.tensor.bitcast %__parameter_model_blk_0_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %36 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%35) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %37 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%34) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %38 = flow.tensor.bitcast %37 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %39 = flow.tensor.bitcast %36 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %40 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%39, %38, %25) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %41 = flow.tensor.reshape %40 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %42 = flow.tensor.bitcast %__parameter_model_blk_0_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %43 = flow.tensor.bitcast %__parameter_model_blk_0_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %44 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%43) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %45 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%42) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %46 = flow.tensor.bitcast %45 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %47 = flow.tensor.bitcast %44 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %48 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%47, %46, %25) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %49 = flow.tensor.reshape %48 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %50 = flow.dispatch @decode_bs4$async_dispatch_17::@decode_bs4$async_dispatch_17_elementwise_4_i64xf32(%2) : (tensor<4xi64>) -> tensor<4xf32>
    %51 = flow.tensor.reshape %50 : tensor<4xf32> -> tensor<4x1x1x1x1xf32>
    %52 = flow.dispatch @decode_bs4$async_dispatch_18::@decode_bs4$async_dispatch_18_batch_mmt4d_4x1x4x1x1x8x1_f32(%51, %__constant_tensor_4x4x1x8x1xf32) : (tensor<4x1x1x1x1xf32>, tensor<4x4x1x8x1xf32>) -> tensor<4x1x4x1x8xf32>
    %53 = flow.dispatch @decode_bs4$async_dispatch_19::@decode_bs4$async_dispatch_19_unpack_f32(%52) : (tensor<4x1x4x1x8xf32>) -> tensor<4x32x1xf32>
    %54 = flow.tensor.reshape %53 : tensor<4x32x1xf32> -> tensor<128xf32>
    %55 = flow.dispatch @decode_bs4$async_dispatch_20::@decode_bs4$async_dispatch_20_elementwise_128_f32xf16(%54) : (tensor<128xf32>) -> tensor<128xf16>
    %56 = flow.dispatch @decode_bs4$async_dispatch_21::@decode_bs4$async_dispatch_21_elementwise_128_f32xf16(%54) : (tensor<128xf32>) -> tensor<128xf16>
    %57 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%33) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %58 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%33) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %59 = flow.tensor.reshape %55 : tensor<128xf16> -> tensor<4x32xf16>
    %60 = flow.tensor.reshape %56 : tensor<128xf16> -> tensor<4x32xf16>
    %61 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%57, %59, %58, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %62 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%41) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %63 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%41) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %64 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%62, %59, %63, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %65 = flow.tensor.reshape %64 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %66 = flow.dispatch @decode_bs4$async_dispatch_28::@decode_bs4$async_dispatch_28_elementwise_4_i64[%7](%7, %2) : (index, tensor<4xi64>) -> tensor<4xi64>
    %67 = flow.dispatch @decode_bs4$async_dispatch_29::@decode_bs4$async_dispatch_29_elementwise_4_i64(%2) : (tensor<4xi64>) -> tensor<4xi64>
    %68 = flow.dispatch @decode_bs4$async_dispatch_30::@decode_bs4$async_dispatch_30_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %69 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%68, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %70 = arith.muli %8, %c5632 overflow<nsw> : index
    %71 = flow.tensor.reshape %6 : tensor<?x360448xf16>{%8} -> tensor<?x2x32x1xf16>{%70}
    %72 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%65, %69, %71, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %71{%70}
    %73 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%68, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %74 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%49, %73, %72, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %72{%70}
    %75 = flow.tensor.reshape %74 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %76 = arith.muli %7, %c32 : index
    %77 = flow.dispatch @decode_bs4$async_dispatch_35::@decode_bs4$async_dispatch_35_gather_4xDx4x32x64xf16_generic[%8, %7](%75, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %78 = arith.muli %7, %c1024 : index
    %79 = flow.dispatch @decode_bs4$async_dispatch_36::@decode_bs4$async_dispatch_36_elementwise_broadcast_4xD_i64xf16[%78, %76](%78, %76, %1) : (index, index, tensor<4xi64>) -> tensor<4x?xf16>{%78}
    %80 = arith.muli %7, %c32 overflow<nsw> : index
    %81 = flow.tensor.reshape %77 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %82 = flow.tensor.reshape %61 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %83 = flow.dispatch @decode_bs4$async_dispatch_37::@decode_bs4$async_dispatch_37_gather_4xDx4x32x64xf16_generic[%8, %7](%75, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %84 = flow.tensor.reshape %83 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %85 = flow.tensor.reshape %79 : tensor<4x?xf16>{%78} -> tensor<128x?xf16>{%76}
    %86 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%82, %81, %84, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %87 = flow.tensor.bitcast %__parameter_model_blk_0_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %88 = flow.tensor.bitcast %__parameter_model_blk_0_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %89 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%88) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %90 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%87) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %91 = flow.tensor.bitcast %90 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %92 = flow.tensor.bitcast %89 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %93 = flow.tensor.reshape %86 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %94 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%92, %91, %93, %18) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %95 = flow.tensor.reshape %94 : tensor<4x2048xf16> -> tensor<8192xf16>
    %96 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%95) : (tensor<8192xf16>) -> tensor<8192xf32>
    %97 = flow.tensor.reshape %96 : tensor<8192xf32> -> tensor<4x2048xf32>
    %98 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%94) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %99 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%97, %98) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %100 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_0_ffn_norm_weight_tensor_2048xf32, %99) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %101 = flow.tensor.reshape %100 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %102 = flow.tensor.bitcast %__parameter_model_blk_0_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %103 = flow.tensor.bitcast %__parameter_model_blk_0_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %104 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%103) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %105 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%102) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %106 = flow.tensor.bitcast %105 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %107 = flow.tensor.bitcast %104 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %108 = flow.tensor.bitcast %__parameter_model_blk_0_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %109 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%108) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %110 = flow.tensor.bitcast %109 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %111 = flow.tensor.bitcast %__parameter_model_blk_0_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %112 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%111) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %113 = flow.tensor.bitcast %112 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %114 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%110, %113, %101) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %115 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%107, %106, %101, %114) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %116 = flow.tensor.reshape %115 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %117 = flow.tensor.bitcast %__parameter_model_blk_0_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %118 = flow.tensor.bitcast %__parameter_model_blk_0_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %119 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%118) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %120 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%117) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %121 = flow.tensor.bitcast %120 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %122 = flow.tensor.bitcast %119 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %123 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%122, %121, %116, %94) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %124 = flow.tensor.reshape %123 : tensor<4x2048xf16> -> tensor<8192xf16>
    %125 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%124) : (tensor<8192xf16>) -> tensor<8192xf32>
    %126 = flow.tensor.reshape %125 : tensor<8192xf32> -> tensor<4x2048xf32>
    %127 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%123) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %128 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%126, %127) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %129 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_1_attn_norm_weight_tensor_2048xf32, %128) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %130 = flow.tensor.reshape %129 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %131 = flow.tensor.bitcast %__parameter_model_blk_1_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %132 = flow.tensor.bitcast %__parameter_model_blk_1_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %133 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%132) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %134 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%131) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %135 = flow.tensor.bitcast %134 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %136 = flow.tensor.bitcast %133 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %137 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%136, %135, %130) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %138 = flow.tensor.reshape %137 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %139 = flow.tensor.bitcast %__parameter_model_blk_1_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %140 = flow.tensor.bitcast %__parameter_model_blk_1_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %141 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%140) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %142 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%139) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %143 = flow.tensor.bitcast %142 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %144 = flow.tensor.bitcast %141 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %145 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%144, %143, %130) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %146 = flow.tensor.reshape %145 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %147 = flow.tensor.bitcast %__parameter_model_blk_1_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %148 = flow.tensor.bitcast %__parameter_model_blk_1_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %149 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%148) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %150 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%147) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %151 = flow.tensor.bitcast %150 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %152 = flow.tensor.bitcast %149 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %153 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%152, %151, %130) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %154 = flow.tensor.reshape %153 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %155 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%138) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %156 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%138) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %157 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%155, %59, %156, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %158 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%146) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %159 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%146) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %160 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%158, %59, %159, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %161 = flow.tensor.reshape %160 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %162 = flow.dispatch @decode_bs4$async_dispatch_74::@decode_bs4$async_dispatch_74_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %163 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%162, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %164 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%161, %163, %74, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %74{%70}
    %165 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%162, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %166 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%154, %165, %164, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %164{%70}
    %167 = flow.tensor.reshape %166 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %168 = flow.dispatch @decode_bs4$async_dispatch_79::@decode_bs4$async_dispatch_79_gather_4xDx4x32x64xf16_generic[%8, %7](%167, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %169 = flow.tensor.reshape %168 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %170 = flow.tensor.reshape %157 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %171 = flow.dispatch @decode_bs4$async_dispatch_80::@decode_bs4$async_dispatch_80_gather_4xDx4x32x64xf16_generic[%8, %7](%167, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %172 = flow.tensor.reshape %171 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %173 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%170, %169, %172, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %174 = flow.tensor.bitcast %__parameter_model_blk_1_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %175 = flow.tensor.bitcast %__parameter_model_blk_1_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %176 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%175) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %177 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%174) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %178 = flow.tensor.bitcast %177 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %179 = flow.tensor.bitcast %176 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %180 = flow.tensor.reshape %173 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %181 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%179, %178, %180, %123) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %182 = flow.tensor.reshape %181 : tensor<4x2048xf16> -> tensor<8192xf16>
    %183 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%182) : (tensor<8192xf16>) -> tensor<8192xf32>
    %184 = flow.tensor.reshape %183 : tensor<8192xf32> -> tensor<4x2048xf32>
    %185 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%181) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %186 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%184, %185) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %187 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_1_ffn_norm_weight_tensor_2048xf32, %186) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %188 = flow.tensor.reshape %187 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %189 = flow.tensor.bitcast %__parameter_model_blk_1_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %190 = flow.tensor.bitcast %__parameter_model_blk_1_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %191 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%190) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %192 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%189) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %193 = flow.tensor.bitcast %192 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %194 = flow.tensor.bitcast %191 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %195 = flow.tensor.bitcast %__parameter_model_blk_1_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %196 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%195) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %197 = flow.tensor.bitcast %196 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %198 = flow.tensor.bitcast %__parameter_model_blk_1_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %199 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%198) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %200 = flow.tensor.bitcast %199 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %201 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%197, %200, %188) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %202 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%194, %193, %188, %201) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %203 = flow.tensor.reshape %202 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %204 = flow.tensor.bitcast %__parameter_model_blk_1_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %205 = flow.tensor.bitcast %__parameter_model_blk_1_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %206 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%205) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %207 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%204) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %208 = flow.tensor.bitcast %207 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %209 = flow.tensor.bitcast %206 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %210 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%209, %208, %203, %181) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %211 = flow.tensor.reshape %210 : tensor<4x2048xf16> -> tensor<8192xf16>
    %212 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%211) : (tensor<8192xf16>) -> tensor<8192xf32>
    %213 = flow.tensor.reshape %212 : tensor<8192xf32> -> tensor<4x2048xf32>
    %214 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%210) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %215 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%213, %214) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %216 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_2_attn_norm_weight_tensor_2048xf32, %215) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %217 = flow.tensor.reshape %216 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %218 = flow.tensor.bitcast %__parameter_model_blk_2_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %219 = flow.tensor.bitcast %__parameter_model_blk_2_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %220 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%219) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %221 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%218) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %222 = flow.tensor.bitcast %221 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %223 = flow.tensor.bitcast %220 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %224 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%223, %222, %217) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %225 = flow.tensor.reshape %224 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %226 = flow.tensor.bitcast %__parameter_model_blk_2_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %227 = flow.tensor.bitcast %__parameter_model_blk_2_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %228 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%227) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %229 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%226) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %230 = flow.tensor.bitcast %229 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %231 = flow.tensor.bitcast %228 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %232 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%231, %230, %217) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %233 = flow.tensor.reshape %232 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %234 = flow.tensor.bitcast %__parameter_model_blk_2_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %235 = flow.tensor.bitcast %__parameter_model_blk_2_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %236 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%235) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %237 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%234) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %238 = flow.tensor.bitcast %237 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %239 = flow.tensor.bitcast %236 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %240 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%239, %238, %217) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %241 = flow.tensor.reshape %240 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %242 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%225) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %243 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%225) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %244 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%242, %59, %243, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %245 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%233) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %246 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%233) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %247 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%245, %59, %246, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %248 = flow.tensor.reshape %247 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %249 = flow.dispatch @decode_bs4$async_dispatch_117::@decode_bs4$async_dispatch_117_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %250 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%249, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %251 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%248, %250, %166, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %166{%70}
    %252 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%249, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %253 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%241, %252, %251, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %251{%70}
    %254 = flow.tensor.reshape %253 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %255 = flow.dispatch @decode_bs4$async_dispatch_122::@decode_bs4$async_dispatch_122_gather_4xDx4x32x64xf16_generic[%8, %7](%254, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %256 = flow.tensor.reshape %255 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %257 = flow.tensor.reshape %244 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %258 = flow.dispatch @decode_bs4$async_dispatch_123::@decode_bs4$async_dispatch_123_gather_4xDx4x32x64xf16_generic[%8, %7](%254, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %259 = flow.tensor.reshape %258 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %260 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%257, %256, %259, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %261 = flow.tensor.bitcast %__parameter_model_blk_2_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %262 = flow.tensor.bitcast %__parameter_model_blk_2_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %263 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%262) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %264 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%261) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %265 = flow.tensor.bitcast %264 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %266 = flow.tensor.bitcast %263 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %267 = flow.tensor.reshape %260 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %268 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%266, %265, %267, %210) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %269 = flow.tensor.reshape %268 : tensor<4x2048xf16> -> tensor<8192xf16>
    %270 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%269) : (tensor<8192xf16>) -> tensor<8192xf32>
    %271 = flow.tensor.reshape %270 : tensor<8192xf32> -> tensor<4x2048xf32>
    %272 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%268) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %273 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%271, %272) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %274 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_2_ffn_norm_weight_tensor_2048xf32, %273) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %275 = flow.tensor.reshape %274 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %276 = flow.tensor.bitcast %__parameter_model_blk_2_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %277 = flow.tensor.bitcast %__parameter_model_blk_2_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %278 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%277) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %279 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%276) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %280 = flow.tensor.bitcast %279 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %281 = flow.tensor.bitcast %278 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %282 = flow.tensor.bitcast %__parameter_model_blk_2_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %283 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%282) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %284 = flow.tensor.bitcast %283 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %285 = flow.tensor.bitcast %__parameter_model_blk_2_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %286 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%285) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %287 = flow.tensor.bitcast %286 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %288 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%284, %287, %275) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %289 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%281, %280, %275, %288) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %290 = flow.tensor.reshape %289 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %291 = flow.tensor.bitcast %__parameter_model_blk_2_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %292 = flow.tensor.bitcast %__parameter_model_blk_2_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %293 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%292) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %294 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%291) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %295 = flow.tensor.bitcast %294 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %296 = flow.tensor.bitcast %293 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %297 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%296, %295, %290, %268) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %298 = flow.tensor.reshape %297 : tensor<4x2048xf16> -> tensor<8192xf16>
    %299 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%298) : (tensor<8192xf16>) -> tensor<8192xf32>
    %300 = flow.tensor.reshape %299 : tensor<8192xf32> -> tensor<4x2048xf32>
    %301 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%297) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %302 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%300, %301) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %303 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_3_attn_norm_weight_tensor_2048xf32, %302) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %304 = flow.tensor.reshape %303 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %305 = flow.tensor.bitcast %__parameter_model_blk_3_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %306 = flow.tensor.bitcast %__parameter_model_blk_3_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %307 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%306) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %308 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%305) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %309 = flow.tensor.bitcast %308 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %310 = flow.tensor.bitcast %307 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %311 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%310, %309, %304) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %312 = flow.tensor.reshape %311 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %313 = flow.tensor.bitcast %__parameter_model_blk_3_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %314 = flow.tensor.bitcast %__parameter_model_blk_3_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %315 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%314) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %316 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%313) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %317 = flow.tensor.bitcast %316 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %318 = flow.tensor.bitcast %315 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %319 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%318, %317, %304) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %320 = flow.tensor.reshape %319 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %321 = flow.tensor.bitcast %__parameter_model_blk_3_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %322 = flow.tensor.bitcast %__parameter_model_blk_3_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %323 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%322) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %324 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%321) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %325 = flow.tensor.bitcast %324 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %326 = flow.tensor.bitcast %323 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %327 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%326, %325, %304) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %328 = flow.tensor.reshape %327 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %329 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%312) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %330 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%312) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %331 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%329, %59, %330, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %332 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%320) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %333 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%320) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %334 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%332, %59, %333, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %335 = flow.tensor.reshape %334 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %336 = flow.dispatch @decode_bs4$async_dispatch_160::@decode_bs4$async_dispatch_160_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %337 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%336, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %338 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%335, %337, %253, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %253{%70}
    %339 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%336, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %340 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%328, %339, %338, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %338{%70}
    %341 = flow.tensor.reshape %340 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %342 = flow.dispatch @decode_bs4$async_dispatch_165::@decode_bs4$async_dispatch_165_gather_4xDx4x32x64xf16_generic[%8, %7](%341, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %343 = flow.tensor.reshape %342 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %344 = flow.tensor.reshape %331 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %345 = flow.dispatch @decode_bs4$async_dispatch_166::@decode_bs4$async_dispatch_166_gather_4xDx4x32x64xf16_generic[%8, %7](%341, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %346 = flow.tensor.reshape %345 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %347 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%344, %343, %346, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %348 = flow.tensor.bitcast %__parameter_model_blk_3_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %349 = flow.tensor.bitcast %__parameter_model_blk_3_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %350 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%349) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %351 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%348) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %352 = flow.tensor.bitcast %351 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %353 = flow.tensor.bitcast %350 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %354 = flow.tensor.reshape %347 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %355 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%353, %352, %354, %297) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %356 = flow.tensor.reshape %355 : tensor<4x2048xf16> -> tensor<8192xf16>
    %357 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%356) : (tensor<8192xf16>) -> tensor<8192xf32>
    %358 = flow.tensor.reshape %357 : tensor<8192xf32> -> tensor<4x2048xf32>
    %359 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%355) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %360 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%358, %359) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %361 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_3_ffn_norm_weight_tensor_2048xf32, %360) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %362 = flow.tensor.reshape %361 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %363 = flow.tensor.bitcast %__parameter_model_blk_3_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %364 = flow.tensor.bitcast %__parameter_model_blk_3_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %365 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%364) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %366 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%363) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %367 = flow.tensor.bitcast %366 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %368 = flow.tensor.bitcast %365 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %369 = flow.tensor.bitcast %__parameter_model_blk_3_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %370 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%369) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %371 = flow.tensor.bitcast %370 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %372 = flow.tensor.bitcast %__parameter_model_blk_3_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %373 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%372) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %374 = flow.tensor.bitcast %373 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %375 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%371, %374, %362) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %376 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%368, %367, %362, %375) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %377 = flow.tensor.reshape %376 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %378 = flow.tensor.bitcast %__parameter_model_blk_3_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %379 = flow.tensor.bitcast %__parameter_model_blk_3_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %380 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%379) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %381 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%378) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %382 = flow.tensor.bitcast %381 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %383 = flow.tensor.bitcast %380 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %384 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%383, %382, %377, %355) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %385 = flow.tensor.reshape %384 : tensor<4x2048xf16> -> tensor<8192xf16>
    %386 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%385) : (tensor<8192xf16>) -> tensor<8192xf32>
    %387 = flow.tensor.reshape %386 : tensor<8192xf32> -> tensor<4x2048xf32>
    %388 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%384) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %389 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%387, %388) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %390 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_4_attn_norm_weight_tensor_2048xf32, %389) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %391 = flow.tensor.reshape %390 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %392 = flow.tensor.bitcast %__parameter_model_blk_4_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %393 = flow.tensor.bitcast %__parameter_model_blk_4_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %394 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%393) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %395 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%392) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %396 = flow.tensor.bitcast %395 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %397 = flow.tensor.bitcast %394 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %398 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%397, %396, %391) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %399 = flow.tensor.reshape %398 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %400 = flow.tensor.bitcast %__parameter_model_blk_4_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %401 = flow.tensor.bitcast %__parameter_model_blk_4_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %402 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%401) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %403 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%400) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %404 = flow.tensor.bitcast %403 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %405 = flow.tensor.bitcast %402 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %406 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%405, %404, %391) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %407 = flow.tensor.reshape %406 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %408 = flow.tensor.bitcast %__parameter_model_blk_4_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %409 = flow.tensor.bitcast %__parameter_model_blk_4_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %410 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%409) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %411 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%408) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %412 = flow.tensor.bitcast %411 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %413 = flow.tensor.bitcast %410 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %414 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%413, %412, %391) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %415 = flow.tensor.reshape %414 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %416 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%399) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %417 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%399) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %418 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%416, %59, %417, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %419 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%407) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %420 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%407) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %421 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%419, %59, %420, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %422 = flow.tensor.reshape %421 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %423 = flow.dispatch @decode_bs4$async_dispatch_203::@decode_bs4$async_dispatch_203_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %424 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%423, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %425 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%422, %424, %340, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %340{%70}
    %426 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%423, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %427 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%415, %426, %425, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %425{%70}
    %428 = flow.tensor.reshape %427 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %429 = flow.dispatch @decode_bs4$async_dispatch_208::@decode_bs4$async_dispatch_208_gather_4xDx4x32x64xf16_generic[%8, %7](%428, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %430 = flow.tensor.reshape %429 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %431 = flow.tensor.reshape %418 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %432 = flow.dispatch @decode_bs4$async_dispatch_209::@decode_bs4$async_dispatch_209_gather_4xDx4x32x64xf16_generic[%8, %7](%428, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %433 = flow.tensor.reshape %432 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %434 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%431, %430, %433, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %435 = flow.tensor.bitcast %__parameter_model_blk_4_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %436 = flow.tensor.bitcast %__parameter_model_blk_4_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %437 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%436) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %438 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%435) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %439 = flow.tensor.bitcast %438 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %440 = flow.tensor.bitcast %437 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %441 = flow.tensor.reshape %434 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %442 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%440, %439, %441, %384) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %443 = flow.tensor.reshape %442 : tensor<4x2048xf16> -> tensor<8192xf16>
    %444 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%443) : (tensor<8192xf16>) -> tensor<8192xf32>
    %445 = flow.tensor.reshape %444 : tensor<8192xf32> -> tensor<4x2048xf32>
    %446 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%442) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %447 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%445, %446) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %448 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_4_ffn_norm_weight_tensor_2048xf32, %447) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %449 = flow.tensor.reshape %448 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %450 = flow.tensor.bitcast %__parameter_model_blk_4_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %451 = flow.tensor.bitcast %__parameter_model_blk_4_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %452 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%451) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %453 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%450) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %454 = flow.tensor.bitcast %453 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %455 = flow.tensor.bitcast %452 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %456 = flow.tensor.bitcast %__parameter_model_blk_4_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %457 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%456) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %458 = flow.tensor.bitcast %457 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %459 = flow.tensor.bitcast %__parameter_model_blk_4_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %460 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%459) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %461 = flow.tensor.bitcast %460 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %462 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%458, %461, %449) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %463 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%455, %454, %449, %462) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %464 = flow.tensor.reshape %463 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %465 = flow.tensor.bitcast %__parameter_model_blk_4_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %466 = flow.tensor.bitcast %__parameter_model_blk_4_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %467 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%466) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %468 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%465) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %469 = flow.tensor.bitcast %468 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %470 = flow.tensor.bitcast %467 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %471 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%470, %469, %464, %442) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %472 = flow.tensor.reshape %471 : tensor<4x2048xf16> -> tensor<8192xf16>
    %473 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%472) : (tensor<8192xf16>) -> tensor<8192xf32>
    %474 = flow.tensor.reshape %473 : tensor<8192xf32> -> tensor<4x2048xf32>
    %475 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%471) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %476 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%474, %475) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %477 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_5_attn_norm_weight_tensor_2048xf32, %476) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %478 = flow.tensor.reshape %477 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %479 = flow.tensor.bitcast %__parameter_model_blk_5_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %480 = flow.tensor.bitcast %__parameter_model_blk_5_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %481 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%480) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %482 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%479) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %483 = flow.tensor.bitcast %482 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %484 = flow.tensor.bitcast %481 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %485 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%484, %483, %478) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %486 = flow.tensor.reshape %485 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %487 = flow.tensor.bitcast %__parameter_model_blk_5_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %488 = flow.tensor.bitcast %__parameter_model_blk_5_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %489 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%488) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %490 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%487) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %491 = flow.tensor.bitcast %490 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %492 = flow.tensor.bitcast %489 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %493 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%492, %491, %478) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %494 = flow.tensor.reshape %493 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %495 = flow.tensor.bitcast %__parameter_model_blk_5_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %496 = flow.tensor.bitcast %__parameter_model_blk_5_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %497 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%496) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %498 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%495) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %499 = flow.tensor.bitcast %498 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %500 = flow.tensor.bitcast %497 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %501 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%500, %499, %478) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %502 = flow.tensor.reshape %501 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %503 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%486) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %504 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%486) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %505 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%503, %59, %504, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %506 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%494) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %507 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%494) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %508 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%506, %59, %507, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %509 = flow.tensor.reshape %508 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %510 = flow.dispatch @decode_bs4$async_dispatch_246::@decode_bs4$async_dispatch_246_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %511 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%510, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %512 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%509, %511, %427, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %427{%70}
    %513 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%510, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %514 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%502, %513, %512, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %512{%70}
    %515 = flow.tensor.reshape %514 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %516 = flow.dispatch @decode_bs4$async_dispatch_251::@decode_bs4$async_dispatch_251_gather_4xDx4x32x64xf16_generic[%8, %7](%515, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %517 = flow.tensor.reshape %516 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %518 = flow.tensor.reshape %505 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %519 = flow.dispatch @decode_bs4$async_dispatch_252::@decode_bs4$async_dispatch_252_gather_4xDx4x32x64xf16_generic[%8, %7](%515, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %520 = flow.tensor.reshape %519 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %521 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%518, %517, %520, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %522 = flow.tensor.bitcast %__parameter_model_blk_5_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %523 = flow.tensor.bitcast %__parameter_model_blk_5_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %524 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%523) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %525 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%522) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %526 = flow.tensor.bitcast %525 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %527 = flow.tensor.bitcast %524 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %528 = flow.tensor.reshape %521 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %529 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%527, %526, %528, %471) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %530 = flow.tensor.reshape %529 : tensor<4x2048xf16> -> tensor<8192xf16>
    %531 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%530) : (tensor<8192xf16>) -> tensor<8192xf32>
    %532 = flow.tensor.reshape %531 : tensor<8192xf32> -> tensor<4x2048xf32>
    %533 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%529) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %534 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%532, %533) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %535 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_5_ffn_norm_weight_tensor_2048xf32, %534) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %536 = flow.tensor.reshape %535 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %537 = flow.tensor.bitcast %__parameter_model_blk_5_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %538 = flow.tensor.bitcast %__parameter_model_blk_5_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %539 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%538) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %540 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%537) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %541 = flow.tensor.bitcast %540 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %542 = flow.tensor.bitcast %539 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %543 = flow.tensor.bitcast %__parameter_model_blk_5_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %544 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%543) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %545 = flow.tensor.bitcast %544 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %546 = flow.tensor.bitcast %__parameter_model_blk_5_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %547 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%546) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %548 = flow.tensor.bitcast %547 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %549 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%545, %548, %536) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %550 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%542, %541, %536, %549) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %551 = flow.tensor.reshape %550 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %552 = flow.tensor.bitcast %__parameter_model_blk_5_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %553 = flow.tensor.bitcast %__parameter_model_blk_5_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %554 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%553) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %555 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%552) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %556 = flow.tensor.bitcast %555 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %557 = flow.tensor.bitcast %554 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %558 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%557, %556, %551, %529) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %559 = flow.tensor.reshape %558 : tensor<4x2048xf16> -> tensor<8192xf16>
    %560 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%559) : (tensor<8192xf16>) -> tensor<8192xf32>
    %561 = flow.tensor.reshape %560 : tensor<8192xf32> -> tensor<4x2048xf32>
    %562 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%558) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %563 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%561, %562) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %564 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_6_attn_norm_weight_tensor_2048xf32, %563) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %565 = flow.tensor.reshape %564 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %566 = flow.tensor.bitcast %__parameter_model_blk_6_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %567 = flow.tensor.bitcast %__parameter_model_blk_6_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %568 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%567) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %569 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%566) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %570 = flow.tensor.bitcast %569 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %571 = flow.tensor.bitcast %568 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %572 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%571, %570, %565) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %573 = flow.tensor.reshape %572 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %574 = flow.tensor.bitcast %__parameter_model_blk_6_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %575 = flow.tensor.bitcast %__parameter_model_blk_6_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %576 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%575) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %577 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%574) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %578 = flow.tensor.bitcast %577 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %579 = flow.tensor.bitcast %576 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %580 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%579, %578, %565) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %581 = flow.tensor.reshape %580 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %582 = flow.tensor.bitcast %__parameter_model_blk_6_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %583 = flow.tensor.bitcast %__parameter_model_blk_6_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %584 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%583) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %585 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%582) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %586 = flow.tensor.bitcast %585 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %587 = flow.tensor.bitcast %584 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %588 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%587, %586, %565) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %589 = flow.tensor.reshape %588 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %590 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%573) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %591 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%573) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %592 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%590, %59, %591, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %593 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%581) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %594 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%581) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %595 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%593, %59, %594, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %596 = flow.tensor.reshape %595 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %597 = flow.dispatch @decode_bs4$async_dispatch_289::@decode_bs4$async_dispatch_289_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %598 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%597, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %599 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%596, %598, %514, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %514{%70}
    %600 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%597, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %601 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%589, %600, %599, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %599{%70}
    %602 = flow.tensor.reshape %601 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %603 = flow.dispatch @decode_bs4$async_dispatch_294::@decode_bs4$async_dispatch_294_gather_4xDx4x32x64xf16_generic[%8, %7](%602, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %604 = flow.tensor.reshape %603 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %605 = flow.tensor.reshape %592 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %606 = flow.dispatch @decode_bs4$async_dispatch_295::@decode_bs4$async_dispatch_295_gather_4xDx4x32x64xf16_generic[%8, %7](%602, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %607 = flow.tensor.reshape %606 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %608 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%605, %604, %607, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %609 = flow.tensor.bitcast %__parameter_model_blk_6_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %610 = flow.tensor.bitcast %__parameter_model_blk_6_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %611 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%610) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %612 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%609) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %613 = flow.tensor.bitcast %612 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %614 = flow.tensor.bitcast %611 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %615 = flow.tensor.reshape %608 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %616 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%614, %613, %615, %558) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %617 = flow.tensor.reshape %616 : tensor<4x2048xf16> -> tensor<8192xf16>
    %618 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%617) : (tensor<8192xf16>) -> tensor<8192xf32>
    %619 = flow.tensor.reshape %618 : tensor<8192xf32> -> tensor<4x2048xf32>
    %620 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%616) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %621 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%619, %620) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %622 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_6_ffn_norm_weight_tensor_2048xf32, %621) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %623 = flow.tensor.reshape %622 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %624 = flow.tensor.bitcast %__parameter_model_blk_6_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %625 = flow.tensor.bitcast %__parameter_model_blk_6_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %626 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%625) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %627 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%624) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %628 = flow.tensor.bitcast %627 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %629 = flow.tensor.bitcast %626 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %630 = flow.tensor.bitcast %__parameter_model_blk_6_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %631 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%630) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %632 = flow.tensor.bitcast %631 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %633 = flow.tensor.bitcast %__parameter_model_blk_6_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %634 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%633) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %635 = flow.tensor.bitcast %634 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %636 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%632, %635, %623) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %637 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%629, %628, %623, %636) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %638 = flow.tensor.reshape %637 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %639 = flow.tensor.bitcast %__parameter_model_blk_6_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %640 = flow.tensor.bitcast %__parameter_model_blk_6_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %641 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%640) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %642 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%639) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %643 = flow.tensor.bitcast %642 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %644 = flow.tensor.bitcast %641 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %645 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%644, %643, %638, %616) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %646 = flow.tensor.reshape %645 : tensor<4x2048xf16> -> tensor<8192xf16>
    %647 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%646) : (tensor<8192xf16>) -> tensor<8192xf32>
    %648 = flow.tensor.reshape %647 : tensor<8192xf32> -> tensor<4x2048xf32>
    %649 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%645) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %650 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%648, %649) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %651 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_7_attn_norm_weight_tensor_2048xf32, %650) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %652 = flow.tensor.reshape %651 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %653 = flow.tensor.bitcast %__parameter_model_blk_7_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %654 = flow.tensor.bitcast %__parameter_model_blk_7_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %655 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%654) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %656 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%653) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %657 = flow.tensor.bitcast %656 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %658 = flow.tensor.bitcast %655 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %659 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%658, %657, %652) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %660 = flow.tensor.reshape %659 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %661 = flow.tensor.bitcast %__parameter_model_blk_7_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %662 = flow.tensor.bitcast %__parameter_model_blk_7_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %663 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%662) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %664 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%661) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %665 = flow.tensor.bitcast %664 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %666 = flow.tensor.bitcast %663 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %667 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%666, %665, %652) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %668 = flow.tensor.reshape %667 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %669 = flow.tensor.bitcast %__parameter_model_blk_7_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %670 = flow.tensor.bitcast %__parameter_model_blk_7_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %671 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%670) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %672 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%669) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %673 = flow.tensor.bitcast %672 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %674 = flow.tensor.bitcast %671 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %675 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%674, %673, %652) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %676 = flow.tensor.reshape %675 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %677 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%660) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %678 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%660) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %679 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%677, %59, %678, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %680 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%668) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %681 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%668) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %682 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%680, %59, %681, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %683 = flow.tensor.reshape %682 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %684 = flow.dispatch @decode_bs4$async_dispatch_332::@decode_bs4$async_dispatch_332_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %685 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%684, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %686 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%683, %685, %601, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %601{%70}
    %687 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%684, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %688 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%676, %687, %686, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %686{%70}
    %689 = flow.tensor.reshape %688 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %690 = flow.dispatch @decode_bs4$async_dispatch_337::@decode_bs4$async_dispatch_337_gather_4xDx4x32x64xf16_generic[%8, %7](%689, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %691 = flow.tensor.reshape %690 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %692 = flow.tensor.reshape %679 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %693 = flow.dispatch @decode_bs4$async_dispatch_338::@decode_bs4$async_dispatch_338_gather_4xDx4x32x64xf16_generic[%8, %7](%689, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %694 = flow.tensor.reshape %693 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %695 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%692, %691, %694, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %696 = flow.tensor.bitcast %__parameter_model_blk_7_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %697 = flow.tensor.bitcast %__parameter_model_blk_7_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %698 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%697) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %699 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%696) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %700 = flow.tensor.bitcast %699 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %701 = flow.tensor.bitcast %698 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %702 = flow.tensor.reshape %695 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %703 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%701, %700, %702, %645) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %704 = flow.tensor.reshape %703 : tensor<4x2048xf16> -> tensor<8192xf16>
    %705 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%704) : (tensor<8192xf16>) -> tensor<8192xf32>
    %706 = flow.tensor.reshape %705 : tensor<8192xf32> -> tensor<4x2048xf32>
    %707 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%703) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %708 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%706, %707) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %709 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_7_ffn_norm_weight_tensor_2048xf32, %708) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %710 = flow.tensor.reshape %709 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %711 = flow.tensor.bitcast %__parameter_model_blk_7_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %712 = flow.tensor.bitcast %__parameter_model_blk_7_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %713 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%712) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %714 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%711) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %715 = flow.tensor.bitcast %714 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %716 = flow.tensor.bitcast %713 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %717 = flow.tensor.bitcast %__parameter_model_blk_7_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %718 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%717) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %719 = flow.tensor.bitcast %718 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %720 = flow.tensor.bitcast %__parameter_model_blk_7_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %721 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%720) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %722 = flow.tensor.bitcast %721 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %723 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%719, %722, %710) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %724 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%716, %715, %710, %723) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %725 = flow.tensor.reshape %724 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %726 = flow.tensor.bitcast %__parameter_model_blk_7_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %727 = flow.tensor.bitcast %__parameter_model_blk_7_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %728 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%727) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %729 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%726) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %730 = flow.tensor.bitcast %729 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %731 = flow.tensor.bitcast %728 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %732 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%731, %730, %725, %703) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %733 = flow.tensor.reshape %732 : tensor<4x2048xf16> -> tensor<8192xf16>
    %734 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%733) : (tensor<8192xf16>) -> tensor<8192xf32>
    %735 = flow.tensor.reshape %734 : tensor<8192xf32> -> tensor<4x2048xf32>
    %736 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%732) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %737 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%735, %736) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %738 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_8_attn_norm_weight_tensor_2048xf32, %737) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %739 = flow.tensor.reshape %738 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %740 = flow.tensor.bitcast %__parameter_model_blk_8_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %741 = flow.tensor.bitcast %__parameter_model_blk_8_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %742 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%741) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %743 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%740) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %744 = flow.tensor.bitcast %743 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %745 = flow.tensor.bitcast %742 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %746 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%745, %744, %739) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %747 = flow.tensor.reshape %746 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %748 = flow.tensor.bitcast %__parameter_model_blk_8_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %749 = flow.tensor.bitcast %__parameter_model_blk_8_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %750 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%749) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %751 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%748) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %752 = flow.tensor.bitcast %751 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %753 = flow.tensor.bitcast %750 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %754 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%753, %752, %739) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %755 = flow.tensor.reshape %754 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %756 = flow.tensor.bitcast %__parameter_model_blk_8_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %757 = flow.tensor.bitcast %__parameter_model_blk_8_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %758 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%757) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %759 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%756) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %760 = flow.tensor.bitcast %759 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %761 = flow.tensor.bitcast %758 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %762 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%761, %760, %739) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %763 = flow.tensor.reshape %762 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %764 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%747) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %765 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%747) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %766 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%764, %59, %765, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %767 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%755) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %768 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%755) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %769 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%767, %59, %768, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %770 = flow.tensor.reshape %769 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %771 = flow.dispatch @decode_bs4$async_dispatch_375::@decode_bs4$async_dispatch_375_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %772 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%771, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %773 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%770, %772, %688, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %688{%70}
    %774 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%771, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %775 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%763, %774, %773, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %773{%70}
    %776 = flow.tensor.reshape %775 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %777 = flow.dispatch @decode_bs4$async_dispatch_380::@decode_bs4$async_dispatch_380_gather_4xDx4x32x64xf16_generic[%8, %7](%776, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %778 = flow.tensor.reshape %777 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %779 = flow.tensor.reshape %766 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %780 = flow.dispatch @decode_bs4$async_dispatch_381::@decode_bs4$async_dispatch_381_gather_4xDx4x32x64xf16_generic[%8, %7](%776, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %781 = flow.tensor.reshape %780 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %782 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%779, %778, %781, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %783 = flow.tensor.bitcast %__parameter_model_blk_8_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %784 = flow.tensor.bitcast %__parameter_model_blk_8_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %785 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%784) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %786 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%783) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %787 = flow.tensor.bitcast %786 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %788 = flow.tensor.bitcast %785 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %789 = flow.tensor.reshape %782 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %790 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%788, %787, %789, %732) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %791 = flow.tensor.reshape %790 : tensor<4x2048xf16> -> tensor<8192xf16>
    %792 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%791) : (tensor<8192xf16>) -> tensor<8192xf32>
    %793 = flow.tensor.reshape %792 : tensor<8192xf32> -> tensor<4x2048xf32>
    %794 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%790) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %795 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%793, %794) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %796 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_8_ffn_norm_weight_tensor_2048xf32, %795) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %797 = flow.tensor.reshape %796 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %798 = flow.tensor.bitcast %__parameter_model_blk_8_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %799 = flow.tensor.bitcast %__parameter_model_blk_8_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %800 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%799) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %801 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%798) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %802 = flow.tensor.bitcast %801 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %803 = flow.tensor.bitcast %800 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %804 = flow.tensor.bitcast %__parameter_model_blk_8_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %805 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%804) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %806 = flow.tensor.bitcast %805 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %807 = flow.tensor.bitcast %__parameter_model_blk_8_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %808 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%807) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %809 = flow.tensor.bitcast %808 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %810 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%806, %809, %797) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %811 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%803, %802, %797, %810) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %812 = flow.tensor.reshape %811 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %813 = flow.tensor.bitcast %__parameter_model_blk_8_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %814 = flow.tensor.bitcast %__parameter_model_blk_8_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %815 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%814) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %816 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%813) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %817 = flow.tensor.bitcast %816 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %818 = flow.tensor.bitcast %815 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %819 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%818, %817, %812, %790) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %820 = flow.tensor.reshape %819 : tensor<4x2048xf16> -> tensor<8192xf16>
    %821 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%820) : (tensor<8192xf16>) -> tensor<8192xf32>
    %822 = flow.tensor.reshape %821 : tensor<8192xf32> -> tensor<4x2048xf32>
    %823 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%819) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %824 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%822, %823) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %825 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_9_attn_norm_weight_tensor_2048xf32, %824) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %826 = flow.tensor.reshape %825 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %827 = flow.tensor.bitcast %__parameter_model_blk_9_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %828 = flow.tensor.bitcast %__parameter_model_blk_9_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %829 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%828) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %830 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%827) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %831 = flow.tensor.bitcast %830 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %832 = flow.tensor.bitcast %829 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %833 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%832, %831, %826) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %834 = flow.tensor.reshape %833 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %835 = flow.tensor.bitcast %__parameter_model_blk_9_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %836 = flow.tensor.bitcast %__parameter_model_blk_9_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %837 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%836) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %838 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%835) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %839 = flow.tensor.bitcast %838 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %840 = flow.tensor.bitcast %837 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %841 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%840, %839, %826) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %842 = flow.tensor.reshape %841 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %843 = flow.tensor.bitcast %__parameter_model_blk_9_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %844 = flow.tensor.bitcast %__parameter_model_blk_9_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %845 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%844) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %846 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%843) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %847 = flow.tensor.bitcast %846 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %848 = flow.tensor.bitcast %845 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %849 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%848, %847, %826) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %850 = flow.tensor.reshape %849 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %851 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%834) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %852 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%834) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %853 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%851, %59, %852, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %854 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%842) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %855 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%842) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %856 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%854, %59, %855, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %857 = flow.tensor.reshape %856 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %858 = flow.dispatch @decode_bs4$async_dispatch_418::@decode_bs4$async_dispatch_418_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %859 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%858, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %860 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%857, %859, %775, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %775{%70}
    %861 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%858, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %862 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%850, %861, %860, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %860{%70}
    %863 = flow.tensor.reshape %862 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %864 = flow.dispatch @decode_bs4$async_dispatch_423::@decode_bs4$async_dispatch_423_gather_4xDx4x32x64xf16_generic[%8, %7](%863, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %865 = flow.tensor.reshape %864 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %866 = flow.tensor.reshape %853 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %867 = flow.dispatch @decode_bs4$async_dispatch_424::@decode_bs4$async_dispatch_424_gather_4xDx4x32x64xf16_generic[%8, %7](%863, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %868 = flow.tensor.reshape %867 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %869 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%866, %865, %868, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %870 = flow.tensor.bitcast %__parameter_model_blk_9_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %871 = flow.tensor.bitcast %__parameter_model_blk_9_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %872 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%871) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %873 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%870) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %874 = flow.tensor.bitcast %873 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %875 = flow.tensor.bitcast %872 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %876 = flow.tensor.reshape %869 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %877 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%875, %874, %876, %819) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %878 = flow.tensor.reshape %877 : tensor<4x2048xf16> -> tensor<8192xf16>
    %879 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%878) : (tensor<8192xf16>) -> tensor<8192xf32>
    %880 = flow.tensor.reshape %879 : tensor<8192xf32> -> tensor<4x2048xf32>
    %881 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%877) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %882 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%880, %881) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %883 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_9_ffn_norm_weight_tensor_2048xf32, %882) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %884 = flow.tensor.reshape %883 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %885 = flow.tensor.bitcast %__parameter_model_blk_9_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %886 = flow.tensor.bitcast %__parameter_model_blk_9_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %887 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%886) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %888 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%885) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %889 = flow.tensor.bitcast %888 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %890 = flow.tensor.bitcast %887 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %891 = flow.tensor.bitcast %__parameter_model_blk_9_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %892 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%891) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %893 = flow.tensor.bitcast %892 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %894 = flow.tensor.bitcast %__parameter_model_blk_9_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %895 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%894) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %896 = flow.tensor.bitcast %895 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %897 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%893, %896, %884) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %898 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%890, %889, %884, %897) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %899 = flow.tensor.reshape %898 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %900 = flow.tensor.bitcast %__parameter_model_blk_9_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %901 = flow.tensor.bitcast %__parameter_model_blk_9_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %902 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%901) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %903 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%900) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %904 = flow.tensor.bitcast %903 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %905 = flow.tensor.bitcast %902 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %906 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%905, %904, %899, %877) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %907 = flow.tensor.reshape %906 : tensor<4x2048xf16> -> tensor<8192xf16>
    %908 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%907) : (tensor<8192xf16>) -> tensor<8192xf32>
    %909 = flow.tensor.reshape %908 : tensor<8192xf32> -> tensor<4x2048xf32>
    %910 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%906) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %911 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%909, %910) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %912 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_10_attn_norm_weight_tensor_2048xf32, %911) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %913 = flow.tensor.reshape %912 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %914 = flow.tensor.bitcast %__parameter_model_blk_10_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %915 = flow.tensor.bitcast %__parameter_model_blk_10_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %916 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%915) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %917 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%914) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %918 = flow.tensor.bitcast %917 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %919 = flow.tensor.bitcast %916 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %920 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%919, %918, %913) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %921 = flow.tensor.reshape %920 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %922 = flow.tensor.bitcast %__parameter_model_blk_10_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %923 = flow.tensor.bitcast %__parameter_model_blk_10_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %924 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%923) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %925 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%922) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %926 = flow.tensor.bitcast %925 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %927 = flow.tensor.bitcast %924 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %928 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%927, %926, %913) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %929 = flow.tensor.reshape %928 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %930 = flow.tensor.bitcast %__parameter_model_blk_10_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %931 = flow.tensor.bitcast %__parameter_model_blk_10_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %932 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%931) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %933 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%930) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %934 = flow.tensor.bitcast %933 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %935 = flow.tensor.bitcast %932 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %936 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%935, %934, %913) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %937 = flow.tensor.reshape %936 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %938 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%921) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %939 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%921) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %940 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%938, %59, %939, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %941 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%929) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %942 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%929) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %943 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%941, %59, %942, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %944 = flow.tensor.reshape %943 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %945 = flow.dispatch @decode_bs4$async_dispatch_461::@decode_bs4$async_dispatch_461_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %946 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%945, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %947 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%944, %946, %862, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %862{%70}
    %948 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%945, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %949 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%937, %948, %947, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %947{%70}
    %950 = flow.tensor.reshape %949 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %951 = flow.dispatch @decode_bs4$async_dispatch_466::@decode_bs4$async_dispatch_466_gather_4xDx4x32x64xf16_generic[%8, %7](%950, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %952 = flow.tensor.reshape %951 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %953 = flow.tensor.reshape %940 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %954 = flow.dispatch @decode_bs4$async_dispatch_467::@decode_bs4$async_dispatch_467_gather_4xDx4x32x64xf16_generic[%8, %7](%950, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %955 = flow.tensor.reshape %954 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %956 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%953, %952, %955, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %957 = flow.tensor.bitcast %__parameter_model_blk_10_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %958 = flow.tensor.bitcast %__parameter_model_blk_10_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %959 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%958) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %960 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%957) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %961 = flow.tensor.bitcast %960 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %962 = flow.tensor.bitcast %959 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %963 = flow.tensor.reshape %956 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %964 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%962, %961, %963, %906) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %965 = flow.tensor.reshape %964 : tensor<4x2048xf16> -> tensor<8192xf16>
    %966 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%965) : (tensor<8192xf16>) -> tensor<8192xf32>
    %967 = flow.tensor.reshape %966 : tensor<8192xf32> -> tensor<4x2048xf32>
    %968 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%964) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %969 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%967, %968) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %970 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_10_ffn_norm_weight_tensor_2048xf32, %969) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %971 = flow.tensor.reshape %970 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %972 = flow.tensor.bitcast %__parameter_model_blk_10_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %973 = flow.tensor.bitcast %__parameter_model_blk_10_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %974 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%973) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %975 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%972) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %976 = flow.tensor.bitcast %975 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %977 = flow.tensor.bitcast %974 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %978 = flow.tensor.bitcast %__parameter_model_blk_10_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %979 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%978) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %980 = flow.tensor.bitcast %979 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %981 = flow.tensor.bitcast %__parameter_model_blk_10_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %982 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%981) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %983 = flow.tensor.bitcast %982 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %984 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%980, %983, %971) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %985 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%977, %976, %971, %984) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %986 = flow.tensor.reshape %985 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %987 = flow.tensor.bitcast %__parameter_model_blk_10_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %988 = flow.tensor.bitcast %__parameter_model_blk_10_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %989 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%988) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %990 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%987) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %991 = flow.tensor.bitcast %990 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %992 = flow.tensor.bitcast %989 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %993 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%992, %991, %986, %964) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %994 = flow.tensor.reshape %993 : tensor<4x2048xf16> -> tensor<8192xf16>
    %995 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%994) : (tensor<8192xf16>) -> tensor<8192xf32>
    %996 = flow.tensor.reshape %995 : tensor<8192xf32> -> tensor<4x2048xf32>
    %997 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%993) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %998 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%996, %997) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %999 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_11_attn_norm_weight_tensor_2048xf32, %998) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1000 = flow.tensor.reshape %999 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1001 = flow.tensor.bitcast %__parameter_model_blk_11_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1002 = flow.tensor.bitcast %__parameter_model_blk_11_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1003 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1002) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1004 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1001) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1005 = flow.tensor.bitcast %1004 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1006 = flow.tensor.bitcast %1003 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1007 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%1006, %1005, %1000) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %1008 = flow.tensor.reshape %1007 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %1009 = flow.tensor.bitcast %__parameter_model_blk_11_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1010 = flow.tensor.bitcast %__parameter_model_blk_11_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1011 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1010) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1012 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1009) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1013 = flow.tensor.bitcast %1012 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1014 = flow.tensor.bitcast %1011 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1015 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1014, %1013, %1000) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1016 = flow.tensor.reshape %1015 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %1017 = flow.tensor.bitcast %__parameter_model_blk_11_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1018 = flow.tensor.bitcast %__parameter_model_blk_11_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1019 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1018) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1020 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1017) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1021 = flow.tensor.bitcast %1020 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1022 = flow.tensor.bitcast %1019 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1023 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1022, %1021, %1000) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1024 = flow.tensor.reshape %1023 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %1025 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%1008) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1026 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%1008) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1027 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%1025, %59, %1026, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %1028 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%1016) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1029 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%1016) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1030 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%1028, %59, %1029, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %1031 = flow.tensor.reshape %1030 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %1032 = flow.dispatch @decode_bs4$async_dispatch_504::@decode_bs4$async_dispatch_504_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %1033 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%1032, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1034 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1031, %1033, %949, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %949{%70}
    %1035 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%1032, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1036 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1024, %1035, %1034, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1034{%70}
    %1037 = flow.tensor.reshape %1036 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %1038 = flow.dispatch @decode_bs4$async_dispatch_509::@decode_bs4$async_dispatch_509_gather_4xDx4x32x64xf16_generic[%8, %7](%1037, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %1039 = flow.tensor.reshape %1038 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %1040 = flow.tensor.reshape %1027 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %1041 = flow.dispatch @decode_bs4$async_dispatch_510::@decode_bs4$async_dispatch_510_gather_4xDx4x32x64xf16_generic[%8, %7](%1037, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %1042 = flow.tensor.reshape %1041 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %1043 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%1040, %1039, %1042, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %1044 = flow.tensor.bitcast %__parameter_model_blk_11_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1045 = flow.tensor.bitcast %__parameter_model_blk_11_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1046 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1045) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1047 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1044) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1048 = flow.tensor.bitcast %1047 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1049 = flow.tensor.bitcast %1046 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1050 = flow.tensor.reshape %1043 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %1051 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%1049, %1048, %1050, %993) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1052 = flow.tensor.reshape %1051 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1053 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1052) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1054 = flow.tensor.reshape %1053 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1055 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1051) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1056 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1054, %1055) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1057 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_11_ffn_norm_weight_tensor_2048xf32, %1056) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1058 = flow.tensor.reshape %1057 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1059 = flow.tensor.bitcast %__parameter_model_blk_11_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1060 = flow.tensor.bitcast %__parameter_model_blk_11_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1061 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1060) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1062 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1059) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1063 = flow.tensor.bitcast %1062 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1064 = flow.tensor.bitcast %1061 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1065 = flow.tensor.bitcast %__parameter_model_blk_11_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1066 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1065) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1067 = flow.tensor.bitcast %1066 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1068 = flow.tensor.bitcast %__parameter_model_blk_11_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1069 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1068) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1070 = flow.tensor.bitcast %1069 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1071 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%1067, %1070, %1058) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %1072 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%1064, %1063, %1058, %1071) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %1073 = flow.tensor.reshape %1072 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %1074 = flow.tensor.bitcast %__parameter_model_blk_11_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1075 = flow.tensor.bitcast %__parameter_model_blk_11_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1076 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1075) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1077 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1074) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1078 = flow.tensor.bitcast %1077 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1079 = flow.tensor.bitcast %1076 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1080 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%1079, %1078, %1073, %1051) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1081 = flow.tensor.reshape %1080 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1082 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1081) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1083 = flow.tensor.reshape %1082 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1084 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1080) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1085 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1083, %1084) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1086 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_12_attn_norm_weight_tensor_2048xf32, %1085) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1087 = flow.tensor.reshape %1086 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1088 = flow.tensor.bitcast %__parameter_model_blk_12_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1089 = flow.tensor.bitcast %__parameter_model_blk_12_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1090 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1089) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1091 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1088) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1092 = flow.tensor.bitcast %1091 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1093 = flow.tensor.bitcast %1090 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1094 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%1093, %1092, %1087) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %1095 = flow.tensor.reshape %1094 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %1096 = flow.tensor.bitcast %__parameter_model_blk_12_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1097 = flow.tensor.bitcast %__parameter_model_blk_12_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1098 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1097) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1099 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1096) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1100 = flow.tensor.bitcast %1099 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1101 = flow.tensor.bitcast %1098 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1102 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1101, %1100, %1087) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1103 = flow.tensor.reshape %1102 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %1104 = flow.tensor.bitcast %__parameter_model_blk_12_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1105 = flow.tensor.bitcast %__parameter_model_blk_12_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1106 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1105) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1107 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1104) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1108 = flow.tensor.bitcast %1107 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1109 = flow.tensor.bitcast %1106 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1110 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1109, %1108, %1087) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1111 = flow.tensor.reshape %1110 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %1112 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%1095) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1113 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%1095) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1114 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%1112, %59, %1113, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %1115 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%1103) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1116 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%1103) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1117 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%1115, %59, %1116, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %1118 = flow.tensor.reshape %1117 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %1119 = flow.dispatch @decode_bs4$async_dispatch_547::@decode_bs4$async_dispatch_547_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %1120 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%1119, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1121 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1118, %1120, %1036, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1036{%70}
    %1122 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%1119, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1123 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1111, %1122, %1121, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1121{%70}
    %1124 = flow.tensor.reshape %1123 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %1125 = flow.dispatch @decode_bs4$async_dispatch_552::@decode_bs4$async_dispatch_552_gather_4xDx4x32x64xf16_generic[%8, %7](%1124, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %1126 = flow.tensor.reshape %1125 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %1127 = flow.tensor.reshape %1114 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %1128 = flow.dispatch @decode_bs4$async_dispatch_553::@decode_bs4$async_dispatch_553_gather_4xDx4x32x64xf16_generic[%8, %7](%1124, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %1129 = flow.tensor.reshape %1128 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %1130 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%1127, %1126, %1129, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %1131 = flow.tensor.bitcast %__parameter_model_blk_12_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1132 = flow.tensor.bitcast %__parameter_model_blk_12_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1133 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1132) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1134 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1131) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1135 = flow.tensor.bitcast %1134 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1136 = flow.tensor.bitcast %1133 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1137 = flow.tensor.reshape %1130 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %1138 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%1136, %1135, %1137, %1080) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1139 = flow.tensor.reshape %1138 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1140 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1139) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1141 = flow.tensor.reshape %1140 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1142 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1138) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1143 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1141, %1142) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1144 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_12_ffn_norm_weight_tensor_2048xf32, %1143) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1145 = flow.tensor.reshape %1144 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1146 = flow.tensor.bitcast %__parameter_model_blk_12_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1147 = flow.tensor.bitcast %__parameter_model_blk_12_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1148 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1147) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1149 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1146) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1150 = flow.tensor.bitcast %1149 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1151 = flow.tensor.bitcast %1148 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1152 = flow.tensor.bitcast %__parameter_model_blk_12_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1153 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1152) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1154 = flow.tensor.bitcast %1153 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1155 = flow.tensor.bitcast %__parameter_model_blk_12_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1156 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1155) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1157 = flow.tensor.bitcast %1156 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1158 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%1154, %1157, %1145) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %1159 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%1151, %1150, %1145, %1158) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %1160 = flow.tensor.reshape %1159 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %1161 = flow.tensor.bitcast %__parameter_model_blk_12_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1162 = flow.tensor.bitcast %__parameter_model_blk_12_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1163 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1162) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1164 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1161) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1165 = flow.tensor.bitcast %1164 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1166 = flow.tensor.bitcast %1163 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1167 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%1166, %1165, %1160, %1138) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1168 = flow.tensor.reshape %1167 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1169 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1168) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1170 = flow.tensor.reshape %1169 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1171 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1167) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1172 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1170, %1171) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1173 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_13_attn_norm_weight_tensor_2048xf32, %1172) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1174 = flow.tensor.reshape %1173 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1175 = flow.tensor.bitcast %__parameter_model_blk_13_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1176 = flow.tensor.bitcast %__parameter_model_blk_13_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1177 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1176) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1178 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1175) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1179 = flow.tensor.bitcast %1178 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1180 = flow.tensor.bitcast %1177 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1181 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%1180, %1179, %1174) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %1182 = flow.tensor.reshape %1181 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %1183 = flow.tensor.bitcast %__parameter_model_blk_13_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1184 = flow.tensor.bitcast %__parameter_model_blk_13_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1185 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1184) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1186 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1183) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1187 = flow.tensor.bitcast %1186 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1188 = flow.tensor.bitcast %1185 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1189 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1188, %1187, %1174) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1190 = flow.tensor.reshape %1189 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %1191 = flow.tensor.bitcast %__parameter_model_blk_13_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1192 = flow.tensor.bitcast %__parameter_model_blk_13_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1193 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1192) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1194 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1191) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1195 = flow.tensor.bitcast %1194 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1196 = flow.tensor.bitcast %1193 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1197 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1196, %1195, %1174) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1198 = flow.tensor.reshape %1197 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %1199 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%1182) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1200 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%1182) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1201 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%1199, %59, %1200, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %1202 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%1190) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1203 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%1190) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1204 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%1202, %59, %1203, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %1205 = flow.tensor.reshape %1204 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %1206 = flow.dispatch @decode_bs4$async_dispatch_590::@decode_bs4$async_dispatch_590_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %1207 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%1206, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1208 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1205, %1207, %1123, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1123{%70}
    %1209 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%1206, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1210 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1198, %1209, %1208, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1208{%70}
    %1211 = flow.tensor.reshape %1210 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %1212 = flow.dispatch @decode_bs4$async_dispatch_595::@decode_bs4$async_dispatch_595_gather_4xDx4x32x64xf16_generic[%8, %7](%1211, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %1213 = flow.tensor.reshape %1212 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %1214 = flow.tensor.reshape %1201 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %1215 = flow.dispatch @decode_bs4$async_dispatch_596::@decode_bs4$async_dispatch_596_gather_4xDx4x32x64xf16_generic[%8, %7](%1211, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %1216 = flow.tensor.reshape %1215 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %1217 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%1214, %1213, %1216, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %1218 = flow.tensor.bitcast %__parameter_model_blk_13_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1219 = flow.tensor.bitcast %__parameter_model_blk_13_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1220 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1219) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1221 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1218) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1222 = flow.tensor.bitcast %1221 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1223 = flow.tensor.bitcast %1220 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1224 = flow.tensor.reshape %1217 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %1225 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%1223, %1222, %1224, %1167) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1226 = flow.tensor.reshape %1225 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1227 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1226) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1228 = flow.tensor.reshape %1227 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1229 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1225) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1230 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1228, %1229) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1231 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_13_ffn_norm_weight_tensor_2048xf32, %1230) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1232 = flow.tensor.reshape %1231 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1233 = flow.tensor.bitcast %__parameter_model_blk_13_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1234 = flow.tensor.bitcast %__parameter_model_blk_13_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1235 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1234) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1236 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1233) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1237 = flow.tensor.bitcast %1236 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1238 = flow.tensor.bitcast %1235 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1239 = flow.tensor.bitcast %__parameter_model_blk_13_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1240 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1239) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1241 = flow.tensor.bitcast %1240 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1242 = flow.tensor.bitcast %__parameter_model_blk_13_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1243 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1242) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1244 = flow.tensor.bitcast %1243 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1245 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%1241, %1244, %1232) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %1246 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%1238, %1237, %1232, %1245) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %1247 = flow.tensor.reshape %1246 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %1248 = flow.tensor.bitcast %__parameter_model_blk_13_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1249 = flow.tensor.bitcast %__parameter_model_blk_13_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1250 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1249) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1251 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1248) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1252 = flow.tensor.bitcast %1251 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1253 = flow.tensor.bitcast %1250 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1254 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%1253, %1252, %1247, %1225) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1255 = flow.tensor.reshape %1254 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1256 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1255) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1257 = flow.tensor.reshape %1256 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1258 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1254) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1259 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1257, %1258) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1260 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_14_attn_norm_weight_tensor_2048xf32, %1259) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1261 = flow.tensor.reshape %1260 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1262 = flow.tensor.bitcast %__parameter_model_blk_14_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1263 = flow.tensor.bitcast %__parameter_model_blk_14_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1264 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1263) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1265 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1262) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1266 = flow.tensor.bitcast %1265 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1267 = flow.tensor.bitcast %1264 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1268 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%1267, %1266, %1261) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %1269 = flow.tensor.reshape %1268 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %1270 = flow.tensor.bitcast %__parameter_model_blk_14_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1271 = flow.tensor.bitcast %__parameter_model_blk_14_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1272 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1271) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1273 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1270) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1274 = flow.tensor.bitcast %1273 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1275 = flow.tensor.bitcast %1272 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1276 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1275, %1274, %1261) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1277 = flow.tensor.reshape %1276 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %1278 = flow.tensor.bitcast %__parameter_model_blk_14_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1279 = flow.tensor.bitcast %__parameter_model_blk_14_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1280 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1279) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1281 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1278) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1282 = flow.tensor.bitcast %1281 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1283 = flow.tensor.bitcast %1280 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1284 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1283, %1282, %1261) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1285 = flow.tensor.reshape %1284 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %1286 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%1269) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1287 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%1269) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1288 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%1286, %59, %1287, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %1289 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%1277) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1290 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%1277) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1291 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%1289, %59, %1290, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %1292 = flow.tensor.reshape %1291 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %1293 = flow.dispatch @decode_bs4$async_dispatch_633::@decode_bs4$async_dispatch_633_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %1294 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%1293, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1295 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1292, %1294, %1210, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1210{%70}
    %1296 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%1293, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1297 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1285, %1296, %1295, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1295{%70}
    %1298 = flow.tensor.reshape %1297 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %1299 = flow.dispatch @decode_bs4$async_dispatch_638::@decode_bs4$async_dispatch_638_gather_4xDx4x32x64xf16_generic[%8, %7](%1298, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %1300 = flow.tensor.reshape %1299 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %1301 = flow.tensor.reshape %1288 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %1302 = flow.dispatch @decode_bs4$async_dispatch_639::@decode_bs4$async_dispatch_639_gather_4xDx4x32x64xf16_generic[%8, %7](%1298, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %1303 = flow.tensor.reshape %1302 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %1304 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%1301, %1300, %1303, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %1305 = flow.tensor.bitcast %__parameter_model_blk_14_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1306 = flow.tensor.bitcast %__parameter_model_blk_14_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1307 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1306) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1308 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1305) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1309 = flow.tensor.bitcast %1308 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1310 = flow.tensor.bitcast %1307 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1311 = flow.tensor.reshape %1304 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %1312 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%1310, %1309, %1311, %1254) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1313 = flow.tensor.reshape %1312 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1314 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1313) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1315 = flow.tensor.reshape %1314 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1316 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1312) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1317 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1315, %1316) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1318 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_14_ffn_norm_weight_tensor_2048xf32, %1317) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1319 = flow.tensor.reshape %1318 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1320 = flow.tensor.bitcast %__parameter_model_blk_14_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1321 = flow.tensor.bitcast %__parameter_model_blk_14_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1322 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1321) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1323 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1320) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1324 = flow.tensor.bitcast %1323 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1325 = flow.tensor.bitcast %1322 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1326 = flow.tensor.bitcast %__parameter_model_blk_14_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1327 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1326) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1328 = flow.tensor.bitcast %1327 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1329 = flow.tensor.bitcast %__parameter_model_blk_14_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1330 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1329) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1331 = flow.tensor.bitcast %1330 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1332 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%1328, %1331, %1319) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %1333 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%1325, %1324, %1319, %1332) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %1334 = flow.tensor.reshape %1333 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %1335 = flow.tensor.bitcast %__parameter_model_blk_14_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1336 = flow.tensor.bitcast %__parameter_model_blk_14_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1337 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1336) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1338 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1335) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1339 = flow.tensor.bitcast %1338 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1340 = flow.tensor.bitcast %1337 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1341 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%1340, %1339, %1334, %1312) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1342 = flow.tensor.reshape %1341 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1343 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1342) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1344 = flow.tensor.reshape %1343 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1345 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1341) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1346 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1344, %1345) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1347 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_15_attn_norm_weight_tensor_2048xf32, %1346) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1348 = flow.tensor.reshape %1347 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1349 = flow.tensor.bitcast %__parameter_model_blk_15_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1350 = flow.tensor.bitcast %__parameter_model_blk_15_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1351 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1350) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1352 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1349) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1353 = flow.tensor.bitcast %1352 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1354 = flow.tensor.bitcast %1351 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1355 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%1354, %1353, %1348) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %1356 = flow.tensor.reshape %1355 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %1357 = flow.tensor.bitcast %__parameter_model_blk_15_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1358 = flow.tensor.bitcast %__parameter_model_blk_15_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1359 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1358) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1360 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1357) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1361 = flow.tensor.bitcast %1360 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1362 = flow.tensor.bitcast %1359 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1363 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1362, %1361, %1348) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1364 = flow.tensor.reshape %1363 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %1365 = flow.tensor.bitcast %__parameter_model_blk_15_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1366 = flow.tensor.bitcast %__parameter_model_blk_15_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1367 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1366) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1368 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1365) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1369 = flow.tensor.bitcast %1368 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1370 = flow.tensor.bitcast %1367 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1371 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1370, %1369, %1348) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1372 = flow.tensor.reshape %1371 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %1373 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%1356) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1374 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%1356) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1375 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%1373, %59, %1374, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %1376 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%1364) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1377 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%1364) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1378 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%1376, %59, %1377, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %1379 = flow.tensor.reshape %1378 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %1380 = flow.dispatch @decode_bs4$async_dispatch_676::@decode_bs4$async_dispatch_676_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %1381 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%1380, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1382 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1379, %1381, %1297, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1297{%70}
    %1383 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%1380, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1384 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1372, %1383, %1382, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1382{%70}
    %1385 = flow.tensor.reshape %1384 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %1386 = flow.dispatch @decode_bs4$async_dispatch_681::@decode_bs4$async_dispatch_681_gather_4xDx4x32x64xf16_generic[%8, %7](%1385, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %1387 = flow.tensor.reshape %1386 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %1388 = flow.tensor.reshape %1375 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %1389 = flow.dispatch @decode_bs4$async_dispatch_682::@decode_bs4$async_dispatch_682_gather_4xDx4x32x64xf16_generic[%8, %7](%1385, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %1390 = flow.tensor.reshape %1389 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %1391 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%1388, %1387, %1390, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %1392 = flow.tensor.bitcast %__parameter_model_blk_15_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1393 = flow.tensor.bitcast %__parameter_model_blk_15_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1394 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1393) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1395 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1392) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1396 = flow.tensor.bitcast %1395 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1397 = flow.tensor.bitcast %1394 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1398 = flow.tensor.reshape %1391 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %1399 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%1397, %1396, %1398, %1341) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1400 = flow.tensor.reshape %1399 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1401 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1400) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1402 = flow.tensor.reshape %1401 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1403 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1399) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1404 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1402, %1403) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1405 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_15_ffn_norm_weight_tensor_2048xf32, %1404) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1406 = flow.tensor.reshape %1405 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1407 = flow.tensor.bitcast %__parameter_model_blk_15_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1408 = flow.tensor.bitcast %__parameter_model_blk_15_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1409 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1408) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1410 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1407) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1411 = flow.tensor.bitcast %1410 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1412 = flow.tensor.bitcast %1409 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1413 = flow.tensor.bitcast %__parameter_model_blk_15_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1414 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1413) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1415 = flow.tensor.bitcast %1414 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1416 = flow.tensor.bitcast %__parameter_model_blk_15_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1417 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1416) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1418 = flow.tensor.bitcast %1417 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1419 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%1415, %1418, %1406) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %1420 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%1412, %1411, %1406, %1419) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %1421 = flow.tensor.reshape %1420 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %1422 = flow.tensor.bitcast %__parameter_model_blk_15_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1423 = flow.tensor.bitcast %__parameter_model_blk_15_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1424 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1423) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1425 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1422) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1426 = flow.tensor.bitcast %1425 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1427 = flow.tensor.bitcast %1424 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1428 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%1427, %1426, %1421, %1399) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1429 = flow.tensor.reshape %1428 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1430 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1429) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1431 = flow.tensor.reshape %1430 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1432 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1428) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1433 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1431, %1432) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1434 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_16_attn_norm_weight_tensor_2048xf32, %1433) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1435 = flow.tensor.reshape %1434 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1436 = flow.tensor.bitcast %__parameter_model_blk_16_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1437 = flow.tensor.bitcast %__parameter_model_blk_16_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1438 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1437) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1439 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1436) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1440 = flow.tensor.bitcast %1439 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1441 = flow.tensor.bitcast %1438 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1442 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%1441, %1440, %1435) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %1443 = flow.tensor.reshape %1442 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %1444 = flow.tensor.bitcast %__parameter_model_blk_16_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1445 = flow.tensor.bitcast %__parameter_model_blk_16_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1446 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1445) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1447 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1444) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1448 = flow.tensor.bitcast %1447 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1449 = flow.tensor.bitcast %1446 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1450 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1449, %1448, %1435) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1451 = flow.tensor.reshape %1450 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %1452 = flow.tensor.bitcast %__parameter_model_blk_16_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1453 = flow.tensor.bitcast %__parameter_model_blk_16_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1454 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1453) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1455 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1452) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1456 = flow.tensor.bitcast %1455 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1457 = flow.tensor.bitcast %1454 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1458 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1457, %1456, %1435) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1459 = flow.tensor.reshape %1458 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %1460 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%1443) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1461 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%1443) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1462 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%1460, %59, %1461, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %1463 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%1451) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1464 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%1451) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1465 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%1463, %59, %1464, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %1466 = flow.tensor.reshape %1465 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %1467 = flow.dispatch @decode_bs4$async_dispatch_719::@decode_bs4$async_dispatch_719_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %1468 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%1467, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1469 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1466, %1468, %1384, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1384{%70}
    %1470 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%1467, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1471 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1459, %1470, %1469, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1469{%70}
    %1472 = flow.tensor.reshape %1471 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %1473 = flow.dispatch @decode_bs4$async_dispatch_724::@decode_bs4$async_dispatch_724_gather_4xDx4x32x64xf16_generic[%8, %7](%1472, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %1474 = flow.tensor.reshape %1473 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %1475 = flow.tensor.reshape %1462 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %1476 = flow.dispatch @decode_bs4$async_dispatch_725::@decode_bs4$async_dispatch_725_gather_4xDx4x32x64xf16_generic[%8, %7](%1472, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %1477 = flow.tensor.reshape %1476 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %1478 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%1475, %1474, %1477, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %1479 = flow.tensor.bitcast %__parameter_model_blk_16_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1480 = flow.tensor.bitcast %__parameter_model_blk_16_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1481 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1480) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1482 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1479) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1483 = flow.tensor.bitcast %1482 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1484 = flow.tensor.bitcast %1481 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1485 = flow.tensor.reshape %1478 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %1486 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%1484, %1483, %1485, %1428) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1487 = flow.tensor.reshape %1486 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1488 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1487) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1489 = flow.tensor.reshape %1488 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1490 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1486) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1491 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1489, %1490) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1492 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_16_ffn_norm_weight_tensor_2048xf32, %1491) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1493 = flow.tensor.reshape %1492 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1494 = flow.tensor.bitcast %__parameter_model_blk_16_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1495 = flow.tensor.bitcast %__parameter_model_blk_16_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1496 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1495) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1497 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1494) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1498 = flow.tensor.bitcast %1497 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1499 = flow.tensor.bitcast %1496 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1500 = flow.tensor.bitcast %__parameter_model_blk_16_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1501 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1500) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1502 = flow.tensor.bitcast %1501 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1503 = flow.tensor.bitcast %__parameter_model_blk_16_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1504 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1503) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1505 = flow.tensor.bitcast %1504 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1506 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%1502, %1505, %1493) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %1507 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%1499, %1498, %1493, %1506) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %1508 = flow.tensor.reshape %1507 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %1509 = flow.tensor.bitcast %__parameter_model_blk_16_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1510 = flow.tensor.bitcast %__parameter_model_blk_16_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1511 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1510) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1512 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1509) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1513 = flow.tensor.bitcast %1512 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1514 = flow.tensor.bitcast %1511 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1515 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%1514, %1513, %1508, %1486) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1516 = flow.tensor.reshape %1515 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1517 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1516) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1518 = flow.tensor.reshape %1517 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1519 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1515) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1520 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1518, %1519) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1521 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_17_attn_norm_weight_tensor_2048xf32, %1520) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1522 = flow.tensor.reshape %1521 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1523 = flow.tensor.bitcast %__parameter_model_blk_17_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1524 = flow.tensor.bitcast %__parameter_model_blk_17_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1525 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1524) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1526 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1523) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1527 = flow.tensor.bitcast %1526 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1528 = flow.tensor.bitcast %1525 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1529 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%1528, %1527, %1522) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %1530 = flow.tensor.reshape %1529 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %1531 = flow.tensor.bitcast %__parameter_model_blk_17_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1532 = flow.tensor.bitcast %__parameter_model_blk_17_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1533 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1532) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1534 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1531) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1535 = flow.tensor.bitcast %1534 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1536 = flow.tensor.bitcast %1533 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1537 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1536, %1535, %1522) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1538 = flow.tensor.reshape %1537 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %1539 = flow.tensor.bitcast %__parameter_model_blk_17_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1540 = flow.tensor.bitcast %__parameter_model_blk_17_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1541 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1540) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1542 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1539) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1543 = flow.tensor.bitcast %1542 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1544 = flow.tensor.bitcast %1541 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1545 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1544, %1543, %1522) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1546 = flow.tensor.reshape %1545 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %1547 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%1530) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1548 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%1530) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1549 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%1547, %59, %1548, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %1550 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%1538) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1551 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%1538) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1552 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%1550, %59, %1551, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %1553 = flow.tensor.reshape %1552 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %1554 = flow.dispatch @decode_bs4$async_dispatch_762::@decode_bs4$async_dispatch_762_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %1555 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%1554, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1556 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1553, %1555, %1471, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1471{%70}
    %1557 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%1554, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1558 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1546, %1557, %1556, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1556{%70}
    %1559 = flow.tensor.reshape %1558 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %1560 = flow.dispatch @decode_bs4$async_dispatch_767::@decode_bs4$async_dispatch_767_gather_4xDx4x32x64xf16_generic[%8, %7](%1559, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %1561 = flow.tensor.reshape %1560 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %1562 = flow.tensor.reshape %1549 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %1563 = flow.dispatch @decode_bs4$async_dispatch_768::@decode_bs4$async_dispatch_768_gather_4xDx4x32x64xf16_generic[%8, %7](%1559, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %1564 = flow.tensor.reshape %1563 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %1565 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%1562, %1561, %1564, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %1566 = flow.tensor.bitcast %__parameter_model_blk_17_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1567 = flow.tensor.bitcast %__parameter_model_blk_17_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1568 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1567) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1569 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1566) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1570 = flow.tensor.bitcast %1569 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1571 = flow.tensor.bitcast %1568 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1572 = flow.tensor.reshape %1565 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %1573 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%1571, %1570, %1572, %1515) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1574 = flow.tensor.reshape %1573 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1575 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1574) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1576 = flow.tensor.reshape %1575 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1577 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1573) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1578 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1576, %1577) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1579 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_17_ffn_norm_weight_tensor_2048xf32, %1578) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1580 = flow.tensor.reshape %1579 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1581 = flow.tensor.bitcast %__parameter_model_blk_17_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1582 = flow.tensor.bitcast %__parameter_model_blk_17_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1583 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1582) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1584 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1581) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1585 = flow.tensor.bitcast %1584 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1586 = flow.tensor.bitcast %1583 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1587 = flow.tensor.bitcast %__parameter_model_blk_17_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1588 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1587) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1589 = flow.tensor.bitcast %1588 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1590 = flow.tensor.bitcast %__parameter_model_blk_17_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1591 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1590) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1592 = flow.tensor.bitcast %1591 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1593 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%1589, %1592, %1580) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %1594 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%1586, %1585, %1580, %1593) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %1595 = flow.tensor.reshape %1594 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %1596 = flow.tensor.bitcast %__parameter_model_blk_17_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1597 = flow.tensor.bitcast %__parameter_model_blk_17_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1598 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1597) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1599 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1596) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1600 = flow.tensor.bitcast %1599 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1601 = flow.tensor.bitcast %1598 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1602 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%1601, %1600, %1595, %1573) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1603 = flow.tensor.reshape %1602 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1604 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1603) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1605 = flow.tensor.reshape %1604 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1606 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1602) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1607 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1605, %1606) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1608 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_18_attn_norm_weight_tensor_2048xf32, %1607) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1609 = flow.tensor.reshape %1608 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1610 = flow.tensor.bitcast %__parameter_model_blk_18_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1611 = flow.tensor.bitcast %__parameter_model_blk_18_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1612 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1611) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1613 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1610) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1614 = flow.tensor.bitcast %1613 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1615 = flow.tensor.bitcast %1612 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1616 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%1615, %1614, %1609) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %1617 = flow.tensor.reshape %1616 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %1618 = flow.tensor.bitcast %__parameter_model_blk_18_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1619 = flow.tensor.bitcast %__parameter_model_blk_18_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1620 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1619) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1621 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1618) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1622 = flow.tensor.bitcast %1621 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1623 = flow.tensor.bitcast %1620 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1624 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1623, %1622, %1609) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1625 = flow.tensor.reshape %1624 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %1626 = flow.tensor.bitcast %__parameter_model_blk_18_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1627 = flow.tensor.bitcast %__parameter_model_blk_18_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1628 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1627) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1629 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1626) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1630 = flow.tensor.bitcast %1629 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1631 = flow.tensor.bitcast %1628 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1632 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1631, %1630, %1609) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1633 = flow.tensor.reshape %1632 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %1634 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%1617) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1635 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%1617) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1636 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%1634, %59, %1635, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %1637 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%1625) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1638 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%1625) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1639 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%1637, %59, %1638, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %1640 = flow.tensor.reshape %1639 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %1641 = flow.dispatch @decode_bs4$async_dispatch_805::@decode_bs4$async_dispatch_805_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %1642 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%1641, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1643 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1640, %1642, %1558, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1558{%70}
    %1644 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%1641, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1645 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1633, %1644, %1643, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1643{%70}
    %1646 = flow.tensor.reshape %1645 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %1647 = flow.dispatch @decode_bs4$async_dispatch_810::@decode_bs4$async_dispatch_810_gather_4xDx4x32x64xf16_generic[%8, %7](%1646, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %1648 = flow.tensor.reshape %1647 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %1649 = flow.tensor.reshape %1636 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %1650 = flow.dispatch @decode_bs4$async_dispatch_811::@decode_bs4$async_dispatch_811_gather_4xDx4x32x64xf16_generic[%8, %7](%1646, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %1651 = flow.tensor.reshape %1650 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %1652 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%1649, %1648, %1651, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %1653 = flow.tensor.bitcast %__parameter_model_blk_18_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1654 = flow.tensor.bitcast %__parameter_model_blk_18_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1655 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1654) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1656 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1653) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1657 = flow.tensor.bitcast %1656 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1658 = flow.tensor.bitcast %1655 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1659 = flow.tensor.reshape %1652 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %1660 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%1658, %1657, %1659, %1602) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1661 = flow.tensor.reshape %1660 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1662 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1661) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1663 = flow.tensor.reshape %1662 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1664 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1660) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1665 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1663, %1664) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1666 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_18_ffn_norm_weight_tensor_2048xf32, %1665) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1667 = flow.tensor.reshape %1666 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1668 = flow.tensor.bitcast %__parameter_model_blk_18_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1669 = flow.tensor.bitcast %__parameter_model_blk_18_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1670 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1669) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1671 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1668) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1672 = flow.tensor.bitcast %1671 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1673 = flow.tensor.bitcast %1670 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1674 = flow.tensor.bitcast %__parameter_model_blk_18_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1675 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1674) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1676 = flow.tensor.bitcast %1675 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1677 = flow.tensor.bitcast %__parameter_model_blk_18_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1678 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1677) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1679 = flow.tensor.bitcast %1678 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1680 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%1676, %1679, %1667) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %1681 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%1673, %1672, %1667, %1680) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %1682 = flow.tensor.reshape %1681 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %1683 = flow.tensor.bitcast %__parameter_model_blk_18_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1684 = flow.tensor.bitcast %__parameter_model_blk_18_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1685 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1684) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1686 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1683) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1687 = flow.tensor.bitcast %1686 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1688 = flow.tensor.bitcast %1685 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1689 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%1688, %1687, %1682, %1660) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1690 = flow.tensor.reshape %1689 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1691 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1690) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1692 = flow.tensor.reshape %1691 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1693 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1689) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1694 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1692, %1693) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1695 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_19_attn_norm_weight_tensor_2048xf32, %1694) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1696 = flow.tensor.reshape %1695 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1697 = flow.tensor.bitcast %__parameter_model_blk_19_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1698 = flow.tensor.bitcast %__parameter_model_blk_19_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1699 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1698) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1700 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1697) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1701 = flow.tensor.bitcast %1700 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1702 = flow.tensor.bitcast %1699 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1703 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%1702, %1701, %1696) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %1704 = flow.tensor.reshape %1703 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %1705 = flow.tensor.bitcast %__parameter_model_blk_19_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1706 = flow.tensor.bitcast %__parameter_model_blk_19_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1707 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1706) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1708 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1705) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1709 = flow.tensor.bitcast %1708 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1710 = flow.tensor.bitcast %1707 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1711 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1710, %1709, %1696) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1712 = flow.tensor.reshape %1711 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %1713 = flow.tensor.bitcast %__parameter_model_blk_19_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1714 = flow.tensor.bitcast %__parameter_model_blk_19_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1715 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1714) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1716 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1713) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1717 = flow.tensor.bitcast %1716 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1718 = flow.tensor.bitcast %1715 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1719 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1718, %1717, %1696) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1720 = flow.tensor.reshape %1719 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %1721 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%1704) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1722 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%1704) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1723 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%1721, %59, %1722, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %1724 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%1712) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1725 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%1712) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1726 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%1724, %59, %1725, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %1727 = flow.tensor.reshape %1726 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %1728 = flow.dispatch @decode_bs4$async_dispatch_848::@decode_bs4$async_dispatch_848_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %1729 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%1728, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1730 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1727, %1729, %1645, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1645{%70}
    %1731 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%1728, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1732 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1720, %1731, %1730, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1730{%70}
    %1733 = flow.tensor.reshape %1732 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %1734 = flow.dispatch @decode_bs4$async_dispatch_853::@decode_bs4$async_dispatch_853_gather_4xDx4x32x64xf16_generic[%8, %7](%1733, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %1735 = flow.tensor.reshape %1734 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %1736 = flow.tensor.reshape %1723 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %1737 = flow.dispatch @decode_bs4$async_dispatch_854::@decode_bs4$async_dispatch_854_gather_4xDx4x32x64xf16_generic[%8, %7](%1733, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %1738 = flow.tensor.reshape %1737 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %1739 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%1736, %1735, %1738, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %1740 = flow.tensor.bitcast %__parameter_model_blk_19_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1741 = flow.tensor.bitcast %__parameter_model_blk_19_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1742 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1741) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1743 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1740) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1744 = flow.tensor.bitcast %1743 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1745 = flow.tensor.bitcast %1742 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1746 = flow.tensor.reshape %1739 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %1747 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%1745, %1744, %1746, %1689) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1748 = flow.tensor.reshape %1747 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1749 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1748) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1750 = flow.tensor.reshape %1749 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1751 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1747) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1752 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1750, %1751) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1753 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_19_ffn_norm_weight_tensor_2048xf32, %1752) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1754 = flow.tensor.reshape %1753 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1755 = flow.tensor.bitcast %__parameter_model_blk_19_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1756 = flow.tensor.bitcast %__parameter_model_blk_19_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1757 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1756) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1758 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1755) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1759 = flow.tensor.bitcast %1758 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1760 = flow.tensor.bitcast %1757 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1761 = flow.tensor.bitcast %__parameter_model_blk_19_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1762 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1761) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1763 = flow.tensor.bitcast %1762 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1764 = flow.tensor.bitcast %__parameter_model_blk_19_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1765 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1764) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1766 = flow.tensor.bitcast %1765 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1767 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%1763, %1766, %1754) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %1768 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%1760, %1759, %1754, %1767) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %1769 = flow.tensor.reshape %1768 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %1770 = flow.tensor.bitcast %__parameter_model_blk_19_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1771 = flow.tensor.bitcast %__parameter_model_blk_19_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1772 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1771) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1773 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1770) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1774 = flow.tensor.bitcast %1773 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1775 = flow.tensor.bitcast %1772 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1776 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%1775, %1774, %1769, %1747) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1777 = flow.tensor.reshape %1776 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1778 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1777) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1779 = flow.tensor.reshape %1778 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1780 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1776) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1781 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1779, %1780) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1782 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_20_attn_norm_weight_tensor_2048xf32, %1781) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1783 = flow.tensor.reshape %1782 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1784 = flow.tensor.bitcast %__parameter_model_blk_20_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1785 = flow.tensor.bitcast %__parameter_model_blk_20_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1786 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1785) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1787 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1784) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1788 = flow.tensor.bitcast %1787 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1789 = flow.tensor.bitcast %1786 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1790 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%1789, %1788, %1783) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %1791 = flow.tensor.reshape %1790 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %1792 = flow.tensor.bitcast %__parameter_model_blk_20_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1793 = flow.tensor.bitcast %__parameter_model_blk_20_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1794 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1793) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1795 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1792) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1796 = flow.tensor.bitcast %1795 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1797 = flow.tensor.bitcast %1794 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1798 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1797, %1796, %1783) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1799 = flow.tensor.reshape %1798 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %1800 = flow.tensor.bitcast %__parameter_model_blk_20_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1801 = flow.tensor.bitcast %__parameter_model_blk_20_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1802 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1801) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1803 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1800) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1804 = flow.tensor.bitcast %1803 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1805 = flow.tensor.bitcast %1802 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1806 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1805, %1804, %1783) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1807 = flow.tensor.reshape %1806 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %1808 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%1791) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1809 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%1791) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1810 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%1808, %59, %1809, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %1811 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%1799) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1812 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%1799) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1813 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%1811, %59, %1812, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %1814 = flow.tensor.reshape %1813 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %1815 = flow.dispatch @decode_bs4$async_dispatch_891::@decode_bs4$async_dispatch_891_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %1816 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%1815, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1817 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1814, %1816, %1732, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1732{%70}
    %1818 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%1815, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1819 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1807, %1818, %1817, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1817{%70}
    %1820 = flow.tensor.reshape %1819 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %1821 = flow.dispatch @decode_bs4$async_dispatch_896::@decode_bs4$async_dispatch_896_gather_4xDx4x32x64xf16_generic[%8, %7](%1820, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %1822 = flow.tensor.reshape %1821 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %1823 = flow.tensor.reshape %1810 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %1824 = flow.dispatch @decode_bs4$async_dispatch_897::@decode_bs4$async_dispatch_897_gather_4xDx4x32x64xf16_generic[%8, %7](%1820, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %1825 = flow.tensor.reshape %1824 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %1826 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%1823, %1822, %1825, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %1827 = flow.tensor.bitcast %__parameter_model_blk_20_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1828 = flow.tensor.bitcast %__parameter_model_blk_20_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1829 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1828) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1830 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1827) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1831 = flow.tensor.bitcast %1830 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1832 = flow.tensor.bitcast %1829 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1833 = flow.tensor.reshape %1826 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %1834 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%1832, %1831, %1833, %1776) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1835 = flow.tensor.reshape %1834 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1836 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1835) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1837 = flow.tensor.reshape %1836 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1838 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1834) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1839 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1837, %1838) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1840 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_20_ffn_norm_weight_tensor_2048xf32, %1839) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1841 = flow.tensor.reshape %1840 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1842 = flow.tensor.bitcast %__parameter_model_blk_20_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1843 = flow.tensor.bitcast %__parameter_model_blk_20_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1844 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1843) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1845 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1842) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1846 = flow.tensor.bitcast %1845 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1847 = flow.tensor.bitcast %1844 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1848 = flow.tensor.bitcast %__parameter_model_blk_20_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1849 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1848) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1850 = flow.tensor.bitcast %1849 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1851 = flow.tensor.bitcast %__parameter_model_blk_20_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1852 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1851) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1853 = flow.tensor.bitcast %1852 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1854 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%1850, %1853, %1841) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %1855 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%1847, %1846, %1841, %1854) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %1856 = flow.tensor.reshape %1855 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %1857 = flow.tensor.bitcast %__parameter_model_blk_20_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1858 = flow.tensor.bitcast %__parameter_model_blk_20_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1859 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1858) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1860 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1857) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1861 = flow.tensor.bitcast %1860 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1862 = flow.tensor.bitcast %1859 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1863 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%1862, %1861, %1856, %1834) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1864 = flow.tensor.reshape %1863 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1865 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1864) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1866 = flow.tensor.reshape %1865 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1867 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1863) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1868 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1866, %1867) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1869 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_21_attn_norm_weight_tensor_2048xf32, %1868) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1870 = flow.tensor.reshape %1869 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1871 = flow.tensor.bitcast %__parameter_model_blk_21_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1872 = flow.tensor.bitcast %__parameter_model_blk_21_attn_q_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1873 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1872) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1874 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1871) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1875 = flow.tensor.bitcast %1874 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1876 = flow.tensor.bitcast %1873 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1877 = flow.dispatch @decode_bs4$async_dispatch_10::@decode_bs4$async_dispatch_10_matmul_like_4x2048x64x32_f16xf16xf32(%1876, %1875, %1870) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x2048xf16>
    %1878 = flow.tensor.reshape %1877 : tensor<4x2048xf16> -> tensor<4x1x32x64xf16>
    %1879 = flow.tensor.bitcast %__parameter_model_blk_21_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1880 = flow.tensor.bitcast %__parameter_model_blk_21_attn_k_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1881 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1880) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1882 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1879) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1883 = flow.tensor.bitcast %1882 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1884 = flow.tensor.bitcast %1881 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1885 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1884, %1883, %1870) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1886 = flow.tensor.reshape %1885 : tensor<4x256xf16> -> tensor<4x1x4x64xf16>
    %1887 = flow.tensor.bitcast %__parameter_model_blk_21_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x17xi16>
    %1888 = flow.tensor.bitcast %__parameter_model_blk_21_attn_v_weight_tensor_256x2176xi8 : tensor<256x2176xi8> -> tensor<256x64x1x17xi16>
    %1889 = flow.dispatch @prefill_bs4$async_dispatch_11::@prefill_bs4$async_dispatch_11_slow_memcpy(%1888) : (tensor<256x64x1x17xi16>) -> tensor<256x64x1xi16>
    %1890 = flow.dispatch @prefill_bs4$async_dispatch_12::@prefill_bs4$async_dispatch_12_slow_memcpy(%1887) : (tensor<256x64x17xi16>) -> tensor<256x64x16xi16>
    %1891 = flow.tensor.bitcast %1890 : tensor<256x64x16xi16> -> tensor<256x64x32xi8>
    %1892 = flow.tensor.bitcast %1889 : tensor<256x64x1xi16> -> tensor<256x64xf16>
    %1893 = flow.dispatch @decode_bs4$async_dispatch_13::@decode_bs4$async_dispatch_13_matmul_like_4x256x64x32_f16xf16xf32(%1892, %1891, %1870) : (tensor<256x64xf16>, tensor<256x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x256xf16>
    %1894 = flow.tensor.reshape %1893 : tensor<4x256xf16> -> tensor<4x4x2x32x1xf16>
    %1895 = flow.dispatch @decode_bs4$async_dispatch_22::@decode_bs4$async_dispatch_22_slow_memcpy(%1878) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1896 = flow.dispatch @decode_bs4$async_dispatch_23::@decode_bs4$async_dispatch_23_slow_memcpy(%1878) : (tensor<4x1x32x64xf16>) -> tensor<4x32x32xf16>
    %1897 = flow.dispatch @decode_bs4$async_dispatch_24::@decode_bs4$async_dispatch_24_elementwise_broadcast_4x32x2x32_f16(%1895, %59, %1896, %60) : (tensor<4x32x32xf16>, tensor<4x32xf16>, tensor<4x32x32xf16>, tensor<4x32xf16>) -> tensor<4x32x2x32xf16>
    %1898 = flow.dispatch @decode_bs4$async_dispatch_25::@decode_bs4$async_dispatch_25_slow_memcpy(%1886) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1899 = flow.dispatch @decode_bs4$async_dispatch_26::@decode_bs4$async_dispatch_26_slow_memcpy(%1886) : (tensor<4x1x4x64xf16>) -> tensor<4x4x32xf16>
    %1900 = flow.dispatch @decode_bs4$async_dispatch_27::@decode_bs4$async_dispatch_27_elementwise_broadcast_4x4x2x32_f16(%1898, %59, %1899, %60) : (tensor<4x4x32xf16>, tensor<4x32xf16>, tensor<4x4x32xf16>, tensor<4x32xf16>) -> tensor<4x4x2x32xf16>
    %1901 = flow.tensor.reshape %1900 : tensor<4x4x2x32xf16> -> tensor<4x4x2x32x1xf16>
    %1902 = flow.dispatch @decode_bs4$async_dispatch_934::@decode_bs4$async_dispatch_934_elementwise_4_i64[%7](%4, %66, %7) : (tensor<4x?xi64>{%7}, tensor<4xi64>, index) -> tensor<4xi64>
    %1903 = flow.dispatch @decode_bs4$async_dispatch_31::@decode_bs4$async_dispatch_31_elementwise_broadcast_4x4_i64(%1902, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1904 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1901, %1903, %1819, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1819{%70}
    %1905 = flow.dispatch @decode_bs4$async_dispatch_33::@decode_bs4$async_dispatch_33_elementwise_broadcast_4x4_i64(%1902, %67) : (tensor<4xi64>, tensor<4xi64>) -> tensor<4x4xi64>
    %1906 = flow.dispatch @decode_bs4$async_dispatch_32::@decode_bs4$async_dispatch_32_scatter_Dx2x32x1xf16_dispatch_tensor_store[%70](%1894, %1905, %1904, %70) : (tensor<4x4x2x32x1xf16>, tensor<4x4xi64>, tensor<?x2x32x1xf16>{%70}, index) -> %1904{%70}
    %1907 = flow.tensor.reshape %1906 : tensor<?x2x32x1xf16>{%70} -> tensor<?x22x2x4x32x64xf16>{%8}
    %1908 = flow.tensor.reshape %1906 : tensor<?x2x32x1xf16>{%70} -> tensor<?x360448xf16>{%8}
    %1909 = flow.dispatch @decode_bs4$async_dispatch_939::@decode_bs4$async_dispatch_939_gather_4xDx4x32x64xf16_generic[%8, %7](%1907, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x?x32x64xf16>{%7}
    %1910 = flow.tensor.reshape %1909 : tensor<4x4x8x?x32x64xf16>{%7} -> tensor<128x?x64xf16>{%80}
    %1911 = flow.tensor.reshape %1897 : tensor<4x32x2x32xf16> -> tensor<128x64xf16>
    %1912 = flow.dispatch @decode_bs4$async_dispatch_940::@decode_bs4$async_dispatch_940_gather_4xDx4x32x64xf16_generic[%8, %7](%1907, %8, %4, %7) : (tensor<?x22x2x4x32x64xf16>{%8}, index, tensor<4x?xi64>{%7}, index) -> tensor<4x4x8x64x?x32xf16>{%7}
    %1913 = flow.tensor.reshape %1912 : tensor<4x4x8x64x?x32xf16>{%7} -> tensor<128x64x?xf16>{%80}
    %1914 = flow.dispatch @decode_bs4$async_dispatch_38::@decode_bs4$async_dispatch_38_attention_128xDx64xf16_dispatch_tensor_store[%80, %76](%1911, %1910, %1913, %85, %80, %76) : (tensor<128x64xf16>, tensor<128x?x64xf16>{%80}, tensor<128x64x?xf16>{%80}, tensor<128x?xf16>{%76}, index, index) -> tensor<128x64xf16>
    %1915 = flow.tensor.bitcast %__parameter_model_blk_21_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x17xi16>
    %1916 = flow.tensor.bitcast %__parameter_model_blk_21_attn_output_weight_tensor_2048x2176xi8 : tensor<2048x2176xi8> -> tensor<2048x64x1x17xi16>
    %1917 = flow.dispatch @prefill_bs4$async_dispatch_8::@prefill_bs4$async_dispatch_8_slow_memcpy(%1916) : (tensor<2048x64x1x17xi16>) -> tensor<2048x64x1xi16>
    %1918 = flow.dispatch @prefill_bs4$async_dispatch_9::@prefill_bs4$async_dispatch_9_slow_memcpy(%1915) : (tensor<2048x64x17xi16>) -> tensor<2048x64x16xi16>
    %1919 = flow.tensor.bitcast %1918 : tensor<2048x64x16xi16> -> tensor<2048x64x32xi8>
    %1920 = flow.tensor.bitcast %1917 : tensor<2048x64x1xi16> -> tensor<2048x64xf16>
    %1921 = flow.tensor.reshape %1914 : tensor<128x64xf16> -> tensor<4x64x32xf16>
    %1922 = flow.dispatch @decode_bs4$async_dispatch_41::@decode_bs4$async_dispatch_41_matmul_like_4x2048x64x32_f16xf16xf32(%1920, %1919, %1921, %1863) : (tensor<2048x64xf16>, tensor<2048x64x32xi8>, tensor<4x64x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1923 = flow.tensor.reshape %1922 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1924 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1923) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1925 = flow.tensor.reshape %1924 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1926 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1922) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1927 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1925, %1926) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1928 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_blk_21_ffn_norm_weight_tensor_2048xf32, %1927) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1929 = flow.tensor.reshape %1928 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1930 = flow.tensor.bitcast %__parameter_model_blk_21_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1931 = flow.tensor.bitcast %__parameter_model_blk_21_ffn_gate_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1932 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1931) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1933 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1930) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1934 = flow.tensor.bitcast %1933 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1935 = flow.tensor.bitcast %1932 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1936 = flow.tensor.bitcast %__parameter_model_blk_21_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x1x17xi16>
    %1937 = flow.dispatch @prefill_bs4$async_dispatch_43::@prefill_bs4$async_dispatch_43_slow_memcpy(%1936) : (tensor<5632x64x1x17xi16>) -> tensor<5632x64x1xi16>
    %1938 = flow.tensor.bitcast %1937 : tensor<5632x64x1xi16> -> tensor<5632x64xf16>
    %1939 = flow.tensor.bitcast %__parameter_model_blk_21_ffn_up_weight_tensor_5632x2176xi8 : tensor<5632x2176xi8> -> tensor<5632x64x17xi16>
    %1940 = flow.dispatch @prefill_bs4$async_dispatch_44::@prefill_bs4$async_dispatch_44_slow_memcpy(%1939) : (tensor<5632x64x17xi16>) -> tensor<5632x64x16xi16>
    %1941 = flow.tensor.bitcast %1940 : tensor<5632x64x16xi16> -> tensor<5632x64x32xi8>
    %1942 = flow.dispatch @decode_bs4$async_dispatch_50::@decode_bs4$async_dispatch_50_matmul_like_4x5632x64x32_f16xf16xf32(%1938, %1941, %1929) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x5632xf16>
    %1943 = flow.dispatch @decode_bs4$async_dispatch_51::@decode_bs4$async_dispatch_51_matmul_like_4x5632x64x32_f16xf16xf32(%1935, %1934, %1929, %1942) : (tensor<5632x64xf16>, tensor<5632x64x32xi8>, tensor<4x64x32xf16>, tensor<4x5632xf16>) -> tensor<4x5632xf16>
    %1944 = flow.tensor.reshape %1943 : tensor<4x5632xf16> -> tensor<4x176x32xf16>
    %1945 = flow.tensor.bitcast %__parameter_model_blk_21_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x17xi16>
    %1946 = flow.tensor.bitcast %__parameter_model_blk_21_ffn_down_weight_tensor_2048x5984xi8 : tensor<2048x5984xi8> -> tensor<2048x176x1x17xi16>
    %1947 = flow.dispatch @prefill_bs4$async_dispatch_49::@prefill_bs4$async_dispatch_49_slow_memcpy(%1946) : (tensor<2048x176x1x17xi16>) -> tensor<2048x176x1xi16>
    %1948 = flow.dispatch @prefill_bs4$async_dispatch_50::@prefill_bs4$async_dispatch_50_slow_memcpy(%1945) : (tensor<2048x176x17xi16>) -> tensor<2048x176x16xi16>
    %1949 = flow.tensor.bitcast %1948 : tensor<2048x176x16xi16> -> tensor<2048x176x32xi8>
    %1950 = flow.tensor.bitcast %1947 : tensor<2048x176x1xi16> -> tensor<2048x176xf16>
    %1951 = flow.dispatch @decode_bs4$async_dispatch_54::@decode_bs4$async_dispatch_54_matmul_like_4x2048x176x32_f16xf16xf32(%1950, %1949, %1944, %1922) : (tensor<2048x176xf16>, tensor<2048x176x32xi8>, tensor<4x176x32xf16>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1952 = flow.tensor.reshape %1951 : tensor<4x2048xf16> -> tensor<8192xf16>
    %1953 = flow.dispatch @decode_bs4$async_dispatch_4::@decode_bs4$async_dispatch_4_elementwise_8192_f16xf32(%1952) : (tensor<8192xf16>) -> tensor<8192xf32>
    %1954 = flow.tensor.reshape %1953 : tensor<8192xf32> -> tensor<4x2048xf32>
    %1955 = flow.dispatch @decode_bs4$async_dispatch_5::@decode_bs4$async_dispatch_5_reduction_4x2048_f32(%1951) : (tensor<4x2048xf16>) -> tensor<4xf32>
    %1956 = flow.dispatch @decode_bs4$async_dispatch_6::@decode_bs4$async_dispatch_6_elementwise_4x2048_f32xf32xf16(%1954, %1955) : (tensor<4x2048xf32>, tensor<4xf32>) -> tensor<4x2048xf16>
    %1957 = flow.dispatch @decode_bs4$async_dispatch_7::@decode_bs4$async_dispatch_7_elementwise_4x2048_f32xf16xf16(%__parameter_model_output_norm_weight_tensor_2048xf32, %1956) : (tensor<2048xf32>, tensor<4x2048xf16>) -> tensor<4x2048xf16>
    %1958 = flow.tensor.reshape %1957 : tensor<4x2048xf16> -> tensor<4x64x32xf16>
    %1959 = flow.tensor.bitcast %__parameter_model_output_weight_tensor_32000x2176xi8 : tensor<32000x2176xi8> -> tensor<32000x64x17xi16>
    %1960 = flow.tensor.bitcast %__parameter_model_output_weight_tensor_32000x2176xi8 : tensor<32000x2176xi8> -> tensor<32000x64x1x17xi16>
    %1961 = flow.dispatch @prefill_bs4$async_dispatch_0::@prefill_bs4$async_dispatch_0_slow_memcpy(%1960) : (tensor<32000x64x1x17xi16>) -> tensor<32000x64x1xi16>
    %1962 = flow.dispatch @prefill_bs4$async_dispatch_1::@prefill_bs4$async_dispatch_1_slow_memcpy(%1959) : (tensor<32000x64x17xi16>) -> tensor<32000x64x16xi16>
    %1963 = flow.tensor.bitcast %1962 : tensor<32000x64x16xi16> -> tensor<32000x64x32xi8>
    %1964 = flow.tensor.bitcast %1961 : tensor<32000x64x1xi16> -> tensor<32000x64xf16>
    %1965 = flow.dispatch @decode_bs4$async_dispatch_964::@decode_bs4$async_dispatch_964_matmul_like_4x32000x64x32_f16xf16xf32(%1964, %1963, %1958) : (tensor<32000x64xf16>, tensor<32000x64x32xi8>, tensor<4x64x32xf16>) -> tensor<4x32000xf16>
    %1966 = flow.tensor.reshape %1965 : tensor<4x32000xf16> -> tensor<4x1x32000xf16>
    %1967 = hal.tensor.alias on(#hal.device.affinity<@__device_0>) wait(%arg5) => %1908 : tensor<?x360448xf16>{%8} to %arg4 : !hal.buffer_view
    %1968:2 = hal.tensor.barrier join(%1967, %1966 : tensor<?x360448xf16>, tensor<4x1x32000xf16>) => %arg6 : !hal.fence
    %1969 = hal.tensor.export %1968#1 : tensor<4x1x32000xf16> -> !hal.buffer_view
    util.return %1969 : !hal.buffer_view
  }
  util.func public @decode_bs4(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view, %arg3: !hal.buffer_view, %arg4: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %0 = util.null : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %1 = util.call @decode_bs4$async(%arg0, %arg1, %arg2, %arg3, %arg4, %0, %fence) : (!hal.buffer_view, !hal.buffer_view, !hal.buffer_view, !hal.buffer_view, !hal.buffer_view, !hal.fence, !hal.fence) -> !hal.buffer_view
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) flags("None") : i32
    util.return %1 : !hal.buffer_view
  }
}
