#map = affine_map<(d0, d1, d2) -> (d0, d1, 0)>
#map1 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map2 = affine_map<(d0, d1, d2, d3, d4) -> (d0, d1, d3, d4)>
#map3 = affine_map<(d0, d1, d2, d3, d4) -> (d2, d3, d4)>
#map4 = affine_map<(d0, d1, d2, d3, d4) -> (d0, d1, d2)>
#map5 = affine_map<(d0, d1, d2, d3, d4) -> (d0, d1, d2, d4)>
#map6 = affine_map<(d0, d1, d2, d3, d4) -> (d0, d1, d2, d3, d4)>
module @module {
  util.global private @__auto.token_embd.weight = #flow.parameter.named<"model"::"token_embd.weight"> : tensor<32000x2176xi8>
  util.global private @__auto.blk.0.attn_norm.weight = #flow.parameter.named<"model"::"blk.0.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.0.attn_q.weight = #flow.parameter.named<"model"::"blk.0.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.0.attn_k.weight = #flow.parameter.named<"model"::"blk.0.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.0.attn_v.weight = #flow.parameter.named<"model"::"blk.0.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.0.attn_output.weight = #flow.parameter.named<"model"::"blk.0.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.0.ffn_norm.weight = #flow.parameter.named<"model"::"blk.0.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.0.ffn_gate.weight = #flow.parameter.named<"model"::"blk.0.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.0.ffn_up.weight = #flow.parameter.named<"model"::"blk.0.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.0.ffn_down.weight = #flow.parameter.named<"model"::"blk.0.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.1.attn_norm.weight = #flow.parameter.named<"model"::"blk.1.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.1.attn_q.weight = #flow.parameter.named<"model"::"blk.1.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.1.attn_k.weight = #flow.parameter.named<"model"::"blk.1.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.1.attn_v.weight = #flow.parameter.named<"model"::"blk.1.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.1.attn_output.weight = #flow.parameter.named<"model"::"blk.1.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.1.ffn_norm.weight = #flow.parameter.named<"model"::"blk.1.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.1.ffn_gate.weight = #flow.parameter.named<"model"::"blk.1.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.1.ffn_up.weight = #flow.parameter.named<"model"::"blk.1.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.1.ffn_down.weight = #flow.parameter.named<"model"::"blk.1.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.2.attn_norm.weight = #flow.parameter.named<"model"::"blk.2.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.2.attn_q.weight = #flow.parameter.named<"model"::"blk.2.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.2.attn_k.weight = #flow.parameter.named<"model"::"blk.2.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.2.attn_v.weight = #flow.parameter.named<"model"::"blk.2.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.2.attn_output.weight = #flow.parameter.named<"model"::"blk.2.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.2.ffn_norm.weight = #flow.parameter.named<"model"::"blk.2.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.2.ffn_gate.weight = #flow.parameter.named<"model"::"blk.2.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.2.ffn_up.weight = #flow.parameter.named<"model"::"blk.2.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.2.ffn_down.weight = #flow.parameter.named<"model"::"blk.2.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.3.attn_norm.weight = #flow.parameter.named<"model"::"blk.3.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.3.attn_q.weight = #flow.parameter.named<"model"::"blk.3.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.3.attn_k.weight = #flow.parameter.named<"model"::"blk.3.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.3.attn_v.weight = #flow.parameter.named<"model"::"blk.3.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.3.attn_output.weight = #flow.parameter.named<"model"::"blk.3.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.3.ffn_norm.weight = #flow.parameter.named<"model"::"blk.3.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.3.ffn_gate.weight = #flow.parameter.named<"model"::"blk.3.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.3.ffn_up.weight = #flow.parameter.named<"model"::"blk.3.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.3.ffn_down.weight = #flow.parameter.named<"model"::"blk.3.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.4.attn_norm.weight = #flow.parameter.named<"model"::"blk.4.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.4.attn_q.weight = #flow.parameter.named<"model"::"blk.4.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.4.attn_k.weight = #flow.parameter.named<"model"::"blk.4.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.4.attn_v.weight = #flow.parameter.named<"model"::"blk.4.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.4.attn_output.weight = #flow.parameter.named<"model"::"blk.4.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.4.ffn_norm.weight = #flow.parameter.named<"model"::"blk.4.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.4.ffn_gate.weight = #flow.parameter.named<"model"::"blk.4.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.4.ffn_up.weight = #flow.parameter.named<"model"::"blk.4.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.4.ffn_down.weight = #flow.parameter.named<"model"::"blk.4.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.5.attn_norm.weight = #flow.parameter.named<"model"::"blk.5.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.5.attn_q.weight = #flow.parameter.named<"model"::"blk.5.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.5.attn_k.weight = #flow.parameter.named<"model"::"blk.5.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.5.attn_v.weight = #flow.parameter.named<"model"::"blk.5.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.5.attn_output.weight = #flow.parameter.named<"model"::"blk.5.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.5.ffn_norm.weight = #flow.parameter.named<"model"::"blk.5.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.5.ffn_gate.weight = #flow.parameter.named<"model"::"blk.5.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.5.ffn_up.weight = #flow.parameter.named<"model"::"blk.5.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.5.ffn_down.weight = #flow.parameter.named<"model"::"blk.5.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.6.attn_norm.weight = #flow.parameter.named<"model"::"blk.6.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.6.attn_q.weight = #flow.parameter.named<"model"::"blk.6.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.6.attn_k.weight = #flow.parameter.named<"model"::"blk.6.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.6.attn_v.weight = #flow.parameter.named<"model"::"blk.6.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.6.attn_output.weight = #flow.parameter.named<"model"::"blk.6.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.6.ffn_norm.weight = #flow.parameter.named<"model"::"blk.6.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.6.ffn_gate.weight = #flow.parameter.named<"model"::"blk.6.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.6.ffn_up.weight = #flow.parameter.named<"model"::"blk.6.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.6.ffn_down.weight = #flow.parameter.named<"model"::"blk.6.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.7.attn_norm.weight = #flow.parameter.named<"model"::"blk.7.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.7.attn_q.weight = #flow.parameter.named<"model"::"blk.7.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.7.attn_k.weight = #flow.parameter.named<"model"::"blk.7.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.7.attn_v.weight = #flow.parameter.named<"model"::"blk.7.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.7.attn_output.weight = #flow.parameter.named<"model"::"blk.7.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.7.ffn_norm.weight = #flow.parameter.named<"model"::"blk.7.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.7.ffn_gate.weight = #flow.parameter.named<"model"::"blk.7.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.7.ffn_up.weight = #flow.parameter.named<"model"::"blk.7.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.7.ffn_down.weight = #flow.parameter.named<"model"::"blk.7.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.8.attn_norm.weight = #flow.parameter.named<"model"::"blk.8.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.8.attn_q.weight = #flow.parameter.named<"model"::"blk.8.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.8.attn_k.weight = #flow.parameter.named<"model"::"blk.8.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.8.attn_v.weight = #flow.parameter.named<"model"::"blk.8.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.8.attn_output.weight = #flow.parameter.named<"model"::"blk.8.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.8.ffn_norm.weight = #flow.parameter.named<"model"::"blk.8.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.8.ffn_gate.weight = #flow.parameter.named<"model"::"blk.8.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.8.ffn_up.weight = #flow.parameter.named<"model"::"blk.8.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.8.ffn_down.weight = #flow.parameter.named<"model"::"blk.8.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.9.attn_norm.weight = #flow.parameter.named<"model"::"blk.9.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.9.attn_q.weight = #flow.parameter.named<"model"::"blk.9.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.9.attn_k.weight = #flow.parameter.named<"model"::"blk.9.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.9.attn_v.weight = #flow.parameter.named<"model"::"blk.9.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.9.attn_output.weight = #flow.parameter.named<"model"::"blk.9.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.9.ffn_norm.weight = #flow.parameter.named<"model"::"blk.9.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.9.ffn_gate.weight = #flow.parameter.named<"model"::"blk.9.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.9.ffn_up.weight = #flow.parameter.named<"model"::"blk.9.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.9.ffn_down.weight = #flow.parameter.named<"model"::"blk.9.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.10.attn_norm.weight = #flow.parameter.named<"model"::"blk.10.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.10.attn_q.weight = #flow.parameter.named<"model"::"blk.10.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.10.attn_k.weight = #flow.parameter.named<"model"::"blk.10.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.10.attn_v.weight = #flow.parameter.named<"model"::"blk.10.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.10.attn_output.weight = #flow.parameter.named<"model"::"blk.10.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.10.ffn_norm.weight = #flow.parameter.named<"model"::"blk.10.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.10.ffn_gate.weight = #flow.parameter.named<"model"::"blk.10.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.10.ffn_up.weight = #flow.parameter.named<"model"::"blk.10.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.10.ffn_down.weight = #flow.parameter.named<"model"::"blk.10.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.11.attn_norm.weight = #flow.parameter.named<"model"::"blk.11.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.11.attn_q.weight = #flow.parameter.named<"model"::"blk.11.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.11.attn_k.weight = #flow.parameter.named<"model"::"blk.11.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.11.attn_v.weight = #flow.parameter.named<"model"::"blk.11.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.11.attn_output.weight = #flow.parameter.named<"model"::"blk.11.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.11.ffn_norm.weight = #flow.parameter.named<"model"::"blk.11.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.11.ffn_gate.weight = #flow.parameter.named<"model"::"blk.11.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.11.ffn_up.weight = #flow.parameter.named<"model"::"blk.11.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.11.ffn_down.weight = #flow.parameter.named<"model"::"blk.11.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.12.attn_norm.weight = #flow.parameter.named<"model"::"blk.12.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.12.attn_q.weight = #flow.parameter.named<"model"::"blk.12.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.12.attn_k.weight = #flow.parameter.named<"model"::"blk.12.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.12.attn_v.weight = #flow.parameter.named<"model"::"blk.12.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.12.attn_output.weight = #flow.parameter.named<"model"::"blk.12.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.12.ffn_norm.weight = #flow.parameter.named<"model"::"blk.12.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.12.ffn_gate.weight = #flow.parameter.named<"model"::"blk.12.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.12.ffn_up.weight = #flow.parameter.named<"model"::"blk.12.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.12.ffn_down.weight = #flow.parameter.named<"model"::"blk.12.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.13.attn_norm.weight = #flow.parameter.named<"model"::"blk.13.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.13.attn_q.weight = #flow.parameter.named<"model"::"blk.13.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.13.attn_k.weight = #flow.parameter.named<"model"::"blk.13.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.13.attn_v.weight = #flow.parameter.named<"model"::"blk.13.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.13.attn_output.weight = #flow.parameter.named<"model"::"blk.13.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.13.ffn_norm.weight = #flow.parameter.named<"model"::"blk.13.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.13.ffn_gate.weight = #flow.parameter.named<"model"::"blk.13.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.13.ffn_up.weight = #flow.parameter.named<"model"::"blk.13.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.13.ffn_down.weight = #flow.parameter.named<"model"::"blk.13.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.14.attn_norm.weight = #flow.parameter.named<"model"::"blk.14.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.14.attn_q.weight = #flow.parameter.named<"model"::"blk.14.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.14.attn_k.weight = #flow.parameter.named<"model"::"blk.14.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.14.attn_v.weight = #flow.parameter.named<"model"::"blk.14.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.14.attn_output.weight = #flow.parameter.named<"model"::"blk.14.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.14.ffn_norm.weight = #flow.parameter.named<"model"::"blk.14.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.14.ffn_gate.weight = #flow.parameter.named<"model"::"blk.14.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.14.ffn_up.weight = #flow.parameter.named<"model"::"blk.14.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.14.ffn_down.weight = #flow.parameter.named<"model"::"blk.14.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.15.attn_norm.weight = #flow.parameter.named<"model"::"blk.15.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.15.attn_q.weight = #flow.parameter.named<"model"::"blk.15.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.15.attn_k.weight = #flow.parameter.named<"model"::"blk.15.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.15.attn_v.weight = #flow.parameter.named<"model"::"blk.15.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.15.attn_output.weight = #flow.parameter.named<"model"::"blk.15.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.15.ffn_norm.weight = #flow.parameter.named<"model"::"blk.15.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.15.ffn_gate.weight = #flow.parameter.named<"model"::"blk.15.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.15.ffn_up.weight = #flow.parameter.named<"model"::"blk.15.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.15.ffn_down.weight = #flow.parameter.named<"model"::"blk.15.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.16.attn_norm.weight = #flow.parameter.named<"model"::"blk.16.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.16.attn_q.weight = #flow.parameter.named<"model"::"blk.16.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.16.attn_k.weight = #flow.parameter.named<"model"::"blk.16.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.16.attn_v.weight = #flow.parameter.named<"model"::"blk.16.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.16.attn_output.weight = #flow.parameter.named<"model"::"blk.16.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.16.ffn_norm.weight = #flow.parameter.named<"model"::"blk.16.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.16.ffn_gate.weight = #flow.parameter.named<"model"::"blk.16.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.16.ffn_up.weight = #flow.parameter.named<"model"::"blk.16.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.16.ffn_down.weight = #flow.parameter.named<"model"::"blk.16.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.17.attn_norm.weight = #flow.parameter.named<"model"::"blk.17.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.17.attn_q.weight = #flow.parameter.named<"model"::"blk.17.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.17.attn_k.weight = #flow.parameter.named<"model"::"blk.17.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.17.attn_v.weight = #flow.parameter.named<"model"::"blk.17.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.17.attn_output.weight = #flow.parameter.named<"model"::"blk.17.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.17.ffn_norm.weight = #flow.parameter.named<"model"::"blk.17.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.17.ffn_gate.weight = #flow.parameter.named<"model"::"blk.17.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.17.ffn_up.weight = #flow.parameter.named<"model"::"blk.17.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.17.ffn_down.weight = #flow.parameter.named<"model"::"blk.17.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.18.attn_norm.weight = #flow.parameter.named<"model"::"blk.18.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.18.attn_q.weight = #flow.parameter.named<"model"::"blk.18.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.18.attn_k.weight = #flow.parameter.named<"model"::"blk.18.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.18.attn_v.weight = #flow.parameter.named<"model"::"blk.18.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.18.attn_output.weight = #flow.parameter.named<"model"::"blk.18.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.18.ffn_norm.weight = #flow.parameter.named<"model"::"blk.18.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.18.ffn_gate.weight = #flow.parameter.named<"model"::"blk.18.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.18.ffn_up.weight = #flow.parameter.named<"model"::"blk.18.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.18.ffn_down.weight = #flow.parameter.named<"model"::"blk.18.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.19.attn_norm.weight = #flow.parameter.named<"model"::"blk.19.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.19.attn_q.weight = #flow.parameter.named<"model"::"blk.19.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.19.attn_k.weight = #flow.parameter.named<"model"::"blk.19.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.19.attn_v.weight = #flow.parameter.named<"model"::"blk.19.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.19.attn_output.weight = #flow.parameter.named<"model"::"blk.19.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.19.ffn_norm.weight = #flow.parameter.named<"model"::"blk.19.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.19.ffn_gate.weight = #flow.parameter.named<"model"::"blk.19.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.19.ffn_up.weight = #flow.parameter.named<"model"::"blk.19.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.19.ffn_down.weight = #flow.parameter.named<"model"::"blk.19.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.20.attn_norm.weight = #flow.parameter.named<"model"::"blk.20.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.20.attn_q.weight = #flow.parameter.named<"model"::"blk.20.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.20.attn_k.weight = #flow.parameter.named<"model"::"blk.20.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.20.attn_v.weight = #flow.parameter.named<"model"::"blk.20.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.20.attn_output.weight = #flow.parameter.named<"model"::"blk.20.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.20.ffn_norm.weight = #flow.parameter.named<"model"::"blk.20.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.20.ffn_gate.weight = #flow.parameter.named<"model"::"blk.20.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.20.ffn_up.weight = #flow.parameter.named<"model"::"blk.20.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.20.ffn_down.weight = #flow.parameter.named<"model"::"blk.20.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.blk.21.attn_norm.weight = #flow.parameter.named<"model"::"blk.21.attn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.21.attn_q.weight = #flow.parameter.named<"model"::"blk.21.attn_q.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.21.attn_k.weight = #flow.parameter.named<"model"::"blk.21.attn_k.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.21.attn_v.weight = #flow.parameter.named<"model"::"blk.21.attn_v.weight"> : tensor<256x2176xi8>
  util.global private @__auto.blk.21.attn_output.weight = #flow.parameter.named<"model"::"blk.21.attn_output.weight"> : tensor<2048x2176xi8>
  util.global private @__auto.blk.21.ffn_norm.weight = #flow.parameter.named<"model"::"blk.21.ffn_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.blk.21.ffn_gate.weight = #flow.parameter.named<"model"::"blk.21.ffn_gate.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.21.ffn_up.weight = #flow.parameter.named<"model"::"blk.21.ffn_up.weight"> : tensor<5632x2176xi8>
  util.global private @__auto.blk.21.ffn_down.weight = #flow.parameter.named<"model"::"blk.21.ffn_down.weight"> : tensor<2048x5984xi8>
  util.global private @__auto.output_norm.weight = #flow.parameter.named<"model"::"output_norm.weight"> : tensor<2048xf32>
  util.global private @__auto.output.weight = #flow.parameter.named<"model"::"output.weight"> : tensor<32000x2176xi8>
  func.func @prefill_bs4(%arg0: !torch.vtensor<[4,?],si64> {iree.abi.affinity = #hal.device.promise<@__device_0>}, %arg1: !torch.vtensor<[4],si64> {iree.abi.affinity = #hal.device.promise<@__device_0>}, %arg2: !torch.vtensor<[4,?],si64> {iree.abi.affinity = #hal.device.promise<@__device_0>}, %arg3: !torch.tensor<[?,360448],f16> {iree.abi.affinity = #hal.device.promise<@__device_0>}) -> !torch.vtensor<[4,?,32000],f16> attributes {torch.assume_strict_symbolic_shapes} {
    %__auto.token_embd.weight = util.global.load @__auto.token_embd.weight : tensor<32000x2176xi8>
    %0 = torch_c.from_builtin_tensor %__auto.token_embd.weight : tensor<32000x2176xi8> -> !torch.vtensor<[32000,2176],ui8>
    %1 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %2 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %3 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.0.attn_norm.weight = util.global.load @__auto.blk.0.attn_norm.weight : tensor<2048xf32>
    %4 = torch_c.from_builtin_tensor %__auto.blk.0.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.0.attn_q.weight = util.global.load @__auto.blk.0.attn_q.weight : tensor<2048x2176xi8>
    %5 = torch_c.from_builtin_tensor %__auto.blk.0.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.0.attn_k.weight = util.global.load @__auto.blk.0.attn_k.weight : tensor<256x2176xi8>
    %6 = torch_c.from_builtin_tensor %__auto.blk.0.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.0.attn_v.weight = util.global.load @__auto.blk.0.attn_v.weight : tensor<256x2176xi8>
    %7 = torch_c.from_builtin_tensor %__auto.blk.0.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.0.attn_output.weight = util.global.load @__auto.blk.0.attn_output.weight : tensor<2048x2176xi8>
    %8 = torch_c.from_builtin_tensor %__auto.blk.0.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.0.ffn_norm.weight = util.global.load @__auto.blk.0.ffn_norm.weight : tensor<2048xf32>
    %9 = torch_c.from_builtin_tensor %__auto.blk.0.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.0.ffn_gate.weight = util.global.load @__auto.blk.0.ffn_gate.weight : tensor<5632x2176xi8>
    %10 = torch_c.from_builtin_tensor %__auto.blk.0.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.0.ffn_up.weight = util.global.load @__auto.blk.0.ffn_up.weight : tensor<5632x2176xi8>
    %11 = torch_c.from_builtin_tensor %__auto.blk.0.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.0.ffn_down.weight = util.global.load @__auto.blk.0.ffn_down.weight : tensor<2048x5984xi8>
    %12 = torch_c.from_builtin_tensor %__auto.blk.0.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %13 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %14 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %15 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.1.attn_norm.weight = util.global.load @__auto.blk.1.attn_norm.weight : tensor<2048xf32>
    %16 = torch_c.from_builtin_tensor %__auto.blk.1.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.1.attn_q.weight = util.global.load @__auto.blk.1.attn_q.weight : tensor<2048x2176xi8>
    %17 = torch_c.from_builtin_tensor %__auto.blk.1.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.1.attn_k.weight = util.global.load @__auto.blk.1.attn_k.weight : tensor<256x2176xi8>
    %18 = torch_c.from_builtin_tensor %__auto.blk.1.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.1.attn_v.weight = util.global.load @__auto.blk.1.attn_v.weight : tensor<256x2176xi8>
    %19 = torch_c.from_builtin_tensor %__auto.blk.1.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.1.attn_output.weight = util.global.load @__auto.blk.1.attn_output.weight : tensor<2048x2176xi8>
    %20 = torch_c.from_builtin_tensor %__auto.blk.1.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.1.ffn_norm.weight = util.global.load @__auto.blk.1.ffn_norm.weight : tensor<2048xf32>
    %21 = torch_c.from_builtin_tensor %__auto.blk.1.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.1.ffn_gate.weight = util.global.load @__auto.blk.1.ffn_gate.weight : tensor<5632x2176xi8>
    %22 = torch_c.from_builtin_tensor %__auto.blk.1.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.1.ffn_up.weight = util.global.load @__auto.blk.1.ffn_up.weight : tensor<5632x2176xi8>
    %23 = torch_c.from_builtin_tensor %__auto.blk.1.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.1.ffn_down.weight = util.global.load @__auto.blk.1.ffn_down.weight : tensor<2048x5984xi8>
    %24 = torch_c.from_builtin_tensor %__auto.blk.1.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %25 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %26 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %27 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.2.attn_norm.weight = util.global.load @__auto.blk.2.attn_norm.weight : tensor<2048xf32>
    %28 = torch_c.from_builtin_tensor %__auto.blk.2.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.2.attn_q.weight = util.global.load @__auto.blk.2.attn_q.weight : tensor<2048x2176xi8>
    %29 = torch_c.from_builtin_tensor %__auto.blk.2.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.2.attn_k.weight = util.global.load @__auto.blk.2.attn_k.weight : tensor<256x2176xi8>
    %30 = torch_c.from_builtin_tensor %__auto.blk.2.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.2.attn_v.weight = util.global.load @__auto.blk.2.attn_v.weight : tensor<256x2176xi8>
    %31 = torch_c.from_builtin_tensor %__auto.blk.2.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.2.attn_output.weight = util.global.load @__auto.blk.2.attn_output.weight : tensor<2048x2176xi8>
    %32 = torch_c.from_builtin_tensor %__auto.blk.2.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.2.ffn_norm.weight = util.global.load @__auto.blk.2.ffn_norm.weight : tensor<2048xf32>
    %33 = torch_c.from_builtin_tensor %__auto.blk.2.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.2.ffn_gate.weight = util.global.load @__auto.blk.2.ffn_gate.weight : tensor<5632x2176xi8>
    %34 = torch_c.from_builtin_tensor %__auto.blk.2.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.2.ffn_up.weight = util.global.load @__auto.blk.2.ffn_up.weight : tensor<5632x2176xi8>
    %35 = torch_c.from_builtin_tensor %__auto.blk.2.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.2.ffn_down.weight = util.global.load @__auto.blk.2.ffn_down.weight : tensor<2048x5984xi8>
    %36 = torch_c.from_builtin_tensor %__auto.blk.2.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %37 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %38 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %39 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.3.attn_norm.weight = util.global.load @__auto.blk.3.attn_norm.weight : tensor<2048xf32>
    %40 = torch_c.from_builtin_tensor %__auto.blk.3.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.3.attn_q.weight = util.global.load @__auto.blk.3.attn_q.weight : tensor<2048x2176xi8>
    %41 = torch_c.from_builtin_tensor %__auto.blk.3.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.3.attn_k.weight = util.global.load @__auto.blk.3.attn_k.weight : tensor<256x2176xi8>
    %42 = torch_c.from_builtin_tensor %__auto.blk.3.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.3.attn_v.weight = util.global.load @__auto.blk.3.attn_v.weight : tensor<256x2176xi8>
    %43 = torch_c.from_builtin_tensor %__auto.blk.3.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.3.attn_output.weight = util.global.load @__auto.blk.3.attn_output.weight : tensor<2048x2176xi8>
    %44 = torch_c.from_builtin_tensor %__auto.blk.3.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.3.ffn_norm.weight = util.global.load @__auto.blk.3.ffn_norm.weight : tensor<2048xf32>
    %45 = torch_c.from_builtin_tensor %__auto.blk.3.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.3.ffn_gate.weight = util.global.load @__auto.blk.3.ffn_gate.weight : tensor<5632x2176xi8>
    %46 = torch_c.from_builtin_tensor %__auto.blk.3.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.3.ffn_up.weight = util.global.load @__auto.blk.3.ffn_up.weight : tensor<5632x2176xi8>
    %47 = torch_c.from_builtin_tensor %__auto.blk.3.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.3.ffn_down.weight = util.global.load @__auto.blk.3.ffn_down.weight : tensor<2048x5984xi8>
    %48 = torch_c.from_builtin_tensor %__auto.blk.3.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %49 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %50 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %51 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.4.attn_norm.weight = util.global.load @__auto.blk.4.attn_norm.weight : tensor<2048xf32>
    %52 = torch_c.from_builtin_tensor %__auto.blk.4.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.4.attn_q.weight = util.global.load @__auto.blk.4.attn_q.weight : tensor<2048x2176xi8>
    %53 = torch_c.from_builtin_tensor %__auto.blk.4.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.4.attn_k.weight = util.global.load @__auto.blk.4.attn_k.weight : tensor<256x2176xi8>
    %54 = torch_c.from_builtin_tensor %__auto.blk.4.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.4.attn_v.weight = util.global.load @__auto.blk.4.attn_v.weight : tensor<256x2176xi8>
    %55 = torch_c.from_builtin_tensor %__auto.blk.4.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.4.attn_output.weight = util.global.load @__auto.blk.4.attn_output.weight : tensor<2048x2176xi8>
    %56 = torch_c.from_builtin_tensor %__auto.blk.4.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.4.ffn_norm.weight = util.global.load @__auto.blk.4.ffn_norm.weight : tensor<2048xf32>
    %57 = torch_c.from_builtin_tensor %__auto.blk.4.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.4.ffn_gate.weight = util.global.load @__auto.blk.4.ffn_gate.weight : tensor<5632x2176xi8>
    %58 = torch_c.from_builtin_tensor %__auto.blk.4.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.4.ffn_up.weight = util.global.load @__auto.blk.4.ffn_up.weight : tensor<5632x2176xi8>
    %59 = torch_c.from_builtin_tensor %__auto.blk.4.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.4.ffn_down.weight = util.global.load @__auto.blk.4.ffn_down.weight : tensor<2048x5984xi8>
    %60 = torch_c.from_builtin_tensor %__auto.blk.4.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %61 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %62 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %63 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.5.attn_norm.weight = util.global.load @__auto.blk.5.attn_norm.weight : tensor<2048xf32>
    %64 = torch_c.from_builtin_tensor %__auto.blk.5.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.5.attn_q.weight = util.global.load @__auto.blk.5.attn_q.weight : tensor<2048x2176xi8>
    %65 = torch_c.from_builtin_tensor %__auto.blk.5.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.5.attn_k.weight = util.global.load @__auto.blk.5.attn_k.weight : tensor<256x2176xi8>
    %66 = torch_c.from_builtin_tensor %__auto.blk.5.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.5.attn_v.weight = util.global.load @__auto.blk.5.attn_v.weight : tensor<256x2176xi8>
    %67 = torch_c.from_builtin_tensor %__auto.blk.5.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.5.attn_output.weight = util.global.load @__auto.blk.5.attn_output.weight : tensor<2048x2176xi8>
    %68 = torch_c.from_builtin_tensor %__auto.blk.5.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.5.ffn_norm.weight = util.global.load @__auto.blk.5.ffn_norm.weight : tensor<2048xf32>
    %69 = torch_c.from_builtin_tensor %__auto.blk.5.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.5.ffn_gate.weight = util.global.load @__auto.blk.5.ffn_gate.weight : tensor<5632x2176xi8>
    %70 = torch_c.from_builtin_tensor %__auto.blk.5.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.5.ffn_up.weight = util.global.load @__auto.blk.5.ffn_up.weight : tensor<5632x2176xi8>
    %71 = torch_c.from_builtin_tensor %__auto.blk.5.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.5.ffn_down.weight = util.global.load @__auto.blk.5.ffn_down.weight : tensor<2048x5984xi8>
    %72 = torch_c.from_builtin_tensor %__auto.blk.5.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %73 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %74 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %75 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.6.attn_norm.weight = util.global.load @__auto.blk.6.attn_norm.weight : tensor<2048xf32>
    %76 = torch_c.from_builtin_tensor %__auto.blk.6.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.6.attn_q.weight = util.global.load @__auto.blk.6.attn_q.weight : tensor<2048x2176xi8>
    %77 = torch_c.from_builtin_tensor %__auto.blk.6.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.6.attn_k.weight = util.global.load @__auto.blk.6.attn_k.weight : tensor<256x2176xi8>
    %78 = torch_c.from_builtin_tensor %__auto.blk.6.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.6.attn_v.weight = util.global.load @__auto.blk.6.attn_v.weight : tensor<256x2176xi8>
    %79 = torch_c.from_builtin_tensor %__auto.blk.6.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.6.attn_output.weight = util.global.load @__auto.blk.6.attn_output.weight : tensor<2048x2176xi8>
    %80 = torch_c.from_builtin_tensor %__auto.blk.6.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.6.ffn_norm.weight = util.global.load @__auto.blk.6.ffn_norm.weight : tensor<2048xf32>
    %81 = torch_c.from_builtin_tensor %__auto.blk.6.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.6.ffn_gate.weight = util.global.load @__auto.blk.6.ffn_gate.weight : tensor<5632x2176xi8>
    %82 = torch_c.from_builtin_tensor %__auto.blk.6.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.6.ffn_up.weight = util.global.load @__auto.blk.6.ffn_up.weight : tensor<5632x2176xi8>
    %83 = torch_c.from_builtin_tensor %__auto.blk.6.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.6.ffn_down.weight = util.global.load @__auto.blk.6.ffn_down.weight : tensor<2048x5984xi8>
    %84 = torch_c.from_builtin_tensor %__auto.blk.6.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %85 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %86 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %87 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.7.attn_norm.weight = util.global.load @__auto.blk.7.attn_norm.weight : tensor<2048xf32>
    %88 = torch_c.from_builtin_tensor %__auto.blk.7.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.7.attn_q.weight = util.global.load @__auto.blk.7.attn_q.weight : tensor<2048x2176xi8>
    %89 = torch_c.from_builtin_tensor %__auto.blk.7.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.7.attn_k.weight = util.global.load @__auto.blk.7.attn_k.weight : tensor<256x2176xi8>
    %90 = torch_c.from_builtin_tensor %__auto.blk.7.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.7.attn_v.weight = util.global.load @__auto.blk.7.attn_v.weight : tensor<256x2176xi8>
    %91 = torch_c.from_builtin_tensor %__auto.blk.7.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.7.attn_output.weight = util.global.load @__auto.blk.7.attn_output.weight : tensor<2048x2176xi8>
    %92 = torch_c.from_builtin_tensor %__auto.blk.7.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.7.ffn_norm.weight = util.global.load @__auto.blk.7.ffn_norm.weight : tensor<2048xf32>
    %93 = torch_c.from_builtin_tensor %__auto.blk.7.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.7.ffn_gate.weight = util.global.load @__auto.blk.7.ffn_gate.weight : tensor<5632x2176xi8>
    %94 = torch_c.from_builtin_tensor %__auto.blk.7.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.7.ffn_up.weight = util.global.load @__auto.blk.7.ffn_up.weight : tensor<5632x2176xi8>
    %95 = torch_c.from_builtin_tensor %__auto.blk.7.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.7.ffn_down.weight = util.global.load @__auto.blk.7.ffn_down.weight : tensor<2048x5984xi8>
    %96 = torch_c.from_builtin_tensor %__auto.blk.7.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %97 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %98 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %99 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.8.attn_norm.weight = util.global.load @__auto.blk.8.attn_norm.weight : tensor<2048xf32>
    %100 = torch_c.from_builtin_tensor %__auto.blk.8.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.8.attn_q.weight = util.global.load @__auto.blk.8.attn_q.weight : tensor<2048x2176xi8>
    %101 = torch_c.from_builtin_tensor %__auto.blk.8.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.8.attn_k.weight = util.global.load @__auto.blk.8.attn_k.weight : tensor<256x2176xi8>
    %102 = torch_c.from_builtin_tensor %__auto.blk.8.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.8.attn_v.weight = util.global.load @__auto.blk.8.attn_v.weight : tensor<256x2176xi8>
    %103 = torch_c.from_builtin_tensor %__auto.blk.8.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.8.attn_output.weight = util.global.load @__auto.blk.8.attn_output.weight : tensor<2048x2176xi8>
    %104 = torch_c.from_builtin_tensor %__auto.blk.8.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.8.ffn_norm.weight = util.global.load @__auto.blk.8.ffn_norm.weight : tensor<2048xf32>
    %105 = torch_c.from_builtin_tensor %__auto.blk.8.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.8.ffn_gate.weight = util.global.load @__auto.blk.8.ffn_gate.weight : tensor<5632x2176xi8>
    %106 = torch_c.from_builtin_tensor %__auto.blk.8.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.8.ffn_up.weight = util.global.load @__auto.blk.8.ffn_up.weight : tensor<5632x2176xi8>
    %107 = torch_c.from_builtin_tensor %__auto.blk.8.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.8.ffn_down.weight = util.global.load @__auto.blk.8.ffn_down.weight : tensor<2048x5984xi8>
    %108 = torch_c.from_builtin_tensor %__auto.blk.8.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %109 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %110 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %111 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.9.attn_norm.weight = util.global.load @__auto.blk.9.attn_norm.weight : tensor<2048xf32>
    %112 = torch_c.from_builtin_tensor %__auto.blk.9.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.9.attn_q.weight = util.global.load @__auto.blk.9.attn_q.weight : tensor<2048x2176xi8>
    %113 = torch_c.from_builtin_tensor %__auto.blk.9.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.9.attn_k.weight = util.global.load @__auto.blk.9.attn_k.weight : tensor<256x2176xi8>
    %114 = torch_c.from_builtin_tensor %__auto.blk.9.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.9.attn_v.weight = util.global.load @__auto.blk.9.attn_v.weight : tensor<256x2176xi8>
    %115 = torch_c.from_builtin_tensor %__auto.blk.9.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.9.attn_output.weight = util.global.load @__auto.blk.9.attn_output.weight : tensor<2048x2176xi8>
    %116 = torch_c.from_builtin_tensor %__auto.blk.9.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.9.ffn_norm.weight = util.global.load @__auto.blk.9.ffn_norm.weight : tensor<2048xf32>
    %117 = torch_c.from_builtin_tensor %__auto.blk.9.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.9.ffn_gate.weight = util.global.load @__auto.blk.9.ffn_gate.weight : tensor<5632x2176xi8>
    %118 = torch_c.from_builtin_tensor %__auto.blk.9.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.9.ffn_up.weight = util.global.load @__auto.blk.9.ffn_up.weight : tensor<5632x2176xi8>
    %119 = torch_c.from_builtin_tensor %__auto.blk.9.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.9.ffn_down.weight = util.global.load @__auto.blk.9.ffn_down.weight : tensor<2048x5984xi8>
    %120 = torch_c.from_builtin_tensor %__auto.blk.9.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %121 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %122 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %123 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.10.attn_norm.weight = util.global.load @__auto.blk.10.attn_norm.weight : tensor<2048xf32>
    %124 = torch_c.from_builtin_tensor %__auto.blk.10.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.10.attn_q.weight = util.global.load @__auto.blk.10.attn_q.weight : tensor<2048x2176xi8>
    %125 = torch_c.from_builtin_tensor %__auto.blk.10.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.10.attn_k.weight = util.global.load @__auto.blk.10.attn_k.weight : tensor<256x2176xi8>
    %126 = torch_c.from_builtin_tensor %__auto.blk.10.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.10.attn_v.weight = util.global.load @__auto.blk.10.attn_v.weight : tensor<256x2176xi8>
    %127 = torch_c.from_builtin_tensor %__auto.blk.10.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.10.attn_output.weight = util.global.load @__auto.blk.10.attn_output.weight : tensor<2048x2176xi8>
    %128 = torch_c.from_builtin_tensor %__auto.blk.10.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.10.ffn_norm.weight = util.global.load @__auto.blk.10.ffn_norm.weight : tensor<2048xf32>
    %129 = torch_c.from_builtin_tensor %__auto.blk.10.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.10.ffn_gate.weight = util.global.load @__auto.blk.10.ffn_gate.weight : tensor<5632x2176xi8>
    %130 = torch_c.from_builtin_tensor %__auto.blk.10.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.10.ffn_up.weight = util.global.load @__auto.blk.10.ffn_up.weight : tensor<5632x2176xi8>
    %131 = torch_c.from_builtin_tensor %__auto.blk.10.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.10.ffn_down.weight = util.global.load @__auto.blk.10.ffn_down.weight : tensor<2048x5984xi8>
    %132 = torch_c.from_builtin_tensor %__auto.blk.10.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %133 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %134 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %135 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.11.attn_norm.weight = util.global.load @__auto.blk.11.attn_norm.weight : tensor<2048xf32>
    %136 = torch_c.from_builtin_tensor %__auto.blk.11.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.11.attn_q.weight = util.global.load @__auto.blk.11.attn_q.weight : tensor<2048x2176xi8>
    %137 = torch_c.from_builtin_tensor %__auto.blk.11.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.11.attn_k.weight = util.global.load @__auto.blk.11.attn_k.weight : tensor<256x2176xi8>
    %138 = torch_c.from_builtin_tensor %__auto.blk.11.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.11.attn_v.weight = util.global.load @__auto.blk.11.attn_v.weight : tensor<256x2176xi8>
    %139 = torch_c.from_builtin_tensor %__auto.blk.11.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.11.attn_output.weight = util.global.load @__auto.blk.11.attn_output.weight : tensor<2048x2176xi8>
    %140 = torch_c.from_builtin_tensor %__auto.blk.11.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.11.ffn_norm.weight = util.global.load @__auto.blk.11.ffn_norm.weight : tensor<2048xf32>
    %141 = torch_c.from_builtin_tensor %__auto.blk.11.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.11.ffn_gate.weight = util.global.load @__auto.blk.11.ffn_gate.weight : tensor<5632x2176xi8>
    %142 = torch_c.from_builtin_tensor %__auto.blk.11.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.11.ffn_up.weight = util.global.load @__auto.blk.11.ffn_up.weight : tensor<5632x2176xi8>
    %143 = torch_c.from_builtin_tensor %__auto.blk.11.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.11.ffn_down.weight = util.global.load @__auto.blk.11.ffn_down.weight : tensor<2048x5984xi8>
    %144 = torch_c.from_builtin_tensor %__auto.blk.11.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %145 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %146 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %147 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.12.attn_norm.weight = util.global.load @__auto.blk.12.attn_norm.weight : tensor<2048xf32>
    %148 = torch_c.from_builtin_tensor %__auto.blk.12.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.12.attn_q.weight = util.global.load @__auto.blk.12.attn_q.weight : tensor<2048x2176xi8>
    %149 = torch_c.from_builtin_tensor %__auto.blk.12.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.12.attn_k.weight = util.global.load @__auto.blk.12.attn_k.weight : tensor<256x2176xi8>
    %150 = torch_c.from_builtin_tensor %__auto.blk.12.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.12.attn_v.weight = util.global.load @__auto.blk.12.attn_v.weight : tensor<256x2176xi8>
    %151 = torch_c.from_builtin_tensor %__auto.blk.12.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.12.attn_output.weight = util.global.load @__auto.blk.12.attn_output.weight : tensor<2048x2176xi8>
    %152 = torch_c.from_builtin_tensor %__auto.blk.12.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.12.ffn_norm.weight = util.global.load @__auto.blk.12.ffn_norm.weight : tensor<2048xf32>
    %153 = torch_c.from_builtin_tensor %__auto.blk.12.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.12.ffn_gate.weight = util.global.load @__auto.blk.12.ffn_gate.weight : tensor<5632x2176xi8>
    %154 = torch_c.from_builtin_tensor %__auto.blk.12.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.12.ffn_up.weight = util.global.load @__auto.blk.12.ffn_up.weight : tensor<5632x2176xi8>
    %155 = torch_c.from_builtin_tensor %__auto.blk.12.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.12.ffn_down.weight = util.global.load @__auto.blk.12.ffn_down.weight : tensor<2048x5984xi8>
    %156 = torch_c.from_builtin_tensor %__auto.blk.12.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %157 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %158 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %159 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.13.attn_norm.weight = util.global.load @__auto.blk.13.attn_norm.weight : tensor<2048xf32>
    %160 = torch_c.from_builtin_tensor %__auto.blk.13.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.13.attn_q.weight = util.global.load @__auto.blk.13.attn_q.weight : tensor<2048x2176xi8>
    %161 = torch_c.from_builtin_tensor %__auto.blk.13.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.13.attn_k.weight = util.global.load @__auto.blk.13.attn_k.weight : tensor<256x2176xi8>
    %162 = torch_c.from_builtin_tensor %__auto.blk.13.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.13.attn_v.weight = util.global.load @__auto.blk.13.attn_v.weight : tensor<256x2176xi8>
    %163 = torch_c.from_builtin_tensor %__auto.blk.13.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.13.attn_output.weight = util.global.load @__auto.blk.13.attn_output.weight : tensor<2048x2176xi8>
    %164 = torch_c.from_builtin_tensor %__auto.blk.13.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.13.ffn_norm.weight = util.global.load @__auto.blk.13.ffn_norm.weight : tensor<2048xf32>
    %165 = torch_c.from_builtin_tensor %__auto.blk.13.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.13.ffn_gate.weight = util.global.load @__auto.blk.13.ffn_gate.weight : tensor<5632x2176xi8>
    %166 = torch_c.from_builtin_tensor %__auto.blk.13.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.13.ffn_up.weight = util.global.load @__auto.blk.13.ffn_up.weight : tensor<5632x2176xi8>
    %167 = torch_c.from_builtin_tensor %__auto.blk.13.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.13.ffn_down.weight = util.global.load @__auto.blk.13.ffn_down.weight : tensor<2048x5984xi8>
    %168 = torch_c.from_builtin_tensor %__auto.blk.13.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %169 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %170 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %171 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.14.attn_norm.weight = util.global.load @__auto.blk.14.attn_norm.weight : tensor<2048xf32>
    %172 = torch_c.from_builtin_tensor %__auto.blk.14.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.14.attn_q.weight = util.global.load @__auto.blk.14.attn_q.weight : tensor<2048x2176xi8>
    %173 = torch_c.from_builtin_tensor %__auto.blk.14.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.14.attn_k.weight = util.global.load @__auto.blk.14.attn_k.weight : tensor<256x2176xi8>
    %174 = torch_c.from_builtin_tensor %__auto.blk.14.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.14.attn_v.weight = util.global.load @__auto.blk.14.attn_v.weight : tensor<256x2176xi8>
    %175 = torch_c.from_builtin_tensor %__auto.blk.14.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.14.attn_output.weight = util.global.load @__auto.blk.14.attn_output.weight : tensor<2048x2176xi8>
    %176 = torch_c.from_builtin_tensor %__auto.blk.14.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.14.ffn_norm.weight = util.global.load @__auto.blk.14.ffn_norm.weight : tensor<2048xf32>
    %177 = torch_c.from_builtin_tensor %__auto.blk.14.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.14.ffn_gate.weight = util.global.load @__auto.blk.14.ffn_gate.weight : tensor<5632x2176xi8>
    %178 = torch_c.from_builtin_tensor %__auto.blk.14.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.14.ffn_up.weight = util.global.load @__auto.blk.14.ffn_up.weight : tensor<5632x2176xi8>
    %179 = torch_c.from_builtin_tensor %__auto.blk.14.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.14.ffn_down.weight = util.global.load @__auto.blk.14.ffn_down.weight : tensor<2048x5984xi8>
    %180 = torch_c.from_builtin_tensor %__auto.blk.14.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %181 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %182 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %183 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.15.attn_norm.weight = util.global.load @__auto.blk.15.attn_norm.weight : tensor<2048xf32>
    %184 = torch_c.from_builtin_tensor %__auto.blk.15.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.15.attn_q.weight = util.global.load @__auto.blk.15.attn_q.weight : tensor<2048x2176xi8>
    %185 = torch_c.from_builtin_tensor %__auto.blk.15.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.15.attn_k.weight = util.global.load @__auto.blk.15.attn_k.weight : tensor<256x2176xi8>
    %186 = torch_c.from_builtin_tensor %__auto.blk.15.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.15.attn_v.weight = util.global.load @__auto.blk.15.attn_v.weight : tensor<256x2176xi8>
    %187 = torch_c.from_builtin_tensor %__auto.blk.15.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.15.attn_output.weight = util.global.load @__auto.blk.15.attn_output.weight : tensor<2048x2176xi8>
    %188 = torch_c.from_builtin_tensor %__auto.blk.15.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.15.ffn_norm.weight = util.global.load @__auto.blk.15.ffn_norm.weight : tensor<2048xf32>
    %189 = torch_c.from_builtin_tensor %__auto.blk.15.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.15.ffn_gate.weight = util.global.load @__auto.blk.15.ffn_gate.weight : tensor<5632x2176xi8>
    %190 = torch_c.from_builtin_tensor %__auto.blk.15.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.15.ffn_up.weight = util.global.load @__auto.blk.15.ffn_up.weight : tensor<5632x2176xi8>
    %191 = torch_c.from_builtin_tensor %__auto.blk.15.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.15.ffn_down.weight = util.global.load @__auto.blk.15.ffn_down.weight : tensor<2048x5984xi8>
    %192 = torch_c.from_builtin_tensor %__auto.blk.15.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %193 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %194 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %195 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.16.attn_norm.weight = util.global.load @__auto.blk.16.attn_norm.weight : tensor<2048xf32>
    %196 = torch_c.from_builtin_tensor %__auto.blk.16.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.16.attn_q.weight = util.global.load @__auto.blk.16.attn_q.weight : tensor<2048x2176xi8>
    %197 = torch_c.from_builtin_tensor %__auto.blk.16.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.16.attn_k.weight = util.global.load @__auto.blk.16.attn_k.weight : tensor<256x2176xi8>
    %198 = torch_c.from_builtin_tensor %__auto.blk.16.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.16.attn_v.weight = util.global.load @__auto.blk.16.attn_v.weight : tensor<256x2176xi8>
    %199 = torch_c.from_builtin_tensor %__auto.blk.16.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.16.attn_output.weight = util.global.load @__auto.blk.16.attn_output.weight : tensor<2048x2176xi8>
    %200 = torch_c.from_builtin_tensor %__auto.blk.16.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.16.ffn_norm.weight = util.global.load @__auto.blk.16.ffn_norm.weight : tensor<2048xf32>
    %201 = torch_c.from_builtin_tensor %__auto.blk.16.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.16.ffn_gate.weight = util.global.load @__auto.blk.16.ffn_gate.weight : tensor<5632x2176xi8>
    %202 = torch_c.from_builtin_tensor %__auto.blk.16.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.16.ffn_up.weight = util.global.load @__auto.blk.16.ffn_up.weight : tensor<5632x2176xi8>
    %203 = torch_c.from_builtin_tensor %__auto.blk.16.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.16.ffn_down.weight = util.global.load @__auto.blk.16.ffn_down.weight : tensor<2048x5984xi8>
    %204 = torch_c.from_builtin_tensor %__auto.blk.16.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %205 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %206 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %207 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.17.attn_norm.weight = util.global.load @__auto.blk.17.attn_norm.weight : tensor<2048xf32>
    %208 = torch_c.from_builtin_tensor %__auto.blk.17.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.17.attn_q.weight = util.global.load @__auto.blk.17.attn_q.weight : tensor<2048x2176xi8>
    %209 = torch_c.from_builtin_tensor %__auto.blk.17.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.17.attn_k.weight = util.global.load @__auto.blk.17.attn_k.weight : tensor<256x2176xi8>
    %210 = torch_c.from_builtin_tensor %__auto.blk.17.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.17.attn_v.weight = util.global.load @__auto.blk.17.attn_v.weight : tensor<256x2176xi8>
    %211 = torch_c.from_builtin_tensor %__auto.blk.17.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.17.attn_output.weight = util.global.load @__auto.blk.17.attn_output.weight : tensor<2048x2176xi8>
    %212 = torch_c.from_builtin_tensor %__auto.blk.17.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.17.ffn_norm.weight = util.global.load @__auto.blk.17.ffn_norm.weight : tensor<2048xf32>
    %213 = torch_c.from_builtin_tensor %__auto.blk.17.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.17.ffn_gate.weight = util.global.load @__auto.blk.17.ffn_gate.weight : tensor<5632x2176xi8>
    %214 = torch_c.from_builtin_tensor %__auto.blk.17.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.17.ffn_up.weight = util.global.load @__auto.blk.17.ffn_up.weight : tensor<5632x2176xi8>
    %215 = torch_c.from_builtin_tensor %__auto.blk.17.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.17.ffn_down.weight = util.global.load @__auto.blk.17.ffn_down.weight : tensor<2048x5984xi8>
    %216 = torch_c.from_builtin_tensor %__auto.blk.17.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %217 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %218 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %219 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.18.attn_norm.weight = util.global.load @__auto.blk.18.attn_norm.weight : tensor<2048xf32>
    %220 = torch_c.from_builtin_tensor %__auto.blk.18.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.18.attn_q.weight = util.global.load @__auto.blk.18.attn_q.weight : tensor<2048x2176xi8>
    %221 = torch_c.from_builtin_tensor %__auto.blk.18.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.18.attn_k.weight = util.global.load @__auto.blk.18.attn_k.weight : tensor<256x2176xi8>
    %222 = torch_c.from_builtin_tensor %__auto.blk.18.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.18.attn_v.weight = util.global.load @__auto.blk.18.attn_v.weight : tensor<256x2176xi8>
    %223 = torch_c.from_builtin_tensor %__auto.blk.18.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.18.attn_output.weight = util.global.load @__auto.blk.18.attn_output.weight : tensor<2048x2176xi8>
    %224 = torch_c.from_builtin_tensor %__auto.blk.18.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.18.ffn_norm.weight = util.global.load @__auto.blk.18.ffn_norm.weight : tensor<2048xf32>
    %225 = torch_c.from_builtin_tensor %__auto.blk.18.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.18.ffn_gate.weight = util.global.load @__auto.blk.18.ffn_gate.weight : tensor<5632x2176xi8>
    %226 = torch_c.from_builtin_tensor %__auto.blk.18.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.18.ffn_up.weight = util.global.load @__auto.blk.18.ffn_up.weight : tensor<5632x2176xi8>
    %227 = torch_c.from_builtin_tensor %__auto.blk.18.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.18.ffn_down.weight = util.global.load @__auto.blk.18.ffn_down.weight : tensor<2048x5984xi8>
    %228 = torch_c.from_builtin_tensor %__auto.blk.18.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %229 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %230 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %231 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.19.attn_norm.weight = util.global.load @__auto.blk.19.attn_norm.weight : tensor<2048xf32>
    %232 = torch_c.from_builtin_tensor %__auto.blk.19.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.19.attn_q.weight = util.global.load @__auto.blk.19.attn_q.weight : tensor<2048x2176xi8>
    %233 = torch_c.from_builtin_tensor %__auto.blk.19.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.19.attn_k.weight = util.global.load @__auto.blk.19.attn_k.weight : tensor<256x2176xi8>
    %234 = torch_c.from_builtin_tensor %__auto.blk.19.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.19.attn_v.weight = util.global.load @__auto.blk.19.attn_v.weight : tensor<256x2176xi8>
    %235 = torch_c.from_builtin_tensor %__auto.blk.19.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.19.attn_output.weight = util.global.load @__auto.blk.19.attn_output.weight : tensor<2048x2176xi8>
    %236 = torch_c.from_builtin_tensor %__auto.blk.19.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.19.ffn_norm.weight = util.global.load @__auto.blk.19.ffn_norm.weight : tensor<2048xf32>
    %237 = torch_c.from_builtin_tensor %__auto.blk.19.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.19.ffn_gate.weight = util.global.load @__auto.blk.19.ffn_gate.weight : tensor<5632x2176xi8>
    %238 = torch_c.from_builtin_tensor %__auto.blk.19.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.19.ffn_up.weight = util.global.load @__auto.blk.19.ffn_up.weight : tensor<5632x2176xi8>
    %239 = torch_c.from_builtin_tensor %__auto.blk.19.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.19.ffn_down.weight = util.global.load @__auto.blk.19.ffn_down.weight : tensor<2048x5984xi8>
    %240 = torch_c.from_builtin_tensor %__auto.blk.19.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %241 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %242 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %243 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.20.attn_norm.weight = util.global.load @__auto.blk.20.attn_norm.weight : tensor<2048xf32>
    %244 = torch_c.from_builtin_tensor %__auto.blk.20.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.20.attn_q.weight = util.global.load @__auto.blk.20.attn_q.weight : tensor<2048x2176xi8>
    %245 = torch_c.from_builtin_tensor %__auto.blk.20.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.20.attn_k.weight = util.global.load @__auto.blk.20.attn_k.weight : tensor<256x2176xi8>
    %246 = torch_c.from_builtin_tensor %__auto.blk.20.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.20.attn_v.weight = util.global.load @__auto.blk.20.attn_v.weight : tensor<256x2176xi8>
    %247 = torch_c.from_builtin_tensor %__auto.blk.20.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.20.attn_output.weight = util.global.load @__auto.blk.20.attn_output.weight : tensor<2048x2176xi8>
    %248 = torch_c.from_builtin_tensor %__auto.blk.20.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.20.ffn_norm.weight = util.global.load @__auto.blk.20.ffn_norm.weight : tensor<2048xf32>
    %249 = torch_c.from_builtin_tensor %__auto.blk.20.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.20.ffn_gate.weight = util.global.load @__auto.blk.20.ffn_gate.weight : tensor<5632x2176xi8>
    %250 = torch_c.from_builtin_tensor %__auto.blk.20.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.20.ffn_up.weight = util.global.load @__auto.blk.20.ffn_up.weight : tensor<5632x2176xi8>
    %251 = torch_c.from_builtin_tensor %__auto.blk.20.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.20.ffn_down.weight = util.global.load @__auto.blk.20.ffn_down.weight : tensor<2048x5984xi8>
    %252 = torch_c.from_builtin_tensor %__auto.blk.20.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %253 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %254 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %255 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.21.attn_norm.weight = util.global.load @__auto.blk.21.attn_norm.weight : tensor<2048xf32>
    %256 = torch_c.from_builtin_tensor %__auto.blk.21.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.21.attn_q.weight = util.global.load @__auto.blk.21.attn_q.weight : tensor<2048x2176xi8>
    %257 = torch_c.from_builtin_tensor %__auto.blk.21.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.21.attn_k.weight = util.global.load @__auto.blk.21.attn_k.weight : tensor<256x2176xi8>
    %258 = torch_c.from_builtin_tensor %__auto.blk.21.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.21.attn_v.weight = util.global.load @__auto.blk.21.attn_v.weight : tensor<256x2176xi8>
    %259 = torch_c.from_builtin_tensor %__auto.blk.21.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.21.attn_output.weight = util.global.load @__auto.blk.21.attn_output.weight : tensor<2048x2176xi8>
    %260 = torch_c.from_builtin_tensor %__auto.blk.21.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.21.ffn_norm.weight = util.global.load @__auto.blk.21.ffn_norm.weight : tensor<2048xf32>
    %261 = torch_c.from_builtin_tensor %__auto.blk.21.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.21.ffn_gate.weight = util.global.load @__auto.blk.21.ffn_gate.weight : tensor<5632x2176xi8>
    %262 = torch_c.from_builtin_tensor %__auto.blk.21.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.21.ffn_up.weight = util.global.load @__auto.blk.21.ffn_up.weight : tensor<5632x2176xi8>
    %263 = torch_c.from_builtin_tensor %__auto.blk.21.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.21.ffn_down.weight = util.global.load @__auto.blk.21.ffn_down.weight : tensor<2048x5984xi8>
    %264 = torch_c.from_builtin_tensor %__auto.blk.21.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %__auto.output_norm.weight = util.global.load @__auto.output_norm.weight : tensor<2048xf32>
    %265 = torch_c.from_builtin_tensor %__auto.output_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.output.weight = util.global.load @__auto.output.weight : tensor<32000x2176xi8>
    %266 = torch_c.from_builtin_tensor %__auto.output.weight : tensor<32000x2176xi8> -> !torch.vtensor<[32000,2176],ui8>
    %267 = torch.copy.to_vtensor %arg3 : !torch.vtensor<[?,360448],f16>
    %268 = torch.symbolic_int "32*s23" {min_val = 64, max_val = 2016} : !torch.int
    %269 = torch.symbolic_int "s23" {min_val = 2, max_val = 63} : !torch.int
    %270 = torch.symbolic_int "s39" {min_val = 0, max_val = 9223372036854775807} : !torch.int
    torch.bind_symbolic_shape %arg0, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %arg2, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %267, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int1 = torch.constant.int 1
    %271 = torch.aten.size.int %arg2, %int1 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.int
    %int0 = torch.constant.int 0
    %272 = torch.aten.size.int %267, %int0 : !torch.vtensor<[?,360448],f16>, !torch.int -> !torch.int
    %int1_0 = torch.constant.int 1
    %273 = torch.aten.size.int %arg0, %int1_0 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.int
    %int2 = torch.constant.int 2
    %274 = torch.aten.view.dtype %0, %int2 : !torch.vtensor<[32000,2176],ui8>, !torch.int -> !torch.vtensor<[32000,1088],si16>
    %275 = torch.aten.detach %274 : !torch.vtensor<[32000,1088],si16> -> !torch.vtensor<[32000,1088],si16>
    %int-1 = torch.constant.int -1
    %int17 = torch.constant.int 17
    %276 = torch.prim.ListConstruct %int-1, %int17 : (!torch.int, !torch.int) -> !torch.list<int>
    %277 = torch.aten.view %275, %276 : !torch.vtensor<[32000,1088],si16>, !torch.list<int> -> !torch.vtensor<[2048000,17],si16>
    %int32000 = torch.constant.int 32000
    %int-1_1 = torch.constant.int -1
    %int17_2 = torch.constant.int 17
    %278 = torch.prim.ListConstruct %int32000, %int-1_1, %int17_2 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %279 = torch.aten.view %277, %278 : !torch.vtensor<[2048000,17],si16>, !torch.list<int> -> !torch.vtensor<[32000,64,17],si16>
    %int2_3 = torch.constant.int 2
    %int0_4 = torch.constant.int 0
    %int1_5 = torch.constant.int 1
    %int1_6 = torch.constant.int 1
    %280 = torch.aten.slice.Tensor %279, %int2_3, %int0_4, %int1_5, %int1_6 : !torch.vtensor<[32000,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[32000,64,1],si16>
    %int5 = torch.constant.int 5
    %281 = torch.aten.view.dtype %280, %int5 : !torch.vtensor<[32000,64,1],si16>, !torch.int -> !torch.vtensor<[32000,64,1],f16>
    %282 = torch.aten.detach %281 : !torch.vtensor<[32000,64,1],f16> -> !torch.vtensor<[32000,64,1],f16>
    %int2_7 = torch.constant.int 2
    %int1_8 = torch.constant.int 1
    %int9223372036854775807 = torch.constant.int 9223372036854775807
    %int1_9 = torch.constant.int 1
    %283 = torch.aten.slice.Tensor %279, %int2_7, %int1_8, %int9223372036854775807, %int1_9 : !torch.vtensor<[32000,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[32000,64,16],si16>
    %int1_10 = torch.constant.int 1
    %284 = torch.aten.view.dtype %283, %int1_10 : !torch.vtensor<[32000,64,16],si16>, !torch.int -> !torch.vtensor<[32000,64,32],si8>
    %285 = torch.aten.detach %284 : !torch.vtensor<[32000,64,32],si8> -> !torch.vtensor<[32000,64,32],si8>
    %none = torch.constant.none
    %none_11 = torch.constant.none
    %int5_12 = torch.constant.int 5
    %cpu = torch.constant.device "cpu"
    %int0_13 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %282, %none, %none_11, %int5_12, %cpu, %int0_13 : !torch.vtensor<[32000,64,1],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_14 = torch.constant.none
    %none_15 = torch.constant.none
    %int1_16 = torch.constant.int 1
    %cpu_17 = torch.constant.device "cpu"
    %int0_18 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %285, %none_14, %none_15, %int1_16, %cpu_17, %int0_18 : !torch.vtensor<[32000,64,32],si8>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_19 = torch.constant.int 5
    %286 = torch.prims.convert_element_type %285, %int5_19 : !torch.vtensor<[32000,64,32],si8>, !torch.int -> !torch.vtensor<[32000,64,32],f16>
    %287 = torch.aten.mul.Tensor %282, %286 : !torch.vtensor<[32000,64,1],f16>, !torch.vtensor<[32000,64,32],f16> -> !torch.vtensor<[32000,64,32],f16>
    %int32000_20 = torch.constant.int 32000
    %int2048 = torch.constant.int 2048
    %288 = torch.prim.ListConstruct %int32000_20, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %289 = torch.aten.view %287, %288 : !torch.vtensor<[32000,64,32],f16>, !torch.list<int> -> !torch.vtensor<[32000,2048],f16>
    %int-1_21 = torch.constant.int -1
    %false = torch.constant.bool false
    %false_22 = torch.constant.bool false
    %290 = torch.aten.embedding %289, %arg0, %int-1_21, %false, %false_22 : !torch.vtensor<[32000,2048],f16>, !torch.vtensor<[4,?],si64>, !torch.int, !torch.bool, !torch.bool -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %290, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_23 = torch.constant.none
    %none_24 = torch.constant.none
    %int5_25 = torch.constant.int 5
    %cpu_26 = torch.constant.device "cpu"
    %int0_27 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %290, %none_23, %none_24, %int5_25, %cpu_26, %int0_27 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6 = torch.constant.int 6
    %291 = torch.prims.convert_element_type %290, %int6 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %291, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_28 = torch.constant.int 2
    %292 = torch.aten.pow.Tensor_Scalar %291, %int2_28 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %292, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_29 = torch.constant.int -1
    %293 = torch.prim.ListConstruct %int-1_29 : (!torch.int) -> !torch.list<int>
    %true = torch.constant.bool true
    %none_30 = torch.constant.none
    %294 = torch.aten.mean.dim %292, %293, %true, %none_30 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %294, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06 = torch.constant.float 9.9999997473787516E-6
    %int1_31 = torch.constant.int 1
    %295 = torch.aten.add.Scalar %294, %float9.999990e-06, %int1_31 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %295, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %296 = torch.aten.rsqrt %295 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %296, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %297 = torch.aten.mul.Tensor %291, %296 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %297, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_32 = torch.constant.none
    %none_33 = torch.constant.none
    %int6_34 = torch.constant.int 6
    %cpu_35 = torch.constant.device "cpu"
    %int0_36 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %297, %none_32, %none_33, %int6_34, %cpu_35, %int0_36 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_37 = torch.constant.int 5
    %298 = torch.prims.convert_element_type %297, %int5_37 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %298, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %299 = torch.aten.mul.Tensor %4, %298 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %299, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_38 = torch.constant.none
    %none_39 = torch.constant.none
    %int6_40 = torch.constant.int 6
    %cpu_41 = torch.constant.device "cpu"
    %int0_42 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %299, %none_38, %none_39, %int6_40, %cpu_41, %int0_42 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_43 = torch.constant.int 5
    %300 = torch.prims.convert_element_type %299, %int5_43 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %300, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_44 = torch.constant.int 2
    %301 = torch.aten.view.dtype %5, %int2_44 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %302 = torch.aten.detach %301 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_45 = torch.constant.int -1
    %int17_46 = torch.constant.int 17
    %303 = torch.prim.ListConstruct %int-1_45, %int17_46 : (!torch.int, !torch.int) -> !torch.list<int>
    %304 = torch.aten.view %302, %303 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_47 = torch.constant.int 2048
    %int-1_48 = torch.constant.int -1
    %int17_49 = torch.constant.int 17
    %305 = torch.prim.ListConstruct %int2048_47, %int-1_48, %int17_49 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %306 = torch.aten.view %304, %305 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_50 = torch.constant.int 2
    %int0_51 = torch.constant.int 0
    %int1_52 = torch.constant.int 1
    %int1_53 = torch.constant.int 1
    %307 = torch.aten.slice.Tensor %306, %int2_50, %int0_51, %int1_52, %int1_53 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_54 = torch.constant.int 5
    %308 = torch.aten.view.dtype %307, %int5_54 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %309 = torch.aten.detach %308 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_55 = torch.constant.int 2
    %int1_56 = torch.constant.int 1
    %int9223372036854775807_57 = torch.constant.int 9223372036854775807
    %int1_58 = torch.constant.int 1
    %310 = torch.aten.slice.Tensor %306, %int2_55, %int1_56, %int9223372036854775807_57, %int1_58 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_59 = torch.constant.int 1
    %311 = torch.aten.view.dtype %310, %int1_59 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %312 = torch.aten.detach %311 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %313 = torch_c.to_builtin_tensor %300 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast = tensor.cast %313 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %314 = torch_c.to_builtin_tensor %309 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %315 = torch_c.to_builtin_tensor %312 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %316 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast, %314, %315) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_60 = tensor.cast %316 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %317 = torch_c.from_builtin_tensor %cast_60 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %317, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_61 = torch.constant.int 2
    %318 = torch.aten.view.dtype %6, %int2_61 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %319 = torch.aten.detach %318 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_62 = torch.constant.int -1
    %int17_63 = torch.constant.int 17
    %320 = torch.prim.ListConstruct %int-1_62, %int17_63 : (!torch.int, !torch.int) -> !torch.list<int>
    %321 = torch.aten.view %319, %320 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256 = torch.constant.int 256
    %int-1_64 = torch.constant.int -1
    %int17_65 = torch.constant.int 17
    %322 = torch.prim.ListConstruct %int256, %int-1_64, %int17_65 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %323 = torch.aten.view %321, %322 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_66 = torch.constant.int 2
    %int0_67 = torch.constant.int 0
    %int1_68 = torch.constant.int 1
    %int1_69 = torch.constant.int 1
    %324 = torch.aten.slice.Tensor %323, %int2_66, %int0_67, %int1_68, %int1_69 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_70 = torch.constant.int 5
    %325 = torch.aten.view.dtype %324, %int5_70 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %326 = torch.aten.detach %325 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_71 = torch.constant.int 2
    %int1_72 = torch.constant.int 1
    %int9223372036854775807_73 = torch.constant.int 9223372036854775807
    %int1_74 = torch.constant.int 1
    %327 = torch.aten.slice.Tensor %323, %int2_71, %int1_72, %int9223372036854775807_73, %int1_74 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_75 = torch.constant.int 1
    %328 = torch.aten.view.dtype %327, %int1_75 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %329 = torch.aten.detach %328 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %330 = torch_c.to_builtin_tensor %300 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_76 = tensor.cast %330 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %331 = torch_c.to_builtin_tensor %326 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %332 = torch_c.to_builtin_tensor %329 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %333 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_76, %331, %332) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_77 = tensor.cast %333 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %334 = torch_c.from_builtin_tensor %cast_77 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %334, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_78 = torch.constant.int 2
    %335 = torch.aten.view.dtype %7, %int2_78 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %336 = torch.aten.detach %335 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_79 = torch.constant.int -1
    %int17_80 = torch.constant.int 17
    %337 = torch.prim.ListConstruct %int-1_79, %int17_80 : (!torch.int, !torch.int) -> !torch.list<int>
    %338 = torch.aten.view %336, %337 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_81 = torch.constant.int 256
    %int-1_82 = torch.constant.int -1
    %int17_83 = torch.constant.int 17
    %339 = torch.prim.ListConstruct %int256_81, %int-1_82, %int17_83 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %340 = torch.aten.view %338, %339 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_84 = torch.constant.int 2
    %int0_85 = torch.constant.int 0
    %int1_86 = torch.constant.int 1
    %int1_87 = torch.constant.int 1
    %341 = torch.aten.slice.Tensor %340, %int2_84, %int0_85, %int1_86, %int1_87 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_88 = torch.constant.int 5
    %342 = torch.aten.view.dtype %341, %int5_88 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %343 = torch.aten.detach %342 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_89 = torch.constant.int 2
    %int1_90 = torch.constant.int 1
    %int9223372036854775807_91 = torch.constant.int 9223372036854775807
    %int1_92 = torch.constant.int 1
    %344 = torch.aten.slice.Tensor %340, %int2_89, %int1_90, %int9223372036854775807_91, %int1_92 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_93 = torch.constant.int 1
    %345 = torch.aten.view.dtype %344, %int1_93 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %346 = torch.aten.detach %345 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %347 = torch_c.to_builtin_tensor %300 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_94 = tensor.cast %347 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %348 = torch_c.to_builtin_tensor %343 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %349 = torch_c.to_builtin_tensor %346 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %350 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_94, %348, %349) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_95 = tensor.cast %350 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %351 = torch_c.from_builtin_tensor %cast_95 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %351, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4 = torch.constant.int 4
    %int32 = torch.constant.int 32
    %int64 = torch.constant.int 64
    %352 = torch.prim.ListConstruct %int4, %273, %int32, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %353 = torch.aten.view %317, %352 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %353, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_96 = torch.constant.int 4
    %int4_97 = torch.constant.int 4
    %int64_98 = torch.constant.int 64
    %354 = torch.prim.ListConstruct %int4_96, %273, %int4_97, %int64_98 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %355 = torch.aten.view %334, %354 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %355, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_99 = torch.constant.int 4
    %int4_100 = torch.constant.int 4
    %int64_101 = torch.constant.int 64
    %356 = torch.prim.ListConstruct %int4_99, %273, %int4_100, %int64_101 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %357 = torch.aten.view %351, %356 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %357, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_102 = torch.constant.int 0
    %none_103 = torch.constant.none
    %none_104 = torch.constant.none
    %cpu_105 = torch.constant.device "cpu"
    %false_106 = torch.constant.bool false
    %358 = torch.aten.arange.start %int0_102, %273, %none_103, %none_104, %cpu_105, %false_106 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %358, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_107 = torch.constant.int 0
    %359 = torch.aten.unsqueeze %358, %int0_107 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %359, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_108 = torch.constant.int 0
    %int64_109 = torch.constant.int 64
    %int2_110 = torch.constant.int 2
    %none_111 = torch.constant.none
    %none_112 = torch.constant.none
    %cpu_113 = torch.constant.device "cpu"
    %false_114 = torch.constant.bool false
    %360 = torch.aten.arange.start_step %int0_108, %int64_109, %int2_110, %none_111, %none_112, %cpu_113, %false_114 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_115 = torch.constant.none
    %none_116 = torch.constant.none
    %int4_117 = torch.constant.int 4
    %cpu_118 = torch.constant.device "cpu"
    %int0_119 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %360, %none_115, %none_116, %int4_117, %cpu_118, %int0_119 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_120 = torch.constant.int 6
    %361 = torch.prims.convert_element_type %360, %int6_120 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_121 = torch.constant.int 64
    %362 = torch.aten.div.Scalar %361, %int64_121 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04 = torch.constant.float 1.000000e+04
    %363 = torch.aten.pow.Scalar %float1.000000e04, %362 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %364 = torch.aten.reciprocal %363 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00 = torch.constant.float 1.000000e+00
    %365 = torch.aten.mul.Scalar %364, %float1.000000e00 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_122 = torch.constant.none
    %366 = torch.aten.clone %1, %none_122 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_123 = torch.constant.int 0
    %367 = torch.aten.unsqueeze %365, %int0_123 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_124 = torch.constant.int 2
    %368 = torch.aten.unsqueeze %367, %int2_124 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_125 = torch.constant.none
    %none_126 = torch.constant.none
    %int6_127 = torch.constant.int 6
    %cpu_128 = torch.constant.device "cpu"
    %int0_129 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %368, %none_125, %none_126, %int6_127, %cpu_128, %int0_129 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_130 = torch.constant.int 1
    %int-1_131 = torch.constant.int -1
    %int1_132 = torch.constant.int 1
    %369 = torch.prim.ListConstruct %int1_130, %int-1_131, %int1_132 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_133 = torch.constant.bool false
    %370 = torch.aten.expand %368, %369, %false_133 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_134 = torch.constant.int 1
    %371 = torch.aten.unsqueeze %359, %int1_134 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %371, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_135 = torch.constant.none
    %none_136 = torch.constant.none
    %int4_137 = torch.constant.int 4
    %cpu_138 = torch.constant.device "cpu"
    %int0_139 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %371, %none_135, %none_136, %int4_137, %cpu_138, %int0_139 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_140 = torch.constant.int 6
    %372 = torch.prims.convert_element_type %371, %int6_140 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %372, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %373 = torch.aten.matmul %370, %372 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %373, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_141 = torch.constant.int 1
    %int2_142 = torch.constant.int 2
    %374 = torch.aten.transpose.int %373, %int1_141, %int2_142 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %374, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %375 = torch.aten.cos %374 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %375, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %376 = torch.aten.mul.Tensor %375, %366 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %376, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_143 = torch.constant.none
    %none_144 = torch.constant.none
    %int6_145 = torch.constant.int 6
    %cpu_146 = torch.constant.device "cpu"
    %int0_147 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %376, %none_143, %none_144, %int6_145, %cpu_146, %int0_147 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_148 = torch.constant.int 5
    %377 = torch.prims.convert_element_type %376, %int5_148 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %377, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %378 = torch.aten.sin %374 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %378, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %379 = torch.aten.mul.Tensor %378, %366 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %379, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_149 = torch.constant.none
    %none_150 = torch.constant.none
    %int6_151 = torch.constant.int 6
    %cpu_152 = torch.constant.device "cpu"
    %int0_153 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %379, %none_149, %none_150, %int6_151, %cpu_152, %int0_153 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_154 = torch.constant.int 5
    %380 = torch.prims.convert_element_type %379, %int5_154 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %380, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_155 = torch.constant.int 2
    %381 = torch.aten.unsqueeze %377, %int2_155 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %381, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_156 = torch.constant.int 2
    %382 = torch.aten.unsqueeze %380, %int2_156 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %382, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_157 = torch.constant.none
    %none_158 = torch.constant.none
    %int5_159 = torch.constant.int 5
    %cpu_160 = torch.constant.device "cpu"
    %int0_161 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %381, %none_157, %none_158, %int5_159, %cpu_160, %int0_161 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_162 = torch.constant.none
    %none_163 = torch.constant.none
    %int5_164 = torch.constant.int 5
    %cpu_165 = torch.constant.device "cpu"
    %int0_166 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %382, %none_162, %none_163, %int5_164, %cpu_165, %int0_166 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_167 = torch.constant.none
    %none_168 = torch.constant.none
    %int5_169 = torch.constant.int 5
    %cpu_170 = torch.constant.device "cpu"
    %int0_171 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %353, %none_167, %none_168, %int5_169, %cpu_170, %int0_171 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3 = torch.constant.int 3
    %int0_172 = torch.constant.int 0
    %int64_173 = torch.constant.int 64
    %int2_174 = torch.constant.int 2
    %383 = torch.aten.slice.Tensor %353, %int3, %int0_172, %int64_173, %int2_174 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %383, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_175 = torch.constant.int 3
    %int1_176 = torch.constant.int 1
    %int64_177 = torch.constant.int 64
    %int2_178 = torch.constant.int 2
    %384 = torch.aten.slice.Tensor %353, %int3_175, %int1_176, %int64_177, %int2_178 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %384, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %385 = torch.aten.mul.Tensor %383, %381 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %385, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %386 = torch.aten.mul.Tensor %384, %382 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %386, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_179 = torch.constant.int 1
    %387 = torch.aten.sub.Tensor %385, %386, %int1_179 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %387, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %388 = torch.aten.mul.Tensor %384, %381 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %388, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %389 = torch.aten.mul.Tensor %383, %382 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %389, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_180 = torch.constant.int 1
    %390 = torch.aten.add.Tensor %388, %389, %int1_180 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %390, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %391 = torch_c.to_builtin_tensor %387 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_181 = tensor.cast %391 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %392 = torch_c.to_builtin_tensor %390 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_182 = tensor.cast %392 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %393 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_181, %cast_182) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_183 = tensor.cast %393 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %394 = torch_c.from_builtin_tensor %cast_183 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %394, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_184 = torch.constant.int 4
    %int32_185 = torch.constant.int 32
    %int64_186 = torch.constant.int 64
    %395 = torch.prim.ListConstruct %int4_184, %273, %int32_185, %int64_186 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %396 = torch.aten.view %394, %395 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %396, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_187 = torch.constant.none
    %none_188 = torch.constant.none
    %int5_189 = torch.constant.int 5
    %cpu_190 = torch.constant.device "cpu"
    %int0_191 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %396, %none_187, %none_188, %int5_189, %cpu_190, %int0_191 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_192 = torch.constant.int 0
    %none_193 = torch.constant.none
    %none_194 = torch.constant.none
    %cpu_195 = torch.constant.device "cpu"
    %false_196 = torch.constant.bool false
    %397 = torch.aten.arange.start %int0_192, %273, %none_193, %none_194, %cpu_195, %false_196 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %397, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_197 = torch.constant.int 0
    %398 = torch.aten.unsqueeze %397, %int0_197 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %398, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_198 = torch.constant.int 0
    %int64_199 = torch.constant.int 64
    %int2_200 = torch.constant.int 2
    %none_201 = torch.constant.none
    %none_202 = torch.constant.none
    %cpu_203 = torch.constant.device "cpu"
    %false_204 = torch.constant.bool false
    %399 = torch.aten.arange.start_step %int0_198, %int64_199, %int2_200, %none_201, %none_202, %cpu_203, %false_204 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_205 = torch.constant.none
    %none_206 = torch.constant.none
    %int4_207 = torch.constant.int 4
    %cpu_208 = torch.constant.device "cpu"
    %int0_209 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %399, %none_205, %none_206, %int4_207, %cpu_208, %int0_209 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_210 = torch.constant.int 6
    %400 = torch.prims.convert_element_type %399, %int6_210 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_211 = torch.constant.int 64
    %401 = torch.aten.div.Scalar %400, %int64_211 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_212 = torch.constant.float 1.000000e+04
    %402 = torch.aten.pow.Scalar %float1.000000e04_212, %401 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %403 = torch.aten.reciprocal %402 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_213 = torch.constant.float 1.000000e+00
    %404 = torch.aten.mul.Scalar %403, %float1.000000e00_213 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_214 = torch.constant.none
    %405 = torch.aten.clone %2, %none_214 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_215 = torch.constant.int 0
    %406 = torch.aten.unsqueeze %404, %int0_215 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_216 = torch.constant.int 2
    %407 = torch.aten.unsqueeze %406, %int2_216 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_217 = torch.constant.none
    %none_218 = torch.constant.none
    %int6_219 = torch.constant.int 6
    %cpu_220 = torch.constant.device "cpu"
    %int0_221 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %407, %none_217, %none_218, %int6_219, %cpu_220, %int0_221 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_222 = torch.constant.int 1
    %int-1_223 = torch.constant.int -1
    %int1_224 = torch.constant.int 1
    %408 = torch.prim.ListConstruct %int1_222, %int-1_223, %int1_224 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_225 = torch.constant.bool false
    %409 = torch.aten.expand %407, %408, %false_225 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_226 = torch.constant.int 1
    %410 = torch.aten.unsqueeze %398, %int1_226 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %410, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_227 = torch.constant.none
    %none_228 = torch.constant.none
    %int4_229 = torch.constant.int 4
    %cpu_230 = torch.constant.device "cpu"
    %int0_231 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %410, %none_227, %none_228, %int4_229, %cpu_230, %int0_231 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_232 = torch.constant.int 6
    %411 = torch.prims.convert_element_type %410, %int6_232 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %411, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %412 = torch.aten.matmul %409, %411 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %412, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_233 = torch.constant.int 1
    %int2_234 = torch.constant.int 2
    %413 = torch.aten.transpose.int %412, %int1_233, %int2_234 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %413, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %414 = torch.aten.cos %413 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %414, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %415 = torch.aten.mul.Tensor %414, %405 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %415, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_235 = torch.constant.none
    %none_236 = torch.constant.none
    %int6_237 = torch.constant.int 6
    %cpu_238 = torch.constant.device "cpu"
    %int0_239 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %415, %none_235, %none_236, %int6_237, %cpu_238, %int0_239 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_240 = torch.constant.int 5
    %416 = torch.prims.convert_element_type %415, %int5_240 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %416, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %417 = torch.aten.sin %413 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %417, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %418 = torch.aten.mul.Tensor %417, %405 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %418, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_241 = torch.constant.none
    %none_242 = torch.constant.none
    %int6_243 = torch.constant.int 6
    %cpu_244 = torch.constant.device "cpu"
    %int0_245 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %418, %none_241, %none_242, %int6_243, %cpu_244, %int0_245 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_246 = torch.constant.int 5
    %419 = torch.prims.convert_element_type %418, %int5_246 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %419, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_247 = torch.constant.int 2
    %420 = torch.aten.unsqueeze %416, %int2_247 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %420, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_248 = torch.constant.int 2
    %421 = torch.aten.unsqueeze %419, %int2_248 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %421, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_249 = torch.constant.none
    %none_250 = torch.constant.none
    %int5_251 = torch.constant.int 5
    %cpu_252 = torch.constant.device "cpu"
    %int0_253 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %420, %none_249, %none_250, %int5_251, %cpu_252, %int0_253 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_254 = torch.constant.none
    %none_255 = torch.constant.none
    %int5_256 = torch.constant.int 5
    %cpu_257 = torch.constant.device "cpu"
    %int0_258 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %421, %none_254, %none_255, %int5_256, %cpu_257, %int0_258 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_259 = torch.constant.none
    %none_260 = torch.constant.none
    %int5_261 = torch.constant.int 5
    %cpu_262 = torch.constant.device "cpu"
    %int0_263 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %355, %none_259, %none_260, %int5_261, %cpu_262, %int0_263 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_264 = torch.constant.int 3
    %int0_265 = torch.constant.int 0
    %int64_266 = torch.constant.int 64
    %int2_267 = torch.constant.int 2
    %422 = torch.aten.slice.Tensor %355, %int3_264, %int0_265, %int64_266, %int2_267 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %422, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_268 = torch.constant.int 3
    %int1_269 = torch.constant.int 1
    %int64_270 = torch.constant.int 64
    %int2_271 = torch.constant.int 2
    %423 = torch.aten.slice.Tensor %355, %int3_268, %int1_269, %int64_270, %int2_271 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %423, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %424 = torch.aten.mul.Tensor %422, %420 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %424, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %425 = torch.aten.mul.Tensor %423, %421 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %425, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_272 = torch.constant.int 1
    %426 = torch.aten.sub.Tensor %424, %425, %int1_272 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %426, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %427 = torch.aten.mul.Tensor %423, %420 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %427, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %428 = torch.aten.mul.Tensor %422, %421 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %428, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_273 = torch.constant.int 1
    %429 = torch.aten.add.Tensor %427, %428, %int1_273 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %429, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %430 = torch_c.to_builtin_tensor %426 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_274 = tensor.cast %430 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %431 = torch_c.to_builtin_tensor %429 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_275 = tensor.cast %431 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %432 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_274, %cast_275) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_276 = tensor.cast %432 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %433 = torch_c.from_builtin_tensor %cast_276 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %433, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_277 = torch.constant.int 4
    %int4_278 = torch.constant.int 4
    %int64_279 = torch.constant.int 64
    %434 = torch.prim.ListConstruct %int4_277, %273, %int4_278, %int64_279 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %435 = torch.aten.view %433, %434 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %435, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_280 = torch.constant.none
    %none_281 = torch.constant.none
    %int5_282 = torch.constant.int 5
    %cpu_283 = torch.constant.device "cpu"
    %int0_284 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %435, %none_280, %none_281, %int5_282, %cpu_283, %int0_284 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22 = torch.constant.int 22
    %int2_285 = torch.constant.int 2
    %int4_286 = torch.constant.int 4
    %int32_287 = torch.constant.int 32
    %int64_288 = torch.constant.int 64
    %436 = torch.prim.ListConstruct %272, %int22, %int2_285, %int4_286, %int32_287, %int64_288 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %437 = torch.aten.view %267, %436 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %437, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int22_289 = torch.constant.int 22
    %438 = torch.aten.mul.int %272, %int22_289 : !torch.int, !torch.int -> !torch.int
    %int2_290 = torch.constant.int 2
    %439 = torch.aten.mul.int %438, %int2_290 : !torch.int, !torch.int -> !torch.int
    %int4_291 = torch.constant.int 4
    %int32_292 = torch.constant.int 32
    %int64_293 = torch.constant.int 64
    %440 = torch.prim.ListConstruct %439, %int4_291, %int32_292, %int64_293 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %441 = torch.aten.view %437, %440 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %441, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_294 = torch.constant.int 22
    %442 = torch.aten.mul.Scalar %arg2, %int22_294 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %442, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_295 = torch.constant.int 0
    %int1_296 = torch.constant.int 1
    %443 = torch.aten.add.Scalar %442, %int0_295, %int1_296 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %443, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_297 = torch.constant.int 2
    %444 = torch.aten.mul.Scalar %443, %int2_297 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %444, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_298 = torch.constant.int 0
    %int1_299 = torch.constant.int 1
    %445 = torch.aten.add.Scalar %444, %int0_298, %int1_299 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %445, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_300 = torch.constant.int 4
    %446 = torch.aten.mul.int %int4_300, %271 : !torch.int, !torch.int -> !torch.int
    %447 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %448 = torch.aten.view %445, %447 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %448, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_301 = torch.constant.int 4
    %int32_302 = torch.constant.int 32
    %int4_303 = torch.constant.int 4
    %int64_304 = torch.constant.int 64
    %449 = torch.prim.ListConstruct %int4_301, %271, %int32_302, %int4_303, %int64_304 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %450 = torch.aten.view %435, %449 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %450, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_305 = torch.constant.int 32
    %int4_306 = torch.constant.int 4
    %int64_307 = torch.constant.int 64
    %451 = torch.prim.ListConstruct %446, %int32_305, %int4_306, %int64_307 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %452 = torch.aten.view %450, %451 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %452, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_308 = torch.constant.int 1
    %int2_309 = torch.constant.int 2
    %453 = torch.aten.transpose.int %452, %int1_308, %int2_309 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %453, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_310 = torch.constant.none
    %none_311 = torch.constant.none
    %int5_312 = torch.constant.int 5
    %cpu_313 = torch.constant.device "cpu"
    %int0_314 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %453, %none_310, %none_311, %int5_312, %cpu_313, %int0_314 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %454 = torch.prim.ListConstruct %448 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_315 = torch.constant.bool false
    %455 = torch.aten.index_put %441, %454, %453, %false_315 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %455, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_316 = torch.constant.int 22
    %int2_317 = torch.constant.int 2
    %int4_318 = torch.constant.int 4
    %int32_319 = torch.constant.int 32
    %int64_320 = torch.constant.int 64
    %456 = torch.prim.ListConstruct %272, %int22_316, %int2_317, %int4_318, %int32_319, %int64_320 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %457 = torch.aten.view %455, %456 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %457, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448 = torch.constant.int 360448
    %458 = torch.prim.ListConstruct %272, %int360448 : (!torch.int, !torch.int) -> !torch.list<int>
    %459 = torch.aten.view %457, %458 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %459, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_321 = torch.constant.int 22
    %int2_322 = torch.constant.int 2
    %int4_323 = torch.constant.int 4
    %int32_324 = torch.constant.int 32
    %int64_325 = torch.constant.int 64
    %460 = torch.prim.ListConstruct %272, %int22_321, %int2_322, %int4_323, %int32_324, %int64_325 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %461 = torch.aten.view %459, %460 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %461, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_326 = torch.constant.int 4
    %int32_327 = torch.constant.int 32
    %int64_328 = torch.constant.int 64
    %462 = torch.prim.ListConstruct %439, %int4_326, %int32_327, %int64_328 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %463 = torch.aten.view %461, %462 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %463, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_329 = torch.constant.int 22
    %464 = torch.aten.mul.Scalar %arg2, %int22_329 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %464, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_330 = torch.constant.int 0
    %int1_331 = torch.constant.int 1
    %465 = torch.aten.add.Scalar %464, %int0_330, %int1_331 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %465, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_332 = torch.constant.int 2
    %466 = torch.aten.mul.Scalar %465, %int2_332 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %466, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_333 = torch.constant.int 1
    %int1_334 = torch.constant.int 1
    %467 = torch.aten.add.Scalar %466, %int1_333, %int1_334 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %467, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %468 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %469 = torch.aten.view %467, %468 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %469, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_335 = torch.constant.int 4
    %int32_336 = torch.constant.int 32
    %int4_337 = torch.constant.int 4
    %int64_338 = torch.constant.int 64
    %470 = torch.prim.ListConstruct %int4_335, %271, %int32_336, %int4_337, %int64_338 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %471 = torch.aten.view %357, %470 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %471, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_339 = torch.constant.int 32
    %int4_340 = torch.constant.int 4
    %int64_341 = torch.constant.int 64
    %472 = torch.prim.ListConstruct %446, %int32_339, %int4_340, %int64_341 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %473 = torch.aten.view %471, %472 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %473, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_342 = torch.constant.int 1
    %int2_343 = torch.constant.int 2
    %474 = torch.aten.transpose.int %473, %int1_342, %int2_343 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %474, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_344 = torch.constant.none
    %none_345 = torch.constant.none
    %int5_346 = torch.constant.int 5
    %cpu_347 = torch.constant.device "cpu"
    %int0_348 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %474, %none_344, %none_345, %int5_346, %cpu_347, %int0_348 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %475 = torch.prim.ListConstruct %469 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_349 = torch.constant.bool false
    %476 = torch.aten.index_put %463, %475, %474, %false_349 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %476, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_350 = torch.constant.int 22
    %int2_351 = torch.constant.int 2
    %int4_352 = torch.constant.int 4
    %int32_353 = torch.constant.int 32
    %int64_354 = torch.constant.int 64
    %477 = torch.prim.ListConstruct %272, %int22_350, %int2_351, %int4_352, %int32_353, %int64_354 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %478 = torch.aten.view %476, %477 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %478, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_355 = torch.constant.int 360448
    %479 = torch.prim.ListConstruct %272, %int360448_355 : (!torch.int, !torch.int) -> !torch.list<int>
    %480 = torch.aten.view %478, %479 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %480, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_356 = torch.constant.int 0
    %int1_357 = torch.constant.int 1
    %none_358 = torch.constant.none
    %none_359 = torch.constant.none
    %cpu_360 = torch.constant.device "cpu"
    %false_361 = torch.constant.bool false
    %481 = torch.aten.arange.start_step %int0_356, %273, %int1_357, %none_358, %none_359, %cpu_360, %false_361 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %481, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_362 = torch.constant.int -1
    %482 = torch.aten.unsqueeze %arg1, %int-1_362 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %483 = torch.aten.ge.Tensor %481, %482 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %483, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_363 = torch.constant.none
    %none_364 = torch.constant.none
    %cpu_365 = torch.constant.device "cpu"
    %false_366 = torch.constant.bool false
    %484 = torch.aten.arange %273, %none_363, %none_364, %cpu_365, %false_366 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %484, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_367 = torch.constant.int 0
    %485 = torch.aten.unsqueeze %484, %int0_367 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %485, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_368 = torch.constant.int 1
    %486 = torch.aten.unsqueeze %485, %int1_368 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %486, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_369 = torch.constant.int 2
    %487 = torch.aten.unsqueeze %486, %int2_369 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %487, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_370 = torch.constant.none
    %none_371 = torch.constant.none
    %cpu_372 = torch.constant.device "cpu"
    %false_373 = torch.constant.bool false
    %488 = torch.aten.arange %273, %none_370, %none_371, %cpu_372, %false_373 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %488, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_374 = torch.constant.int 0
    %489 = torch.aten.unsqueeze %488, %int0_374 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %489, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_375 = torch.constant.int 1
    %490 = torch.aten.unsqueeze %489, %int1_375 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %490, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_376 = torch.constant.int 3
    %491 = torch.aten.unsqueeze %490, %int3_376 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %491, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %492 = torch.aten.gt.Tensor %487, %491 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %492, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_377 = torch.constant.int 1
    %493 = torch.aten.unsqueeze %483, %int1_377 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %493, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_378 = torch.constant.int 2
    %494 = torch.aten.unsqueeze %493, %int2_378 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %494, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %495 = torch.aten.logical_or %492, %494 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %495, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_379 = torch.constant.none
    %496 = torch.aten.clone %3, %none_379 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_380 = torch.constant.int 0
    %497 = torch.aten.where.ScalarOther %495, %496, %int0_380 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %497, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_381 = torch.constant.none
    %none_382 = torch.constant.none
    %int5_383 = torch.constant.int 5
    %cpu_384 = torch.constant.device "cpu"
    %int0_385 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %497, %none_381, %none_382, %int5_383, %cpu_384, %int0_385 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2 = torch.constant.int -2
    %498 = torch.aten.unsqueeze %435, %int-2 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %498, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_386 = torch.constant.int 4
    %int4_387 = torch.constant.int 4
    %int8 = torch.constant.int 8
    %int64_388 = torch.constant.int 64
    %499 = torch.prim.ListConstruct %int4_386, %273, %int4_387, %int8, %int64_388 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_389 = torch.constant.bool false
    %500 = torch.aten.expand %498, %499, %false_389 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %500, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_390 = torch.constant.int 0
    %501 = torch.aten.clone %500, %int0_390 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %501, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_391 = torch.constant.int 4
    %int32_392 = torch.constant.int 32
    %int64_393 = torch.constant.int 64
    %502 = torch.prim.ListConstruct %int4_391, %273, %int32_392, %int64_393 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %503 = torch.aten._unsafe_view %501, %502 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %503, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_394 = torch.constant.int -2
    %504 = torch.aten.unsqueeze %357, %int-2_394 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %504, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_395 = torch.constant.int 4
    %int4_396 = torch.constant.int 4
    %int8_397 = torch.constant.int 8
    %int64_398 = torch.constant.int 64
    %505 = torch.prim.ListConstruct %int4_395, %273, %int4_396, %int8_397, %int64_398 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_399 = torch.constant.bool false
    %506 = torch.aten.expand %504, %505, %false_399 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %506, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_400 = torch.constant.int 0
    %507 = torch.aten.clone %506, %int0_400 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %507, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_401 = torch.constant.int 4
    %int32_402 = torch.constant.int 32
    %int64_403 = torch.constant.int 64
    %508 = torch.prim.ListConstruct %int4_401, %273, %int32_402, %int64_403 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %509 = torch.aten._unsafe_view %507, %508 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %509, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_404 = torch.constant.int 1
    %int2_405 = torch.constant.int 2
    %510 = torch.aten.transpose.int %396, %int1_404, %int2_405 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %510, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_406 = torch.constant.int 1
    %int2_407 = torch.constant.int 2
    %511 = torch.aten.transpose.int %503, %int1_406, %int2_407 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %511, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_408 = torch.constant.int 1
    %int2_409 = torch.constant.int 2
    %512 = torch.aten.transpose.int %509, %int1_408, %int2_409 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %512, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00 = torch.constant.float 0.000000e+00
    %false_410 = torch.constant.bool false
    %none_411 = torch.constant.none
    %false_412 = torch.constant.bool false
    %513 = torch.aten.scaled_dot_product_attention %510, %511, %512, %497, %float0.000000e00, %false_410, %none_411, %false_412 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %513, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_413 = torch.constant.int 1
    %int2_414 = torch.constant.int 2
    %514 = torch.aten.transpose.int %513, %int1_413, %int2_414 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %514, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_415 = torch.constant.int 4
    %int2048_416 = torch.constant.int 2048
    %515 = torch.prim.ListConstruct %int4_415, %273, %int2048_416 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %516 = torch.aten.view %514, %515 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %516, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_417 = torch.constant.int 2
    %517 = torch.aten.view.dtype %8, %int2_417 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %518 = torch.aten.detach %517 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_418 = torch.constant.int -1
    %int17_419 = torch.constant.int 17
    %519 = torch.prim.ListConstruct %int-1_418, %int17_419 : (!torch.int, !torch.int) -> !torch.list<int>
    %520 = torch.aten.view %518, %519 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_420 = torch.constant.int 2048
    %int-1_421 = torch.constant.int -1
    %int17_422 = torch.constant.int 17
    %521 = torch.prim.ListConstruct %int2048_420, %int-1_421, %int17_422 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %522 = torch.aten.view %520, %521 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_423 = torch.constant.int 2
    %int0_424 = torch.constant.int 0
    %int1_425 = torch.constant.int 1
    %int1_426 = torch.constant.int 1
    %523 = torch.aten.slice.Tensor %522, %int2_423, %int0_424, %int1_425, %int1_426 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_427 = torch.constant.int 5
    %524 = torch.aten.view.dtype %523, %int5_427 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %525 = torch.aten.detach %524 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_428 = torch.constant.int 2
    %int1_429 = torch.constant.int 1
    %int9223372036854775807_430 = torch.constant.int 9223372036854775807
    %int1_431 = torch.constant.int 1
    %526 = torch.aten.slice.Tensor %522, %int2_428, %int1_429, %int9223372036854775807_430, %int1_431 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_432 = torch.constant.int 1
    %527 = torch.aten.view.dtype %526, %int1_432 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %528 = torch.aten.detach %527 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %529 = torch_c.to_builtin_tensor %516 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_433 = tensor.cast %529 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %530 = torch_c.to_builtin_tensor %525 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %531 = torch_c.to_builtin_tensor %528 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %532 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_433, %530, %531) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_434 = tensor.cast %532 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %533 = torch_c.from_builtin_tensor %cast_434 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %533, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_435 = torch.constant.none
    %none_436 = torch.constant.none
    %int5_437 = torch.constant.int 5
    %cpu_438 = torch.constant.device "cpu"
    %int0_439 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %533, %none_435, %none_436, %int5_437, %cpu_438, %int0_439 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_440 = torch.constant.int 1
    %534 = torch.aten.add.Tensor %290, %533, %int1_440 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %534, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_441 = torch.constant.none
    %none_442 = torch.constant.none
    %int5_443 = torch.constant.int 5
    %cpu_444 = torch.constant.device "cpu"
    %int0_445 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %534, %none_441, %none_442, %int5_443, %cpu_444, %int0_445 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_446 = torch.constant.int 6
    %535 = torch.prims.convert_element_type %534, %int6_446 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %535, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_447 = torch.constant.int 2
    %536 = torch.aten.pow.Tensor_Scalar %535, %int2_447 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %536, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_448 = torch.constant.int -1
    %537 = torch.prim.ListConstruct %int-1_448 : (!torch.int) -> !torch.list<int>
    %true_449 = torch.constant.bool true
    %none_450 = torch.constant.none
    %538 = torch.aten.mean.dim %536, %537, %true_449, %none_450 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %538, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_451 = torch.constant.float 9.9999997473787516E-6
    %int1_452 = torch.constant.int 1
    %539 = torch.aten.add.Scalar %538, %float9.999990e-06_451, %int1_452 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %539, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %540 = torch.aten.rsqrt %539 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %540, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %541 = torch.aten.mul.Tensor %535, %540 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %541, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_453 = torch.constant.none
    %none_454 = torch.constant.none
    %int6_455 = torch.constant.int 6
    %cpu_456 = torch.constant.device "cpu"
    %int0_457 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %541, %none_453, %none_454, %int6_455, %cpu_456, %int0_457 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_458 = torch.constant.int 5
    %542 = torch.prims.convert_element_type %541, %int5_458 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %542, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %543 = torch.aten.mul.Tensor %9, %542 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %543, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_459 = torch.constant.none
    %none_460 = torch.constant.none
    %int6_461 = torch.constant.int 6
    %cpu_462 = torch.constant.device "cpu"
    %int0_463 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %543, %none_459, %none_460, %int6_461, %cpu_462, %int0_463 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_464 = torch.constant.int 5
    %544 = torch.prims.convert_element_type %543, %int5_464 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %544, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_465 = torch.constant.int 2
    %545 = torch.aten.view.dtype %10, %int2_465 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %546 = torch.aten.detach %545 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_466 = torch.constant.int -1
    %int17_467 = torch.constant.int 17
    %547 = torch.prim.ListConstruct %int-1_466, %int17_467 : (!torch.int, !torch.int) -> !torch.list<int>
    %548 = torch.aten.view %546, %547 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632 = torch.constant.int 5632
    %int-1_468 = torch.constant.int -1
    %int17_469 = torch.constant.int 17
    %549 = torch.prim.ListConstruct %int5632, %int-1_468, %int17_469 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %550 = torch.aten.view %548, %549 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_470 = torch.constant.int 2
    %int0_471 = torch.constant.int 0
    %int1_472 = torch.constant.int 1
    %int1_473 = torch.constant.int 1
    %551 = torch.aten.slice.Tensor %550, %int2_470, %int0_471, %int1_472, %int1_473 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_474 = torch.constant.int 5
    %552 = torch.aten.view.dtype %551, %int5_474 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %553 = torch.aten.detach %552 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_475 = torch.constant.int 2
    %int1_476 = torch.constant.int 1
    %int9223372036854775807_477 = torch.constant.int 9223372036854775807
    %int1_478 = torch.constant.int 1
    %554 = torch.aten.slice.Tensor %550, %int2_475, %int1_476, %int9223372036854775807_477, %int1_478 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_479 = torch.constant.int 1
    %555 = torch.aten.view.dtype %554, %int1_479 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %556 = torch.aten.detach %555 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %557 = torch_c.to_builtin_tensor %544 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_480 = tensor.cast %557 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %558 = torch_c.to_builtin_tensor %553 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %559 = torch_c.to_builtin_tensor %556 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %560 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_480, %558, %559) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_481 = tensor.cast %560 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %561 = torch_c.from_builtin_tensor %cast_481 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %561, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %562 = torch.aten.silu %561 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %562, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_482 = torch.constant.int 2
    %563 = torch.aten.view.dtype %11, %int2_482 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %564 = torch.aten.detach %563 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_483 = torch.constant.int -1
    %int17_484 = torch.constant.int 17
    %565 = torch.prim.ListConstruct %int-1_483, %int17_484 : (!torch.int, !torch.int) -> !torch.list<int>
    %566 = torch.aten.view %564, %565 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_485 = torch.constant.int 5632
    %int-1_486 = torch.constant.int -1
    %int17_487 = torch.constant.int 17
    %567 = torch.prim.ListConstruct %int5632_485, %int-1_486, %int17_487 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %568 = torch.aten.view %566, %567 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_488 = torch.constant.int 2
    %int0_489 = torch.constant.int 0
    %int1_490 = torch.constant.int 1
    %int1_491 = torch.constant.int 1
    %569 = torch.aten.slice.Tensor %568, %int2_488, %int0_489, %int1_490, %int1_491 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_492 = torch.constant.int 5
    %570 = torch.aten.view.dtype %569, %int5_492 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %571 = torch.aten.detach %570 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_493 = torch.constant.int 2
    %int1_494 = torch.constant.int 1
    %int9223372036854775807_495 = torch.constant.int 9223372036854775807
    %int1_496 = torch.constant.int 1
    %572 = torch.aten.slice.Tensor %568, %int2_493, %int1_494, %int9223372036854775807_495, %int1_496 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_497 = torch.constant.int 1
    %573 = torch.aten.view.dtype %572, %int1_497 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %574 = torch.aten.detach %573 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %575 = torch_c.to_builtin_tensor %544 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_498 = tensor.cast %575 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %576 = torch_c.to_builtin_tensor %571 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %577 = torch_c.to_builtin_tensor %574 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %578 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_498, %576, %577) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_499 = tensor.cast %578 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %579 = torch_c.from_builtin_tensor %cast_499 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %579, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %580 = torch.aten.mul.Tensor %562, %579 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %580, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_500 = torch.constant.int 2
    %581 = torch.aten.view.dtype %12, %int2_500 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %582 = torch.aten.detach %581 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_501 = torch.constant.int -1
    %int17_502 = torch.constant.int 17
    %583 = torch.prim.ListConstruct %int-1_501, %int17_502 : (!torch.int, !torch.int) -> !torch.list<int>
    %584 = torch.aten.view %582, %583 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_503 = torch.constant.int 2048
    %int-1_504 = torch.constant.int -1
    %int17_505 = torch.constant.int 17
    %585 = torch.prim.ListConstruct %int2048_503, %int-1_504, %int17_505 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %586 = torch.aten.view %584, %585 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_506 = torch.constant.int 2
    %int0_507 = torch.constant.int 0
    %int1_508 = torch.constant.int 1
    %int1_509 = torch.constant.int 1
    %587 = torch.aten.slice.Tensor %586, %int2_506, %int0_507, %int1_508, %int1_509 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_510 = torch.constant.int 5
    %588 = torch.aten.view.dtype %587, %int5_510 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %589 = torch.aten.detach %588 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_511 = torch.constant.int 2
    %int1_512 = torch.constant.int 1
    %int9223372036854775807_513 = torch.constant.int 9223372036854775807
    %int1_514 = torch.constant.int 1
    %590 = torch.aten.slice.Tensor %586, %int2_511, %int1_512, %int9223372036854775807_513, %int1_514 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_515 = torch.constant.int 1
    %591 = torch.aten.view.dtype %590, %int1_515 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %592 = torch.aten.detach %591 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %593 = torch_c.to_builtin_tensor %580 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_516 = tensor.cast %593 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %594 = torch_c.to_builtin_tensor %589 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %595 = torch_c.to_builtin_tensor %592 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %596 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_516, %594, %595) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_517 = tensor.cast %596 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %597 = torch_c.from_builtin_tensor %cast_517 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %597, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_518 = torch.constant.int 1
    %598 = torch.aten.add.Tensor %534, %597, %int1_518 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %598, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_519 = torch.constant.none
    %none_520 = torch.constant.none
    %int5_521 = torch.constant.int 5
    %cpu_522 = torch.constant.device "cpu"
    %int0_523 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %598, %none_519, %none_520, %int5_521, %cpu_522, %int0_523 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_524 = torch.constant.int 6
    %599 = torch.prims.convert_element_type %598, %int6_524 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %599, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_525 = torch.constant.int 2
    %600 = torch.aten.pow.Tensor_Scalar %599, %int2_525 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %600, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_526 = torch.constant.int -1
    %601 = torch.prim.ListConstruct %int-1_526 : (!torch.int) -> !torch.list<int>
    %true_527 = torch.constant.bool true
    %none_528 = torch.constant.none
    %602 = torch.aten.mean.dim %600, %601, %true_527, %none_528 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %602, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_529 = torch.constant.float 9.9999997473787516E-6
    %int1_530 = torch.constant.int 1
    %603 = torch.aten.add.Scalar %602, %float9.999990e-06_529, %int1_530 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %603, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %604 = torch.aten.rsqrt %603 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %604, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %605 = torch.aten.mul.Tensor %599, %604 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %605, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_531 = torch.constant.none
    %none_532 = torch.constant.none
    %int6_533 = torch.constant.int 6
    %cpu_534 = torch.constant.device "cpu"
    %int0_535 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %605, %none_531, %none_532, %int6_533, %cpu_534, %int0_535 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_536 = torch.constant.int 5
    %606 = torch.prims.convert_element_type %605, %int5_536 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %606, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %607 = torch.aten.mul.Tensor %16, %606 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %607, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_537 = torch.constant.none
    %none_538 = torch.constant.none
    %int6_539 = torch.constant.int 6
    %cpu_540 = torch.constant.device "cpu"
    %int0_541 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %607, %none_537, %none_538, %int6_539, %cpu_540, %int0_541 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_542 = torch.constant.int 5
    %608 = torch.prims.convert_element_type %607, %int5_542 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %608, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_543 = torch.constant.int 2
    %609 = torch.aten.view.dtype %17, %int2_543 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %610 = torch.aten.detach %609 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_544 = torch.constant.int -1
    %int17_545 = torch.constant.int 17
    %611 = torch.prim.ListConstruct %int-1_544, %int17_545 : (!torch.int, !torch.int) -> !torch.list<int>
    %612 = torch.aten.view %610, %611 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_546 = torch.constant.int 2048
    %int-1_547 = torch.constant.int -1
    %int17_548 = torch.constant.int 17
    %613 = torch.prim.ListConstruct %int2048_546, %int-1_547, %int17_548 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %614 = torch.aten.view %612, %613 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_549 = torch.constant.int 2
    %int0_550 = torch.constant.int 0
    %int1_551 = torch.constant.int 1
    %int1_552 = torch.constant.int 1
    %615 = torch.aten.slice.Tensor %614, %int2_549, %int0_550, %int1_551, %int1_552 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_553 = torch.constant.int 5
    %616 = torch.aten.view.dtype %615, %int5_553 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %617 = torch.aten.detach %616 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_554 = torch.constant.int 2
    %int1_555 = torch.constant.int 1
    %int9223372036854775807_556 = torch.constant.int 9223372036854775807
    %int1_557 = torch.constant.int 1
    %618 = torch.aten.slice.Tensor %614, %int2_554, %int1_555, %int9223372036854775807_556, %int1_557 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_558 = torch.constant.int 1
    %619 = torch.aten.view.dtype %618, %int1_558 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %620 = torch.aten.detach %619 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %621 = torch_c.to_builtin_tensor %608 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_559 = tensor.cast %621 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %622 = torch_c.to_builtin_tensor %617 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %623 = torch_c.to_builtin_tensor %620 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %624 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_559, %622, %623) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_560 = tensor.cast %624 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %625 = torch_c.from_builtin_tensor %cast_560 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %625, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_561 = torch.constant.int 2
    %626 = torch.aten.view.dtype %18, %int2_561 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %627 = torch.aten.detach %626 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_562 = torch.constant.int -1
    %int17_563 = torch.constant.int 17
    %628 = torch.prim.ListConstruct %int-1_562, %int17_563 : (!torch.int, !torch.int) -> !torch.list<int>
    %629 = torch.aten.view %627, %628 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_564 = torch.constant.int 256
    %int-1_565 = torch.constant.int -1
    %int17_566 = torch.constant.int 17
    %630 = torch.prim.ListConstruct %int256_564, %int-1_565, %int17_566 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %631 = torch.aten.view %629, %630 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_567 = torch.constant.int 2
    %int0_568 = torch.constant.int 0
    %int1_569 = torch.constant.int 1
    %int1_570 = torch.constant.int 1
    %632 = torch.aten.slice.Tensor %631, %int2_567, %int0_568, %int1_569, %int1_570 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_571 = torch.constant.int 5
    %633 = torch.aten.view.dtype %632, %int5_571 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %634 = torch.aten.detach %633 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_572 = torch.constant.int 2
    %int1_573 = torch.constant.int 1
    %int9223372036854775807_574 = torch.constant.int 9223372036854775807
    %int1_575 = torch.constant.int 1
    %635 = torch.aten.slice.Tensor %631, %int2_572, %int1_573, %int9223372036854775807_574, %int1_575 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_576 = torch.constant.int 1
    %636 = torch.aten.view.dtype %635, %int1_576 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %637 = torch.aten.detach %636 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %638 = torch_c.to_builtin_tensor %608 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_577 = tensor.cast %638 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %639 = torch_c.to_builtin_tensor %634 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %640 = torch_c.to_builtin_tensor %637 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %641 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_577, %639, %640) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_578 = tensor.cast %641 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %642 = torch_c.from_builtin_tensor %cast_578 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %642, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_579 = torch.constant.int 2
    %643 = torch.aten.view.dtype %19, %int2_579 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %644 = torch.aten.detach %643 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_580 = torch.constant.int -1
    %int17_581 = torch.constant.int 17
    %645 = torch.prim.ListConstruct %int-1_580, %int17_581 : (!torch.int, !torch.int) -> !torch.list<int>
    %646 = torch.aten.view %644, %645 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_582 = torch.constant.int 256
    %int-1_583 = torch.constant.int -1
    %int17_584 = torch.constant.int 17
    %647 = torch.prim.ListConstruct %int256_582, %int-1_583, %int17_584 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %648 = torch.aten.view %646, %647 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_585 = torch.constant.int 2
    %int0_586 = torch.constant.int 0
    %int1_587 = torch.constant.int 1
    %int1_588 = torch.constant.int 1
    %649 = torch.aten.slice.Tensor %648, %int2_585, %int0_586, %int1_587, %int1_588 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_589 = torch.constant.int 5
    %650 = torch.aten.view.dtype %649, %int5_589 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %651 = torch.aten.detach %650 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_590 = torch.constant.int 2
    %int1_591 = torch.constant.int 1
    %int9223372036854775807_592 = torch.constant.int 9223372036854775807
    %int1_593 = torch.constant.int 1
    %652 = torch.aten.slice.Tensor %648, %int2_590, %int1_591, %int9223372036854775807_592, %int1_593 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_594 = torch.constant.int 1
    %653 = torch.aten.view.dtype %652, %int1_594 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %654 = torch.aten.detach %653 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %655 = torch_c.to_builtin_tensor %608 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_595 = tensor.cast %655 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %656 = torch_c.to_builtin_tensor %651 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %657 = torch_c.to_builtin_tensor %654 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %658 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_595, %656, %657) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_596 = tensor.cast %658 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %659 = torch_c.from_builtin_tensor %cast_596 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %659, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_597 = torch.constant.int 4
    %int32_598 = torch.constant.int 32
    %int64_599 = torch.constant.int 64
    %660 = torch.prim.ListConstruct %int4_597, %273, %int32_598, %int64_599 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %661 = torch.aten.view %625, %660 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %661, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_600 = torch.constant.int 4
    %int4_601 = torch.constant.int 4
    %int64_602 = torch.constant.int 64
    %662 = torch.prim.ListConstruct %int4_600, %273, %int4_601, %int64_602 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %663 = torch.aten.view %642, %662 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %663, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_603 = torch.constant.int 4
    %int4_604 = torch.constant.int 4
    %int64_605 = torch.constant.int 64
    %664 = torch.prim.ListConstruct %int4_603, %273, %int4_604, %int64_605 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %665 = torch.aten.view %659, %664 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %665, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_606 = torch.constant.int 0
    %none_607 = torch.constant.none
    %none_608 = torch.constant.none
    %cpu_609 = torch.constant.device "cpu"
    %false_610 = torch.constant.bool false
    %666 = torch.aten.arange.start %int0_606, %273, %none_607, %none_608, %cpu_609, %false_610 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %666, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_611 = torch.constant.int 0
    %667 = torch.aten.unsqueeze %666, %int0_611 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %667, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_612 = torch.constant.int 0
    %int64_613 = torch.constant.int 64
    %int2_614 = torch.constant.int 2
    %none_615 = torch.constant.none
    %none_616 = torch.constant.none
    %cpu_617 = torch.constant.device "cpu"
    %false_618 = torch.constant.bool false
    %668 = torch.aten.arange.start_step %int0_612, %int64_613, %int2_614, %none_615, %none_616, %cpu_617, %false_618 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_619 = torch.constant.none
    %none_620 = torch.constant.none
    %int4_621 = torch.constant.int 4
    %cpu_622 = torch.constant.device "cpu"
    %int0_623 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %668, %none_619, %none_620, %int4_621, %cpu_622, %int0_623 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_624 = torch.constant.int 6
    %669 = torch.prims.convert_element_type %668, %int6_624 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_625 = torch.constant.int 64
    %670 = torch.aten.div.Scalar %669, %int64_625 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_626 = torch.constant.float 1.000000e+04
    %671 = torch.aten.pow.Scalar %float1.000000e04_626, %670 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %672 = torch.aten.reciprocal %671 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_627 = torch.constant.float 1.000000e+00
    %673 = torch.aten.mul.Scalar %672, %float1.000000e00_627 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_628 = torch.constant.none
    %674 = torch.aten.clone %13, %none_628 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_629 = torch.constant.int 0
    %675 = torch.aten.unsqueeze %673, %int0_629 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_630 = torch.constant.int 2
    %676 = torch.aten.unsqueeze %675, %int2_630 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_631 = torch.constant.none
    %none_632 = torch.constant.none
    %int6_633 = torch.constant.int 6
    %cpu_634 = torch.constant.device "cpu"
    %int0_635 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %676, %none_631, %none_632, %int6_633, %cpu_634, %int0_635 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_636 = torch.constant.int 1
    %int-1_637 = torch.constant.int -1
    %int1_638 = torch.constant.int 1
    %677 = torch.prim.ListConstruct %int1_636, %int-1_637, %int1_638 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_639 = torch.constant.bool false
    %678 = torch.aten.expand %676, %677, %false_639 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_640 = torch.constant.int 1
    %679 = torch.aten.unsqueeze %667, %int1_640 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %679, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_641 = torch.constant.none
    %none_642 = torch.constant.none
    %int4_643 = torch.constant.int 4
    %cpu_644 = torch.constant.device "cpu"
    %int0_645 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %679, %none_641, %none_642, %int4_643, %cpu_644, %int0_645 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_646 = torch.constant.int 6
    %680 = torch.prims.convert_element_type %679, %int6_646 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %680, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %681 = torch.aten.matmul %678, %680 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %681, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_647 = torch.constant.int 1
    %int2_648 = torch.constant.int 2
    %682 = torch.aten.transpose.int %681, %int1_647, %int2_648 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %682, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %683 = torch.aten.cos %682 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %683, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %684 = torch.aten.mul.Tensor %683, %674 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %684, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_649 = torch.constant.none
    %none_650 = torch.constant.none
    %int6_651 = torch.constant.int 6
    %cpu_652 = torch.constant.device "cpu"
    %int0_653 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %684, %none_649, %none_650, %int6_651, %cpu_652, %int0_653 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_654 = torch.constant.int 5
    %685 = torch.prims.convert_element_type %684, %int5_654 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %685, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %686 = torch.aten.sin %682 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %686, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %687 = torch.aten.mul.Tensor %686, %674 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %687, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_655 = torch.constant.none
    %none_656 = torch.constant.none
    %int6_657 = torch.constant.int 6
    %cpu_658 = torch.constant.device "cpu"
    %int0_659 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %687, %none_655, %none_656, %int6_657, %cpu_658, %int0_659 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_660 = torch.constant.int 5
    %688 = torch.prims.convert_element_type %687, %int5_660 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %688, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_661 = torch.constant.int 2
    %689 = torch.aten.unsqueeze %685, %int2_661 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %689, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_662 = torch.constant.int 2
    %690 = torch.aten.unsqueeze %688, %int2_662 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %690, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_663 = torch.constant.none
    %none_664 = torch.constant.none
    %int5_665 = torch.constant.int 5
    %cpu_666 = torch.constant.device "cpu"
    %int0_667 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %689, %none_663, %none_664, %int5_665, %cpu_666, %int0_667 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_668 = torch.constant.none
    %none_669 = torch.constant.none
    %int5_670 = torch.constant.int 5
    %cpu_671 = torch.constant.device "cpu"
    %int0_672 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %690, %none_668, %none_669, %int5_670, %cpu_671, %int0_672 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_673 = torch.constant.none
    %none_674 = torch.constant.none
    %int5_675 = torch.constant.int 5
    %cpu_676 = torch.constant.device "cpu"
    %int0_677 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %661, %none_673, %none_674, %int5_675, %cpu_676, %int0_677 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_678 = torch.constant.int 3
    %int0_679 = torch.constant.int 0
    %int64_680 = torch.constant.int 64
    %int2_681 = torch.constant.int 2
    %691 = torch.aten.slice.Tensor %661, %int3_678, %int0_679, %int64_680, %int2_681 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %691, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_682 = torch.constant.int 3
    %int1_683 = torch.constant.int 1
    %int64_684 = torch.constant.int 64
    %int2_685 = torch.constant.int 2
    %692 = torch.aten.slice.Tensor %661, %int3_682, %int1_683, %int64_684, %int2_685 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %692, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %693 = torch.aten.mul.Tensor %691, %689 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %693, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %694 = torch.aten.mul.Tensor %692, %690 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %694, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_686 = torch.constant.int 1
    %695 = torch.aten.sub.Tensor %693, %694, %int1_686 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %695, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %696 = torch.aten.mul.Tensor %692, %689 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %696, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %697 = torch.aten.mul.Tensor %691, %690 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %697, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_687 = torch.constant.int 1
    %698 = torch.aten.add.Tensor %696, %697, %int1_687 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %698, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %699 = torch_c.to_builtin_tensor %695 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_688 = tensor.cast %699 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %700 = torch_c.to_builtin_tensor %698 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_689 = tensor.cast %700 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %701 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_688, %cast_689) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_690 = tensor.cast %701 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %702 = torch_c.from_builtin_tensor %cast_690 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %702, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_691 = torch.constant.int 4
    %int32_692 = torch.constant.int 32
    %int64_693 = torch.constant.int 64
    %703 = torch.prim.ListConstruct %int4_691, %273, %int32_692, %int64_693 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %704 = torch.aten.view %702, %703 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %704, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_694 = torch.constant.none
    %none_695 = torch.constant.none
    %int5_696 = torch.constant.int 5
    %cpu_697 = torch.constant.device "cpu"
    %int0_698 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %704, %none_694, %none_695, %int5_696, %cpu_697, %int0_698 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_699 = torch.constant.int 0
    %none_700 = torch.constant.none
    %none_701 = torch.constant.none
    %cpu_702 = torch.constant.device "cpu"
    %false_703 = torch.constant.bool false
    %705 = torch.aten.arange.start %int0_699, %273, %none_700, %none_701, %cpu_702, %false_703 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %705, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_704 = torch.constant.int 0
    %706 = torch.aten.unsqueeze %705, %int0_704 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %706, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_705 = torch.constant.int 0
    %int64_706 = torch.constant.int 64
    %int2_707 = torch.constant.int 2
    %none_708 = torch.constant.none
    %none_709 = torch.constant.none
    %cpu_710 = torch.constant.device "cpu"
    %false_711 = torch.constant.bool false
    %707 = torch.aten.arange.start_step %int0_705, %int64_706, %int2_707, %none_708, %none_709, %cpu_710, %false_711 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_712 = torch.constant.none
    %none_713 = torch.constant.none
    %int4_714 = torch.constant.int 4
    %cpu_715 = torch.constant.device "cpu"
    %int0_716 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %707, %none_712, %none_713, %int4_714, %cpu_715, %int0_716 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_717 = torch.constant.int 6
    %708 = torch.prims.convert_element_type %707, %int6_717 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_718 = torch.constant.int 64
    %709 = torch.aten.div.Scalar %708, %int64_718 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_719 = torch.constant.float 1.000000e+04
    %710 = torch.aten.pow.Scalar %float1.000000e04_719, %709 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %711 = torch.aten.reciprocal %710 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_720 = torch.constant.float 1.000000e+00
    %712 = torch.aten.mul.Scalar %711, %float1.000000e00_720 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_721 = torch.constant.none
    %713 = torch.aten.clone %14, %none_721 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_722 = torch.constant.int 0
    %714 = torch.aten.unsqueeze %712, %int0_722 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_723 = torch.constant.int 2
    %715 = torch.aten.unsqueeze %714, %int2_723 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_724 = torch.constant.none
    %none_725 = torch.constant.none
    %int6_726 = torch.constant.int 6
    %cpu_727 = torch.constant.device "cpu"
    %int0_728 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %715, %none_724, %none_725, %int6_726, %cpu_727, %int0_728 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_729 = torch.constant.int 1
    %int-1_730 = torch.constant.int -1
    %int1_731 = torch.constant.int 1
    %716 = torch.prim.ListConstruct %int1_729, %int-1_730, %int1_731 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_732 = torch.constant.bool false
    %717 = torch.aten.expand %715, %716, %false_732 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_733 = torch.constant.int 1
    %718 = torch.aten.unsqueeze %706, %int1_733 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %718, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_734 = torch.constant.none
    %none_735 = torch.constant.none
    %int4_736 = torch.constant.int 4
    %cpu_737 = torch.constant.device "cpu"
    %int0_738 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %718, %none_734, %none_735, %int4_736, %cpu_737, %int0_738 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_739 = torch.constant.int 6
    %719 = torch.prims.convert_element_type %718, %int6_739 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %719, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %720 = torch.aten.matmul %717, %719 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %720, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_740 = torch.constant.int 1
    %int2_741 = torch.constant.int 2
    %721 = torch.aten.transpose.int %720, %int1_740, %int2_741 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %721, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %722 = torch.aten.cos %721 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %722, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %723 = torch.aten.mul.Tensor %722, %713 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %723, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_742 = torch.constant.none
    %none_743 = torch.constant.none
    %int6_744 = torch.constant.int 6
    %cpu_745 = torch.constant.device "cpu"
    %int0_746 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %723, %none_742, %none_743, %int6_744, %cpu_745, %int0_746 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_747 = torch.constant.int 5
    %724 = torch.prims.convert_element_type %723, %int5_747 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %724, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %725 = torch.aten.sin %721 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %725, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %726 = torch.aten.mul.Tensor %725, %713 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %726, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_748 = torch.constant.none
    %none_749 = torch.constant.none
    %int6_750 = torch.constant.int 6
    %cpu_751 = torch.constant.device "cpu"
    %int0_752 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %726, %none_748, %none_749, %int6_750, %cpu_751, %int0_752 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_753 = torch.constant.int 5
    %727 = torch.prims.convert_element_type %726, %int5_753 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %727, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_754 = torch.constant.int 2
    %728 = torch.aten.unsqueeze %724, %int2_754 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %728, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_755 = torch.constant.int 2
    %729 = torch.aten.unsqueeze %727, %int2_755 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %729, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_756 = torch.constant.none
    %none_757 = torch.constant.none
    %int5_758 = torch.constant.int 5
    %cpu_759 = torch.constant.device "cpu"
    %int0_760 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %728, %none_756, %none_757, %int5_758, %cpu_759, %int0_760 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_761 = torch.constant.none
    %none_762 = torch.constant.none
    %int5_763 = torch.constant.int 5
    %cpu_764 = torch.constant.device "cpu"
    %int0_765 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %729, %none_761, %none_762, %int5_763, %cpu_764, %int0_765 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_766 = torch.constant.none
    %none_767 = torch.constant.none
    %int5_768 = torch.constant.int 5
    %cpu_769 = torch.constant.device "cpu"
    %int0_770 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %663, %none_766, %none_767, %int5_768, %cpu_769, %int0_770 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_771 = torch.constant.int 3
    %int0_772 = torch.constant.int 0
    %int64_773 = torch.constant.int 64
    %int2_774 = torch.constant.int 2
    %730 = torch.aten.slice.Tensor %663, %int3_771, %int0_772, %int64_773, %int2_774 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %730, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_775 = torch.constant.int 3
    %int1_776 = torch.constant.int 1
    %int64_777 = torch.constant.int 64
    %int2_778 = torch.constant.int 2
    %731 = torch.aten.slice.Tensor %663, %int3_775, %int1_776, %int64_777, %int2_778 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %731, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %732 = torch.aten.mul.Tensor %730, %728 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %732, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %733 = torch.aten.mul.Tensor %731, %729 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %733, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_779 = torch.constant.int 1
    %734 = torch.aten.sub.Tensor %732, %733, %int1_779 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %734, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %735 = torch.aten.mul.Tensor %731, %728 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %735, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %736 = torch.aten.mul.Tensor %730, %729 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %736, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_780 = torch.constant.int 1
    %737 = torch.aten.add.Tensor %735, %736, %int1_780 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %737, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %738 = torch_c.to_builtin_tensor %734 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_781 = tensor.cast %738 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %739 = torch_c.to_builtin_tensor %737 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_782 = tensor.cast %739 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %740 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_781, %cast_782) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_783 = tensor.cast %740 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %741 = torch_c.from_builtin_tensor %cast_783 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %741, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_784 = torch.constant.int 4
    %int4_785 = torch.constant.int 4
    %int64_786 = torch.constant.int 64
    %742 = torch.prim.ListConstruct %int4_784, %273, %int4_785, %int64_786 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %743 = torch.aten.view %741, %742 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %743, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_787 = torch.constant.none
    %none_788 = torch.constant.none
    %int5_789 = torch.constant.int 5
    %cpu_790 = torch.constant.device "cpu"
    %int0_791 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %743, %none_787, %none_788, %int5_789, %cpu_790, %int0_791 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_792 = torch.constant.int 22
    %744 = torch.aten.mul.Scalar %arg2, %int22_792 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %744, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_793 = torch.constant.int 1
    %int1_794 = torch.constant.int 1
    %745 = torch.aten.add.Scalar %744, %int1_793, %int1_794 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %745, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_795 = torch.constant.int 2
    %746 = torch.aten.mul.Scalar %745, %int2_795 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %746, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_796 = torch.constant.int 0
    %int1_797 = torch.constant.int 1
    %747 = torch.aten.add.Scalar %746, %int0_796, %int1_797 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %747, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %748 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %749 = torch.aten.view %747, %748 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %749, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_798 = torch.constant.int 4
    %int32_799 = torch.constant.int 32
    %int4_800 = torch.constant.int 4
    %int64_801 = torch.constant.int 64
    %750 = torch.prim.ListConstruct %int4_798, %271, %int32_799, %int4_800, %int64_801 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %751 = torch.aten.view %743, %750 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %751, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_802 = torch.constant.int 32
    %int4_803 = torch.constant.int 4
    %int64_804 = torch.constant.int 64
    %752 = torch.prim.ListConstruct %446, %int32_802, %int4_803, %int64_804 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %753 = torch.aten.view %751, %752 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %753, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_805 = torch.constant.int 1
    %int2_806 = torch.constant.int 2
    %754 = torch.aten.transpose.int %753, %int1_805, %int2_806 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %754, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_807 = torch.constant.none
    %none_808 = torch.constant.none
    %int5_809 = torch.constant.int 5
    %cpu_810 = torch.constant.device "cpu"
    %int0_811 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %754, %none_807, %none_808, %int5_809, %cpu_810, %int0_811 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_812 = torch.constant.int 22
    %int2_813 = torch.constant.int 2
    %int4_814 = torch.constant.int 4
    %int32_815 = torch.constant.int 32
    %int64_816 = torch.constant.int 64
    %755 = torch.prim.ListConstruct %272, %int22_812, %int2_813, %int4_814, %int32_815, %int64_816 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %756 = torch.aten.view %480, %755 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %756, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_817 = torch.constant.int 4
    %int32_818 = torch.constant.int 32
    %int64_819 = torch.constant.int 64
    %757 = torch.prim.ListConstruct %439, %int4_817, %int32_818, %int64_819 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %758 = torch.aten.view %756, %757 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %758, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %759 = torch.prim.ListConstruct %749 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_820 = torch.constant.bool false
    %760 = torch.aten.index_put %758, %759, %754, %false_820 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %760, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_821 = torch.constant.int 22
    %int2_822 = torch.constant.int 2
    %int4_823 = torch.constant.int 4
    %int32_824 = torch.constant.int 32
    %int64_825 = torch.constant.int 64
    %761 = torch.prim.ListConstruct %272, %int22_821, %int2_822, %int4_823, %int32_824, %int64_825 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %762 = torch.aten.view %760, %761 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %762, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_826 = torch.constant.int 360448
    %763 = torch.prim.ListConstruct %272, %int360448_826 : (!torch.int, !torch.int) -> !torch.list<int>
    %764 = torch.aten.view %762, %763 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %764, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_827 = torch.constant.int 22
    %int2_828 = torch.constant.int 2
    %int4_829 = torch.constant.int 4
    %int32_830 = torch.constant.int 32
    %int64_831 = torch.constant.int 64
    %765 = torch.prim.ListConstruct %272, %int22_827, %int2_828, %int4_829, %int32_830, %int64_831 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %766 = torch.aten.view %764, %765 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %766, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_832 = torch.constant.int 4
    %int32_833 = torch.constant.int 32
    %int64_834 = torch.constant.int 64
    %767 = torch.prim.ListConstruct %439, %int4_832, %int32_833, %int64_834 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %768 = torch.aten.view %766, %767 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %768, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_835 = torch.constant.int 22
    %769 = torch.aten.mul.Scalar %arg2, %int22_835 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %769, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_836 = torch.constant.int 1
    %int1_837 = torch.constant.int 1
    %770 = torch.aten.add.Scalar %769, %int1_836, %int1_837 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %770, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_838 = torch.constant.int 2
    %771 = torch.aten.mul.Scalar %770, %int2_838 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %771, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_839 = torch.constant.int 1
    %int1_840 = torch.constant.int 1
    %772 = torch.aten.add.Scalar %771, %int1_839, %int1_840 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %772, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %773 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %774 = torch.aten.view %772, %773 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %774, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_841 = torch.constant.int 4
    %int32_842 = torch.constant.int 32
    %int4_843 = torch.constant.int 4
    %int64_844 = torch.constant.int 64
    %775 = torch.prim.ListConstruct %int4_841, %271, %int32_842, %int4_843, %int64_844 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %776 = torch.aten.view %665, %775 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %776, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_845 = torch.constant.int 32
    %int4_846 = torch.constant.int 4
    %int64_847 = torch.constant.int 64
    %777 = torch.prim.ListConstruct %446, %int32_845, %int4_846, %int64_847 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %778 = torch.aten.view %776, %777 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %778, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_848 = torch.constant.int 1
    %int2_849 = torch.constant.int 2
    %779 = torch.aten.transpose.int %778, %int1_848, %int2_849 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %779, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_850 = torch.constant.none
    %none_851 = torch.constant.none
    %int5_852 = torch.constant.int 5
    %cpu_853 = torch.constant.device "cpu"
    %int0_854 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %779, %none_850, %none_851, %int5_852, %cpu_853, %int0_854 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %780 = torch.prim.ListConstruct %774 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_855 = torch.constant.bool false
    %781 = torch.aten.index_put %768, %780, %779, %false_855 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %781, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_856 = torch.constant.int 22
    %int2_857 = torch.constant.int 2
    %int4_858 = torch.constant.int 4
    %int32_859 = torch.constant.int 32
    %int64_860 = torch.constant.int 64
    %782 = torch.prim.ListConstruct %272, %int22_856, %int2_857, %int4_858, %int32_859, %int64_860 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %783 = torch.aten.view %781, %782 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %783, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_861 = torch.constant.int 360448
    %784 = torch.prim.ListConstruct %272, %int360448_861 : (!torch.int, !torch.int) -> !torch.list<int>
    %785 = torch.aten.view %783, %784 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %785, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_862 = torch.constant.int 0
    %int1_863 = torch.constant.int 1
    %none_864 = torch.constant.none
    %none_865 = torch.constant.none
    %cpu_866 = torch.constant.device "cpu"
    %false_867 = torch.constant.bool false
    %786 = torch.aten.arange.start_step %int0_862, %273, %int1_863, %none_864, %none_865, %cpu_866, %false_867 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %786, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_868 = torch.constant.int -1
    %787 = torch.aten.unsqueeze %arg1, %int-1_868 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %788 = torch.aten.ge.Tensor %786, %787 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %788, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_869 = torch.constant.none
    %none_870 = torch.constant.none
    %cpu_871 = torch.constant.device "cpu"
    %false_872 = torch.constant.bool false
    %789 = torch.aten.arange %273, %none_869, %none_870, %cpu_871, %false_872 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %789, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_873 = torch.constant.int 0
    %790 = torch.aten.unsqueeze %789, %int0_873 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %790, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_874 = torch.constant.int 1
    %791 = torch.aten.unsqueeze %790, %int1_874 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %791, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_875 = torch.constant.int 2
    %792 = torch.aten.unsqueeze %791, %int2_875 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %792, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_876 = torch.constant.none
    %none_877 = torch.constant.none
    %cpu_878 = torch.constant.device "cpu"
    %false_879 = torch.constant.bool false
    %793 = torch.aten.arange %273, %none_876, %none_877, %cpu_878, %false_879 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %793, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_880 = torch.constant.int 0
    %794 = torch.aten.unsqueeze %793, %int0_880 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %794, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_881 = torch.constant.int 1
    %795 = torch.aten.unsqueeze %794, %int1_881 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %795, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_882 = torch.constant.int 3
    %796 = torch.aten.unsqueeze %795, %int3_882 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %796, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %797 = torch.aten.gt.Tensor %792, %796 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %797, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_883 = torch.constant.int 1
    %798 = torch.aten.unsqueeze %788, %int1_883 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %798, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_884 = torch.constant.int 2
    %799 = torch.aten.unsqueeze %798, %int2_884 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %799, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %800 = torch.aten.logical_or %797, %799 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %800, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_885 = torch.constant.none
    %801 = torch.aten.clone %15, %none_885 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_886 = torch.constant.int 0
    %802 = torch.aten.where.ScalarOther %800, %801, %int0_886 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %802, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_887 = torch.constant.none
    %none_888 = torch.constant.none
    %int5_889 = torch.constant.int 5
    %cpu_890 = torch.constant.device "cpu"
    %int0_891 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %802, %none_887, %none_888, %int5_889, %cpu_890, %int0_891 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_892 = torch.constant.int -2
    %803 = torch.aten.unsqueeze %743, %int-2_892 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %803, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_893 = torch.constant.int 4
    %int4_894 = torch.constant.int 4
    %int8_895 = torch.constant.int 8
    %int64_896 = torch.constant.int 64
    %804 = torch.prim.ListConstruct %int4_893, %273, %int4_894, %int8_895, %int64_896 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_897 = torch.constant.bool false
    %805 = torch.aten.expand %803, %804, %false_897 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %805, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_898 = torch.constant.int 0
    %806 = torch.aten.clone %805, %int0_898 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %806, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_899 = torch.constant.int 4
    %int32_900 = torch.constant.int 32
    %int64_901 = torch.constant.int 64
    %807 = torch.prim.ListConstruct %int4_899, %273, %int32_900, %int64_901 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %808 = torch.aten._unsafe_view %806, %807 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %808, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_902 = torch.constant.int -2
    %809 = torch.aten.unsqueeze %665, %int-2_902 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %809, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_903 = torch.constant.int 4
    %int4_904 = torch.constant.int 4
    %int8_905 = torch.constant.int 8
    %int64_906 = torch.constant.int 64
    %810 = torch.prim.ListConstruct %int4_903, %273, %int4_904, %int8_905, %int64_906 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_907 = torch.constant.bool false
    %811 = torch.aten.expand %809, %810, %false_907 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %811, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_908 = torch.constant.int 0
    %812 = torch.aten.clone %811, %int0_908 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %812, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_909 = torch.constant.int 4
    %int32_910 = torch.constant.int 32
    %int64_911 = torch.constant.int 64
    %813 = torch.prim.ListConstruct %int4_909, %273, %int32_910, %int64_911 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %814 = torch.aten._unsafe_view %812, %813 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %814, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_912 = torch.constant.int 1
    %int2_913 = torch.constant.int 2
    %815 = torch.aten.transpose.int %704, %int1_912, %int2_913 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %815, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_914 = torch.constant.int 1
    %int2_915 = torch.constant.int 2
    %816 = torch.aten.transpose.int %808, %int1_914, %int2_915 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %816, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_916 = torch.constant.int 1
    %int2_917 = torch.constant.int 2
    %817 = torch.aten.transpose.int %814, %int1_916, %int2_917 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %817, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_918 = torch.constant.float 0.000000e+00
    %false_919 = torch.constant.bool false
    %none_920 = torch.constant.none
    %false_921 = torch.constant.bool false
    %818 = torch.aten.scaled_dot_product_attention %815, %816, %817, %802, %float0.000000e00_918, %false_919, %none_920, %false_921 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %818, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_922 = torch.constant.int 1
    %int2_923 = torch.constant.int 2
    %819 = torch.aten.transpose.int %818, %int1_922, %int2_923 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %819, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_924 = torch.constant.int 4
    %int2048_925 = torch.constant.int 2048
    %820 = torch.prim.ListConstruct %int4_924, %273, %int2048_925 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %821 = torch.aten.view %819, %820 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %821, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_926 = torch.constant.int 2
    %822 = torch.aten.view.dtype %20, %int2_926 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %823 = torch.aten.detach %822 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_927 = torch.constant.int -1
    %int17_928 = torch.constant.int 17
    %824 = torch.prim.ListConstruct %int-1_927, %int17_928 : (!torch.int, !torch.int) -> !torch.list<int>
    %825 = torch.aten.view %823, %824 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_929 = torch.constant.int 2048
    %int-1_930 = torch.constant.int -1
    %int17_931 = torch.constant.int 17
    %826 = torch.prim.ListConstruct %int2048_929, %int-1_930, %int17_931 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %827 = torch.aten.view %825, %826 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_932 = torch.constant.int 2
    %int0_933 = torch.constant.int 0
    %int1_934 = torch.constant.int 1
    %int1_935 = torch.constant.int 1
    %828 = torch.aten.slice.Tensor %827, %int2_932, %int0_933, %int1_934, %int1_935 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_936 = torch.constant.int 5
    %829 = torch.aten.view.dtype %828, %int5_936 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %830 = torch.aten.detach %829 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_937 = torch.constant.int 2
    %int1_938 = torch.constant.int 1
    %int9223372036854775807_939 = torch.constant.int 9223372036854775807
    %int1_940 = torch.constant.int 1
    %831 = torch.aten.slice.Tensor %827, %int2_937, %int1_938, %int9223372036854775807_939, %int1_940 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_941 = torch.constant.int 1
    %832 = torch.aten.view.dtype %831, %int1_941 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %833 = torch.aten.detach %832 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %834 = torch_c.to_builtin_tensor %821 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_942 = tensor.cast %834 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %835 = torch_c.to_builtin_tensor %830 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %836 = torch_c.to_builtin_tensor %833 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %837 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_942, %835, %836) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_943 = tensor.cast %837 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %838 = torch_c.from_builtin_tensor %cast_943 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %838, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_944 = torch.constant.none
    %none_945 = torch.constant.none
    %int5_946 = torch.constant.int 5
    %cpu_947 = torch.constant.device "cpu"
    %int0_948 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %838, %none_944, %none_945, %int5_946, %cpu_947, %int0_948 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_949 = torch.constant.int 1
    %839 = torch.aten.add.Tensor %598, %838, %int1_949 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %839, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_950 = torch.constant.none
    %none_951 = torch.constant.none
    %int5_952 = torch.constant.int 5
    %cpu_953 = torch.constant.device "cpu"
    %int0_954 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %839, %none_950, %none_951, %int5_952, %cpu_953, %int0_954 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_955 = torch.constant.int 6
    %840 = torch.prims.convert_element_type %839, %int6_955 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %840, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_956 = torch.constant.int 2
    %841 = torch.aten.pow.Tensor_Scalar %840, %int2_956 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %841, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_957 = torch.constant.int -1
    %842 = torch.prim.ListConstruct %int-1_957 : (!torch.int) -> !torch.list<int>
    %true_958 = torch.constant.bool true
    %none_959 = torch.constant.none
    %843 = torch.aten.mean.dim %841, %842, %true_958, %none_959 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %843, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_960 = torch.constant.float 9.9999997473787516E-6
    %int1_961 = torch.constant.int 1
    %844 = torch.aten.add.Scalar %843, %float9.999990e-06_960, %int1_961 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %844, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %845 = torch.aten.rsqrt %844 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %845, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %846 = torch.aten.mul.Tensor %840, %845 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %846, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_962 = torch.constant.none
    %none_963 = torch.constant.none
    %int6_964 = torch.constant.int 6
    %cpu_965 = torch.constant.device "cpu"
    %int0_966 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %846, %none_962, %none_963, %int6_964, %cpu_965, %int0_966 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_967 = torch.constant.int 5
    %847 = torch.prims.convert_element_type %846, %int5_967 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %847, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %848 = torch.aten.mul.Tensor %21, %847 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %848, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_968 = torch.constant.none
    %none_969 = torch.constant.none
    %int6_970 = torch.constant.int 6
    %cpu_971 = torch.constant.device "cpu"
    %int0_972 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %848, %none_968, %none_969, %int6_970, %cpu_971, %int0_972 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_973 = torch.constant.int 5
    %849 = torch.prims.convert_element_type %848, %int5_973 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %849, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_974 = torch.constant.int 2
    %850 = torch.aten.view.dtype %22, %int2_974 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %851 = torch.aten.detach %850 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_975 = torch.constant.int -1
    %int17_976 = torch.constant.int 17
    %852 = torch.prim.ListConstruct %int-1_975, %int17_976 : (!torch.int, !torch.int) -> !torch.list<int>
    %853 = torch.aten.view %851, %852 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_977 = torch.constant.int 5632
    %int-1_978 = torch.constant.int -1
    %int17_979 = torch.constant.int 17
    %854 = torch.prim.ListConstruct %int5632_977, %int-1_978, %int17_979 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %855 = torch.aten.view %853, %854 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_980 = torch.constant.int 2
    %int0_981 = torch.constant.int 0
    %int1_982 = torch.constant.int 1
    %int1_983 = torch.constant.int 1
    %856 = torch.aten.slice.Tensor %855, %int2_980, %int0_981, %int1_982, %int1_983 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_984 = torch.constant.int 5
    %857 = torch.aten.view.dtype %856, %int5_984 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %858 = torch.aten.detach %857 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_985 = torch.constant.int 2
    %int1_986 = torch.constant.int 1
    %int9223372036854775807_987 = torch.constant.int 9223372036854775807
    %int1_988 = torch.constant.int 1
    %859 = torch.aten.slice.Tensor %855, %int2_985, %int1_986, %int9223372036854775807_987, %int1_988 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_989 = torch.constant.int 1
    %860 = torch.aten.view.dtype %859, %int1_989 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %861 = torch.aten.detach %860 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %862 = torch_c.to_builtin_tensor %849 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_990 = tensor.cast %862 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %863 = torch_c.to_builtin_tensor %858 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %864 = torch_c.to_builtin_tensor %861 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %865 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_990, %863, %864) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_991 = tensor.cast %865 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %866 = torch_c.from_builtin_tensor %cast_991 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %866, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %867 = torch.aten.silu %866 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %867, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_992 = torch.constant.int 2
    %868 = torch.aten.view.dtype %23, %int2_992 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %869 = torch.aten.detach %868 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_993 = torch.constant.int -1
    %int17_994 = torch.constant.int 17
    %870 = torch.prim.ListConstruct %int-1_993, %int17_994 : (!torch.int, !torch.int) -> !torch.list<int>
    %871 = torch.aten.view %869, %870 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_995 = torch.constant.int 5632
    %int-1_996 = torch.constant.int -1
    %int17_997 = torch.constant.int 17
    %872 = torch.prim.ListConstruct %int5632_995, %int-1_996, %int17_997 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %873 = torch.aten.view %871, %872 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_998 = torch.constant.int 2
    %int0_999 = torch.constant.int 0
    %int1_1000 = torch.constant.int 1
    %int1_1001 = torch.constant.int 1
    %874 = torch.aten.slice.Tensor %873, %int2_998, %int0_999, %int1_1000, %int1_1001 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_1002 = torch.constant.int 5
    %875 = torch.aten.view.dtype %874, %int5_1002 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %876 = torch.aten.detach %875 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_1003 = torch.constant.int 2
    %int1_1004 = torch.constant.int 1
    %int9223372036854775807_1005 = torch.constant.int 9223372036854775807
    %int1_1006 = torch.constant.int 1
    %877 = torch.aten.slice.Tensor %873, %int2_1003, %int1_1004, %int9223372036854775807_1005, %int1_1006 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_1007 = torch.constant.int 1
    %878 = torch.aten.view.dtype %877, %int1_1007 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %879 = torch.aten.detach %878 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %880 = torch_c.to_builtin_tensor %849 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_1008 = tensor.cast %880 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %881 = torch_c.to_builtin_tensor %876 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %882 = torch_c.to_builtin_tensor %879 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %883 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_1008, %881, %882) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_1009 = tensor.cast %883 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %884 = torch_c.from_builtin_tensor %cast_1009 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %884, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %885 = torch.aten.mul.Tensor %867, %884 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %885, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_1010 = torch.constant.int 2
    %886 = torch.aten.view.dtype %24, %int2_1010 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %887 = torch.aten.detach %886 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_1011 = torch.constant.int -1
    %int17_1012 = torch.constant.int 17
    %888 = torch.prim.ListConstruct %int-1_1011, %int17_1012 : (!torch.int, !torch.int) -> !torch.list<int>
    %889 = torch.aten.view %887, %888 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_1013 = torch.constant.int 2048
    %int-1_1014 = torch.constant.int -1
    %int17_1015 = torch.constant.int 17
    %890 = torch.prim.ListConstruct %int2048_1013, %int-1_1014, %int17_1015 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %891 = torch.aten.view %889, %890 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_1016 = torch.constant.int 2
    %int0_1017 = torch.constant.int 0
    %int1_1018 = torch.constant.int 1
    %int1_1019 = torch.constant.int 1
    %892 = torch.aten.slice.Tensor %891, %int2_1016, %int0_1017, %int1_1018, %int1_1019 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_1020 = torch.constant.int 5
    %893 = torch.aten.view.dtype %892, %int5_1020 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %894 = torch.aten.detach %893 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_1021 = torch.constant.int 2
    %int1_1022 = torch.constant.int 1
    %int9223372036854775807_1023 = torch.constant.int 9223372036854775807
    %int1_1024 = torch.constant.int 1
    %895 = torch.aten.slice.Tensor %891, %int2_1021, %int1_1022, %int9223372036854775807_1023, %int1_1024 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_1025 = torch.constant.int 1
    %896 = torch.aten.view.dtype %895, %int1_1025 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %897 = torch.aten.detach %896 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %898 = torch_c.to_builtin_tensor %885 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_1026 = tensor.cast %898 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %899 = torch_c.to_builtin_tensor %894 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %900 = torch_c.to_builtin_tensor %897 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %901 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_1026, %899, %900) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_1027 = tensor.cast %901 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %902 = torch_c.from_builtin_tensor %cast_1027 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %902, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_1028 = torch.constant.int 1
    %903 = torch.aten.add.Tensor %839, %902, %int1_1028 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %903, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_1029 = torch.constant.none
    %none_1030 = torch.constant.none
    %int5_1031 = torch.constant.int 5
    %cpu_1032 = torch.constant.device "cpu"
    %int0_1033 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %903, %none_1029, %none_1030, %int5_1031, %cpu_1032, %int0_1033 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1034 = torch.constant.int 6
    %904 = torch.prims.convert_element_type %903, %int6_1034 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %904, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_1035 = torch.constant.int 2
    %905 = torch.aten.pow.Tensor_Scalar %904, %int2_1035 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %905, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_1036 = torch.constant.int -1
    %906 = torch.prim.ListConstruct %int-1_1036 : (!torch.int) -> !torch.list<int>
    %true_1037 = torch.constant.bool true
    %none_1038 = torch.constant.none
    %907 = torch.aten.mean.dim %905, %906, %true_1037, %none_1038 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %907, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_1039 = torch.constant.float 9.9999997473787516E-6
    %int1_1040 = torch.constant.int 1
    %908 = torch.aten.add.Scalar %907, %float9.999990e-06_1039, %int1_1040 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %908, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %909 = torch.aten.rsqrt %908 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %909, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %910 = torch.aten.mul.Tensor %904, %909 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %910, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_1041 = torch.constant.none
    %none_1042 = torch.constant.none
    %int6_1043 = torch.constant.int 6
    %cpu_1044 = torch.constant.device "cpu"
    %int0_1045 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %910, %none_1041, %none_1042, %int6_1043, %cpu_1044, %int0_1045 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1046 = torch.constant.int 5
    %911 = torch.prims.convert_element_type %910, %int5_1046 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %911, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %912 = torch.aten.mul.Tensor %28, %911 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %912, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_1047 = torch.constant.none
    %none_1048 = torch.constant.none
    %int6_1049 = torch.constant.int 6
    %cpu_1050 = torch.constant.device "cpu"
    %int0_1051 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %912, %none_1047, %none_1048, %int6_1049, %cpu_1050, %int0_1051 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1052 = torch.constant.int 5
    %913 = torch.prims.convert_element_type %912, %int5_1052 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %913, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_1053 = torch.constant.int 2
    %914 = torch.aten.view.dtype %29, %int2_1053 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %915 = torch.aten.detach %914 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_1054 = torch.constant.int -1
    %int17_1055 = torch.constant.int 17
    %916 = torch.prim.ListConstruct %int-1_1054, %int17_1055 : (!torch.int, !torch.int) -> !torch.list<int>
    %917 = torch.aten.view %915, %916 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_1056 = torch.constant.int 2048
    %int-1_1057 = torch.constant.int -1
    %int17_1058 = torch.constant.int 17
    %918 = torch.prim.ListConstruct %int2048_1056, %int-1_1057, %int17_1058 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %919 = torch.aten.view %917, %918 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_1059 = torch.constant.int 2
    %int0_1060 = torch.constant.int 0
    %int1_1061 = torch.constant.int 1
    %int1_1062 = torch.constant.int 1
    %920 = torch.aten.slice.Tensor %919, %int2_1059, %int0_1060, %int1_1061, %int1_1062 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_1063 = torch.constant.int 5
    %921 = torch.aten.view.dtype %920, %int5_1063 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %922 = torch.aten.detach %921 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_1064 = torch.constant.int 2
    %int1_1065 = torch.constant.int 1
    %int9223372036854775807_1066 = torch.constant.int 9223372036854775807
    %int1_1067 = torch.constant.int 1
    %923 = torch.aten.slice.Tensor %919, %int2_1064, %int1_1065, %int9223372036854775807_1066, %int1_1067 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_1068 = torch.constant.int 1
    %924 = torch.aten.view.dtype %923, %int1_1068 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %925 = torch.aten.detach %924 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %926 = torch_c.to_builtin_tensor %913 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_1069 = tensor.cast %926 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %927 = torch_c.to_builtin_tensor %922 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %928 = torch_c.to_builtin_tensor %925 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %929 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_1069, %927, %928) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_1070 = tensor.cast %929 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %930 = torch_c.from_builtin_tensor %cast_1070 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %930, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_1071 = torch.constant.int 2
    %931 = torch.aten.view.dtype %30, %int2_1071 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %932 = torch.aten.detach %931 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_1072 = torch.constant.int -1
    %int17_1073 = torch.constant.int 17
    %933 = torch.prim.ListConstruct %int-1_1072, %int17_1073 : (!torch.int, !torch.int) -> !torch.list<int>
    %934 = torch.aten.view %932, %933 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_1074 = torch.constant.int 256
    %int-1_1075 = torch.constant.int -1
    %int17_1076 = torch.constant.int 17
    %935 = torch.prim.ListConstruct %int256_1074, %int-1_1075, %int17_1076 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %936 = torch.aten.view %934, %935 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_1077 = torch.constant.int 2
    %int0_1078 = torch.constant.int 0
    %int1_1079 = torch.constant.int 1
    %int1_1080 = torch.constant.int 1
    %937 = torch.aten.slice.Tensor %936, %int2_1077, %int0_1078, %int1_1079, %int1_1080 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_1081 = torch.constant.int 5
    %938 = torch.aten.view.dtype %937, %int5_1081 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %939 = torch.aten.detach %938 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_1082 = torch.constant.int 2
    %int1_1083 = torch.constant.int 1
    %int9223372036854775807_1084 = torch.constant.int 9223372036854775807
    %int1_1085 = torch.constant.int 1
    %940 = torch.aten.slice.Tensor %936, %int2_1082, %int1_1083, %int9223372036854775807_1084, %int1_1085 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_1086 = torch.constant.int 1
    %941 = torch.aten.view.dtype %940, %int1_1086 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %942 = torch.aten.detach %941 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %943 = torch_c.to_builtin_tensor %913 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_1087 = tensor.cast %943 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %944 = torch_c.to_builtin_tensor %939 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %945 = torch_c.to_builtin_tensor %942 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %946 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_1087, %944, %945) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_1088 = tensor.cast %946 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %947 = torch_c.from_builtin_tensor %cast_1088 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %947, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_1089 = torch.constant.int 2
    %948 = torch.aten.view.dtype %31, %int2_1089 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %949 = torch.aten.detach %948 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_1090 = torch.constant.int -1
    %int17_1091 = torch.constant.int 17
    %950 = torch.prim.ListConstruct %int-1_1090, %int17_1091 : (!torch.int, !torch.int) -> !torch.list<int>
    %951 = torch.aten.view %949, %950 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_1092 = torch.constant.int 256
    %int-1_1093 = torch.constant.int -1
    %int17_1094 = torch.constant.int 17
    %952 = torch.prim.ListConstruct %int256_1092, %int-1_1093, %int17_1094 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %953 = torch.aten.view %951, %952 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_1095 = torch.constant.int 2
    %int0_1096 = torch.constant.int 0
    %int1_1097 = torch.constant.int 1
    %int1_1098 = torch.constant.int 1
    %954 = torch.aten.slice.Tensor %953, %int2_1095, %int0_1096, %int1_1097, %int1_1098 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_1099 = torch.constant.int 5
    %955 = torch.aten.view.dtype %954, %int5_1099 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %956 = torch.aten.detach %955 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_1100 = torch.constant.int 2
    %int1_1101 = torch.constant.int 1
    %int9223372036854775807_1102 = torch.constant.int 9223372036854775807
    %int1_1103 = torch.constant.int 1
    %957 = torch.aten.slice.Tensor %953, %int2_1100, %int1_1101, %int9223372036854775807_1102, %int1_1103 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_1104 = torch.constant.int 1
    %958 = torch.aten.view.dtype %957, %int1_1104 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %959 = torch.aten.detach %958 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %960 = torch_c.to_builtin_tensor %913 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_1105 = tensor.cast %960 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %961 = torch_c.to_builtin_tensor %956 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %962 = torch_c.to_builtin_tensor %959 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %963 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_1105, %961, %962) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_1106 = tensor.cast %963 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %964 = torch_c.from_builtin_tensor %cast_1106 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %964, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_1107 = torch.constant.int 4
    %int32_1108 = torch.constant.int 32
    %int64_1109 = torch.constant.int 64
    %965 = torch.prim.ListConstruct %int4_1107, %273, %int32_1108, %int64_1109 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %966 = torch.aten.view %930, %965 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %966, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_1110 = torch.constant.int 4
    %int4_1111 = torch.constant.int 4
    %int64_1112 = torch.constant.int 64
    %967 = torch.prim.ListConstruct %int4_1110, %273, %int4_1111, %int64_1112 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %968 = torch.aten.view %947, %967 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %968, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_1113 = torch.constant.int 4
    %int4_1114 = torch.constant.int 4
    %int64_1115 = torch.constant.int 64
    %969 = torch.prim.ListConstruct %int4_1113, %273, %int4_1114, %int64_1115 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %970 = torch.aten.view %964, %969 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %970, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_1116 = torch.constant.int 0
    %none_1117 = torch.constant.none
    %none_1118 = torch.constant.none
    %cpu_1119 = torch.constant.device "cpu"
    %false_1120 = torch.constant.bool false
    %971 = torch.aten.arange.start %int0_1116, %273, %none_1117, %none_1118, %cpu_1119, %false_1120 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %971, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_1121 = torch.constant.int 0
    %972 = torch.aten.unsqueeze %971, %int0_1121 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %972, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_1122 = torch.constant.int 0
    %int64_1123 = torch.constant.int 64
    %int2_1124 = torch.constant.int 2
    %none_1125 = torch.constant.none
    %none_1126 = torch.constant.none
    %cpu_1127 = torch.constant.device "cpu"
    %false_1128 = torch.constant.bool false
    %973 = torch.aten.arange.start_step %int0_1122, %int64_1123, %int2_1124, %none_1125, %none_1126, %cpu_1127, %false_1128 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_1129 = torch.constant.none
    %none_1130 = torch.constant.none
    %int4_1131 = torch.constant.int 4
    %cpu_1132 = torch.constant.device "cpu"
    %int0_1133 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %973, %none_1129, %none_1130, %int4_1131, %cpu_1132, %int0_1133 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1134 = torch.constant.int 6
    %974 = torch.prims.convert_element_type %973, %int6_1134 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_1135 = torch.constant.int 64
    %975 = torch.aten.div.Scalar %974, %int64_1135 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_1136 = torch.constant.float 1.000000e+04
    %976 = torch.aten.pow.Scalar %float1.000000e04_1136, %975 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %977 = torch.aten.reciprocal %976 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_1137 = torch.constant.float 1.000000e+00
    %978 = torch.aten.mul.Scalar %977, %float1.000000e00_1137 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_1138 = torch.constant.none
    %979 = torch.aten.clone %25, %none_1138 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_1139 = torch.constant.int 0
    %980 = torch.aten.unsqueeze %978, %int0_1139 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_1140 = torch.constant.int 2
    %981 = torch.aten.unsqueeze %980, %int2_1140 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_1141 = torch.constant.none
    %none_1142 = torch.constant.none
    %int6_1143 = torch.constant.int 6
    %cpu_1144 = torch.constant.device "cpu"
    %int0_1145 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %981, %none_1141, %none_1142, %int6_1143, %cpu_1144, %int0_1145 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_1146 = torch.constant.int 1
    %int-1_1147 = torch.constant.int -1
    %int1_1148 = torch.constant.int 1
    %982 = torch.prim.ListConstruct %int1_1146, %int-1_1147, %int1_1148 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1149 = torch.constant.bool false
    %983 = torch.aten.expand %981, %982, %false_1149 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_1150 = torch.constant.int 1
    %984 = torch.aten.unsqueeze %972, %int1_1150 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %984, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_1151 = torch.constant.none
    %none_1152 = torch.constant.none
    %int4_1153 = torch.constant.int 4
    %cpu_1154 = torch.constant.device "cpu"
    %int0_1155 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %984, %none_1151, %none_1152, %int4_1153, %cpu_1154, %int0_1155 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1156 = torch.constant.int 6
    %985 = torch.prims.convert_element_type %984, %int6_1156 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %985, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %986 = torch.aten.matmul %983, %985 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %986, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_1157 = torch.constant.int 1
    %int2_1158 = torch.constant.int 2
    %987 = torch.aten.transpose.int %986, %int1_1157, %int2_1158 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %987, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %988 = torch.aten.cos %987 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %988, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %989 = torch.aten.mul.Tensor %988, %979 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %989, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_1159 = torch.constant.none
    %none_1160 = torch.constant.none
    %int6_1161 = torch.constant.int 6
    %cpu_1162 = torch.constant.device "cpu"
    %int0_1163 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %989, %none_1159, %none_1160, %int6_1161, %cpu_1162, %int0_1163 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1164 = torch.constant.int 5
    %990 = torch.prims.convert_element_type %989, %int5_1164 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %990, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %991 = torch.aten.sin %987 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %991, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %992 = torch.aten.mul.Tensor %991, %979 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %992, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_1165 = torch.constant.none
    %none_1166 = torch.constant.none
    %int6_1167 = torch.constant.int 6
    %cpu_1168 = torch.constant.device "cpu"
    %int0_1169 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %992, %none_1165, %none_1166, %int6_1167, %cpu_1168, %int0_1169 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1170 = torch.constant.int 5
    %993 = torch.prims.convert_element_type %992, %int5_1170 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %993, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_1171 = torch.constant.int 2
    %994 = torch.aten.unsqueeze %990, %int2_1171 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %994, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_1172 = torch.constant.int 2
    %995 = torch.aten.unsqueeze %993, %int2_1172 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %995, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_1173 = torch.constant.none
    %none_1174 = torch.constant.none
    %int5_1175 = torch.constant.int 5
    %cpu_1176 = torch.constant.device "cpu"
    %int0_1177 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %994, %none_1173, %none_1174, %int5_1175, %cpu_1176, %int0_1177 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_1178 = torch.constant.none
    %none_1179 = torch.constant.none
    %int5_1180 = torch.constant.int 5
    %cpu_1181 = torch.constant.device "cpu"
    %int0_1182 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %995, %none_1178, %none_1179, %int5_1180, %cpu_1181, %int0_1182 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_1183 = torch.constant.none
    %none_1184 = torch.constant.none
    %int5_1185 = torch.constant.int 5
    %cpu_1186 = torch.constant.device "cpu"
    %int0_1187 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %966, %none_1183, %none_1184, %int5_1185, %cpu_1186, %int0_1187 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_1188 = torch.constant.int 3
    %int0_1189 = torch.constant.int 0
    %int64_1190 = torch.constant.int 64
    %int2_1191 = torch.constant.int 2
    %996 = torch.aten.slice.Tensor %966, %int3_1188, %int0_1189, %int64_1190, %int2_1191 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %996, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_1192 = torch.constant.int 3
    %int1_1193 = torch.constant.int 1
    %int64_1194 = torch.constant.int 64
    %int2_1195 = torch.constant.int 2
    %997 = torch.aten.slice.Tensor %966, %int3_1192, %int1_1193, %int64_1194, %int2_1195 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %997, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %998 = torch.aten.mul.Tensor %996, %994 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %998, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %999 = torch.aten.mul.Tensor %997, %995 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %999, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_1196 = torch.constant.int 1
    %1000 = torch.aten.sub.Tensor %998, %999, %int1_1196 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1000, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %1001 = torch.aten.mul.Tensor %997, %994 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1001, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %1002 = torch.aten.mul.Tensor %996, %995 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1002, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_1197 = torch.constant.int 1
    %1003 = torch.aten.add.Tensor %1001, %1002, %int1_1197 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1003, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %1004 = torch_c.to_builtin_tensor %1000 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_1198 = tensor.cast %1004 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %1005 = torch_c.to_builtin_tensor %1003 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_1199 = tensor.cast %1005 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %1006 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_1198, %cast_1199) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_1200 = tensor.cast %1006 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %1007 = torch_c.from_builtin_tensor %cast_1200 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %1007, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_1201 = torch.constant.int 4
    %int32_1202 = torch.constant.int 32
    %int64_1203 = torch.constant.int 64
    %1008 = torch.prim.ListConstruct %int4_1201, %273, %int32_1202, %int64_1203 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1009 = torch.aten.view %1007, %1008 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1009, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_1204 = torch.constant.none
    %none_1205 = torch.constant.none
    %int5_1206 = torch.constant.int 5
    %cpu_1207 = torch.constant.device "cpu"
    %int0_1208 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1009, %none_1204, %none_1205, %int5_1206, %cpu_1207, %int0_1208 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_1209 = torch.constant.int 0
    %none_1210 = torch.constant.none
    %none_1211 = torch.constant.none
    %cpu_1212 = torch.constant.device "cpu"
    %false_1213 = torch.constant.bool false
    %1010 = torch.aten.arange.start %int0_1209, %273, %none_1210, %none_1211, %cpu_1212, %false_1213 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1010, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_1214 = torch.constant.int 0
    %1011 = torch.aten.unsqueeze %1010, %int0_1214 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %1011, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_1215 = torch.constant.int 0
    %int64_1216 = torch.constant.int 64
    %int2_1217 = torch.constant.int 2
    %none_1218 = torch.constant.none
    %none_1219 = torch.constant.none
    %cpu_1220 = torch.constant.device "cpu"
    %false_1221 = torch.constant.bool false
    %1012 = torch.aten.arange.start_step %int0_1215, %int64_1216, %int2_1217, %none_1218, %none_1219, %cpu_1220, %false_1221 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_1222 = torch.constant.none
    %none_1223 = torch.constant.none
    %int4_1224 = torch.constant.int 4
    %cpu_1225 = torch.constant.device "cpu"
    %int0_1226 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1012, %none_1222, %none_1223, %int4_1224, %cpu_1225, %int0_1226 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1227 = torch.constant.int 6
    %1013 = torch.prims.convert_element_type %1012, %int6_1227 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_1228 = torch.constant.int 64
    %1014 = torch.aten.div.Scalar %1013, %int64_1228 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_1229 = torch.constant.float 1.000000e+04
    %1015 = torch.aten.pow.Scalar %float1.000000e04_1229, %1014 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %1016 = torch.aten.reciprocal %1015 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_1230 = torch.constant.float 1.000000e+00
    %1017 = torch.aten.mul.Scalar %1016, %float1.000000e00_1230 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_1231 = torch.constant.none
    %1018 = torch.aten.clone %26, %none_1231 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_1232 = torch.constant.int 0
    %1019 = torch.aten.unsqueeze %1017, %int0_1232 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_1233 = torch.constant.int 2
    %1020 = torch.aten.unsqueeze %1019, %int2_1233 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_1234 = torch.constant.none
    %none_1235 = torch.constant.none
    %int6_1236 = torch.constant.int 6
    %cpu_1237 = torch.constant.device "cpu"
    %int0_1238 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1020, %none_1234, %none_1235, %int6_1236, %cpu_1237, %int0_1238 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_1239 = torch.constant.int 1
    %int-1_1240 = torch.constant.int -1
    %int1_1241 = torch.constant.int 1
    %1021 = torch.prim.ListConstruct %int1_1239, %int-1_1240, %int1_1241 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1242 = torch.constant.bool false
    %1022 = torch.aten.expand %1020, %1021, %false_1242 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_1243 = torch.constant.int 1
    %1023 = torch.aten.unsqueeze %1011, %int1_1243 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %1023, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_1244 = torch.constant.none
    %none_1245 = torch.constant.none
    %int4_1246 = torch.constant.int 4
    %cpu_1247 = torch.constant.device "cpu"
    %int0_1248 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1023, %none_1244, %none_1245, %int4_1246, %cpu_1247, %int0_1248 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1249 = torch.constant.int 6
    %1024 = torch.prims.convert_element_type %1023, %int6_1249 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %1024, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %1025 = torch.aten.matmul %1022, %1024 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %1025, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_1250 = torch.constant.int 1
    %int2_1251 = torch.constant.int 2
    %1026 = torch.aten.transpose.int %1025, %int1_1250, %int2_1251 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1026, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1027 = torch.aten.cos %1026 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1027, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1028 = torch.aten.mul.Tensor %1027, %1018 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1028, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_1252 = torch.constant.none
    %none_1253 = torch.constant.none
    %int6_1254 = torch.constant.int 6
    %cpu_1255 = torch.constant.device "cpu"
    %int0_1256 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1028, %none_1252, %none_1253, %int6_1254, %cpu_1255, %int0_1256 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1257 = torch.constant.int 5
    %1029 = torch.prims.convert_element_type %1028, %int5_1257 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %1029, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %1030 = torch.aten.sin %1026 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1030, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1031 = torch.aten.mul.Tensor %1030, %1018 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1031, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_1258 = torch.constant.none
    %none_1259 = torch.constant.none
    %int6_1260 = torch.constant.int 6
    %cpu_1261 = torch.constant.device "cpu"
    %int0_1262 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1031, %none_1258, %none_1259, %int6_1260, %cpu_1261, %int0_1262 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1263 = torch.constant.int 5
    %1032 = torch.prims.convert_element_type %1031, %int5_1263 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %1032, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_1264 = torch.constant.int 2
    %1033 = torch.aten.unsqueeze %1029, %int2_1264 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %1033, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_1265 = torch.constant.int 2
    %1034 = torch.aten.unsqueeze %1032, %int2_1265 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %1034, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_1266 = torch.constant.none
    %none_1267 = torch.constant.none
    %int5_1268 = torch.constant.int 5
    %cpu_1269 = torch.constant.device "cpu"
    %int0_1270 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1033, %none_1266, %none_1267, %int5_1268, %cpu_1269, %int0_1270 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_1271 = torch.constant.none
    %none_1272 = torch.constant.none
    %int5_1273 = torch.constant.int 5
    %cpu_1274 = torch.constant.device "cpu"
    %int0_1275 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1034, %none_1271, %none_1272, %int5_1273, %cpu_1274, %int0_1275 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_1276 = torch.constant.none
    %none_1277 = torch.constant.none
    %int5_1278 = torch.constant.int 5
    %cpu_1279 = torch.constant.device "cpu"
    %int0_1280 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %968, %none_1276, %none_1277, %int5_1278, %cpu_1279, %int0_1280 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_1281 = torch.constant.int 3
    %int0_1282 = torch.constant.int 0
    %int64_1283 = torch.constant.int 64
    %int2_1284 = torch.constant.int 2
    %1035 = torch.aten.slice.Tensor %968, %int3_1281, %int0_1282, %int64_1283, %int2_1284 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1035, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_1285 = torch.constant.int 3
    %int1_1286 = torch.constant.int 1
    %int64_1287 = torch.constant.int 64
    %int2_1288 = torch.constant.int 2
    %1036 = torch.aten.slice.Tensor %968, %int3_1285, %int1_1286, %int64_1287, %int2_1288 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1036, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1037 = torch.aten.mul.Tensor %1035, %1033 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1037, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1038 = torch.aten.mul.Tensor %1036, %1034 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1038, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_1289 = torch.constant.int 1
    %1039 = torch.aten.sub.Tensor %1037, %1038, %int1_1289 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1039, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1040 = torch.aten.mul.Tensor %1036, %1033 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1040, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1041 = torch.aten.mul.Tensor %1035, %1034 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1041, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_1290 = torch.constant.int 1
    %1042 = torch.aten.add.Tensor %1040, %1041, %int1_1290 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1042, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1043 = torch_c.to_builtin_tensor %1039 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_1291 = tensor.cast %1043 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %1044 = torch_c.to_builtin_tensor %1042 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_1292 = tensor.cast %1044 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %1045 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_1291, %cast_1292) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_1293 = tensor.cast %1045 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %1046 = torch_c.from_builtin_tensor %cast_1293 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %1046, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_1294 = torch.constant.int 4
    %int4_1295 = torch.constant.int 4
    %int64_1296 = torch.constant.int 64
    %1047 = torch.prim.ListConstruct %int4_1294, %273, %int4_1295, %int64_1296 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1048 = torch.aten.view %1046, %1047 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %1048, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_1297 = torch.constant.none
    %none_1298 = torch.constant.none
    %int5_1299 = torch.constant.int 5
    %cpu_1300 = torch.constant.device "cpu"
    %int0_1301 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1048, %none_1297, %none_1298, %int5_1299, %cpu_1300, %int0_1301 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_1302 = torch.constant.int 22
    %1049 = torch.aten.mul.Scalar %arg2, %int22_1302 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1049, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_1303 = torch.constant.int 2
    %int1_1304 = torch.constant.int 1
    %1050 = torch.aten.add.Scalar %1049, %int2_1303, %int1_1304 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1050, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_1305 = torch.constant.int 2
    %1051 = torch.aten.mul.Scalar %1050, %int2_1305 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1051, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_1306 = torch.constant.int 0
    %int1_1307 = torch.constant.int 1
    %1052 = torch.aten.add.Scalar %1051, %int0_1306, %int1_1307 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1052, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %1053 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %1054 = torch.aten.view %1052, %1053 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1054, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_1308 = torch.constant.int 4
    %int32_1309 = torch.constant.int 32
    %int4_1310 = torch.constant.int 4
    %int64_1311 = torch.constant.int 64
    %1055 = torch.prim.ListConstruct %int4_1308, %271, %int32_1309, %int4_1310, %int64_1311 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1056 = torch.aten.view %1048, %1055 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1056, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_1312 = torch.constant.int 32
    %int4_1313 = torch.constant.int 4
    %int64_1314 = torch.constant.int 64
    %1057 = torch.prim.ListConstruct %446, %int32_1312, %int4_1313, %int64_1314 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1058 = torch.aten.view %1056, %1057 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %1058, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_1315 = torch.constant.int 1
    %int2_1316 = torch.constant.int 2
    %1059 = torch.aten.transpose.int %1058, %int1_1315, %int2_1316 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1059, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_1317 = torch.constant.none
    %none_1318 = torch.constant.none
    %int5_1319 = torch.constant.int 5
    %cpu_1320 = torch.constant.device "cpu"
    %int0_1321 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1059, %none_1317, %none_1318, %int5_1319, %cpu_1320, %int0_1321 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_1322 = torch.constant.int 22
    %int2_1323 = torch.constant.int 2
    %int4_1324 = torch.constant.int 4
    %int32_1325 = torch.constant.int 32
    %int64_1326 = torch.constant.int 64
    %1060 = torch.prim.ListConstruct %272, %int22_1322, %int2_1323, %int4_1324, %int32_1325, %int64_1326 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1061 = torch.aten.view %785, %1060 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1061, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_1327 = torch.constant.int 4
    %int32_1328 = torch.constant.int 32
    %int64_1329 = torch.constant.int 64
    %1062 = torch.prim.ListConstruct %439, %int4_1327, %int32_1328, %int64_1329 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1063 = torch.aten.view %1061, %1062 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1063, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %1064 = torch.prim.ListConstruct %1054 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_1330 = torch.constant.bool false
    %1065 = torch.aten.index_put %1063, %1064, %1059, %false_1330 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1065, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_1331 = torch.constant.int 22
    %int2_1332 = torch.constant.int 2
    %int4_1333 = torch.constant.int 4
    %int32_1334 = torch.constant.int 32
    %int64_1335 = torch.constant.int 64
    %1066 = torch.prim.ListConstruct %272, %int22_1331, %int2_1332, %int4_1333, %int32_1334, %int64_1335 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1067 = torch.aten.view %1065, %1066 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1067, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_1336 = torch.constant.int 360448
    %1068 = torch.prim.ListConstruct %272, %int360448_1336 : (!torch.int, !torch.int) -> !torch.list<int>
    %1069 = torch.aten.view %1067, %1068 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %1069, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_1337 = torch.constant.int 22
    %int2_1338 = torch.constant.int 2
    %int4_1339 = torch.constant.int 4
    %int32_1340 = torch.constant.int 32
    %int64_1341 = torch.constant.int 64
    %1070 = torch.prim.ListConstruct %272, %int22_1337, %int2_1338, %int4_1339, %int32_1340, %int64_1341 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1071 = torch.aten.view %1069, %1070 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1071, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_1342 = torch.constant.int 4
    %int32_1343 = torch.constant.int 32
    %int64_1344 = torch.constant.int 64
    %1072 = torch.prim.ListConstruct %439, %int4_1342, %int32_1343, %int64_1344 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1073 = torch.aten.view %1071, %1072 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1073, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_1345 = torch.constant.int 22
    %1074 = torch.aten.mul.Scalar %arg2, %int22_1345 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1074, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_1346 = torch.constant.int 2
    %int1_1347 = torch.constant.int 1
    %1075 = torch.aten.add.Scalar %1074, %int2_1346, %int1_1347 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1075, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_1348 = torch.constant.int 2
    %1076 = torch.aten.mul.Scalar %1075, %int2_1348 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1076, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_1349 = torch.constant.int 1
    %int1_1350 = torch.constant.int 1
    %1077 = torch.aten.add.Scalar %1076, %int1_1349, %int1_1350 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1077, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %1078 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %1079 = torch.aten.view %1077, %1078 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1079, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_1351 = torch.constant.int 4
    %int32_1352 = torch.constant.int 32
    %int4_1353 = torch.constant.int 4
    %int64_1354 = torch.constant.int 64
    %1080 = torch.prim.ListConstruct %int4_1351, %271, %int32_1352, %int4_1353, %int64_1354 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1081 = torch.aten.view %970, %1080 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1081, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_1355 = torch.constant.int 32
    %int4_1356 = torch.constant.int 4
    %int64_1357 = torch.constant.int 64
    %1082 = torch.prim.ListConstruct %446, %int32_1355, %int4_1356, %int64_1357 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1083 = torch.aten.view %1081, %1082 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %1083, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_1358 = torch.constant.int 1
    %int2_1359 = torch.constant.int 2
    %1084 = torch.aten.transpose.int %1083, %int1_1358, %int2_1359 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1084, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_1360 = torch.constant.none
    %none_1361 = torch.constant.none
    %int5_1362 = torch.constant.int 5
    %cpu_1363 = torch.constant.device "cpu"
    %int0_1364 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1084, %none_1360, %none_1361, %int5_1362, %cpu_1363, %int0_1364 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %1085 = torch.prim.ListConstruct %1079 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_1365 = torch.constant.bool false
    %1086 = torch.aten.index_put %1073, %1085, %1084, %false_1365 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1086, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_1366 = torch.constant.int 22
    %int2_1367 = torch.constant.int 2
    %int4_1368 = torch.constant.int 4
    %int32_1369 = torch.constant.int 32
    %int64_1370 = torch.constant.int 64
    %1087 = torch.prim.ListConstruct %272, %int22_1366, %int2_1367, %int4_1368, %int32_1369, %int64_1370 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1088 = torch.aten.view %1086, %1087 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1088, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_1371 = torch.constant.int 360448
    %1089 = torch.prim.ListConstruct %272, %int360448_1371 : (!torch.int, !torch.int) -> !torch.list<int>
    %1090 = torch.aten.view %1088, %1089 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %1090, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_1372 = torch.constant.int 0
    %int1_1373 = torch.constant.int 1
    %none_1374 = torch.constant.none
    %none_1375 = torch.constant.none
    %cpu_1376 = torch.constant.device "cpu"
    %false_1377 = torch.constant.bool false
    %1091 = torch.aten.arange.start_step %int0_1372, %273, %int1_1373, %none_1374, %none_1375, %cpu_1376, %false_1377 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1091, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_1378 = torch.constant.int -1
    %1092 = torch.aten.unsqueeze %arg1, %int-1_1378 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %1093 = torch.aten.ge.Tensor %1091, %1092 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %1093, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_1379 = torch.constant.none
    %none_1380 = torch.constant.none
    %cpu_1381 = torch.constant.device "cpu"
    %false_1382 = torch.constant.bool false
    %1094 = torch.aten.arange %273, %none_1379, %none_1380, %cpu_1381, %false_1382 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1094, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_1383 = torch.constant.int 0
    %1095 = torch.aten.unsqueeze %1094, %int0_1383 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %1095, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_1384 = torch.constant.int 1
    %1096 = torch.aten.unsqueeze %1095, %int1_1384 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %1096, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_1385 = torch.constant.int 2
    %1097 = torch.aten.unsqueeze %1096, %int2_1385 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %1097, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_1386 = torch.constant.none
    %none_1387 = torch.constant.none
    %cpu_1388 = torch.constant.device "cpu"
    %false_1389 = torch.constant.bool false
    %1098 = torch.aten.arange %273, %none_1386, %none_1387, %cpu_1388, %false_1389 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1098, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_1390 = torch.constant.int 0
    %1099 = torch.aten.unsqueeze %1098, %int0_1390 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %1099, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_1391 = torch.constant.int 1
    %1100 = torch.aten.unsqueeze %1099, %int1_1391 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %1100, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_1392 = torch.constant.int 3
    %1101 = torch.aten.unsqueeze %1100, %int3_1392 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %1101, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %1102 = torch.aten.gt.Tensor %1097, %1101 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %1102, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_1393 = torch.constant.int 1
    %1103 = torch.aten.unsqueeze %1093, %int1_1393 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %1103, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_1394 = torch.constant.int 2
    %1104 = torch.aten.unsqueeze %1103, %int2_1394 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %1104, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %1105 = torch.aten.logical_or %1102, %1104 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %1105, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_1395 = torch.constant.none
    %1106 = torch.aten.clone %27, %none_1395 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_1396 = torch.constant.int 0
    %1107 = torch.aten.where.ScalarOther %1105, %1106, %int0_1396 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %1107, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_1397 = torch.constant.none
    %none_1398 = torch.constant.none
    %int5_1399 = torch.constant.int 5
    %cpu_1400 = torch.constant.device "cpu"
    %int0_1401 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1107, %none_1397, %none_1398, %int5_1399, %cpu_1400, %int0_1401 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_1402 = torch.constant.int -2
    %1108 = torch.aten.unsqueeze %1048, %int-2_1402 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %1108, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_1403 = torch.constant.int 4
    %int4_1404 = torch.constant.int 4
    %int8_1405 = torch.constant.int 8
    %int64_1406 = torch.constant.int 64
    %1109 = torch.prim.ListConstruct %int4_1403, %273, %int4_1404, %int8_1405, %int64_1406 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1407 = torch.constant.bool false
    %1110 = torch.aten.expand %1108, %1109, %false_1407 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1110, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_1408 = torch.constant.int 0
    %1111 = torch.aten.clone %1110, %int0_1408 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1111, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_1409 = torch.constant.int 4
    %int32_1410 = torch.constant.int 32
    %int64_1411 = torch.constant.int 64
    %1112 = torch.prim.ListConstruct %int4_1409, %273, %int32_1410, %int64_1411 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1113 = torch.aten._unsafe_view %1111, %1112 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1113, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_1412 = torch.constant.int -2
    %1114 = torch.aten.unsqueeze %970, %int-2_1412 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %1114, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_1413 = torch.constant.int 4
    %int4_1414 = torch.constant.int 4
    %int8_1415 = torch.constant.int 8
    %int64_1416 = torch.constant.int 64
    %1115 = torch.prim.ListConstruct %int4_1413, %273, %int4_1414, %int8_1415, %int64_1416 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1417 = torch.constant.bool false
    %1116 = torch.aten.expand %1114, %1115, %false_1417 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1116, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_1418 = torch.constant.int 0
    %1117 = torch.aten.clone %1116, %int0_1418 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1117, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_1419 = torch.constant.int 4
    %int32_1420 = torch.constant.int 32
    %int64_1421 = torch.constant.int 64
    %1118 = torch.prim.ListConstruct %int4_1419, %273, %int32_1420, %int64_1421 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1119 = torch.aten._unsafe_view %1117, %1118 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1119, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_1422 = torch.constant.int 1
    %int2_1423 = torch.constant.int 2
    %1120 = torch.aten.transpose.int %1009, %int1_1422, %int2_1423 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %1120, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_1424 = torch.constant.int 1
    %int2_1425 = torch.constant.int 2
    %1121 = torch.aten.transpose.int %1113, %int1_1424, %int2_1425 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %1121, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_1426 = torch.constant.int 1
    %int2_1427 = torch.constant.int 2
    %1122 = torch.aten.transpose.int %1119, %int1_1426, %int2_1427 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %1122, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_1428 = torch.constant.float 0.000000e+00
    %false_1429 = torch.constant.bool false
    %none_1430 = torch.constant.none
    %false_1431 = torch.constant.bool false
    %1123 = torch.aten.scaled_dot_product_attention %1120, %1121, %1122, %1107, %float0.000000e00_1428, %false_1429, %none_1430, %false_1431 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %1123, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_1432 = torch.constant.int 1
    %int2_1433 = torch.constant.int 2
    %1124 = torch.aten.transpose.int %1123, %int1_1432, %int2_1433 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1124, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_1434 = torch.constant.int 4
    %int2048_1435 = torch.constant.int 2048
    %1125 = torch.prim.ListConstruct %int4_1434, %273, %int2048_1435 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1126 = torch.aten.view %1124, %1125 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1126, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_1436 = torch.constant.int 2
    %1127 = torch.aten.view.dtype %32, %int2_1436 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %1128 = torch.aten.detach %1127 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_1437 = torch.constant.int -1
    %int17_1438 = torch.constant.int 17
    %1129 = torch.prim.ListConstruct %int-1_1437, %int17_1438 : (!torch.int, !torch.int) -> !torch.list<int>
    %1130 = torch.aten.view %1128, %1129 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_1439 = torch.constant.int 2048
    %int-1_1440 = torch.constant.int -1
    %int17_1441 = torch.constant.int 17
    %1131 = torch.prim.ListConstruct %int2048_1439, %int-1_1440, %int17_1441 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1132 = torch.aten.view %1130, %1131 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_1442 = torch.constant.int 2
    %int0_1443 = torch.constant.int 0
    %int1_1444 = torch.constant.int 1
    %int1_1445 = torch.constant.int 1
    %1133 = torch.aten.slice.Tensor %1132, %int2_1442, %int0_1443, %int1_1444, %int1_1445 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_1446 = torch.constant.int 5
    %1134 = torch.aten.view.dtype %1133, %int5_1446 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %1135 = torch.aten.detach %1134 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_1447 = torch.constant.int 2
    %int1_1448 = torch.constant.int 1
    %int9223372036854775807_1449 = torch.constant.int 9223372036854775807
    %int1_1450 = torch.constant.int 1
    %1136 = torch.aten.slice.Tensor %1132, %int2_1447, %int1_1448, %int9223372036854775807_1449, %int1_1450 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_1451 = torch.constant.int 1
    %1137 = torch.aten.view.dtype %1136, %int1_1451 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %1138 = torch.aten.detach %1137 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %1139 = torch_c.to_builtin_tensor %1126 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_1452 = tensor.cast %1139 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %1140 = torch_c.to_builtin_tensor %1135 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %1141 = torch_c.to_builtin_tensor %1138 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %1142 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_1452, %1140, %1141) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_1453 = tensor.cast %1142 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %1143 = torch_c.from_builtin_tensor %cast_1453 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1143, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_1454 = torch.constant.none
    %none_1455 = torch.constant.none
    %int5_1456 = torch.constant.int 5
    %cpu_1457 = torch.constant.device "cpu"
    %int0_1458 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1143, %none_1454, %none_1455, %int5_1456, %cpu_1457, %int0_1458 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_1459 = torch.constant.int 1
    %1144 = torch.aten.add.Tensor %903, %1143, %int1_1459 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1144, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_1460 = torch.constant.none
    %none_1461 = torch.constant.none
    %int5_1462 = torch.constant.int 5
    %cpu_1463 = torch.constant.device "cpu"
    %int0_1464 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1144, %none_1460, %none_1461, %int5_1462, %cpu_1463, %int0_1464 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1465 = torch.constant.int 6
    %1145 = torch.prims.convert_element_type %1144, %int6_1465 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1145, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_1466 = torch.constant.int 2
    %1146 = torch.aten.pow.Tensor_Scalar %1145, %int2_1466 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1146, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_1467 = torch.constant.int -1
    %1147 = torch.prim.ListConstruct %int-1_1467 : (!torch.int) -> !torch.list<int>
    %true_1468 = torch.constant.bool true
    %none_1469 = torch.constant.none
    %1148 = torch.aten.mean.dim %1146, %1147, %true_1468, %none_1469 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1148, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_1470 = torch.constant.float 9.9999997473787516E-6
    %int1_1471 = torch.constant.int 1
    %1149 = torch.aten.add.Scalar %1148, %float9.999990e-06_1470, %int1_1471 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1149, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1150 = torch.aten.rsqrt %1149 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1150, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1151 = torch.aten.mul.Tensor %1145, %1150 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1151, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_1472 = torch.constant.none
    %none_1473 = torch.constant.none
    %int6_1474 = torch.constant.int 6
    %cpu_1475 = torch.constant.device "cpu"
    %int0_1476 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1151, %none_1472, %none_1473, %int6_1474, %cpu_1475, %int0_1476 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1477 = torch.constant.int 5
    %1152 = torch.prims.convert_element_type %1151, %int5_1477 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1152, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %1153 = torch.aten.mul.Tensor %33, %1152 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1153, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_1478 = torch.constant.none
    %none_1479 = torch.constant.none
    %int6_1480 = torch.constant.int 6
    %cpu_1481 = torch.constant.device "cpu"
    %int0_1482 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1153, %none_1478, %none_1479, %int6_1480, %cpu_1481, %int0_1482 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1483 = torch.constant.int 5
    %1154 = torch.prims.convert_element_type %1153, %int5_1483 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1154, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_1484 = torch.constant.int 2
    %1155 = torch.aten.view.dtype %34, %int2_1484 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %1156 = torch.aten.detach %1155 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_1485 = torch.constant.int -1
    %int17_1486 = torch.constant.int 17
    %1157 = torch.prim.ListConstruct %int-1_1485, %int17_1486 : (!torch.int, !torch.int) -> !torch.list<int>
    %1158 = torch.aten.view %1156, %1157 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_1487 = torch.constant.int 5632
    %int-1_1488 = torch.constant.int -1
    %int17_1489 = torch.constant.int 17
    %1159 = torch.prim.ListConstruct %int5632_1487, %int-1_1488, %int17_1489 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1160 = torch.aten.view %1158, %1159 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_1490 = torch.constant.int 2
    %int0_1491 = torch.constant.int 0
    %int1_1492 = torch.constant.int 1
    %int1_1493 = torch.constant.int 1
    %1161 = torch.aten.slice.Tensor %1160, %int2_1490, %int0_1491, %int1_1492, %int1_1493 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_1494 = torch.constant.int 5
    %1162 = torch.aten.view.dtype %1161, %int5_1494 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %1163 = torch.aten.detach %1162 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_1495 = torch.constant.int 2
    %int1_1496 = torch.constant.int 1
    %int9223372036854775807_1497 = torch.constant.int 9223372036854775807
    %int1_1498 = torch.constant.int 1
    %1164 = torch.aten.slice.Tensor %1160, %int2_1495, %int1_1496, %int9223372036854775807_1497, %int1_1498 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_1499 = torch.constant.int 1
    %1165 = torch.aten.view.dtype %1164, %int1_1499 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %1166 = torch.aten.detach %1165 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %1167 = torch_c.to_builtin_tensor %1154 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_1500 = tensor.cast %1167 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %1168 = torch_c.to_builtin_tensor %1163 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %1169 = torch_c.to_builtin_tensor %1166 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %1170 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_1500, %1168, %1169) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_1501 = tensor.cast %1170 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %1171 = torch_c.from_builtin_tensor %cast_1501 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %1171, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %1172 = torch.aten.silu %1171 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %1172, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_1502 = torch.constant.int 2
    %1173 = torch.aten.view.dtype %35, %int2_1502 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %1174 = torch.aten.detach %1173 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_1503 = torch.constant.int -1
    %int17_1504 = torch.constant.int 17
    %1175 = torch.prim.ListConstruct %int-1_1503, %int17_1504 : (!torch.int, !torch.int) -> !torch.list<int>
    %1176 = torch.aten.view %1174, %1175 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_1505 = torch.constant.int 5632
    %int-1_1506 = torch.constant.int -1
    %int17_1507 = torch.constant.int 17
    %1177 = torch.prim.ListConstruct %int5632_1505, %int-1_1506, %int17_1507 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1178 = torch.aten.view %1176, %1177 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_1508 = torch.constant.int 2
    %int0_1509 = torch.constant.int 0
    %int1_1510 = torch.constant.int 1
    %int1_1511 = torch.constant.int 1
    %1179 = torch.aten.slice.Tensor %1178, %int2_1508, %int0_1509, %int1_1510, %int1_1511 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_1512 = torch.constant.int 5
    %1180 = torch.aten.view.dtype %1179, %int5_1512 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %1181 = torch.aten.detach %1180 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_1513 = torch.constant.int 2
    %int1_1514 = torch.constant.int 1
    %int9223372036854775807_1515 = torch.constant.int 9223372036854775807
    %int1_1516 = torch.constant.int 1
    %1182 = torch.aten.slice.Tensor %1178, %int2_1513, %int1_1514, %int9223372036854775807_1515, %int1_1516 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_1517 = torch.constant.int 1
    %1183 = torch.aten.view.dtype %1182, %int1_1517 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %1184 = torch.aten.detach %1183 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %1185 = torch_c.to_builtin_tensor %1154 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_1518 = tensor.cast %1185 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %1186 = torch_c.to_builtin_tensor %1181 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %1187 = torch_c.to_builtin_tensor %1184 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %1188 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_1518, %1186, %1187) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_1519 = tensor.cast %1188 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %1189 = torch_c.from_builtin_tensor %cast_1519 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %1189, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %1190 = torch.aten.mul.Tensor %1172, %1189 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %1190, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_1520 = torch.constant.int 2
    %1191 = torch.aten.view.dtype %36, %int2_1520 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %1192 = torch.aten.detach %1191 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_1521 = torch.constant.int -1
    %int17_1522 = torch.constant.int 17
    %1193 = torch.prim.ListConstruct %int-1_1521, %int17_1522 : (!torch.int, !torch.int) -> !torch.list<int>
    %1194 = torch.aten.view %1192, %1193 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_1523 = torch.constant.int 2048
    %int-1_1524 = torch.constant.int -1
    %int17_1525 = torch.constant.int 17
    %1195 = torch.prim.ListConstruct %int2048_1523, %int-1_1524, %int17_1525 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1196 = torch.aten.view %1194, %1195 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_1526 = torch.constant.int 2
    %int0_1527 = torch.constant.int 0
    %int1_1528 = torch.constant.int 1
    %int1_1529 = torch.constant.int 1
    %1197 = torch.aten.slice.Tensor %1196, %int2_1526, %int0_1527, %int1_1528, %int1_1529 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_1530 = torch.constant.int 5
    %1198 = torch.aten.view.dtype %1197, %int5_1530 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %1199 = torch.aten.detach %1198 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_1531 = torch.constant.int 2
    %int1_1532 = torch.constant.int 1
    %int9223372036854775807_1533 = torch.constant.int 9223372036854775807
    %int1_1534 = torch.constant.int 1
    %1200 = torch.aten.slice.Tensor %1196, %int2_1531, %int1_1532, %int9223372036854775807_1533, %int1_1534 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_1535 = torch.constant.int 1
    %1201 = torch.aten.view.dtype %1200, %int1_1535 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %1202 = torch.aten.detach %1201 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %1203 = torch_c.to_builtin_tensor %1190 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_1536 = tensor.cast %1203 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %1204 = torch_c.to_builtin_tensor %1199 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %1205 = torch_c.to_builtin_tensor %1202 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %1206 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_1536, %1204, %1205) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_1537 = tensor.cast %1206 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %1207 = torch_c.from_builtin_tensor %cast_1537 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1207, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_1538 = torch.constant.int 1
    %1208 = torch.aten.add.Tensor %1144, %1207, %int1_1538 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1208, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_1539 = torch.constant.none
    %none_1540 = torch.constant.none
    %int5_1541 = torch.constant.int 5
    %cpu_1542 = torch.constant.device "cpu"
    %int0_1543 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1208, %none_1539, %none_1540, %int5_1541, %cpu_1542, %int0_1543 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1544 = torch.constant.int 6
    %1209 = torch.prims.convert_element_type %1208, %int6_1544 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1209, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_1545 = torch.constant.int 2
    %1210 = torch.aten.pow.Tensor_Scalar %1209, %int2_1545 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1210, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_1546 = torch.constant.int -1
    %1211 = torch.prim.ListConstruct %int-1_1546 : (!torch.int) -> !torch.list<int>
    %true_1547 = torch.constant.bool true
    %none_1548 = torch.constant.none
    %1212 = torch.aten.mean.dim %1210, %1211, %true_1547, %none_1548 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1212, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_1549 = torch.constant.float 9.9999997473787516E-6
    %int1_1550 = torch.constant.int 1
    %1213 = torch.aten.add.Scalar %1212, %float9.999990e-06_1549, %int1_1550 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1213, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1214 = torch.aten.rsqrt %1213 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1214, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1215 = torch.aten.mul.Tensor %1209, %1214 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1215, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_1551 = torch.constant.none
    %none_1552 = torch.constant.none
    %int6_1553 = torch.constant.int 6
    %cpu_1554 = torch.constant.device "cpu"
    %int0_1555 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1215, %none_1551, %none_1552, %int6_1553, %cpu_1554, %int0_1555 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1556 = torch.constant.int 5
    %1216 = torch.prims.convert_element_type %1215, %int5_1556 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1216, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %1217 = torch.aten.mul.Tensor %40, %1216 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1217, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_1557 = torch.constant.none
    %none_1558 = torch.constant.none
    %int6_1559 = torch.constant.int 6
    %cpu_1560 = torch.constant.device "cpu"
    %int0_1561 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1217, %none_1557, %none_1558, %int6_1559, %cpu_1560, %int0_1561 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1562 = torch.constant.int 5
    %1218 = torch.prims.convert_element_type %1217, %int5_1562 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1218, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_1563 = torch.constant.int 2
    %1219 = torch.aten.view.dtype %41, %int2_1563 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %1220 = torch.aten.detach %1219 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_1564 = torch.constant.int -1
    %int17_1565 = torch.constant.int 17
    %1221 = torch.prim.ListConstruct %int-1_1564, %int17_1565 : (!torch.int, !torch.int) -> !torch.list<int>
    %1222 = torch.aten.view %1220, %1221 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_1566 = torch.constant.int 2048
    %int-1_1567 = torch.constant.int -1
    %int17_1568 = torch.constant.int 17
    %1223 = torch.prim.ListConstruct %int2048_1566, %int-1_1567, %int17_1568 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1224 = torch.aten.view %1222, %1223 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_1569 = torch.constant.int 2
    %int0_1570 = torch.constant.int 0
    %int1_1571 = torch.constant.int 1
    %int1_1572 = torch.constant.int 1
    %1225 = torch.aten.slice.Tensor %1224, %int2_1569, %int0_1570, %int1_1571, %int1_1572 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_1573 = torch.constant.int 5
    %1226 = torch.aten.view.dtype %1225, %int5_1573 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %1227 = torch.aten.detach %1226 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_1574 = torch.constant.int 2
    %int1_1575 = torch.constant.int 1
    %int9223372036854775807_1576 = torch.constant.int 9223372036854775807
    %int1_1577 = torch.constant.int 1
    %1228 = torch.aten.slice.Tensor %1224, %int2_1574, %int1_1575, %int9223372036854775807_1576, %int1_1577 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_1578 = torch.constant.int 1
    %1229 = torch.aten.view.dtype %1228, %int1_1578 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %1230 = torch.aten.detach %1229 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %1231 = torch_c.to_builtin_tensor %1218 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_1579 = tensor.cast %1231 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %1232 = torch_c.to_builtin_tensor %1227 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %1233 = torch_c.to_builtin_tensor %1230 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %1234 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_1579, %1232, %1233) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_1580 = tensor.cast %1234 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %1235 = torch_c.from_builtin_tensor %cast_1580 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1235, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_1581 = torch.constant.int 2
    %1236 = torch.aten.view.dtype %42, %int2_1581 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %1237 = torch.aten.detach %1236 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_1582 = torch.constant.int -1
    %int17_1583 = torch.constant.int 17
    %1238 = torch.prim.ListConstruct %int-1_1582, %int17_1583 : (!torch.int, !torch.int) -> !torch.list<int>
    %1239 = torch.aten.view %1237, %1238 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_1584 = torch.constant.int 256
    %int-1_1585 = torch.constant.int -1
    %int17_1586 = torch.constant.int 17
    %1240 = torch.prim.ListConstruct %int256_1584, %int-1_1585, %int17_1586 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1241 = torch.aten.view %1239, %1240 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_1587 = torch.constant.int 2
    %int0_1588 = torch.constant.int 0
    %int1_1589 = torch.constant.int 1
    %int1_1590 = torch.constant.int 1
    %1242 = torch.aten.slice.Tensor %1241, %int2_1587, %int0_1588, %int1_1589, %int1_1590 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_1591 = torch.constant.int 5
    %1243 = torch.aten.view.dtype %1242, %int5_1591 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %1244 = torch.aten.detach %1243 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_1592 = torch.constant.int 2
    %int1_1593 = torch.constant.int 1
    %int9223372036854775807_1594 = torch.constant.int 9223372036854775807
    %int1_1595 = torch.constant.int 1
    %1245 = torch.aten.slice.Tensor %1241, %int2_1592, %int1_1593, %int9223372036854775807_1594, %int1_1595 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_1596 = torch.constant.int 1
    %1246 = torch.aten.view.dtype %1245, %int1_1596 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %1247 = torch.aten.detach %1246 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %1248 = torch_c.to_builtin_tensor %1218 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_1597 = tensor.cast %1248 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %1249 = torch_c.to_builtin_tensor %1244 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %1250 = torch_c.to_builtin_tensor %1247 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %1251 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_1597, %1249, %1250) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_1598 = tensor.cast %1251 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %1252 = torch_c.from_builtin_tensor %cast_1598 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %1252, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_1599 = torch.constant.int 2
    %1253 = torch.aten.view.dtype %43, %int2_1599 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %1254 = torch.aten.detach %1253 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_1600 = torch.constant.int -1
    %int17_1601 = torch.constant.int 17
    %1255 = torch.prim.ListConstruct %int-1_1600, %int17_1601 : (!torch.int, !torch.int) -> !torch.list<int>
    %1256 = torch.aten.view %1254, %1255 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_1602 = torch.constant.int 256
    %int-1_1603 = torch.constant.int -1
    %int17_1604 = torch.constant.int 17
    %1257 = torch.prim.ListConstruct %int256_1602, %int-1_1603, %int17_1604 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1258 = torch.aten.view %1256, %1257 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_1605 = torch.constant.int 2
    %int0_1606 = torch.constant.int 0
    %int1_1607 = torch.constant.int 1
    %int1_1608 = torch.constant.int 1
    %1259 = torch.aten.slice.Tensor %1258, %int2_1605, %int0_1606, %int1_1607, %int1_1608 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_1609 = torch.constant.int 5
    %1260 = torch.aten.view.dtype %1259, %int5_1609 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %1261 = torch.aten.detach %1260 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_1610 = torch.constant.int 2
    %int1_1611 = torch.constant.int 1
    %int9223372036854775807_1612 = torch.constant.int 9223372036854775807
    %int1_1613 = torch.constant.int 1
    %1262 = torch.aten.slice.Tensor %1258, %int2_1610, %int1_1611, %int9223372036854775807_1612, %int1_1613 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_1614 = torch.constant.int 1
    %1263 = torch.aten.view.dtype %1262, %int1_1614 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %1264 = torch.aten.detach %1263 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %1265 = torch_c.to_builtin_tensor %1218 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_1615 = tensor.cast %1265 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %1266 = torch_c.to_builtin_tensor %1261 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %1267 = torch_c.to_builtin_tensor %1264 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %1268 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_1615, %1266, %1267) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_1616 = tensor.cast %1268 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %1269 = torch_c.from_builtin_tensor %cast_1616 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %1269, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_1617 = torch.constant.int 4
    %int32_1618 = torch.constant.int 32
    %int64_1619 = torch.constant.int 64
    %1270 = torch.prim.ListConstruct %int4_1617, %273, %int32_1618, %int64_1619 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1271 = torch.aten.view %1235, %1270 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1271, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_1620 = torch.constant.int 4
    %int4_1621 = torch.constant.int 4
    %int64_1622 = torch.constant.int 64
    %1272 = torch.prim.ListConstruct %int4_1620, %273, %int4_1621, %int64_1622 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1273 = torch.aten.view %1252, %1272 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %1273, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_1623 = torch.constant.int 4
    %int4_1624 = torch.constant.int 4
    %int64_1625 = torch.constant.int 64
    %1274 = torch.prim.ListConstruct %int4_1623, %273, %int4_1624, %int64_1625 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1275 = torch.aten.view %1269, %1274 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %1275, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_1626 = torch.constant.int 0
    %none_1627 = torch.constant.none
    %none_1628 = torch.constant.none
    %cpu_1629 = torch.constant.device "cpu"
    %false_1630 = torch.constant.bool false
    %1276 = torch.aten.arange.start %int0_1626, %273, %none_1627, %none_1628, %cpu_1629, %false_1630 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1276, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_1631 = torch.constant.int 0
    %1277 = torch.aten.unsqueeze %1276, %int0_1631 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %1277, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_1632 = torch.constant.int 0
    %int64_1633 = torch.constant.int 64
    %int2_1634 = torch.constant.int 2
    %none_1635 = torch.constant.none
    %none_1636 = torch.constant.none
    %cpu_1637 = torch.constant.device "cpu"
    %false_1638 = torch.constant.bool false
    %1278 = torch.aten.arange.start_step %int0_1632, %int64_1633, %int2_1634, %none_1635, %none_1636, %cpu_1637, %false_1638 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_1639 = torch.constant.none
    %none_1640 = torch.constant.none
    %int4_1641 = torch.constant.int 4
    %cpu_1642 = torch.constant.device "cpu"
    %int0_1643 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1278, %none_1639, %none_1640, %int4_1641, %cpu_1642, %int0_1643 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1644 = torch.constant.int 6
    %1279 = torch.prims.convert_element_type %1278, %int6_1644 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_1645 = torch.constant.int 64
    %1280 = torch.aten.div.Scalar %1279, %int64_1645 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_1646 = torch.constant.float 1.000000e+04
    %1281 = torch.aten.pow.Scalar %float1.000000e04_1646, %1280 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %1282 = torch.aten.reciprocal %1281 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_1647 = torch.constant.float 1.000000e+00
    %1283 = torch.aten.mul.Scalar %1282, %float1.000000e00_1647 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_1648 = torch.constant.none
    %1284 = torch.aten.clone %37, %none_1648 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_1649 = torch.constant.int 0
    %1285 = torch.aten.unsqueeze %1283, %int0_1649 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_1650 = torch.constant.int 2
    %1286 = torch.aten.unsqueeze %1285, %int2_1650 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_1651 = torch.constant.none
    %none_1652 = torch.constant.none
    %int6_1653 = torch.constant.int 6
    %cpu_1654 = torch.constant.device "cpu"
    %int0_1655 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1286, %none_1651, %none_1652, %int6_1653, %cpu_1654, %int0_1655 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_1656 = torch.constant.int 1
    %int-1_1657 = torch.constant.int -1
    %int1_1658 = torch.constant.int 1
    %1287 = torch.prim.ListConstruct %int1_1656, %int-1_1657, %int1_1658 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1659 = torch.constant.bool false
    %1288 = torch.aten.expand %1286, %1287, %false_1659 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_1660 = torch.constant.int 1
    %1289 = torch.aten.unsqueeze %1277, %int1_1660 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %1289, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_1661 = torch.constant.none
    %none_1662 = torch.constant.none
    %int4_1663 = torch.constant.int 4
    %cpu_1664 = torch.constant.device "cpu"
    %int0_1665 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1289, %none_1661, %none_1662, %int4_1663, %cpu_1664, %int0_1665 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1666 = torch.constant.int 6
    %1290 = torch.prims.convert_element_type %1289, %int6_1666 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %1290, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %1291 = torch.aten.matmul %1288, %1290 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %1291, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_1667 = torch.constant.int 1
    %int2_1668 = torch.constant.int 2
    %1292 = torch.aten.transpose.int %1291, %int1_1667, %int2_1668 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1292, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1293 = torch.aten.cos %1292 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1293, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1294 = torch.aten.mul.Tensor %1293, %1284 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1294, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_1669 = torch.constant.none
    %none_1670 = torch.constant.none
    %int6_1671 = torch.constant.int 6
    %cpu_1672 = torch.constant.device "cpu"
    %int0_1673 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1294, %none_1669, %none_1670, %int6_1671, %cpu_1672, %int0_1673 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1674 = torch.constant.int 5
    %1295 = torch.prims.convert_element_type %1294, %int5_1674 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %1295, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %1296 = torch.aten.sin %1292 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1296, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1297 = torch.aten.mul.Tensor %1296, %1284 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1297, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_1675 = torch.constant.none
    %none_1676 = torch.constant.none
    %int6_1677 = torch.constant.int 6
    %cpu_1678 = torch.constant.device "cpu"
    %int0_1679 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1297, %none_1675, %none_1676, %int6_1677, %cpu_1678, %int0_1679 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1680 = torch.constant.int 5
    %1298 = torch.prims.convert_element_type %1297, %int5_1680 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %1298, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_1681 = torch.constant.int 2
    %1299 = torch.aten.unsqueeze %1295, %int2_1681 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %1299, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_1682 = torch.constant.int 2
    %1300 = torch.aten.unsqueeze %1298, %int2_1682 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %1300, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_1683 = torch.constant.none
    %none_1684 = torch.constant.none
    %int5_1685 = torch.constant.int 5
    %cpu_1686 = torch.constant.device "cpu"
    %int0_1687 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1299, %none_1683, %none_1684, %int5_1685, %cpu_1686, %int0_1687 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_1688 = torch.constant.none
    %none_1689 = torch.constant.none
    %int5_1690 = torch.constant.int 5
    %cpu_1691 = torch.constant.device "cpu"
    %int0_1692 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1300, %none_1688, %none_1689, %int5_1690, %cpu_1691, %int0_1692 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_1693 = torch.constant.none
    %none_1694 = torch.constant.none
    %int5_1695 = torch.constant.int 5
    %cpu_1696 = torch.constant.device "cpu"
    %int0_1697 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1271, %none_1693, %none_1694, %int5_1695, %cpu_1696, %int0_1697 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_1698 = torch.constant.int 3
    %int0_1699 = torch.constant.int 0
    %int64_1700 = torch.constant.int 64
    %int2_1701 = torch.constant.int 2
    %1301 = torch.aten.slice.Tensor %1271, %int3_1698, %int0_1699, %int64_1700, %int2_1701 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1301, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_1702 = torch.constant.int 3
    %int1_1703 = torch.constant.int 1
    %int64_1704 = torch.constant.int 64
    %int2_1705 = torch.constant.int 2
    %1302 = torch.aten.slice.Tensor %1271, %int3_1702, %int1_1703, %int64_1704, %int2_1705 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1302, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %1303 = torch.aten.mul.Tensor %1301, %1299 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1303, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %1304 = torch.aten.mul.Tensor %1302, %1300 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1304, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_1706 = torch.constant.int 1
    %1305 = torch.aten.sub.Tensor %1303, %1304, %int1_1706 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1305, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %1306 = torch.aten.mul.Tensor %1302, %1299 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1306, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %1307 = torch.aten.mul.Tensor %1301, %1300 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1307, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_1707 = torch.constant.int 1
    %1308 = torch.aten.add.Tensor %1306, %1307, %int1_1707 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1308, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %1309 = torch_c.to_builtin_tensor %1305 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_1708 = tensor.cast %1309 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %1310 = torch_c.to_builtin_tensor %1308 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_1709 = tensor.cast %1310 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %1311 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_1708, %cast_1709) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_1710 = tensor.cast %1311 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %1312 = torch_c.from_builtin_tensor %cast_1710 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %1312, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_1711 = torch.constant.int 4
    %int32_1712 = torch.constant.int 32
    %int64_1713 = torch.constant.int 64
    %1313 = torch.prim.ListConstruct %int4_1711, %273, %int32_1712, %int64_1713 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1314 = torch.aten.view %1312, %1313 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1314, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_1714 = torch.constant.none
    %none_1715 = torch.constant.none
    %int5_1716 = torch.constant.int 5
    %cpu_1717 = torch.constant.device "cpu"
    %int0_1718 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1314, %none_1714, %none_1715, %int5_1716, %cpu_1717, %int0_1718 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_1719 = torch.constant.int 0
    %none_1720 = torch.constant.none
    %none_1721 = torch.constant.none
    %cpu_1722 = torch.constant.device "cpu"
    %false_1723 = torch.constant.bool false
    %1315 = torch.aten.arange.start %int0_1719, %273, %none_1720, %none_1721, %cpu_1722, %false_1723 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1315, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_1724 = torch.constant.int 0
    %1316 = torch.aten.unsqueeze %1315, %int0_1724 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %1316, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_1725 = torch.constant.int 0
    %int64_1726 = torch.constant.int 64
    %int2_1727 = torch.constant.int 2
    %none_1728 = torch.constant.none
    %none_1729 = torch.constant.none
    %cpu_1730 = torch.constant.device "cpu"
    %false_1731 = torch.constant.bool false
    %1317 = torch.aten.arange.start_step %int0_1725, %int64_1726, %int2_1727, %none_1728, %none_1729, %cpu_1730, %false_1731 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_1732 = torch.constant.none
    %none_1733 = torch.constant.none
    %int4_1734 = torch.constant.int 4
    %cpu_1735 = torch.constant.device "cpu"
    %int0_1736 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1317, %none_1732, %none_1733, %int4_1734, %cpu_1735, %int0_1736 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1737 = torch.constant.int 6
    %1318 = torch.prims.convert_element_type %1317, %int6_1737 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_1738 = torch.constant.int 64
    %1319 = torch.aten.div.Scalar %1318, %int64_1738 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_1739 = torch.constant.float 1.000000e+04
    %1320 = torch.aten.pow.Scalar %float1.000000e04_1739, %1319 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %1321 = torch.aten.reciprocal %1320 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_1740 = torch.constant.float 1.000000e+00
    %1322 = torch.aten.mul.Scalar %1321, %float1.000000e00_1740 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_1741 = torch.constant.none
    %1323 = torch.aten.clone %38, %none_1741 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_1742 = torch.constant.int 0
    %1324 = torch.aten.unsqueeze %1322, %int0_1742 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_1743 = torch.constant.int 2
    %1325 = torch.aten.unsqueeze %1324, %int2_1743 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_1744 = torch.constant.none
    %none_1745 = torch.constant.none
    %int6_1746 = torch.constant.int 6
    %cpu_1747 = torch.constant.device "cpu"
    %int0_1748 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1325, %none_1744, %none_1745, %int6_1746, %cpu_1747, %int0_1748 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_1749 = torch.constant.int 1
    %int-1_1750 = torch.constant.int -1
    %int1_1751 = torch.constant.int 1
    %1326 = torch.prim.ListConstruct %int1_1749, %int-1_1750, %int1_1751 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1752 = torch.constant.bool false
    %1327 = torch.aten.expand %1325, %1326, %false_1752 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_1753 = torch.constant.int 1
    %1328 = torch.aten.unsqueeze %1316, %int1_1753 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %1328, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_1754 = torch.constant.none
    %none_1755 = torch.constant.none
    %int4_1756 = torch.constant.int 4
    %cpu_1757 = torch.constant.device "cpu"
    %int0_1758 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1328, %none_1754, %none_1755, %int4_1756, %cpu_1757, %int0_1758 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1759 = torch.constant.int 6
    %1329 = torch.prims.convert_element_type %1328, %int6_1759 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %1329, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %1330 = torch.aten.matmul %1327, %1329 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %1330, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_1760 = torch.constant.int 1
    %int2_1761 = torch.constant.int 2
    %1331 = torch.aten.transpose.int %1330, %int1_1760, %int2_1761 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1331, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1332 = torch.aten.cos %1331 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1332, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1333 = torch.aten.mul.Tensor %1332, %1323 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1333, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_1762 = torch.constant.none
    %none_1763 = torch.constant.none
    %int6_1764 = torch.constant.int 6
    %cpu_1765 = torch.constant.device "cpu"
    %int0_1766 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1333, %none_1762, %none_1763, %int6_1764, %cpu_1765, %int0_1766 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1767 = torch.constant.int 5
    %1334 = torch.prims.convert_element_type %1333, %int5_1767 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %1334, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %1335 = torch.aten.sin %1331 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1335, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1336 = torch.aten.mul.Tensor %1335, %1323 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1336, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_1768 = torch.constant.none
    %none_1769 = torch.constant.none
    %int6_1770 = torch.constant.int 6
    %cpu_1771 = torch.constant.device "cpu"
    %int0_1772 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1336, %none_1768, %none_1769, %int6_1770, %cpu_1771, %int0_1772 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1773 = torch.constant.int 5
    %1337 = torch.prims.convert_element_type %1336, %int5_1773 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %1337, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_1774 = torch.constant.int 2
    %1338 = torch.aten.unsqueeze %1334, %int2_1774 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %1338, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_1775 = torch.constant.int 2
    %1339 = torch.aten.unsqueeze %1337, %int2_1775 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %1339, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_1776 = torch.constant.none
    %none_1777 = torch.constant.none
    %int5_1778 = torch.constant.int 5
    %cpu_1779 = torch.constant.device "cpu"
    %int0_1780 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1338, %none_1776, %none_1777, %int5_1778, %cpu_1779, %int0_1780 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_1781 = torch.constant.none
    %none_1782 = torch.constant.none
    %int5_1783 = torch.constant.int 5
    %cpu_1784 = torch.constant.device "cpu"
    %int0_1785 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1339, %none_1781, %none_1782, %int5_1783, %cpu_1784, %int0_1785 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_1786 = torch.constant.none
    %none_1787 = torch.constant.none
    %int5_1788 = torch.constant.int 5
    %cpu_1789 = torch.constant.device "cpu"
    %int0_1790 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1273, %none_1786, %none_1787, %int5_1788, %cpu_1789, %int0_1790 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_1791 = torch.constant.int 3
    %int0_1792 = torch.constant.int 0
    %int64_1793 = torch.constant.int 64
    %int2_1794 = torch.constant.int 2
    %1340 = torch.aten.slice.Tensor %1273, %int3_1791, %int0_1792, %int64_1793, %int2_1794 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1340, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_1795 = torch.constant.int 3
    %int1_1796 = torch.constant.int 1
    %int64_1797 = torch.constant.int 64
    %int2_1798 = torch.constant.int 2
    %1341 = torch.aten.slice.Tensor %1273, %int3_1795, %int1_1796, %int64_1797, %int2_1798 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1341, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1342 = torch.aten.mul.Tensor %1340, %1338 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1342, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1343 = torch.aten.mul.Tensor %1341, %1339 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1343, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_1799 = torch.constant.int 1
    %1344 = torch.aten.sub.Tensor %1342, %1343, %int1_1799 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1344, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1345 = torch.aten.mul.Tensor %1341, %1338 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1345, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1346 = torch.aten.mul.Tensor %1340, %1339 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1346, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_1800 = torch.constant.int 1
    %1347 = torch.aten.add.Tensor %1345, %1346, %int1_1800 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1347, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1348 = torch_c.to_builtin_tensor %1344 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_1801 = tensor.cast %1348 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %1349 = torch_c.to_builtin_tensor %1347 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_1802 = tensor.cast %1349 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %1350 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_1801, %cast_1802) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_1803 = tensor.cast %1350 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %1351 = torch_c.from_builtin_tensor %cast_1803 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %1351, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_1804 = torch.constant.int 4
    %int4_1805 = torch.constant.int 4
    %int64_1806 = torch.constant.int 64
    %1352 = torch.prim.ListConstruct %int4_1804, %273, %int4_1805, %int64_1806 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1353 = torch.aten.view %1351, %1352 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %1353, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_1807 = torch.constant.none
    %none_1808 = torch.constant.none
    %int5_1809 = torch.constant.int 5
    %cpu_1810 = torch.constant.device "cpu"
    %int0_1811 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1353, %none_1807, %none_1808, %int5_1809, %cpu_1810, %int0_1811 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_1812 = torch.constant.int 22
    %1354 = torch.aten.mul.Scalar %arg2, %int22_1812 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1354, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int3_1813 = torch.constant.int 3
    %int1_1814 = torch.constant.int 1
    %1355 = torch.aten.add.Scalar %1354, %int3_1813, %int1_1814 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1355, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_1815 = torch.constant.int 2
    %1356 = torch.aten.mul.Scalar %1355, %int2_1815 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1356, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_1816 = torch.constant.int 0
    %int1_1817 = torch.constant.int 1
    %1357 = torch.aten.add.Scalar %1356, %int0_1816, %int1_1817 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1357, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %1358 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %1359 = torch.aten.view %1357, %1358 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1359, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_1818 = torch.constant.int 4
    %int32_1819 = torch.constant.int 32
    %int4_1820 = torch.constant.int 4
    %int64_1821 = torch.constant.int 64
    %1360 = torch.prim.ListConstruct %int4_1818, %271, %int32_1819, %int4_1820, %int64_1821 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1361 = torch.aten.view %1353, %1360 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1361, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_1822 = torch.constant.int 32
    %int4_1823 = torch.constant.int 4
    %int64_1824 = torch.constant.int 64
    %1362 = torch.prim.ListConstruct %446, %int32_1822, %int4_1823, %int64_1824 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1363 = torch.aten.view %1361, %1362 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %1363, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_1825 = torch.constant.int 1
    %int2_1826 = torch.constant.int 2
    %1364 = torch.aten.transpose.int %1363, %int1_1825, %int2_1826 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1364, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_1827 = torch.constant.none
    %none_1828 = torch.constant.none
    %int5_1829 = torch.constant.int 5
    %cpu_1830 = torch.constant.device "cpu"
    %int0_1831 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1364, %none_1827, %none_1828, %int5_1829, %cpu_1830, %int0_1831 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_1832 = torch.constant.int 22
    %int2_1833 = torch.constant.int 2
    %int4_1834 = torch.constant.int 4
    %int32_1835 = torch.constant.int 32
    %int64_1836 = torch.constant.int 64
    %1365 = torch.prim.ListConstruct %272, %int22_1832, %int2_1833, %int4_1834, %int32_1835, %int64_1836 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1366 = torch.aten.view %1090, %1365 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1366, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_1837 = torch.constant.int 4
    %int32_1838 = torch.constant.int 32
    %int64_1839 = torch.constant.int 64
    %1367 = torch.prim.ListConstruct %439, %int4_1837, %int32_1838, %int64_1839 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1368 = torch.aten.view %1366, %1367 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1368, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %1369 = torch.prim.ListConstruct %1359 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_1840 = torch.constant.bool false
    %1370 = torch.aten.index_put %1368, %1369, %1364, %false_1840 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1370, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_1841 = torch.constant.int 22
    %int2_1842 = torch.constant.int 2
    %int4_1843 = torch.constant.int 4
    %int32_1844 = torch.constant.int 32
    %int64_1845 = torch.constant.int 64
    %1371 = torch.prim.ListConstruct %272, %int22_1841, %int2_1842, %int4_1843, %int32_1844, %int64_1845 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1372 = torch.aten.view %1370, %1371 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1372, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_1846 = torch.constant.int 360448
    %1373 = torch.prim.ListConstruct %272, %int360448_1846 : (!torch.int, !torch.int) -> !torch.list<int>
    %1374 = torch.aten.view %1372, %1373 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %1374, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_1847 = torch.constant.int 22
    %int2_1848 = torch.constant.int 2
    %int4_1849 = torch.constant.int 4
    %int32_1850 = torch.constant.int 32
    %int64_1851 = torch.constant.int 64
    %1375 = torch.prim.ListConstruct %272, %int22_1847, %int2_1848, %int4_1849, %int32_1850, %int64_1851 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1376 = torch.aten.view %1374, %1375 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1376, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_1852 = torch.constant.int 4
    %int32_1853 = torch.constant.int 32
    %int64_1854 = torch.constant.int 64
    %1377 = torch.prim.ListConstruct %439, %int4_1852, %int32_1853, %int64_1854 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1378 = torch.aten.view %1376, %1377 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1378, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_1855 = torch.constant.int 22
    %1379 = torch.aten.mul.Scalar %arg2, %int22_1855 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1379, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int3_1856 = torch.constant.int 3
    %int1_1857 = torch.constant.int 1
    %1380 = torch.aten.add.Scalar %1379, %int3_1856, %int1_1857 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1380, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_1858 = torch.constant.int 2
    %1381 = torch.aten.mul.Scalar %1380, %int2_1858 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1381, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_1859 = torch.constant.int 1
    %int1_1860 = torch.constant.int 1
    %1382 = torch.aten.add.Scalar %1381, %int1_1859, %int1_1860 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1382, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %1383 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %1384 = torch.aten.view %1382, %1383 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1384, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_1861 = torch.constant.int 4
    %int32_1862 = torch.constant.int 32
    %int4_1863 = torch.constant.int 4
    %int64_1864 = torch.constant.int 64
    %1385 = torch.prim.ListConstruct %int4_1861, %271, %int32_1862, %int4_1863, %int64_1864 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1386 = torch.aten.view %1275, %1385 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1386, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_1865 = torch.constant.int 32
    %int4_1866 = torch.constant.int 4
    %int64_1867 = torch.constant.int 64
    %1387 = torch.prim.ListConstruct %446, %int32_1865, %int4_1866, %int64_1867 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1388 = torch.aten.view %1386, %1387 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %1388, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_1868 = torch.constant.int 1
    %int2_1869 = torch.constant.int 2
    %1389 = torch.aten.transpose.int %1388, %int1_1868, %int2_1869 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1389, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_1870 = torch.constant.none
    %none_1871 = torch.constant.none
    %int5_1872 = torch.constant.int 5
    %cpu_1873 = torch.constant.device "cpu"
    %int0_1874 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1389, %none_1870, %none_1871, %int5_1872, %cpu_1873, %int0_1874 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %1390 = torch.prim.ListConstruct %1384 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_1875 = torch.constant.bool false
    %1391 = torch.aten.index_put %1378, %1390, %1389, %false_1875 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1391, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_1876 = torch.constant.int 22
    %int2_1877 = torch.constant.int 2
    %int4_1878 = torch.constant.int 4
    %int32_1879 = torch.constant.int 32
    %int64_1880 = torch.constant.int 64
    %1392 = torch.prim.ListConstruct %272, %int22_1876, %int2_1877, %int4_1878, %int32_1879, %int64_1880 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1393 = torch.aten.view %1391, %1392 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1393, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_1881 = torch.constant.int 360448
    %1394 = torch.prim.ListConstruct %272, %int360448_1881 : (!torch.int, !torch.int) -> !torch.list<int>
    %1395 = torch.aten.view %1393, %1394 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %1395, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_1882 = torch.constant.int 0
    %int1_1883 = torch.constant.int 1
    %none_1884 = torch.constant.none
    %none_1885 = torch.constant.none
    %cpu_1886 = torch.constant.device "cpu"
    %false_1887 = torch.constant.bool false
    %1396 = torch.aten.arange.start_step %int0_1882, %273, %int1_1883, %none_1884, %none_1885, %cpu_1886, %false_1887 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1396, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_1888 = torch.constant.int -1
    %1397 = torch.aten.unsqueeze %arg1, %int-1_1888 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %1398 = torch.aten.ge.Tensor %1396, %1397 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %1398, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_1889 = torch.constant.none
    %none_1890 = torch.constant.none
    %cpu_1891 = torch.constant.device "cpu"
    %false_1892 = torch.constant.bool false
    %1399 = torch.aten.arange %273, %none_1889, %none_1890, %cpu_1891, %false_1892 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1399, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_1893 = torch.constant.int 0
    %1400 = torch.aten.unsqueeze %1399, %int0_1893 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %1400, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_1894 = torch.constant.int 1
    %1401 = torch.aten.unsqueeze %1400, %int1_1894 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %1401, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_1895 = torch.constant.int 2
    %1402 = torch.aten.unsqueeze %1401, %int2_1895 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %1402, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_1896 = torch.constant.none
    %none_1897 = torch.constant.none
    %cpu_1898 = torch.constant.device "cpu"
    %false_1899 = torch.constant.bool false
    %1403 = torch.aten.arange %273, %none_1896, %none_1897, %cpu_1898, %false_1899 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1403, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_1900 = torch.constant.int 0
    %1404 = torch.aten.unsqueeze %1403, %int0_1900 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %1404, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_1901 = torch.constant.int 1
    %1405 = torch.aten.unsqueeze %1404, %int1_1901 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %1405, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_1902 = torch.constant.int 3
    %1406 = torch.aten.unsqueeze %1405, %int3_1902 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %1406, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %1407 = torch.aten.gt.Tensor %1402, %1406 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %1407, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_1903 = torch.constant.int 1
    %1408 = torch.aten.unsqueeze %1398, %int1_1903 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %1408, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_1904 = torch.constant.int 2
    %1409 = torch.aten.unsqueeze %1408, %int2_1904 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %1409, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %1410 = torch.aten.logical_or %1407, %1409 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %1410, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_1905 = torch.constant.none
    %1411 = torch.aten.clone %39, %none_1905 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_1906 = torch.constant.int 0
    %1412 = torch.aten.where.ScalarOther %1410, %1411, %int0_1906 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %1412, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_1907 = torch.constant.none
    %none_1908 = torch.constant.none
    %int5_1909 = torch.constant.int 5
    %cpu_1910 = torch.constant.device "cpu"
    %int0_1911 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1412, %none_1907, %none_1908, %int5_1909, %cpu_1910, %int0_1911 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_1912 = torch.constant.int -2
    %1413 = torch.aten.unsqueeze %1353, %int-2_1912 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %1413, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_1913 = torch.constant.int 4
    %int4_1914 = torch.constant.int 4
    %int8_1915 = torch.constant.int 8
    %int64_1916 = torch.constant.int 64
    %1414 = torch.prim.ListConstruct %int4_1913, %273, %int4_1914, %int8_1915, %int64_1916 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1917 = torch.constant.bool false
    %1415 = torch.aten.expand %1413, %1414, %false_1917 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1415, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_1918 = torch.constant.int 0
    %1416 = torch.aten.clone %1415, %int0_1918 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1416, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_1919 = torch.constant.int 4
    %int32_1920 = torch.constant.int 32
    %int64_1921 = torch.constant.int 64
    %1417 = torch.prim.ListConstruct %int4_1919, %273, %int32_1920, %int64_1921 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1418 = torch.aten._unsafe_view %1416, %1417 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1418, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_1922 = torch.constant.int -2
    %1419 = torch.aten.unsqueeze %1275, %int-2_1922 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %1419, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_1923 = torch.constant.int 4
    %int4_1924 = torch.constant.int 4
    %int8_1925 = torch.constant.int 8
    %int64_1926 = torch.constant.int 64
    %1420 = torch.prim.ListConstruct %int4_1923, %273, %int4_1924, %int8_1925, %int64_1926 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1927 = torch.constant.bool false
    %1421 = torch.aten.expand %1419, %1420, %false_1927 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1421, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_1928 = torch.constant.int 0
    %1422 = torch.aten.clone %1421, %int0_1928 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1422, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_1929 = torch.constant.int 4
    %int32_1930 = torch.constant.int 32
    %int64_1931 = torch.constant.int 64
    %1423 = torch.prim.ListConstruct %int4_1929, %273, %int32_1930, %int64_1931 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1424 = torch.aten._unsafe_view %1422, %1423 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1424, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_1932 = torch.constant.int 1
    %int2_1933 = torch.constant.int 2
    %1425 = torch.aten.transpose.int %1314, %int1_1932, %int2_1933 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %1425, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_1934 = torch.constant.int 1
    %int2_1935 = torch.constant.int 2
    %1426 = torch.aten.transpose.int %1418, %int1_1934, %int2_1935 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %1426, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_1936 = torch.constant.int 1
    %int2_1937 = torch.constant.int 2
    %1427 = torch.aten.transpose.int %1424, %int1_1936, %int2_1937 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %1427, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_1938 = torch.constant.float 0.000000e+00
    %false_1939 = torch.constant.bool false
    %none_1940 = torch.constant.none
    %false_1941 = torch.constant.bool false
    %1428 = torch.aten.scaled_dot_product_attention %1425, %1426, %1427, %1412, %float0.000000e00_1938, %false_1939, %none_1940, %false_1941 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %1428, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_1942 = torch.constant.int 1
    %int2_1943 = torch.constant.int 2
    %1429 = torch.aten.transpose.int %1428, %int1_1942, %int2_1943 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1429, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_1944 = torch.constant.int 4
    %int2048_1945 = torch.constant.int 2048
    %1430 = torch.prim.ListConstruct %int4_1944, %273, %int2048_1945 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1431 = torch.aten.view %1429, %1430 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1431, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_1946 = torch.constant.int 2
    %1432 = torch.aten.view.dtype %44, %int2_1946 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %1433 = torch.aten.detach %1432 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_1947 = torch.constant.int -1
    %int17_1948 = torch.constant.int 17
    %1434 = torch.prim.ListConstruct %int-1_1947, %int17_1948 : (!torch.int, !torch.int) -> !torch.list<int>
    %1435 = torch.aten.view %1433, %1434 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_1949 = torch.constant.int 2048
    %int-1_1950 = torch.constant.int -1
    %int17_1951 = torch.constant.int 17
    %1436 = torch.prim.ListConstruct %int2048_1949, %int-1_1950, %int17_1951 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1437 = torch.aten.view %1435, %1436 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_1952 = torch.constant.int 2
    %int0_1953 = torch.constant.int 0
    %int1_1954 = torch.constant.int 1
    %int1_1955 = torch.constant.int 1
    %1438 = torch.aten.slice.Tensor %1437, %int2_1952, %int0_1953, %int1_1954, %int1_1955 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_1956 = torch.constant.int 5
    %1439 = torch.aten.view.dtype %1438, %int5_1956 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %1440 = torch.aten.detach %1439 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_1957 = torch.constant.int 2
    %int1_1958 = torch.constant.int 1
    %int9223372036854775807_1959 = torch.constant.int 9223372036854775807
    %int1_1960 = torch.constant.int 1
    %1441 = torch.aten.slice.Tensor %1437, %int2_1957, %int1_1958, %int9223372036854775807_1959, %int1_1960 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_1961 = torch.constant.int 1
    %1442 = torch.aten.view.dtype %1441, %int1_1961 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %1443 = torch.aten.detach %1442 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %1444 = torch_c.to_builtin_tensor %1431 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_1962 = tensor.cast %1444 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %1445 = torch_c.to_builtin_tensor %1440 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %1446 = torch_c.to_builtin_tensor %1443 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %1447 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_1962, %1445, %1446) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_1963 = tensor.cast %1447 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %1448 = torch_c.from_builtin_tensor %cast_1963 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1448, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_1964 = torch.constant.none
    %none_1965 = torch.constant.none
    %int5_1966 = torch.constant.int 5
    %cpu_1967 = torch.constant.device "cpu"
    %int0_1968 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1448, %none_1964, %none_1965, %int5_1966, %cpu_1967, %int0_1968 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_1969 = torch.constant.int 1
    %1449 = torch.aten.add.Tensor %1208, %1448, %int1_1969 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1449, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_1970 = torch.constant.none
    %none_1971 = torch.constant.none
    %int5_1972 = torch.constant.int 5
    %cpu_1973 = torch.constant.device "cpu"
    %int0_1974 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1449, %none_1970, %none_1971, %int5_1972, %cpu_1973, %int0_1974 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1975 = torch.constant.int 6
    %1450 = torch.prims.convert_element_type %1449, %int6_1975 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1450, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_1976 = torch.constant.int 2
    %1451 = torch.aten.pow.Tensor_Scalar %1450, %int2_1976 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1451, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_1977 = torch.constant.int -1
    %1452 = torch.prim.ListConstruct %int-1_1977 : (!torch.int) -> !torch.list<int>
    %true_1978 = torch.constant.bool true
    %none_1979 = torch.constant.none
    %1453 = torch.aten.mean.dim %1451, %1452, %true_1978, %none_1979 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1453, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_1980 = torch.constant.float 9.9999997473787516E-6
    %int1_1981 = torch.constant.int 1
    %1454 = torch.aten.add.Scalar %1453, %float9.999990e-06_1980, %int1_1981 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1454, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1455 = torch.aten.rsqrt %1454 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1455, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1456 = torch.aten.mul.Tensor %1450, %1455 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1456, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_1982 = torch.constant.none
    %none_1983 = torch.constant.none
    %int6_1984 = torch.constant.int 6
    %cpu_1985 = torch.constant.device "cpu"
    %int0_1986 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1456, %none_1982, %none_1983, %int6_1984, %cpu_1985, %int0_1986 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1987 = torch.constant.int 5
    %1457 = torch.prims.convert_element_type %1456, %int5_1987 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1457, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %1458 = torch.aten.mul.Tensor %45, %1457 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1458, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_1988 = torch.constant.none
    %none_1989 = torch.constant.none
    %int6_1990 = torch.constant.int 6
    %cpu_1991 = torch.constant.device "cpu"
    %int0_1992 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1458, %none_1988, %none_1989, %int6_1990, %cpu_1991, %int0_1992 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1993 = torch.constant.int 5
    %1459 = torch.prims.convert_element_type %1458, %int5_1993 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1459, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_1994 = torch.constant.int 2
    %1460 = torch.aten.view.dtype %46, %int2_1994 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %1461 = torch.aten.detach %1460 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_1995 = torch.constant.int -1
    %int17_1996 = torch.constant.int 17
    %1462 = torch.prim.ListConstruct %int-1_1995, %int17_1996 : (!torch.int, !torch.int) -> !torch.list<int>
    %1463 = torch.aten.view %1461, %1462 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_1997 = torch.constant.int 5632
    %int-1_1998 = torch.constant.int -1
    %int17_1999 = torch.constant.int 17
    %1464 = torch.prim.ListConstruct %int5632_1997, %int-1_1998, %int17_1999 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1465 = torch.aten.view %1463, %1464 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_2000 = torch.constant.int 2
    %int0_2001 = torch.constant.int 0
    %int1_2002 = torch.constant.int 1
    %int1_2003 = torch.constant.int 1
    %1466 = torch.aten.slice.Tensor %1465, %int2_2000, %int0_2001, %int1_2002, %int1_2003 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_2004 = torch.constant.int 5
    %1467 = torch.aten.view.dtype %1466, %int5_2004 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %1468 = torch.aten.detach %1467 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_2005 = torch.constant.int 2
    %int1_2006 = torch.constant.int 1
    %int9223372036854775807_2007 = torch.constant.int 9223372036854775807
    %int1_2008 = torch.constant.int 1
    %1469 = torch.aten.slice.Tensor %1465, %int2_2005, %int1_2006, %int9223372036854775807_2007, %int1_2008 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_2009 = torch.constant.int 1
    %1470 = torch.aten.view.dtype %1469, %int1_2009 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %1471 = torch.aten.detach %1470 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %1472 = torch_c.to_builtin_tensor %1459 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_2010 = tensor.cast %1472 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %1473 = torch_c.to_builtin_tensor %1468 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %1474 = torch_c.to_builtin_tensor %1471 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %1475 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_2010, %1473, %1474) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_2011 = tensor.cast %1475 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %1476 = torch_c.from_builtin_tensor %cast_2011 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %1476, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %1477 = torch.aten.silu %1476 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %1477, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_2012 = torch.constant.int 2
    %1478 = torch.aten.view.dtype %47, %int2_2012 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %1479 = torch.aten.detach %1478 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_2013 = torch.constant.int -1
    %int17_2014 = torch.constant.int 17
    %1480 = torch.prim.ListConstruct %int-1_2013, %int17_2014 : (!torch.int, !torch.int) -> !torch.list<int>
    %1481 = torch.aten.view %1479, %1480 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_2015 = torch.constant.int 5632
    %int-1_2016 = torch.constant.int -1
    %int17_2017 = torch.constant.int 17
    %1482 = torch.prim.ListConstruct %int5632_2015, %int-1_2016, %int17_2017 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1483 = torch.aten.view %1481, %1482 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_2018 = torch.constant.int 2
    %int0_2019 = torch.constant.int 0
    %int1_2020 = torch.constant.int 1
    %int1_2021 = torch.constant.int 1
    %1484 = torch.aten.slice.Tensor %1483, %int2_2018, %int0_2019, %int1_2020, %int1_2021 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_2022 = torch.constant.int 5
    %1485 = torch.aten.view.dtype %1484, %int5_2022 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %1486 = torch.aten.detach %1485 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_2023 = torch.constant.int 2
    %int1_2024 = torch.constant.int 1
    %int9223372036854775807_2025 = torch.constant.int 9223372036854775807
    %int1_2026 = torch.constant.int 1
    %1487 = torch.aten.slice.Tensor %1483, %int2_2023, %int1_2024, %int9223372036854775807_2025, %int1_2026 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_2027 = torch.constant.int 1
    %1488 = torch.aten.view.dtype %1487, %int1_2027 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %1489 = torch.aten.detach %1488 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %1490 = torch_c.to_builtin_tensor %1459 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_2028 = tensor.cast %1490 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %1491 = torch_c.to_builtin_tensor %1486 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %1492 = torch_c.to_builtin_tensor %1489 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %1493 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_2028, %1491, %1492) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_2029 = tensor.cast %1493 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %1494 = torch_c.from_builtin_tensor %cast_2029 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %1494, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %1495 = torch.aten.mul.Tensor %1477, %1494 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %1495, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_2030 = torch.constant.int 2
    %1496 = torch.aten.view.dtype %48, %int2_2030 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %1497 = torch.aten.detach %1496 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_2031 = torch.constant.int -1
    %int17_2032 = torch.constant.int 17
    %1498 = torch.prim.ListConstruct %int-1_2031, %int17_2032 : (!torch.int, !torch.int) -> !torch.list<int>
    %1499 = torch.aten.view %1497, %1498 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_2033 = torch.constant.int 2048
    %int-1_2034 = torch.constant.int -1
    %int17_2035 = torch.constant.int 17
    %1500 = torch.prim.ListConstruct %int2048_2033, %int-1_2034, %int17_2035 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1501 = torch.aten.view %1499, %1500 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_2036 = torch.constant.int 2
    %int0_2037 = torch.constant.int 0
    %int1_2038 = torch.constant.int 1
    %int1_2039 = torch.constant.int 1
    %1502 = torch.aten.slice.Tensor %1501, %int2_2036, %int0_2037, %int1_2038, %int1_2039 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_2040 = torch.constant.int 5
    %1503 = torch.aten.view.dtype %1502, %int5_2040 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %1504 = torch.aten.detach %1503 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_2041 = torch.constant.int 2
    %int1_2042 = torch.constant.int 1
    %int9223372036854775807_2043 = torch.constant.int 9223372036854775807
    %int1_2044 = torch.constant.int 1
    %1505 = torch.aten.slice.Tensor %1501, %int2_2041, %int1_2042, %int9223372036854775807_2043, %int1_2044 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_2045 = torch.constant.int 1
    %1506 = torch.aten.view.dtype %1505, %int1_2045 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %1507 = torch.aten.detach %1506 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %1508 = torch_c.to_builtin_tensor %1495 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_2046 = tensor.cast %1508 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %1509 = torch_c.to_builtin_tensor %1504 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %1510 = torch_c.to_builtin_tensor %1507 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %1511 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_2046, %1509, %1510) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_2047 = tensor.cast %1511 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %1512 = torch_c.from_builtin_tensor %cast_2047 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1512, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_2048 = torch.constant.int 1
    %1513 = torch.aten.add.Tensor %1449, %1512, %int1_2048 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1513, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_2049 = torch.constant.none
    %none_2050 = torch.constant.none
    %int5_2051 = torch.constant.int 5
    %cpu_2052 = torch.constant.device "cpu"
    %int0_2053 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1513, %none_2049, %none_2050, %int5_2051, %cpu_2052, %int0_2053 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2054 = torch.constant.int 6
    %1514 = torch.prims.convert_element_type %1513, %int6_2054 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1514, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_2055 = torch.constant.int 2
    %1515 = torch.aten.pow.Tensor_Scalar %1514, %int2_2055 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1515, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_2056 = torch.constant.int -1
    %1516 = torch.prim.ListConstruct %int-1_2056 : (!torch.int) -> !torch.list<int>
    %true_2057 = torch.constant.bool true
    %none_2058 = torch.constant.none
    %1517 = torch.aten.mean.dim %1515, %1516, %true_2057, %none_2058 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1517, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_2059 = torch.constant.float 9.9999997473787516E-6
    %int1_2060 = torch.constant.int 1
    %1518 = torch.aten.add.Scalar %1517, %float9.999990e-06_2059, %int1_2060 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1518, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1519 = torch.aten.rsqrt %1518 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1519, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1520 = torch.aten.mul.Tensor %1514, %1519 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1520, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_2061 = torch.constant.none
    %none_2062 = torch.constant.none
    %int6_2063 = torch.constant.int 6
    %cpu_2064 = torch.constant.device "cpu"
    %int0_2065 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1520, %none_2061, %none_2062, %int6_2063, %cpu_2064, %int0_2065 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2066 = torch.constant.int 5
    %1521 = torch.prims.convert_element_type %1520, %int5_2066 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1521, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %1522 = torch.aten.mul.Tensor %52, %1521 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1522, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_2067 = torch.constant.none
    %none_2068 = torch.constant.none
    %int6_2069 = torch.constant.int 6
    %cpu_2070 = torch.constant.device "cpu"
    %int0_2071 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1522, %none_2067, %none_2068, %int6_2069, %cpu_2070, %int0_2071 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2072 = torch.constant.int 5
    %1523 = torch.prims.convert_element_type %1522, %int5_2072 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1523, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_2073 = torch.constant.int 2
    %1524 = torch.aten.view.dtype %53, %int2_2073 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %1525 = torch.aten.detach %1524 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_2074 = torch.constant.int -1
    %int17_2075 = torch.constant.int 17
    %1526 = torch.prim.ListConstruct %int-1_2074, %int17_2075 : (!torch.int, !torch.int) -> !torch.list<int>
    %1527 = torch.aten.view %1525, %1526 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_2076 = torch.constant.int 2048
    %int-1_2077 = torch.constant.int -1
    %int17_2078 = torch.constant.int 17
    %1528 = torch.prim.ListConstruct %int2048_2076, %int-1_2077, %int17_2078 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1529 = torch.aten.view %1527, %1528 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_2079 = torch.constant.int 2
    %int0_2080 = torch.constant.int 0
    %int1_2081 = torch.constant.int 1
    %int1_2082 = torch.constant.int 1
    %1530 = torch.aten.slice.Tensor %1529, %int2_2079, %int0_2080, %int1_2081, %int1_2082 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_2083 = torch.constant.int 5
    %1531 = torch.aten.view.dtype %1530, %int5_2083 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %1532 = torch.aten.detach %1531 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_2084 = torch.constant.int 2
    %int1_2085 = torch.constant.int 1
    %int9223372036854775807_2086 = torch.constant.int 9223372036854775807
    %int1_2087 = torch.constant.int 1
    %1533 = torch.aten.slice.Tensor %1529, %int2_2084, %int1_2085, %int9223372036854775807_2086, %int1_2087 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_2088 = torch.constant.int 1
    %1534 = torch.aten.view.dtype %1533, %int1_2088 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %1535 = torch.aten.detach %1534 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %1536 = torch_c.to_builtin_tensor %1523 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_2089 = tensor.cast %1536 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %1537 = torch_c.to_builtin_tensor %1532 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %1538 = torch_c.to_builtin_tensor %1535 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %1539 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_2089, %1537, %1538) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_2090 = tensor.cast %1539 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %1540 = torch_c.from_builtin_tensor %cast_2090 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1540, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_2091 = torch.constant.int 2
    %1541 = torch.aten.view.dtype %54, %int2_2091 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %1542 = torch.aten.detach %1541 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_2092 = torch.constant.int -1
    %int17_2093 = torch.constant.int 17
    %1543 = torch.prim.ListConstruct %int-1_2092, %int17_2093 : (!torch.int, !torch.int) -> !torch.list<int>
    %1544 = torch.aten.view %1542, %1543 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_2094 = torch.constant.int 256
    %int-1_2095 = torch.constant.int -1
    %int17_2096 = torch.constant.int 17
    %1545 = torch.prim.ListConstruct %int256_2094, %int-1_2095, %int17_2096 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1546 = torch.aten.view %1544, %1545 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_2097 = torch.constant.int 2
    %int0_2098 = torch.constant.int 0
    %int1_2099 = torch.constant.int 1
    %int1_2100 = torch.constant.int 1
    %1547 = torch.aten.slice.Tensor %1546, %int2_2097, %int0_2098, %int1_2099, %int1_2100 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_2101 = torch.constant.int 5
    %1548 = torch.aten.view.dtype %1547, %int5_2101 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %1549 = torch.aten.detach %1548 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_2102 = torch.constant.int 2
    %int1_2103 = torch.constant.int 1
    %int9223372036854775807_2104 = torch.constant.int 9223372036854775807
    %int1_2105 = torch.constant.int 1
    %1550 = torch.aten.slice.Tensor %1546, %int2_2102, %int1_2103, %int9223372036854775807_2104, %int1_2105 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_2106 = torch.constant.int 1
    %1551 = torch.aten.view.dtype %1550, %int1_2106 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %1552 = torch.aten.detach %1551 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %1553 = torch_c.to_builtin_tensor %1523 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_2107 = tensor.cast %1553 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %1554 = torch_c.to_builtin_tensor %1549 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %1555 = torch_c.to_builtin_tensor %1552 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %1556 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_2107, %1554, %1555) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_2108 = tensor.cast %1556 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %1557 = torch_c.from_builtin_tensor %cast_2108 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %1557, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_2109 = torch.constant.int 2
    %1558 = torch.aten.view.dtype %55, %int2_2109 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %1559 = torch.aten.detach %1558 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_2110 = torch.constant.int -1
    %int17_2111 = torch.constant.int 17
    %1560 = torch.prim.ListConstruct %int-1_2110, %int17_2111 : (!torch.int, !torch.int) -> !torch.list<int>
    %1561 = torch.aten.view %1559, %1560 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_2112 = torch.constant.int 256
    %int-1_2113 = torch.constant.int -1
    %int17_2114 = torch.constant.int 17
    %1562 = torch.prim.ListConstruct %int256_2112, %int-1_2113, %int17_2114 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1563 = torch.aten.view %1561, %1562 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_2115 = torch.constant.int 2
    %int0_2116 = torch.constant.int 0
    %int1_2117 = torch.constant.int 1
    %int1_2118 = torch.constant.int 1
    %1564 = torch.aten.slice.Tensor %1563, %int2_2115, %int0_2116, %int1_2117, %int1_2118 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_2119 = torch.constant.int 5
    %1565 = torch.aten.view.dtype %1564, %int5_2119 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %1566 = torch.aten.detach %1565 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_2120 = torch.constant.int 2
    %int1_2121 = torch.constant.int 1
    %int9223372036854775807_2122 = torch.constant.int 9223372036854775807
    %int1_2123 = torch.constant.int 1
    %1567 = torch.aten.slice.Tensor %1563, %int2_2120, %int1_2121, %int9223372036854775807_2122, %int1_2123 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_2124 = torch.constant.int 1
    %1568 = torch.aten.view.dtype %1567, %int1_2124 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %1569 = torch.aten.detach %1568 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %1570 = torch_c.to_builtin_tensor %1523 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_2125 = tensor.cast %1570 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %1571 = torch_c.to_builtin_tensor %1566 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %1572 = torch_c.to_builtin_tensor %1569 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %1573 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_2125, %1571, %1572) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_2126 = tensor.cast %1573 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %1574 = torch_c.from_builtin_tensor %cast_2126 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %1574, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_2127 = torch.constant.int 4
    %int32_2128 = torch.constant.int 32
    %int64_2129 = torch.constant.int 64
    %1575 = torch.prim.ListConstruct %int4_2127, %273, %int32_2128, %int64_2129 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1576 = torch.aten.view %1540, %1575 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1576, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_2130 = torch.constant.int 4
    %int4_2131 = torch.constant.int 4
    %int64_2132 = torch.constant.int 64
    %1577 = torch.prim.ListConstruct %int4_2130, %273, %int4_2131, %int64_2132 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1578 = torch.aten.view %1557, %1577 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %1578, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_2133 = torch.constant.int 4
    %int4_2134 = torch.constant.int 4
    %int64_2135 = torch.constant.int 64
    %1579 = torch.prim.ListConstruct %int4_2133, %273, %int4_2134, %int64_2135 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1580 = torch.aten.view %1574, %1579 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %1580, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_2136 = torch.constant.int 0
    %none_2137 = torch.constant.none
    %none_2138 = torch.constant.none
    %cpu_2139 = torch.constant.device "cpu"
    %false_2140 = torch.constant.bool false
    %1581 = torch.aten.arange.start %int0_2136, %273, %none_2137, %none_2138, %cpu_2139, %false_2140 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1581, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_2141 = torch.constant.int 0
    %1582 = torch.aten.unsqueeze %1581, %int0_2141 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %1582, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_2142 = torch.constant.int 0
    %int64_2143 = torch.constant.int 64
    %int2_2144 = torch.constant.int 2
    %none_2145 = torch.constant.none
    %none_2146 = torch.constant.none
    %cpu_2147 = torch.constant.device "cpu"
    %false_2148 = torch.constant.bool false
    %1583 = torch.aten.arange.start_step %int0_2142, %int64_2143, %int2_2144, %none_2145, %none_2146, %cpu_2147, %false_2148 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_2149 = torch.constant.none
    %none_2150 = torch.constant.none
    %int4_2151 = torch.constant.int 4
    %cpu_2152 = torch.constant.device "cpu"
    %int0_2153 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1583, %none_2149, %none_2150, %int4_2151, %cpu_2152, %int0_2153 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2154 = torch.constant.int 6
    %1584 = torch.prims.convert_element_type %1583, %int6_2154 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_2155 = torch.constant.int 64
    %1585 = torch.aten.div.Scalar %1584, %int64_2155 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_2156 = torch.constant.float 1.000000e+04
    %1586 = torch.aten.pow.Scalar %float1.000000e04_2156, %1585 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %1587 = torch.aten.reciprocal %1586 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_2157 = torch.constant.float 1.000000e+00
    %1588 = torch.aten.mul.Scalar %1587, %float1.000000e00_2157 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_2158 = torch.constant.none
    %1589 = torch.aten.clone %49, %none_2158 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_2159 = torch.constant.int 0
    %1590 = torch.aten.unsqueeze %1588, %int0_2159 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_2160 = torch.constant.int 2
    %1591 = torch.aten.unsqueeze %1590, %int2_2160 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_2161 = torch.constant.none
    %none_2162 = torch.constant.none
    %int6_2163 = torch.constant.int 6
    %cpu_2164 = torch.constant.device "cpu"
    %int0_2165 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1591, %none_2161, %none_2162, %int6_2163, %cpu_2164, %int0_2165 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_2166 = torch.constant.int 1
    %int-1_2167 = torch.constant.int -1
    %int1_2168 = torch.constant.int 1
    %1592 = torch.prim.ListConstruct %int1_2166, %int-1_2167, %int1_2168 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2169 = torch.constant.bool false
    %1593 = torch.aten.expand %1591, %1592, %false_2169 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_2170 = torch.constant.int 1
    %1594 = torch.aten.unsqueeze %1582, %int1_2170 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %1594, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_2171 = torch.constant.none
    %none_2172 = torch.constant.none
    %int4_2173 = torch.constant.int 4
    %cpu_2174 = torch.constant.device "cpu"
    %int0_2175 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1594, %none_2171, %none_2172, %int4_2173, %cpu_2174, %int0_2175 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2176 = torch.constant.int 6
    %1595 = torch.prims.convert_element_type %1594, %int6_2176 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %1595, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %1596 = torch.aten.matmul %1593, %1595 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %1596, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_2177 = torch.constant.int 1
    %int2_2178 = torch.constant.int 2
    %1597 = torch.aten.transpose.int %1596, %int1_2177, %int2_2178 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1597, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1598 = torch.aten.cos %1597 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1598, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1599 = torch.aten.mul.Tensor %1598, %1589 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1599, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_2179 = torch.constant.none
    %none_2180 = torch.constant.none
    %int6_2181 = torch.constant.int 6
    %cpu_2182 = torch.constant.device "cpu"
    %int0_2183 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1599, %none_2179, %none_2180, %int6_2181, %cpu_2182, %int0_2183 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2184 = torch.constant.int 5
    %1600 = torch.prims.convert_element_type %1599, %int5_2184 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %1600, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %1601 = torch.aten.sin %1597 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1601, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1602 = torch.aten.mul.Tensor %1601, %1589 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1602, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_2185 = torch.constant.none
    %none_2186 = torch.constant.none
    %int6_2187 = torch.constant.int 6
    %cpu_2188 = torch.constant.device "cpu"
    %int0_2189 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1602, %none_2185, %none_2186, %int6_2187, %cpu_2188, %int0_2189 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2190 = torch.constant.int 5
    %1603 = torch.prims.convert_element_type %1602, %int5_2190 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %1603, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_2191 = torch.constant.int 2
    %1604 = torch.aten.unsqueeze %1600, %int2_2191 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %1604, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_2192 = torch.constant.int 2
    %1605 = torch.aten.unsqueeze %1603, %int2_2192 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %1605, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_2193 = torch.constant.none
    %none_2194 = torch.constant.none
    %int5_2195 = torch.constant.int 5
    %cpu_2196 = torch.constant.device "cpu"
    %int0_2197 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1604, %none_2193, %none_2194, %int5_2195, %cpu_2196, %int0_2197 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_2198 = torch.constant.none
    %none_2199 = torch.constant.none
    %int5_2200 = torch.constant.int 5
    %cpu_2201 = torch.constant.device "cpu"
    %int0_2202 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1605, %none_2198, %none_2199, %int5_2200, %cpu_2201, %int0_2202 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_2203 = torch.constant.none
    %none_2204 = torch.constant.none
    %int5_2205 = torch.constant.int 5
    %cpu_2206 = torch.constant.device "cpu"
    %int0_2207 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1576, %none_2203, %none_2204, %int5_2205, %cpu_2206, %int0_2207 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_2208 = torch.constant.int 3
    %int0_2209 = torch.constant.int 0
    %int64_2210 = torch.constant.int 64
    %int2_2211 = torch.constant.int 2
    %1606 = torch.aten.slice.Tensor %1576, %int3_2208, %int0_2209, %int64_2210, %int2_2211 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1606, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_2212 = torch.constant.int 3
    %int1_2213 = torch.constant.int 1
    %int64_2214 = torch.constant.int 64
    %int2_2215 = torch.constant.int 2
    %1607 = torch.aten.slice.Tensor %1576, %int3_2212, %int1_2213, %int64_2214, %int2_2215 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1607, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %1608 = torch.aten.mul.Tensor %1606, %1604 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1608, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %1609 = torch.aten.mul.Tensor %1607, %1605 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1609, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_2216 = torch.constant.int 1
    %1610 = torch.aten.sub.Tensor %1608, %1609, %int1_2216 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1610, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %1611 = torch.aten.mul.Tensor %1607, %1604 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1611, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %1612 = torch.aten.mul.Tensor %1606, %1605 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1612, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_2217 = torch.constant.int 1
    %1613 = torch.aten.add.Tensor %1611, %1612, %int1_2217 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1613, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %1614 = torch_c.to_builtin_tensor %1610 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_2218 = tensor.cast %1614 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %1615 = torch_c.to_builtin_tensor %1613 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_2219 = tensor.cast %1615 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %1616 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_2218, %cast_2219) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_2220 = tensor.cast %1616 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %1617 = torch_c.from_builtin_tensor %cast_2220 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %1617, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_2221 = torch.constant.int 4
    %int32_2222 = torch.constant.int 32
    %int64_2223 = torch.constant.int 64
    %1618 = torch.prim.ListConstruct %int4_2221, %273, %int32_2222, %int64_2223 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1619 = torch.aten.view %1617, %1618 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1619, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_2224 = torch.constant.none
    %none_2225 = torch.constant.none
    %int5_2226 = torch.constant.int 5
    %cpu_2227 = torch.constant.device "cpu"
    %int0_2228 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1619, %none_2224, %none_2225, %int5_2226, %cpu_2227, %int0_2228 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_2229 = torch.constant.int 0
    %none_2230 = torch.constant.none
    %none_2231 = torch.constant.none
    %cpu_2232 = torch.constant.device "cpu"
    %false_2233 = torch.constant.bool false
    %1620 = torch.aten.arange.start %int0_2229, %273, %none_2230, %none_2231, %cpu_2232, %false_2233 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1620, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_2234 = torch.constant.int 0
    %1621 = torch.aten.unsqueeze %1620, %int0_2234 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %1621, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_2235 = torch.constant.int 0
    %int64_2236 = torch.constant.int 64
    %int2_2237 = torch.constant.int 2
    %none_2238 = torch.constant.none
    %none_2239 = torch.constant.none
    %cpu_2240 = torch.constant.device "cpu"
    %false_2241 = torch.constant.bool false
    %1622 = torch.aten.arange.start_step %int0_2235, %int64_2236, %int2_2237, %none_2238, %none_2239, %cpu_2240, %false_2241 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_2242 = torch.constant.none
    %none_2243 = torch.constant.none
    %int4_2244 = torch.constant.int 4
    %cpu_2245 = torch.constant.device "cpu"
    %int0_2246 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1622, %none_2242, %none_2243, %int4_2244, %cpu_2245, %int0_2246 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2247 = torch.constant.int 6
    %1623 = torch.prims.convert_element_type %1622, %int6_2247 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_2248 = torch.constant.int 64
    %1624 = torch.aten.div.Scalar %1623, %int64_2248 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_2249 = torch.constant.float 1.000000e+04
    %1625 = torch.aten.pow.Scalar %float1.000000e04_2249, %1624 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %1626 = torch.aten.reciprocal %1625 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_2250 = torch.constant.float 1.000000e+00
    %1627 = torch.aten.mul.Scalar %1626, %float1.000000e00_2250 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_2251 = torch.constant.none
    %1628 = torch.aten.clone %50, %none_2251 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_2252 = torch.constant.int 0
    %1629 = torch.aten.unsqueeze %1627, %int0_2252 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_2253 = torch.constant.int 2
    %1630 = torch.aten.unsqueeze %1629, %int2_2253 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_2254 = torch.constant.none
    %none_2255 = torch.constant.none
    %int6_2256 = torch.constant.int 6
    %cpu_2257 = torch.constant.device "cpu"
    %int0_2258 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1630, %none_2254, %none_2255, %int6_2256, %cpu_2257, %int0_2258 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_2259 = torch.constant.int 1
    %int-1_2260 = torch.constant.int -1
    %int1_2261 = torch.constant.int 1
    %1631 = torch.prim.ListConstruct %int1_2259, %int-1_2260, %int1_2261 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2262 = torch.constant.bool false
    %1632 = torch.aten.expand %1630, %1631, %false_2262 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_2263 = torch.constant.int 1
    %1633 = torch.aten.unsqueeze %1621, %int1_2263 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %1633, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_2264 = torch.constant.none
    %none_2265 = torch.constant.none
    %int4_2266 = torch.constant.int 4
    %cpu_2267 = torch.constant.device "cpu"
    %int0_2268 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1633, %none_2264, %none_2265, %int4_2266, %cpu_2267, %int0_2268 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2269 = torch.constant.int 6
    %1634 = torch.prims.convert_element_type %1633, %int6_2269 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %1634, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %1635 = torch.aten.matmul %1632, %1634 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %1635, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_2270 = torch.constant.int 1
    %int2_2271 = torch.constant.int 2
    %1636 = torch.aten.transpose.int %1635, %int1_2270, %int2_2271 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1636, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1637 = torch.aten.cos %1636 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1637, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1638 = torch.aten.mul.Tensor %1637, %1628 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1638, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_2272 = torch.constant.none
    %none_2273 = torch.constant.none
    %int6_2274 = torch.constant.int 6
    %cpu_2275 = torch.constant.device "cpu"
    %int0_2276 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1638, %none_2272, %none_2273, %int6_2274, %cpu_2275, %int0_2276 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2277 = torch.constant.int 5
    %1639 = torch.prims.convert_element_type %1638, %int5_2277 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %1639, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %1640 = torch.aten.sin %1636 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1640, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1641 = torch.aten.mul.Tensor %1640, %1628 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1641, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_2278 = torch.constant.none
    %none_2279 = torch.constant.none
    %int6_2280 = torch.constant.int 6
    %cpu_2281 = torch.constant.device "cpu"
    %int0_2282 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1641, %none_2278, %none_2279, %int6_2280, %cpu_2281, %int0_2282 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2283 = torch.constant.int 5
    %1642 = torch.prims.convert_element_type %1641, %int5_2283 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %1642, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_2284 = torch.constant.int 2
    %1643 = torch.aten.unsqueeze %1639, %int2_2284 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %1643, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_2285 = torch.constant.int 2
    %1644 = torch.aten.unsqueeze %1642, %int2_2285 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %1644, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_2286 = torch.constant.none
    %none_2287 = torch.constant.none
    %int5_2288 = torch.constant.int 5
    %cpu_2289 = torch.constant.device "cpu"
    %int0_2290 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1643, %none_2286, %none_2287, %int5_2288, %cpu_2289, %int0_2290 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_2291 = torch.constant.none
    %none_2292 = torch.constant.none
    %int5_2293 = torch.constant.int 5
    %cpu_2294 = torch.constant.device "cpu"
    %int0_2295 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1644, %none_2291, %none_2292, %int5_2293, %cpu_2294, %int0_2295 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_2296 = torch.constant.none
    %none_2297 = torch.constant.none
    %int5_2298 = torch.constant.int 5
    %cpu_2299 = torch.constant.device "cpu"
    %int0_2300 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1578, %none_2296, %none_2297, %int5_2298, %cpu_2299, %int0_2300 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_2301 = torch.constant.int 3
    %int0_2302 = torch.constant.int 0
    %int64_2303 = torch.constant.int 64
    %int2_2304 = torch.constant.int 2
    %1645 = torch.aten.slice.Tensor %1578, %int3_2301, %int0_2302, %int64_2303, %int2_2304 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1645, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_2305 = torch.constant.int 3
    %int1_2306 = torch.constant.int 1
    %int64_2307 = torch.constant.int 64
    %int2_2308 = torch.constant.int 2
    %1646 = torch.aten.slice.Tensor %1578, %int3_2305, %int1_2306, %int64_2307, %int2_2308 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1646, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1647 = torch.aten.mul.Tensor %1645, %1643 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1647, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1648 = torch.aten.mul.Tensor %1646, %1644 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1648, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_2309 = torch.constant.int 1
    %1649 = torch.aten.sub.Tensor %1647, %1648, %int1_2309 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1649, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1650 = torch.aten.mul.Tensor %1646, %1643 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1650, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1651 = torch.aten.mul.Tensor %1645, %1644 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1651, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_2310 = torch.constant.int 1
    %1652 = torch.aten.add.Tensor %1650, %1651, %int1_2310 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1652, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1653 = torch_c.to_builtin_tensor %1649 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_2311 = tensor.cast %1653 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %1654 = torch_c.to_builtin_tensor %1652 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_2312 = tensor.cast %1654 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %1655 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_2311, %cast_2312) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_2313 = tensor.cast %1655 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %1656 = torch_c.from_builtin_tensor %cast_2313 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %1656, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_2314 = torch.constant.int 4
    %int4_2315 = torch.constant.int 4
    %int64_2316 = torch.constant.int 64
    %1657 = torch.prim.ListConstruct %int4_2314, %273, %int4_2315, %int64_2316 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1658 = torch.aten.view %1656, %1657 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %1658, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_2317 = torch.constant.none
    %none_2318 = torch.constant.none
    %int5_2319 = torch.constant.int 5
    %cpu_2320 = torch.constant.device "cpu"
    %int0_2321 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1658, %none_2317, %none_2318, %int5_2319, %cpu_2320, %int0_2321 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_2322 = torch.constant.int 22
    %1659 = torch.aten.mul.Scalar %arg2, %int22_2322 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1659, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_2323 = torch.constant.int 4
    %int1_2324 = torch.constant.int 1
    %1660 = torch.aten.add.Scalar %1659, %int4_2323, %int1_2324 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1660, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_2325 = torch.constant.int 2
    %1661 = torch.aten.mul.Scalar %1660, %int2_2325 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1661, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_2326 = torch.constant.int 0
    %int1_2327 = torch.constant.int 1
    %1662 = torch.aten.add.Scalar %1661, %int0_2326, %int1_2327 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1662, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %1663 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %1664 = torch.aten.view %1662, %1663 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1664, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_2328 = torch.constant.int 4
    %int32_2329 = torch.constant.int 32
    %int4_2330 = torch.constant.int 4
    %int64_2331 = torch.constant.int 64
    %1665 = torch.prim.ListConstruct %int4_2328, %271, %int32_2329, %int4_2330, %int64_2331 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1666 = torch.aten.view %1658, %1665 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1666, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_2332 = torch.constant.int 32
    %int4_2333 = torch.constant.int 4
    %int64_2334 = torch.constant.int 64
    %1667 = torch.prim.ListConstruct %446, %int32_2332, %int4_2333, %int64_2334 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1668 = torch.aten.view %1666, %1667 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %1668, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_2335 = torch.constant.int 1
    %int2_2336 = torch.constant.int 2
    %1669 = torch.aten.transpose.int %1668, %int1_2335, %int2_2336 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1669, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_2337 = torch.constant.none
    %none_2338 = torch.constant.none
    %int5_2339 = torch.constant.int 5
    %cpu_2340 = torch.constant.device "cpu"
    %int0_2341 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1669, %none_2337, %none_2338, %int5_2339, %cpu_2340, %int0_2341 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_2342 = torch.constant.int 22
    %int2_2343 = torch.constant.int 2
    %int4_2344 = torch.constant.int 4
    %int32_2345 = torch.constant.int 32
    %int64_2346 = torch.constant.int 64
    %1670 = torch.prim.ListConstruct %272, %int22_2342, %int2_2343, %int4_2344, %int32_2345, %int64_2346 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1671 = torch.aten.view %1395, %1670 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1671, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_2347 = torch.constant.int 4
    %int32_2348 = torch.constant.int 32
    %int64_2349 = torch.constant.int 64
    %1672 = torch.prim.ListConstruct %439, %int4_2347, %int32_2348, %int64_2349 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1673 = torch.aten.view %1671, %1672 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1673, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %1674 = torch.prim.ListConstruct %1664 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_2350 = torch.constant.bool false
    %1675 = torch.aten.index_put %1673, %1674, %1669, %false_2350 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1675, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_2351 = torch.constant.int 22
    %int2_2352 = torch.constant.int 2
    %int4_2353 = torch.constant.int 4
    %int32_2354 = torch.constant.int 32
    %int64_2355 = torch.constant.int 64
    %1676 = torch.prim.ListConstruct %272, %int22_2351, %int2_2352, %int4_2353, %int32_2354, %int64_2355 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1677 = torch.aten.view %1675, %1676 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1677, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_2356 = torch.constant.int 360448
    %1678 = torch.prim.ListConstruct %272, %int360448_2356 : (!torch.int, !torch.int) -> !torch.list<int>
    %1679 = torch.aten.view %1677, %1678 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %1679, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_2357 = torch.constant.int 22
    %int2_2358 = torch.constant.int 2
    %int4_2359 = torch.constant.int 4
    %int32_2360 = torch.constant.int 32
    %int64_2361 = torch.constant.int 64
    %1680 = torch.prim.ListConstruct %272, %int22_2357, %int2_2358, %int4_2359, %int32_2360, %int64_2361 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1681 = torch.aten.view %1679, %1680 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1681, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_2362 = torch.constant.int 4
    %int32_2363 = torch.constant.int 32
    %int64_2364 = torch.constant.int 64
    %1682 = torch.prim.ListConstruct %439, %int4_2362, %int32_2363, %int64_2364 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1683 = torch.aten.view %1681, %1682 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1683, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_2365 = torch.constant.int 22
    %1684 = torch.aten.mul.Scalar %arg2, %int22_2365 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1684, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_2366 = torch.constant.int 4
    %int1_2367 = torch.constant.int 1
    %1685 = torch.aten.add.Scalar %1684, %int4_2366, %int1_2367 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1685, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_2368 = torch.constant.int 2
    %1686 = torch.aten.mul.Scalar %1685, %int2_2368 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1686, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_2369 = torch.constant.int 1
    %int1_2370 = torch.constant.int 1
    %1687 = torch.aten.add.Scalar %1686, %int1_2369, %int1_2370 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1687, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %1688 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %1689 = torch.aten.view %1687, %1688 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1689, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_2371 = torch.constant.int 4
    %int32_2372 = torch.constant.int 32
    %int4_2373 = torch.constant.int 4
    %int64_2374 = torch.constant.int 64
    %1690 = torch.prim.ListConstruct %int4_2371, %271, %int32_2372, %int4_2373, %int64_2374 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1691 = torch.aten.view %1580, %1690 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1691, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_2375 = torch.constant.int 32
    %int4_2376 = torch.constant.int 4
    %int64_2377 = torch.constant.int 64
    %1692 = torch.prim.ListConstruct %446, %int32_2375, %int4_2376, %int64_2377 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1693 = torch.aten.view %1691, %1692 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %1693, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_2378 = torch.constant.int 1
    %int2_2379 = torch.constant.int 2
    %1694 = torch.aten.transpose.int %1693, %int1_2378, %int2_2379 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1694, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_2380 = torch.constant.none
    %none_2381 = torch.constant.none
    %int5_2382 = torch.constant.int 5
    %cpu_2383 = torch.constant.device "cpu"
    %int0_2384 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1694, %none_2380, %none_2381, %int5_2382, %cpu_2383, %int0_2384 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %1695 = torch.prim.ListConstruct %1689 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_2385 = torch.constant.bool false
    %1696 = torch.aten.index_put %1683, %1695, %1694, %false_2385 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1696, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_2386 = torch.constant.int 22
    %int2_2387 = torch.constant.int 2
    %int4_2388 = torch.constant.int 4
    %int32_2389 = torch.constant.int 32
    %int64_2390 = torch.constant.int 64
    %1697 = torch.prim.ListConstruct %272, %int22_2386, %int2_2387, %int4_2388, %int32_2389, %int64_2390 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1698 = torch.aten.view %1696, %1697 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1698, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_2391 = torch.constant.int 360448
    %1699 = torch.prim.ListConstruct %272, %int360448_2391 : (!torch.int, !torch.int) -> !torch.list<int>
    %1700 = torch.aten.view %1698, %1699 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %1700, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_2392 = torch.constant.int 0
    %int1_2393 = torch.constant.int 1
    %none_2394 = torch.constant.none
    %none_2395 = torch.constant.none
    %cpu_2396 = torch.constant.device "cpu"
    %false_2397 = torch.constant.bool false
    %1701 = torch.aten.arange.start_step %int0_2392, %273, %int1_2393, %none_2394, %none_2395, %cpu_2396, %false_2397 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1701, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_2398 = torch.constant.int -1
    %1702 = torch.aten.unsqueeze %arg1, %int-1_2398 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %1703 = torch.aten.ge.Tensor %1701, %1702 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %1703, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_2399 = torch.constant.none
    %none_2400 = torch.constant.none
    %cpu_2401 = torch.constant.device "cpu"
    %false_2402 = torch.constant.bool false
    %1704 = torch.aten.arange %273, %none_2399, %none_2400, %cpu_2401, %false_2402 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1704, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_2403 = torch.constant.int 0
    %1705 = torch.aten.unsqueeze %1704, %int0_2403 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %1705, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_2404 = torch.constant.int 1
    %1706 = torch.aten.unsqueeze %1705, %int1_2404 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %1706, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_2405 = torch.constant.int 2
    %1707 = torch.aten.unsqueeze %1706, %int2_2405 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %1707, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_2406 = torch.constant.none
    %none_2407 = torch.constant.none
    %cpu_2408 = torch.constant.device "cpu"
    %false_2409 = torch.constant.bool false
    %1708 = torch.aten.arange %273, %none_2406, %none_2407, %cpu_2408, %false_2409 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1708, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_2410 = torch.constant.int 0
    %1709 = torch.aten.unsqueeze %1708, %int0_2410 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %1709, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_2411 = torch.constant.int 1
    %1710 = torch.aten.unsqueeze %1709, %int1_2411 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %1710, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_2412 = torch.constant.int 3
    %1711 = torch.aten.unsqueeze %1710, %int3_2412 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %1711, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %1712 = torch.aten.gt.Tensor %1707, %1711 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %1712, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_2413 = torch.constant.int 1
    %1713 = torch.aten.unsqueeze %1703, %int1_2413 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %1713, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_2414 = torch.constant.int 2
    %1714 = torch.aten.unsqueeze %1713, %int2_2414 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %1714, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %1715 = torch.aten.logical_or %1712, %1714 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %1715, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_2415 = torch.constant.none
    %1716 = torch.aten.clone %51, %none_2415 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_2416 = torch.constant.int 0
    %1717 = torch.aten.where.ScalarOther %1715, %1716, %int0_2416 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %1717, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_2417 = torch.constant.none
    %none_2418 = torch.constant.none
    %int5_2419 = torch.constant.int 5
    %cpu_2420 = torch.constant.device "cpu"
    %int0_2421 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1717, %none_2417, %none_2418, %int5_2419, %cpu_2420, %int0_2421 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_2422 = torch.constant.int -2
    %1718 = torch.aten.unsqueeze %1658, %int-2_2422 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %1718, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_2423 = torch.constant.int 4
    %int4_2424 = torch.constant.int 4
    %int8_2425 = torch.constant.int 8
    %int64_2426 = torch.constant.int 64
    %1719 = torch.prim.ListConstruct %int4_2423, %273, %int4_2424, %int8_2425, %int64_2426 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2427 = torch.constant.bool false
    %1720 = torch.aten.expand %1718, %1719, %false_2427 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1720, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_2428 = torch.constant.int 0
    %1721 = torch.aten.clone %1720, %int0_2428 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1721, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_2429 = torch.constant.int 4
    %int32_2430 = torch.constant.int 32
    %int64_2431 = torch.constant.int 64
    %1722 = torch.prim.ListConstruct %int4_2429, %273, %int32_2430, %int64_2431 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1723 = torch.aten._unsafe_view %1721, %1722 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1723, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_2432 = torch.constant.int -2
    %1724 = torch.aten.unsqueeze %1580, %int-2_2432 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %1724, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_2433 = torch.constant.int 4
    %int4_2434 = torch.constant.int 4
    %int8_2435 = torch.constant.int 8
    %int64_2436 = torch.constant.int 64
    %1725 = torch.prim.ListConstruct %int4_2433, %273, %int4_2434, %int8_2435, %int64_2436 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2437 = torch.constant.bool false
    %1726 = torch.aten.expand %1724, %1725, %false_2437 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1726, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_2438 = torch.constant.int 0
    %1727 = torch.aten.clone %1726, %int0_2438 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1727, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_2439 = torch.constant.int 4
    %int32_2440 = torch.constant.int 32
    %int64_2441 = torch.constant.int 64
    %1728 = torch.prim.ListConstruct %int4_2439, %273, %int32_2440, %int64_2441 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1729 = torch.aten._unsafe_view %1727, %1728 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1729, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_2442 = torch.constant.int 1
    %int2_2443 = torch.constant.int 2
    %1730 = torch.aten.transpose.int %1619, %int1_2442, %int2_2443 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %1730, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_2444 = torch.constant.int 1
    %int2_2445 = torch.constant.int 2
    %1731 = torch.aten.transpose.int %1723, %int1_2444, %int2_2445 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %1731, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_2446 = torch.constant.int 1
    %int2_2447 = torch.constant.int 2
    %1732 = torch.aten.transpose.int %1729, %int1_2446, %int2_2447 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %1732, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_2448 = torch.constant.float 0.000000e+00
    %false_2449 = torch.constant.bool false
    %none_2450 = torch.constant.none
    %false_2451 = torch.constant.bool false
    %1733 = torch.aten.scaled_dot_product_attention %1730, %1731, %1732, %1717, %float0.000000e00_2448, %false_2449, %none_2450, %false_2451 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %1733, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_2452 = torch.constant.int 1
    %int2_2453 = torch.constant.int 2
    %1734 = torch.aten.transpose.int %1733, %int1_2452, %int2_2453 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1734, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_2454 = torch.constant.int 4
    %int2048_2455 = torch.constant.int 2048
    %1735 = torch.prim.ListConstruct %int4_2454, %273, %int2048_2455 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1736 = torch.aten.view %1734, %1735 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1736, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_2456 = torch.constant.int 2
    %1737 = torch.aten.view.dtype %56, %int2_2456 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %1738 = torch.aten.detach %1737 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_2457 = torch.constant.int -1
    %int17_2458 = torch.constant.int 17
    %1739 = torch.prim.ListConstruct %int-1_2457, %int17_2458 : (!torch.int, !torch.int) -> !torch.list<int>
    %1740 = torch.aten.view %1738, %1739 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_2459 = torch.constant.int 2048
    %int-1_2460 = torch.constant.int -1
    %int17_2461 = torch.constant.int 17
    %1741 = torch.prim.ListConstruct %int2048_2459, %int-1_2460, %int17_2461 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1742 = torch.aten.view %1740, %1741 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_2462 = torch.constant.int 2
    %int0_2463 = torch.constant.int 0
    %int1_2464 = torch.constant.int 1
    %int1_2465 = torch.constant.int 1
    %1743 = torch.aten.slice.Tensor %1742, %int2_2462, %int0_2463, %int1_2464, %int1_2465 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_2466 = torch.constant.int 5
    %1744 = torch.aten.view.dtype %1743, %int5_2466 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %1745 = torch.aten.detach %1744 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_2467 = torch.constant.int 2
    %int1_2468 = torch.constant.int 1
    %int9223372036854775807_2469 = torch.constant.int 9223372036854775807
    %int1_2470 = torch.constant.int 1
    %1746 = torch.aten.slice.Tensor %1742, %int2_2467, %int1_2468, %int9223372036854775807_2469, %int1_2470 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_2471 = torch.constant.int 1
    %1747 = torch.aten.view.dtype %1746, %int1_2471 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %1748 = torch.aten.detach %1747 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %1749 = torch_c.to_builtin_tensor %1736 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_2472 = tensor.cast %1749 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %1750 = torch_c.to_builtin_tensor %1745 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %1751 = torch_c.to_builtin_tensor %1748 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %1752 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_2472, %1750, %1751) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_2473 = tensor.cast %1752 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %1753 = torch_c.from_builtin_tensor %cast_2473 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1753, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_2474 = torch.constant.none
    %none_2475 = torch.constant.none
    %int5_2476 = torch.constant.int 5
    %cpu_2477 = torch.constant.device "cpu"
    %int0_2478 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1753, %none_2474, %none_2475, %int5_2476, %cpu_2477, %int0_2478 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_2479 = torch.constant.int 1
    %1754 = torch.aten.add.Tensor %1513, %1753, %int1_2479 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1754, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_2480 = torch.constant.none
    %none_2481 = torch.constant.none
    %int5_2482 = torch.constant.int 5
    %cpu_2483 = torch.constant.device "cpu"
    %int0_2484 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1754, %none_2480, %none_2481, %int5_2482, %cpu_2483, %int0_2484 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2485 = torch.constant.int 6
    %1755 = torch.prims.convert_element_type %1754, %int6_2485 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1755, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_2486 = torch.constant.int 2
    %1756 = torch.aten.pow.Tensor_Scalar %1755, %int2_2486 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1756, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_2487 = torch.constant.int -1
    %1757 = torch.prim.ListConstruct %int-1_2487 : (!torch.int) -> !torch.list<int>
    %true_2488 = torch.constant.bool true
    %none_2489 = torch.constant.none
    %1758 = torch.aten.mean.dim %1756, %1757, %true_2488, %none_2489 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1758, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_2490 = torch.constant.float 9.9999997473787516E-6
    %int1_2491 = torch.constant.int 1
    %1759 = torch.aten.add.Scalar %1758, %float9.999990e-06_2490, %int1_2491 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1759, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1760 = torch.aten.rsqrt %1759 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1760, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1761 = torch.aten.mul.Tensor %1755, %1760 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1761, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_2492 = torch.constant.none
    %none_2493 = torch.constant.none
    %int6_2494 = torch.constant.int 6
    %cpu_2495 = torch.constant.device "cpu"
    %int0_2496 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1761, %none_2492, %none_2493, %int6_2494, %cpu_2495, %int0_2496 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2497 = torch.constant.int 5
    %1762 = torch.prims.convert_element_type %1761, %int5_2497 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1762, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %1763 = torch.aten.mul.Tensor %57, %1762 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1763, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_2498 = torch.constant.none
    %none_2499 = torch.constant.none
    %int6_2500 = torch.constant.int 6
    %cpu_2501 = torch.constant.device "cpu"
    %int0_2502 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1763, %none_2498, %none_2499, %int6_2500, %cpu_2501, %int0_2502 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2503 = torch.constant.int 5
    %1764 = torch.prims.convert_element_type %1763, %int5_2503 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1764, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_2504 = torch.constant.int 2
    %1765 = torch.aten.view.dtype %58, %int2_2504 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %1766 = torch.aten.detach %1765 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_2505 = torch.constant.int -1
    %int17_2506 = torch.constant.int 17
    %1767 = torch.prim.ListConstruct %int-1_2505, %int17_2506 : (!torch.int, !torch.int) -> !torch.list<int>
    %1768 = torch.aten.view %1766, %1767 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_2507 = torch.constant.int 5632
    %int-1_2508 = torch.constant.int -1
    %int17_2509 = torch.constant.int 17
    %1769 = torch.prim.ListConstruct %int5632_2507, %int-1_2508, %int17_2509 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1770 = torch.aten.view %1768, %1769 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_2510 = torch.constant.int 2
    %int0_2511 = torch.constant.int 0
    %int1_2512 = torch.constant.int 1
    %int1_2513 = torch.constant.int 1
    %1771 = torch.aten.slice.Tensor %1770, %int2_2510, %int0_2511, %int1_2512, %int1_2513 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_2514 = torch.constant.int 5
    %1772 = torch.aten.view.dtype %1771, %int5_2514 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %1773 = torch.aten.detach %1772 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_2515 = torch.constant.int 2
    %int1_2516 = torch.constant.int 1
    %int9223372036854775807_2517 = torch.constant.int 9223372036854775807
    %int1_2518 = torch.constant.int 1
    %1774 = torch.aten.slice.Tensor %1770, %int2_2515, %int1_2516, %int9223372036854775807_2517, %int1_2518 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_2519 = torch.constant.int 1
    %1775 = torch.aten.view.dtype %1774, %int1_2519 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %1776 = torch.aten.detach %1775 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %1777 = torch_c.to_builtin_tensor %1764 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_2520 = tensor.cast %1777 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %1778 = torch_c.to_builtin_tensor %1773 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %1779 = torch_c.to_builtin_tensor %1776 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %1780 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_2520, %1778, %1779) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_2521 = tensor.cast %1780 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %1781 = torch_c.from_builtin_tensor %cast_2521 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %1781, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %1782 = torch.aten.silu %1781 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %1782, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_2522 = torch.constant.int 2
    %1783 = torch.aten.view.dtype %59, %int2_2522 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %1784 = torch.aten.detach %1783 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_2523 = torch.constant.int -1
    %int17_2524 = torch.constant.int 17
    %1785 = torch.prim.ListConstruct %int-1_2523, %int17_2524 : (!torch.int, !torch.int) -> !torch.list<int>
    %1786 = torch.aten.view %1784, %1785 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_2525 = torch.constant.int 5632
    %int-1_2526 = torch.constant.int -1
    %int17_2527 = torch.constant.int 17
    %1787 = torch.prim.ListConstruct %int5632_2525, %int-1_2526, %int17_2527 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1788 = torch.aten.view %1786, %1787 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_2528 = torch.constant.int 2
    %int0_2529 = torch.constant.int 0
    %int1_2530 = torch.constant.int 1
    %int1_2531 = torch.constant.int 1
    %1789 = torch.aten.slice.Tensor %1788, %int2_2528, %int0_2529, %int1_2530, %int1_2531 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_2532 = torch.constant.int 5
    %1790 = torch.aten.view.dtype %1789, %int5_2532 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %1791 = torch.aten.detach %1790 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_2533 = torch.constant.int 2
    %int1_2534 = torch.constant.int 1
    %int9223372036854775807_2535 = torch.constant.int 9223372036854775807
    %int1_2536 = torch.constant.int 1
    %1792 = torch.aten.slice.Tensor %1788, %int2_2533, %int1_2534, %int9223372036854775807_2535, %int1_2536 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_2537 = torch.constant.int 1
    %1793 = torch.aten.view.dtype %1792, %int1_2537 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %1794 = torch.aten.detach %1793 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %1795 = torch_c.to_builtin_tensor %1764 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_2538 = tensor.cast %1795 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %1796 = torch_c.to_builtin_tensor %1791 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %1797 = torch_c.to_builtin_tensor %1794 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %1798 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_2538, %1796, %1797) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_2539 = tensor.cast %1798 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %1799 = torch_c.from_builtin_tensor %cast_2539 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %1799, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %1800 = torch.aten.mul.Tensor %1782, %1799 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %1800, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_2540 = torch.constant.int 2
    %1801 = torch.aten.view.dtype %60, %int2_2540 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %1802 = torch.aten.detach %1801 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_2541 = torch.constant.int -1
    %int17_2542 = torch.constant.int 17
    %1803 = torch.prim.ListConstruct %int-1_2541, %int17_2542 : (!torch.int, !torch.int) -> !torch.list<int>
    %1804 = torch.aten.view %1802, %1803 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_2543 = torch.constant.int 2048
    %int-1_2544 = torch.constant.int -1
    %int17_2545 = torch.constant.int 17
    %1805 = torch.prim.ListConstruct %int2048_2543, %int-1_2544, %int17_2545 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1806 = torch.aten.view %1804, %1805 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_2546 = torch.constant.int 2
    %int0_2547 = torch.constant.int 0
    %int1_2548 = torch.constant.int 1
    %int1_2549 = torch.constant.int 1
    %1807 = torch.aten.slice.Tensor %1806, %int2_2546, %int0_2547, %int1_2548, %int1_2549 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_2550 = torch.constant.int 5
    %1808 = torch.aten.view.dtype %1807, %int5_2550 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %1809 = torch.aten.detach %1808 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_2551 = torch.constant.int 2
    %int1_2552 = torch.constant.int 1
    %int9223372036854775807_2553 = torch.constant.int 9223372036854775807
    %int1_2554 = torch.constant.int 1
    %1810 = torch.aten.slice.Tensor %1806, %int2_2551, %int1_2552, %int9223372036854775807_2553, %int1_2554 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_2555 = torch.constant.int 1
    %1811 = torch.aten.view.dtype %1810, %int1_2555 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %1812 = torch.aten.detach %1811 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %1813 = torch_c.to_builtin_tensor %1800 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_2556 = tensor.cast %1813 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %1814 = torch_c.to_builtin_tensor %1809 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %1815 = torch_c.to_builtin_tensor %1812 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %1816 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_2556, %1814, %1815) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_2557 = tensor.cast %1816 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %1817 = torch_c.from_builtin_tensor %cast_2557 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1817, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_2558 = torch.constant.int 1
    %1818 = torch.aten.add.Tensor %1754, %1817, %int1_2558 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1818, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_2559 = torch.constant.none
    %none_2560 = torch.constant.none
    %int5_2561 = torch.constant.int 5
    %cpu_2562 = torch.constant.device "cpu"
    %int0_2563 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1818, %none_2559, %none_2560, %int5_2561, %cpu_2562, %int0_2563 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2564 = torch.constant.int 6
    %1819 = torch.prims.convert_element_type %1818, %int6_2564 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1819, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_2565 = torch.constant.int 2
    %1820 = torch.aten.pow.Tensor_Scalar %1819, %int2_2565 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1820, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_2566 = torch.constant.int -1
    %1821 = torch.prim.ListConstruct %int-1_2566 : (!torch.int) -> !torch.list<int>
    %true_2567 = torch.constant.bool true
    %none_2568 = torch.constant.none
    %1822 = torch.aten.mean.dim %1820, %1821, %true_2567, %none_2568 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1822, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_2569 = torch.constant.float 9.9999997473787516E-6
    %int1_2570 = torch.constant.int 1
    %1823 = torch.aten.add.Scalar %1822, %float9.999990e-06_2569, %int1_2570 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1823, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1824 = torch.aten.rsqrt %1823 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1824, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1825 = torch.aten.mul.Tensor %1819, %1824 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1825, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_2571 = torch.constant.none
    %none_2572 = torch.constant.none
    %int6_2573 = torch.constant.int 6
    %cpu_2574 = torch.constant.device "cpu"
    %int0_2575 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1825, %none_2571, %none_2572, %int6_2573, %cpu_2574, %int0_2575 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2576 = torch.constant.int 5
    %1826 = torch.prims.convert_element_type %1825, %int5_2576 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1826, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %1827 = torch.aten.mul.Tensor %64, %1826 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %1827, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_2577 = torch.constant.none
    %none_2578 = torch.constant.none
    %int6_2579 = torch.constant.int 6
    %cpu_2580 = torch.constant.device "cpu"
    %int0_2581 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1827, %none_2577, %none_2578, %int6_2579, %cpu_2580, %int0_2581 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2582 = torch.constant.int 5
    %1828 = torch.prims.convert_element_type %1827, %int5_2582 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1828, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_2583 = torch.constant.int 2
    %1829 = torch.aten.view.dtype %65, %int2_2583 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %1830 = torch.aten.detach %1829 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_2584 = torch.constant.int -1
    %int17_2585 = torch.constant.int 17
    %1831 = torch.prim.ListConstruct %int-1_2584, %int17_2585 : (!torch.int, !torch.int) -> !torch.list<int>
    %1832 = torch.aten.view %1830, %1831 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_2586 = torch.constant.int 2048
    %int-1_2587 = torch.constant.int -1
    %int17_2588 = torch.constant.int 17
    %1833 = torch.prim.ListConstruct %int2048_2586, %int-1_2587, %int17_2588 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1834 = torch.aten.view %1832, %1833 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_2589 = torch.constant.int 2
    %int0_2590 = torch.constant.int 0
    %int1_2591 = torch.constant.int 1
    %int1_2592 = torch.constant.int 1
    %1835 = torch.aten.slice.Tensor %1834, %int2_2589, %int0_2590, %int1_2591, %int1_2592 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_2593 = torch.constant.int 5
    %1836 = torch.aten.view.dtype %1835, %int5_2593 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %1837 = torch.aten.detach %1836 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_2594 = torch.constant.int 2
    %int1_2595 = torch.constant.int 1
    %int9223372036854775807_2596 = torch.constant.int 9223372036854775807
    %int1_2597 = torch.constant.int 1
    %1838 = torch.aten.slice.Tensor %1834, %int2_2594, %int1_2595, %int9223372036854775807_2596, %int1_2597 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_2598 = torch.constant.int 1
    %1839 = torch.aten.view.dtype %1838, %int1_2598 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %1840 = torch.aten.detach %1839 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %1841 = torch_c.to_builtin_tensor %1828 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_2599 = tensor.cast %1841 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %1842 = torch_c.to_builtin_tensor %1837 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %1843 = torch_c.to_builtin_tensor %1840 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %1844 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_2599, %1842, %1843) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_2600 = tensor.cast %1844 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %1845 = torch_c.from_builtin_tensor %cast_2600 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %1845, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_2601 = torch.constant.int 2
    %1846 = torch.aten.view.dtype %66, %int2_2601 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %1847 = torch.aten.detach %1846 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_2602 = torch.constant.int -1
    %int17_2603 = torch.constant.int 17
    %1848 = torch.prim.ListConstruct %int-1_2602, %int17_2603 : (!torch.int, !torch.int) -> !torch.list<int>
    %1849 = torch.aten.view %1847, %1848 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_2604 = torch.constant.int 256
    %int-1_2605 = torch.constant.int -1
    %int17_2606 = torch.constant.int 17
    %1850 = torch.prim.ListConstruct %int256_2604, %int-1_2605, %int17_2606 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1851 = torch.aten.view %1849, %1850 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_2607 = torch.constant.int 2
    %int0_2608 = torch.constant.int 0
    %int1_2609 = torch.constant.int 1
    %int1_2610 = torch.constant.int 1
    %1852 = torch.aten.slice.Tensor %1851, %int2_2607, %int0_2608, %int1_2609, %int1_2610 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_2611 = torch.constant.int 5
    %1853 = torch.aten.view.dtype %1852, %int5_2611 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %1854 = torch.aten.detach %1853 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_2612 = torch.constant.int 2
    %int1_2613 = torch.constant.int 1
    %int9223372036854775807_2614 = torch.constant.int 9223372036854775807
    %int1_2615 = torch.constant.int 1
    %1855 = torch.aten.slice.Tensor %1851, %int2_2612, %int1_2613, %int9223372036854775807_2614, %int1_2615 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_2616 = torch.constant.int 1
    %1856 = torch.aten.view.dtype %1855, %int1_2616 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %1857 = torch.aten.detach %1856 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %1858 = torch_c.to_builtin_tensor %1828 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_2617 = tensor.cast %1858 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %1859 = torch_c.to_builtin_tensor %1854 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %1860 = torch_c.to_builtin_tensor %1857 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %1861 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_2617, %1859, %1860) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_2618 = tensor.cast %1861 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %1862 = torch_c.from_builtin_tensor %cast_2618 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %1862, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_2619 = torch.constant.int 2
    %1863 = torch.aten.view.dtype %67, %int2_2619 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %1864 = torch.aten.detach %1863 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_2620 = torch.constant.int -1
    %int17_2621 = torch.constant.int 17
    %1865 = torch.prim.ListConstruct %int-1_2620, %int17_2621 : (!torch.int, !torch.int) -> !torch.list<int>
    %1866 = torch.aten.view %1864, %1865 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_2622 = torch.constant.int 256
    %int-1_2623 = torch.constant.int -1
    %int17_2624 = torch.constant.int 17
    %1867 = torch.prim.ListConstruct %int256_2622, %int-1_2623, %int17_2624 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1868 = torch.aten.view %1866, %1867 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_2625 = torch.constant.int 2
    %int0_2626 = torch.constant.int 0
    %int1_2627 = torch.constant.int 1
    %int1_2628 = torch.constant.int 1
    %1869 = torch.aten.slice.Tensor %1868, %int2_2625, %int0_2626, %int1_2627, %int1_2628 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_2629 = torch.constant.int 5
    %1870 = torch.aten.view.dtype %1869, %int5_2629 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %1871 = torch.aten.detach %1870 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_2630 = torch.constant.int 2
    %int1_2631 = torch.constant.int 1
    %int9223372036854775807_2632 = torch.constant.int 9223372036854775807
    %int1_2633 = torch.constant.int 1
    %1872 = torch.aten.slice.Tensor %1868, %int2_2630, %int1_2631, %int9223372036854775807_2632, %int1_2633 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_2634 = torch.constant.int 1
    %1873 = torch.aten.view.dtype %1872, %int1_2634 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %1874 = torch.aten.detach %1873 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %1875 = torch_c.to_builtin_tensor %1828 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_2635 = tensor.cast %1875 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %1876 = torch_c.to_builtin_tensor %1871 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %1877 = torch_c.to_builtin_tensor %1874 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %1878 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_2635, %1876, %1877) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_2636 = tensor.cast %1878 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %1879 = torch_c.from_builtin_tensor %cast_2636 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %1879, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_2637 = torch.constant.int 4
    %int32_2638 = torch.constant.int 32
    %int64_2639 = torch.constant.int 64
    %1880 = torch.prim.ListConstruct %int4_2637, %273, %int32_2638, %int64_2639 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1881 = torch.aten.view %1845, %1880 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1881, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_2640 = torch.constant.int 4
    %int4_2641 = torch.constant.int 4
    %int64_2642 = torch.constant.int 64
    %1882 = torch.prim.ListConstruct %int4_2640, %273, %int4_2641, %int64_2642 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1883 = torch.aten.view %1862, %1882 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %1883, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_2643 = torch.constant.int 4
    %int4_2644 = torch.constant.int 4
    %int64_2645 = torch.constant.int 64
    %1884 = torch.prim.ListConstruct %int4_2643, %273, %int4_2644, %int64_2645 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1885 = torch.aten.view %1879, %1884 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %1885, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_2646 = torch.constant.int 0
    %none_2647 = torch.constant.none
    %none_2648 = torch.constant.none
    %cpu_2649 = torch.constant.device "cpu"
    %false_2650 = torch.constant.bool false
    %1886 = torch.aten.arange.start %int0_2646, %273, %none_2647, %none_2648, %cpu_2649, %false_2650 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1886, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_2651 = torch.constant.int 0
    %1887 = torch.aten.unsqueeze %1886, %int0_2651 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %1887, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_2652 = torch.constant.int 0
    %int64_2653 = torch.constant.int 64
    %int2_2654 = torch.constant.int 2
    %none_2655 = torch.constant.none
    %none_2656 = torch.constant.none
    %cpu_2657 = torch.constant.device "cpu"
    %false_2658 = torch.constant.bool false
    %1888 = torch.aten.arange.start_step %int0_2652, %int64_2653, %int2_2654, %none_2655, %none_2656, %cpu_2657, %false_2658 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_2659 = torch.constant.none
    %none_2660 = torch.constant.none
    %int4_2661 = torch.constant.int 4
    %cpu_2662 = torch.constant.device "cpu"
    %int0_2663 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1888, %none_2659, %none_2660, %int4_2661, %cpu_2662, %int0_2663 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2664 = torch.constant.int 6
    %1889 = torch.prims.convert_element_type %1888, %int6_2664 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_2665 = torch.constant.int 64
    %1890 = torch.aten.div.Scalar %1889, %int64_2665 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_2666 = torch.constant.float 1.000000e+04
    %1891 = torch.aten.pow.Scalar %float1.000000e04_2666, %1890 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %1892 = torch.aten.reciprocal %1891 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_2667 = torch.constant.float 1.000000e+00
    %1893 = torch.aten.mul.Scalar %1892, %float1.000000e00_2667 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_2668 = torch.constant.none
    %1894 = torch.aten.clone %61, %none_2668 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_2669 = torch.constant.int 0
    %1895 = torch.aten.unsqueeze %1893, %int0_2669 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_2670 = torch.constant.int 2
    %1896 = torch.aten.unsqueeze %1895, %int2_2670 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_2671 = torch.constant.none
    %none_2672 = torch.constant.none
    %int6_2673 = torch.constant.int 6
    %cpu_2674 = torch.constant.device "cpu"
    %int0_2675 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1896, %none_2671, %none_2672, %int6_2673, %cpu_2674, %int0_2675 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_2676 = torch.constant.int 1
    %int-1_2677 = torch.constant.int -1
    %int1_2678 = torch.constant.int 1
    %1897 = torch.prim.ListConstruct %int1_2676, %int-1_2677, %int1_2678 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2679 = torch.constant.bool false
    %1898 = torch.aten.expand %1896, %1897, %false_2679 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_2680 = torch.constant.int 1
    %1899 = torch.aten.unsqueeze %1887, %int1_2680 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %1899, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_2681 = torch.constant.none
    %none_2682 = torch.constant.none
    %int4_2683 = torch.constant.int 4
    %cpu_2684 = torch.constant.device "cpu"
    %int0_2685 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1899, %none_2681, %none_2682, %int4_2683, %cpu_2684, %int0_2685 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2686 = torch.constant.int 6
    %1900 = torch.prims.convert_element_type %1899, %int6_2686 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %1900, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %1901 = torch.aten.matmul %1898, %1900 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %1901, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_2687 = torch.constant.int 1
    %int2_2688 = torch.constant.int 2
    %1902 = torch.aten.transpose.int %1901, %int1_2687, %int2_2688 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1902, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1903 = torch.aten.cos %1902 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1903, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1904 = torch.aten.mul.Tensor %1903, %1894 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1904, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_2689 = torch.constant.none
    %none_2690 = torch.constant.none
    %int6_2691 = torch.constant.int 6
    %cpu_2692 = torch.constant.device "cpu"
    %int0_2693 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1904, %none_2689, %none_2690, %int6_2691, %cpu_2692, %int0_2693 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2694 = torch.constant.int 5
    %1905 = torch.prims.convert_element_type %1904, %int5_2694 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %1905, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %1906 = torch.aten.sin %1902 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1906, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1907 = torch.aten.mul.Tensor %1906, %1894 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1907, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_2695 = torch.constant.none
    %none_2696 = torch.constant.none
    %int6_2697 = torch.constant.int 6
    %cpu_2698 = torch.constant.device "cpu"
    %int0_2699 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1907, %none_2695, %none_2696, %int6_2697, %cpu_2698, %int0_2699 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2700 = torch.constant.int 5
    %1908 = torch.prims.convert_element_type %1907, %int5_2700 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %1908, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_2701 = torch.constant.int 2
    %1909 = torch.aten.unsqueeze %1905, %int2_2701 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %1909, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_2702 = torch.constant.int 2
    %1910 = torch.aten.unsqueeze %1908, %int2_2702 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %1910, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_2703 = torch.constant.none
    %none_2704 = torch.constant.none
    %int5_2705 = torch.constant.int 5
    %cpu_2706 = torch.constant.device "cpu"
    %int0_2707 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1909, %none_2703, %none_2704, %int5_2705, %cpu_2706, %int0_2707 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_2708 = torch.constant.none
    %none_2709 = torch.constant.none
    %int5_2710 = torch.constant.int 5
    %cpu_2711 = torch.constant.device "cpu"
    %int0_2712 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1910, %none_2708, %none_2709, %int5_2710, %cpu_2711, %int0_2712 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_2713 = torch.constant.none
    %none_2714 = torch.constant.none
    %int5_2715 = torch.constant.int 5
    %cpu_2716 = torch.constant.device "cpu"
    %int0_2717 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1881, %none_2713, %none_2714, %int5_2715, %cpu_2716, %int0_2717 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_2718 = torch.constant.int 3
    %int0_2719 = torch.constant.int 0
    %int64_2720 = torch.constant.int 64
    %int2_2721 = torch.constant.int 2
    %1911 = torch.aten.slice.Tensor %1881, %int3_2718, %int0_2719, %int64_2720, %int2_2721 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1911, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_2722 = torch.constant.int 3
    %int1_2723 = torch.constant.int 1
    %int64_2724 = torch.constant.int 64
    %int2_2725 = torch.constant.int 2
    %1912 = torch.aten.slice.Tensor %1881, %int3_2722, %int1_2723, %int64_2724, %int2_2725 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1912, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %1913 = torch.aten.mul.Tensor %1911, %1909 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1913, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %1914 = torch.aten.mul.Tensor %1912, %1910 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1914, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_2726 = torch.constant.int 1
    %1915 = torch.aten.sub.Tensor %1913, %1914, %int1_2726 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1915, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %1916 = torch.aten.mul.Tensor %1912, %1909 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1916, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %1917 = torch.aten.mul.Tensor %1911, %1910 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1917, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_2727 = torch.constant.int 1
    %1918 = torch.aten.add.Tensor %1916, %1917, %int1_2727 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %1918, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %1919 = torch_c.to_builtin_tensor %1915 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_2728 = tensor.cast %1919 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %1920 = torch_c.to_builtin_tensor %1918 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_2729 = tensor.cast %1920 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %1921 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_2728, %cast_2729) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_2730 = tensor.cast %1921 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %1922 = torch_c.from_builtin_tensor %cast_2730 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %1922, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_2731 = torch.constant.int 4
    %int32_2732 = torch.constant.int 32
    %int64_2733 = torch.constant.int 64
    %1923 = torch.prim.ListConstruct %int4_2731, %273, %int32_2732, %int64_2733 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1924 = torch.aten.view %1922, %1923 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1924, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_2734 = torch.constant.none
    %none_2735 = torch.constant.none
    %int5_2736 = torch.constant.int 5
    %cpu_2737 = torch.constant.device "cpu"
    %int0_2738 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1924, %none_2734, %none_2735, %int5_2736, %cpu_2737, %int0_2738 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_2739 = torch.constant.int 0
    %none_2740 = torch.constant.none
    %none_2741 = torch.constant.none
    %cpu_2742 = torch.constant.device "cpu"
    %false_2743 = torch.constant.bool false
    %1925 = torch.aten.arange.start %int0_2739, %273, %none_2740, %none_2741, %cpu_2742, %false_2743 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1925, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_2744 = torch.constant.int 0
    %1926 = torch.aten.unsqueeze %1925, %int0_2744 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %1926, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_2745 = torch.constant.int 0
    %int64_2746 = torch.constant.int 64
    %int2_2747 = torch.constant.int 2
    %none_2748 = torch.constant.none
    %none_2749 = torch.constant.none
    %cpu_2750 = torch.constant.device "cpu"
    %false_2751 = torch.constant.bool false
    %1927 = torch.aten.arange.start_step %int0_2745, %int64_2746, %int2_2747, %none_2748, %none_2749, %cpu_2750, %false_2751 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_2752 = torch.constant.none
    %none_2753 = torch.constant.none
    %int4_2754 = torch.constant.int 4
    %cpu_2755 = torch.constant.device "cpu"
    %int0_2756 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1927, %none_2752, %none_2753, %int4_2754, %cpu_2755, %int0_2756 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2757 = torch.constant.int 6
    %1928 = torch.prims.convert_element_type %1927, %int6_2757 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_2758 = torch.constant.int 64
    %1929 = torch.aten.div.Scalar %1928, %int64_2758 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_2759 = torch.constant.float 1.000000e+04
    %1930 = torch.aten.pow.Scalar %float1.000000e04_2759, %1929 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %1931 = torch.aten.reciprocal %1930 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_2760 = torch.constant.float 1.000000e+00
    %1932 = torch.aten.mul.Scalar %1931, %float1.000000e00_2760 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_2761 = torch.constant.none
    %1933 = torch.aten.clone %62, %none_2761 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_2762 = torch.constant.int 0
    %1934 = torch.aten.unsqueeze %1932, %int0_2762 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_2763 = torch.constant.int 2
    %1935 = torch.aten.unsqueeze %1934, %int2_2763 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_2764 = torch.constant.none
    %none_2765 = torch.constant.none
    %int6_2766 = torch.constant.int 6
    %cpu_2767 = torch.constant.device "cpu"
    %int0_2768 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1935, %none_2764, %none_2765, %int6_2766, %cpu_2767, %int0_2768 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_2769 = torch.constant.int 1
    %int-1_2770 = torch.constant.int -1
    %int1_2771 = torch.constant.int 1
    %1936 = torch.prim.ListConstruct %int1_2769, %int-1_2770, %int1_2771 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2772 = torch.constant.bool false
    %1937 = torch.aten.expand %1935, %1936, %false_2772 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_2773 = torch.constant.int 1
    %1938 = torch.aten.unsqueeze %1926, %int1_2773 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %1938, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_2774 = torch.constant.none
    %none_2775 = torch.constant.none
    %int4_2776 = torch.constant.int 4
    %cpu_2777 = torch.constant.device "cpu"
    %int0_2778 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1938, %none_2774, %none_2775, %int4_2776, %cpu_2777, %int0_2778 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2779 = torch.constant.int 6
    %1939 = torch.prims.convert_element_type %1938, %int6_2779 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %1939, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %1940 = torch.aten.matmul %1937, %1939 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %1940, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_2780 = torch.constant.int 1
    %int2_2781 = torch.constant.int 2
    %1941 = torch.aten.transpose.int %1940, %int1_2780, %int2_2781 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1941, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1942 = torch.aten.cos %1941 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1942, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1943 = torch.aten.mul.Tensor %1942, %1933 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1943, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_2782 = torch.constant.none
    %none_2783 = torch.constant.none
    %int6_2784 = torch.constant.int 6
    %cpu_2785 = torch.constant.device "cpu"
    %int0_2786 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1943, %none_2782, %none_2783, %int6_2784, %cpu_2785, %int0_2786 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2787 = torch.constant.int 5
    %1944 = torch.prims.convert_element_type %1943, %int5_2787 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %1944, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %1945 = torch.aten.sin %1941 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1945, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %1946 = torch.aten.mul.Tensor %1945, %1933 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %1946, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_2788 = torch.constant.none
    %none_2789 = torch.constant.none
    %int6_2790 = torch.constant.int 6
    %cpu_2791 = torch.constant.device "cpu"
    %int0_2792 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1946, %none_2788, %none_2789, %int6_2790, %cpu_2791, %int0_2792 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2793 = torch.constant.int 5
    %1947 = torch.prims.convert_element_type %1946, %int5_2793 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %1947, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_2794 = torch.constant.int 2
    %1948 = torch.aten.unsqueeze %1944, %int2_2794 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %1948, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_2795 = torch.constant.int 2
    %1949 = torch.aten.unsqueeze %1947, %int2_2795 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %1949, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_2796 = torch.constant.none
    %none_2797 = torch.constant.none
    %int5_2798 = torch.constant.int 5
    %cpu_2799 = torch.constant.device "cpu"
    %int0_2800 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1948, %none_2796, %none_2797, %int5_2798, %cpu_2799, %int0_2800 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_2801 = torch.constant.none
    %none_2802 = torch.constant.none
    %int5_2803 = torch.constant.int 5
    %cpu_2804 = torch.constant.device "cpu"
    %int0_2805 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1949, %none_2801, %none_2802, %int5_2803, %cpu_2804, %int0_2805 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_2806 = torch.constant.none
    %none_2807 = torch.constant.none
    %int5_2808 = torch.constant.int 5
    %cpu_2809 = torch.constant.device "cpu"
    %int0_2810 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1883, %none_2806, %none_2807, %int5_2808, %cpu_2809, %int0_2810 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_2811 = torch.constant.int 3
    %int0_2812 = torch.constant.int 0
    %int64_2813 = torch.constant.int 64
    %int2_2814 = torch.constant.int 2
    %1950 = torch.aten.slice.Tensor %1883, %int3_2811, %int0_2812, %int64_2813, %int2_2814 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1950, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_2815 = torch.constant.int 3
    %int1_2816 = torch.constant.int 1
    %int64_2817 = torch.constant.int 64
    %int2_2818 = torch.constant.int 2
    %1951 = torch.aten.slice.Tensor %1883, %int3_2815, %int1_2816, %int64_2817, %int2_2818 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1951, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1952 = torch.aten.mul.Tensor %1950, %1948 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1952, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1953 = torch.aten.mul.Tensor %1951, %1949 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1953, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_2819 = torch.constant.int 1
    %1954 = torch.aten.sub.Tensor %1952, %1953, %int1_2819 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1954, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1955 = torch.aten.mul.Tensor %1951, %1948 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1955, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1956 = torch.aten.mul.Tensor %1950, %1949 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1956, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_2820 = torch.constant.int 1
    %1957 = torch.aten.add.Tensor %1955, %1956, %int1_2820 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %1957, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %1958 = torch_c.to_builtin_tensor %1954 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_2821 = tensor.cast %1958 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %1959 = torch_c.to_builtin_tensor %1957 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_2822 = tensor.cast %1959 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %1960 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_2821, %cast_2822) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_2823 = tensor.cast %1960 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %1961 = torch_c.from_builtin_tensor %cast_2823 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %1961, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_2824 = torch.constant.int 4
    %int4_2825 = torch.constant.int 4
    %int64_2826 = torch.constant.int 64
    %1962 = torch.prim.ListConstruct %int4_2824, %273, %int4_2825, %int64_2826 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1963 = torch.aten.view %1961, %1962 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %1963, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_2827 = torch.constant.none
    %none_2828 = torch.constant.none
    %int5_2829 = torch.constant.int 5
    %cpu_2830 = torch.constant.device "cpu"
    %int0_2831 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1963, %none_2827, %none_2828, %int5_2829, %cpu_2830, %int0_2831 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_2832 = torch.constant.int 22
    %1964 = torch.aten.mul.Scalar %arg2, %int22_2832 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1964, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int5_2833 = torch.constant.int 5
    %int1_2834 = torch.constant.int 1
    %1965 = torch.aten.add.Scalar %1964, %int5_2833, %int1_2834 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1965, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_2835 = torch.constant.int 2
    %1966 = torch.aten.mul.Scalar %1965, %int2_2835 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1966, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_2836 = torch.constant.int 0
    %int1_2837 = torch.constant.int 1
    %1967 = torch.aten.add.Scalar %1966, %int0_2836, %int1_2837 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1967, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %1968 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %1969 = torch.aten.view %1967, %1968 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1969, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_2838 = torch.constant.int 4
    %int32_2839 = torch.constant.int 32
    %int4_2840 = torch.constant.int 4
    %int64_2841 = torch.constant.int 64
    %1970 = torch.prim.ListConstruct %int4_2838, %271, %int32_2839, %int4_2840, %int64_2841 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1971 = torch.aten.view %1963, %1970 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1971, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_2842 = torch.constant.int 32
    %int4_2843 = torch.constant.int 4
    %int64_2844 = torch.constant.int 64
    %1972 = torch.prim.ListConstruct %446, %int32_2842, %int4_2843, %int64_2844 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1973 = torch.aten.view %1971, %1972 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %1973, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_2845 = torch.constant.int 1
    %int2_2846 = torch.constant.int 2
    %1974 = torch.aten.transpose.int %1973, %int1_2845, %int2_2846 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1974, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_2847 = torch.constant.none
    %none_2848 = torch.constant.none
    %int5_2849 = torch.constant.int 5
    %cpu_2850 = torch.constant.device "cpu"
    %int0_2851 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1974, %none_2847, %none_2848, %int5_2849, %cpu_2850, %int0_2851 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_2852 = torch.constant.int 22
    %int2_2853 = torch.constant.int 2
    %int4_2854 = torch.constant.int 4
    %int32_2855 = torch.constant.int 32
    %int64_2856 = torch.constant.int 64
    %1975 = torch.prim.ListConstruct %272, %int22_2852, %int2_2853, %int4_2854, %int32_2855, %int64_2856 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1976 = torch.aten.view %1700, %1975 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1976, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_2857 = torch.constant.int 4
    %int32_2858 = torch.constant.int 32
    %int64_2859 = torch.constant.int 64
    %1977 = torch.prim.ListConstruct %439, %int4_2857, %int32_2858, %int64_2859 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1978 = torch.aten.view %1976, %1977 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1978, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %1979 = torch.prim.ListConstruct %1969 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_2860 = torch.constant.bool false
    %1980 = torch.aten.index_put %1978, %1979, %1974, %false_2860 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1980, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_2861 = torch.constant.int 22
    %int2_2862 = torch.constant.int 2
    %int4_2863 = torch.constant.int 4
    %int32_2864 = torch.constant.int 32
    %int64_2865 = torch.constant.int 64
    %1981 = torch.prim.ListConstruct %272, %int22_2861, %int2_2862, %int4_2863, %int32_2864, %int64_2865 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1982 = torch.aten.view %1980, %1981 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1982, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_2866 = torch.constant.int 360448
    %1983 = torch.prim.ListConstruct %272, %int360448_2866 : (!torch.int, !torch.int) -> !torch.list<int>
    %1984 = torch.aten.view %1982, %1983 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %1984, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_2867 = torch.constant.int 22
    %int2_2868 = torch.constant.int 2
    %int4_2869 = torch.constant.int 4
    %int32_2870 = torch.constant.int 32
    %int64_2871 = torch.constant.int 64
    %1985 = torch.prim.ListConstruct %272, %int22_2867, %int2_2868, %int4_2869, %int32_2870, %int64_2871 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1986 = torch.aten.view %1984, %1985 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1986, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_2872 = torch.constant.int 4
    %int32_2873 = torch.constant.int 32
    %int64_2874 = torch.constant.int 64
    %1987 = torch.prim.ListConstruct %439, %int4_2872, %int32_2873, %int64_2874 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1988 = torch.aten.view %1986, %1987 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1988, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_2875 = torch.constant.int 22
    %1989 = torch.aten.mul.Scalar %arg2, %int22_2875 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1989, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int5_2876 = torch.constant.int 5
    %int1_2877 = torch.constant.int 1
    %1990 = torch.aten.add.Scalar %1989, %int5_2876, %int1_2877 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1990, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_2878 = torch.constant.int 2
    %1991 = torch.aten.mul.Scalar %1990, %int2_2878 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1991, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_2879 = torch.constant.int 1
    %int1_2880 = torch.constant.int 1
    %1992 = torch.aten.add.Scalar %1991, %int1_2879, %int1_2880 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1992, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %1993 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %1994 = torch.aten.view %1992, %1993 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1994, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_2881 = torch.constant.int 4
    %int32_2882 = torch.constant.int 32
    %int4_2883 = torch.constant.int 4
    %int64_2884 = torch.constant.int 64
    %1995 = torch.prim.ListConstruct %int4_2881, %271, %int32_2882, %int4_2883, %int64_2884 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1996 = torch.aten.view %1885, %1995 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1996, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_2885 = torch.constant.int 32
    %int4_2886 = torch.constant.int 4
    %int64_2887 = torch.constant.int 64
    %1997 = torch.prim.ListConstruct %446, %int32_2885, %int4_2886, %int64_2887 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1998 = torch.aten.view %1996, %1997 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %1998, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_2888 = torch.constant.int 1
    %int2_2889 = torch.constant.int 2
    %1999 = torch.aten.transpose.int %1998, %int1_2888, %int2_2889 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %1999, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_2890 = torch.constant.none
    %none_2891 = torch.constant.none
    %int5_2892 = torch.constant.int 5
    %cpu_2893 = torch.constant.device "cpu"
    %int0_2894 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1999, %none_2890, %none_2891, %int5_2892, %cpu_2893, %int0_2894 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %2000 = torch.prim.ListConstruct %1994 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_2895 = torch.constant.bool false
    %2001 = torch.aten.index_put %1988, %2000, %1999, %false_2895 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2001, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_2896 = torch.constant.int 22
    %int2_2897 = torch.constant.int 2
    %int4_2898 = torch.constant.int 4
    %int32_2899 = torch.constant.int 32
    %int64_2900 = torch.constant.int 64
    %2002 = torch.prim.ListConstruct %272, %int22_2896, %int2_2897, %int4_2898, %int32_2899, %int64_2900 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2003 = torch.aten.view %2001, %2002 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2003, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_2901 = torch.constant.int 360448
    %2004 = torch.prim.ListConstruct %272, %int360448_2901 : (!torch.int, !torch.int) -> !torch.list<int>
    %2005 = torch.aten.view %2003, %2004 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %2005, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_2902 = torch.constant.int 0
    %int1_2903 = torch.constant.int 1
    %none_2904 = torch.constant.none
    %none_2905 = torch.constant.none
    %cpu_2906 = torch.constant.device "cpu"
    %false_2907 = torch.constant.bool false
    %2006 = torch.aten.arange.start_step %int0_2902, %273, %int1_2903, %none_2904, %none_2905, %cpu_2906, %false_2907 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2006, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_2908 = torch.constant.int -1
    %2007 = torch.aten.unsqueeze %arg1, %int-1_2908 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %2008 = torch.aten.ge.Tensor %2006, %2007 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %2008, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_2909 = torch.constant.none
    %none_2910 = torch.constant.none
    %cpu_2911 = torch.constant.device "cpu"
    %false_2912 = torch.constant.bool false
    %2009 = torch.aten.arange %273, %none_2909, %none_2910, %cpu_2911, %false_2912 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2009, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_2913 = torch.constant.int 0
    %2010 = torch.aten.unsqueeze %2009, %int0_2913 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %2010, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_2914 = torch.constant.int 1
    %2011 = torch.aten.unsqueeze %2010, %int1_2914 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %2011, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_2915 = torch.constant.int 2
    %2012 = torch.aten.unsqueeze %2011, %int2_2915 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %2012, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_2916 = torch.constant.none
    %none_2917 = torch.constant.none
    %cpu_2918 = torch.constant.device "cpu"
    %false_2919 = torch.constant.bool false
    %2013 = torch.aten.arange %273, %none_2916, %none_2917, %cpu_2918, %false_2919 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2013, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_2920 = torch.constant.int 0
    %2014 = torch.aten.unsqueeze %2013, %int0_2920 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %2014, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_2921 = torch.constant.int 1
    %2015 = torch.aten.unsqueeze %2014, %int1_2921 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %2015, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_2922 = torch.constant.int 3
    %2016 = torch.aten.unsqueeze %2015, %int3_2922 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %2016, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %2017 = torch.aten.gt.Tensor %2012, %2016 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %2017, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_2923 = torch.constant.int 1
    %2018 = torch.aten.unsqueeze %2008, %int1_2923 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %2018, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_2924 = torch.constant.int 2
    %2019 = torch.aten.unsqueeze %2018, %int2_2924 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %2019, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %2020 = torch.aten.logical_or %2017, %2019 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %2020, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_2925 = torch.constant.none
    %2021 = torch.aten.clone %63, %none_2925 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_2926 = torch.constant.int 0
    %2022 = torch.aten.where.ScalarOther %2020, %2021, %int0_2926 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %2022, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_2927 = torch.constant.none
    %none_2928 = torch.constant.none
    %int5_2929 = torch.constant.int 5
    %cpu_2930 = torch.constant.device "cpu"
    %int0_2931 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2022, %none_2927, %none_2928, %int5_2929, %cpu_2930, %int0_2931 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_2932 = torch.constant.int -2
    %2023 = torch.aten.unsqueeze %1963, %int-2_2932 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %2023, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_2933 = torch.constant.int 4
    %int4_2934 = torch.constant.int 4
    %int8_2935 = torch.constant.int 8
    %int64_2936 = torch.constant.int 64
    %2024 = torch.prim.ListConstruct %int4_2933, %273, %int4_2934, %int8_2935, %int64_2936 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2937 = torch.constant.bool false
    %2025 = torch.aten.expand %2023, %2024, %false_2937 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2025, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_2938 = torch.constant.int 0
    %2026 = torch.aten.clone %2025, %int0_2938 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2026, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_2939 = torch.constant.int 4
    %int32_2940 = torch.constant.int 32
    %int64_2941 = torch.constant.int 64
    %2027 = torch.prim.ListConstruct %int4_2939, %273, %int32_2940, %int64_2941 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2028 = torch.aten._unsafe_view %2026, %2027 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2028, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_2942 = torch.constant.int -2
    %2029 = torch.aten.unsqueeze %1885, %int-2_2942 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %2029, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_2943 = torch.constant.int 4
    %int4_2944 = torch.constant.int 4
    %int8_2945 = torch.constant.int 8
    %int64_2946 = torch.constant.int 64
    %2030 = torch.prim.ListConstruct %int4_2943, %273, %int4_2944, %int8_2945, %int64_2946 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2947 = torch.constant.bool false
    %2031 = torch.aten.expand %2029, %2030, %false_2947 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2031, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_2948 = torch.constant.int 0
    %2032 = torch.aten.clone %2031, %int0_2948 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2032, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_2949 = torch.constant.int 4
    %int32_2950 = torch.constant.int 32
    %int64_2951 = torch.constant.int 64
    %2033 = torch.prim.ListConstruct %int4_2949, %273, %int32_2950, %int64_2951 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2034 = torch.aten._unsafe_view %2032, %2033 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2034, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_2952 = torch.constant.int 1
    %int2_2953 = torch.constant.int 2
    %2035 = torch.aten.transpose.int %1924, %int1_2952, %int2_2953 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2035, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_2954 = torch.constant.int 1
    %int2_2955 = torch.constant.int 2
    %2036 = torch.aten.transpose.int %2028, %int1_2954, %int2_2955 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2036, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_2956 = torch.constant.int 1
    %int2_2957 = torch.constant.int 2
    %2037 = torch.aten.transpose.int %2034, %int1_2956, %int2_2957 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2037, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_2958 = torch.constant.float 0.000000e+00
    %false_2959 = torch.constant.bool false
    %none_2960 = torch.constant.none
    %false_2961 = torch.constant.bool false
    %2038 = torch.aten.scaled_dot_product_attention %2035, %2036, %2037, %2022, %float0.000000e00_2958, %false_2959, %none_2960, %false_2961 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2038, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_2962 = torch.constant.int 1
    %int2_2963 = torch.constant.int 2
    %2039 = torch.aten.transpose.int %2038, %int1_2962, %int2_2963 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2039, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_2964 = torch.constant.int 4
    %int2048_2965 = torch.constant.int 2048
    %2040 = torch.prim.ListConstruct %int4_2964, %273, %int2048_2965 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2041 = torch.aten.view %2039, %2040 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2041, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_2966 = torch.constant.int 2
    %2042 = torch.aten.view.dtype %68, %int2_2966 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %2043 = torch.aten.detach %2042 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_2967 = torch.constant.int -1
    %int17_2968 = torch.constant.int 17
    %2044 = torch.prim.ListConstruct %int-1_2967, %int17_2968 : (!torch.int, !torch.int) -> !torch.list<int>
    %2045 = torch.aten.view %2043, %2044 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_2969 = torch.constant.int 2048
    %int-1_2970 = torch.constant.int -1
    %int17_2971 = torch.constant.int 17
    %2046 = torch.prim.ListConstruct %int2048_2969, %int-1_2970, %int17_2971 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2047 = torch.aten.view %2045, %2046 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_2972 = torch.constant.int 2
    %int0_2973 = torch.constant.int 0
    %int1_2974 = torch.constant.int 1
    %int1_2975 = torch.constant.int 1
    %2048 = torch.aten.slice.Tensor %2047, %int2_2972, %int0_2973, %int1_2974, %int1_2975 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_2976 = torch.constant.int 5
    %2049 = torch.aten.view.dtype %2048, %int5_2976 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %2050 = torch.aten.detach %2049 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_2977 = torch.constant.int 2
    %int1_2978 = torch.constant.int 1
    %int9223372036854775807_2979 = torch.constant.int 9223372036854775807
    %int1_2980 = torch.constant.int 1
    %2051 = torch.aten.slice.Tensor %2047, %int2_2977, %int1_2978, %int9223372036854775807_2979, %int1_2980 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_2981 = torch.constant.int 1
    %2052 = torch.aten.view.dtype %2051, %int1_2981 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %2053 = torch.aten.detach %2052 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %2054 = torch_c.to_builtin_tensor %2041 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_2982 = tensor.cast %2054 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2055 = torch_c.to_builtin_tensor %2050 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %2056 = torch_c.to_builtin_tensor %2053 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %2057 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_2982, %2055, %2056) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_2983 = tensor.cast %2057 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %2058 = torch_c.from_builtin_tensor %cast_2983 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2058, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_2984 = torch.constant.none
    %none_2985 = torch.constant.none
    %int5_2986 = torch.constant.int 5
    %cpu_2987 = torch.constant.device "cpu"
    %int0_2988 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2058, %none_2984, %none_2985, %int5_2986, %cpu_2987, %int0_2988 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_2989 = torch.constant.int 1
    %2059 = torch.aten.add.Tensor %1818, %2058, %int1_2989 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2059, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_2990 = torch.constant.none
    %none_2991 = torch.constant.none
    %int5_2992 = torch.constant.int 5
    %cpu_2993 = torch.constant.device "cpu"
    %int0_2994 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2059, %none_2990, %none_2991, %int5_2992, %cpu_2993, %int0_2994 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2995 = torch.constant.int 6
    %2060 = torch.prims.convert_element_type %2059, %int6_2995 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2060, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_2996 = torch.constant.int 2
    %2061 = torch.aten.pow.Tensor_Scalar %2060, %int2_2996 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2061, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_2997 = torch.constant.int -1
    %2062 = torch.prim.ListConstruct %int-1_2997 : (!torch.int) -> !torch.list<int>
    %true_2998 = torch.constant.bool true
    %none_2999 = torch.constant.none
    %2063 = torch.aten.mean.dim %2061, %2062, %true_2998, %none_2999 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2063, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_3000 = torch.constant.float 9.9999997473787516E-6
    %int1_3001 = torch.constant.int 1
    %2064 = torch.aten.add.Scalar %2063, %float9.999990e-06_3000, %int1_3001 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2064, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2065 = torch.aten.rsqrt %2064 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2065, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2066 = torch.aten.mul.Tensor %2060, %2065 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2066, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_3002 = torch.constant.none
    %none_3003 = torch.constant.none
    %int6_3004 = torch.constant.int 6
    %cpu_3005 = torch.constant.device "cpu"
    %int0_3006 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2066, %none_3002, %none_3003, %int6_3004, %cpu_3005, %int0_3006 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3007 = torch.constant.int 5
    %2067 = torch.prims.convert_element_type %2066, %int5_3007 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2067, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %2068 = torch.aten.mul.Tensor %69, %2067 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2068, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_3008 = torch.constant.none
    %none_3009 = torch.constant.none
    %int6_3010 = torch.constant.int 6
    %cpu_3011 = torch.constant.device "cpu"
    %int0_3012 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2068, %none_3008, %none_3009, %int6_3010, %cpu_3011, %int0_3012 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3013 = torch.constant.int 5
    %2069 = torch.prims.convert_element_type %2068, %int5_3013 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2069, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_3014 = torch.constant.int 2
    %2070 = torch.aten.view.dtype %70, %int2_3014 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %2071 = torch.aten.detach %2070 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_3015 = torch.constant.int -1
    %int17_3016 = torch.constant.int 17
    %2072 = torch.prim.ListConstruct %int-1_3015, %int17_3016 : (!torch.int, !torch.int) -> !torch.list<int>
    %2073 = torch.aten.view %2071, %2072 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_3017 = torch.constant.int 5632
    %int-1_3018 = torch.constant.int -1
    %int17_3019 = torch.constant.int 17
    %2074 = torch.prim.ListConstruct %int5632_3017, %int-1_3018, %int17_3019 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2075 = torch.aten.view %2073, %2074 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_3020 = torch.constant.int 2
    %int0_3021 = torch.constant.int 0
    %int1_3022 = torch.constant.int 1
    %int1_3023 = torch.constant.int 1
    %2076 = torch.aten.slice.Tensor %2075, %int2_3020, %int0_3021, %int1_3022, %int1_3023 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_3024 = torch.constant.int 5
    %2077 = torch.aten.view.dtype %2076, %int5_3024 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %2078 = torch.aten.detach %2077 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_3025 = torch.constant.int 2
    %int1_3026 = torch.constant.int 1
    %int9223372036854775807_3027 = torch.constant.int 9223372036854775807
    %int1_3028 = torch.constant.int 1
    %2079 = torch.aten.slice.Tensor %2075, %int2_3025, %int1_3026, %int9223372036854775807_3027, %int1_3028 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_3029 = torch.constant.int 1
    %2080 = torch.aten.view.dtype %2079, %int1_3029 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %2081 = torch.aten.detach %2080 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %2082 = torch_c.to_builtin_tensor %2069 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_3030 = tensor.cast %2082 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2083 = torch_c.to_builtin_tensor %2078 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %2084 = torch_c.to_builtin_tensor %2081 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %2085 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_3030, %2083, %2084) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_3031 = tensor.cast %2085 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %2086 = torch_c.from_builtin_tensor %cast_3031 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %2086, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %2087 = torch.aten.silu %2086 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %2087, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_3032 = torch.constant.int 2
    %2088 = torch.aten.view.dtype %71, %int2_3032 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %2089 = torch.aten.detach %2088 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_3033 = torch.constant.int -1
    %int17_3034 = torch.constant.int 17
    %2090 = torch.prim.ListConstruct %int-1_3033, %int17_3034 : (!torch.int, !torch.int) -> !torch.list<int>
    %2091 = torch.aten.view %2089, %2090 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_3035 = torch.constant.int 5632
    %int-1_3036 = torch.constant.int -1
    %int17_3037 = torch.constant.int 17
    %2092 = torch.prim.ListConstruct %int5632_3035, %int-1_3036, %int17_3037 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2093 = torch.aten.view %2091, %2092 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_3038 = torch.constant.int 2
    %int0_3039 = torch.constant.int 0
    %int1_3040 = torch.constant.int 1
    %int1_3041 = torch.constant.int 1
    %2094 = torch.aten.slice.Tensor %2093, %int2_3038, %int0_3039, %int1_3040, %int1_3041 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_3042 = torch.constant.int 5
    %2095 = torch.aten.view.dtype %2094, %int5_3042 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %2096 = torch.aten.detach %2095 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_3043 = torch.constant.int 2
    %int1_3044 = torch.constant.int 1
    %int9223372036854775807_3045 = torch.constant.int 9223372036854775807
    %int1_3046 = torch.constant.int 1
    %2097 = torch.aten.slice.Tensor %2093, %int2_3043, %int1_3044, %int9223372036854775807_3045, %int1_3046 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_3047 = torch.constant.int 1
    %2098 = torch.aten.view.dtype %2097, %int1_3047 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %2099 = torch.aten.detach %2098 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %2100 = torch_c.to_builtin_tensor %2069 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_3048 = tensor.cast %2100 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2101 = torch_c.to_builtin_tensor %2096 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %2102 = torch_c.to_builtin_tensor %2099 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %2103 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_3048, %2101, %2102) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_3049 = tensor.cast %2103 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %2104 = torch_c.from_builtin_tensor %cast_3049 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %2104, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %2105 = torch.aten.mul.Tensor %2087, %2104 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %2105, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_3050 = torch.constant.int 2
    %2106 = torch.aten.view.dtype %72, %int2_3050 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %2107 = torch.aten.detach %2106 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_3051 = torch.constant.int -1
    %int17_3052 = torch.constant.int 17
    %2108 = torch.prim.ListConstruct %int-1_3051, %int17_3052 : (!torch.int, !torch.int) -> !torch.list<int>
    %2109 = torch.aten.view %2107, %2108 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_3053 = torch.constant.int 2048
    %int-1_3054 = torch.constant.int -1
    %int17_3055 = torch.constant.int 17
    %2110 = torch.prim.ListConstruct %int2048_3053, %int-1_3054, %int17_3055 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2111 = torch.aten.view %2109, %2110 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_3056 = torch.constant.int 2
    %int0_3057 = torch.constant.int 0
    %int1_3058 = torch.constant.int 1
    %int1_3059 = torch.constant.int 1
    %2112 = torch.aten.slice.Tensor %2111, %int2_3056, %int0_3057, %int1_3058, %int1_3059 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_3060 = torch.constant.int 5
    %2113 = torch.aten.view.dtype %2112, %int5_3060 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %2114 = torch.aten.detach %2113 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_3061 = torch.constant.int 2
    %int1_3062 = torch.constant.int 1
    %int9223372036854775807_3063 = torch.constant.int 9223372036854775807
    %int1_3064 = torch.constant.int 1
    %2115 = torch.aten.slice.Tensor %2111, %int2_3061, %int1_3062, %int9223372036854775807_3063, %int1_3064 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_3065 = torch.constant.int 1
    %2116 = torch.aten.view.dtype %2115, %int1_3065 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %2117 = torch.aten.detach %2116 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %2118 = torch_c.to_builtin_tensor %2105 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_3066 = tensor.cast %2118 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %2119 = torch_c.to_builtin_tensor %2114 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %2120 = torch_c.to_builtin_tensor %2117 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %2121 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_3066, %2119, %2120) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_3067 = tensor.cast %2121 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %2122 = torch_c.from_builtin_tensor %cast_3067 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2122, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_3068 = torch.constant.int 1
    %2123 = torch.aten.add.Tensor %2059, %2122, %int1_3068 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2123, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_3069 = torch.constant.none
    %none_3070 = torch.constant.none
    %int5_3071 = torch.constant.int 5
    %cpu_3072 = torch.constant.device "cpu"
    %int0_3073 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2123, %none_3069, %none_3070, %int5_3071, %cpu_3072, %int0_3073 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3074 = torch.constant.int 6
    %2124 = torch.prims.convert_element_type %2123, %int6_3074 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2124, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_3075 = torch.constant.int 2
    %2125 = torch.aten.pow.Tensor_Scalar %2124, %int2_3075 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2125, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_3076 = torch.constant.int -1
    %2126 = torch.prim.ListConstruct %int-1_3076 : (!torch.int) -> !torch.list<int>
    %true_3077 = torch.constant.bool true
    %none_3078 = torch.constant.none
    %2127 = torch.aten.mean.dim %2125, %2126, %true_3077, %none_3078 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2127, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_3079 = torch.constant.float 9.9999997473787516E-6
    %int1_3080 = torch.constant.int 1
    %2128 = torch.aten.add.Scalar %2127, %float9.999990e-06_3079, %int1_3080 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2128, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2129 = torch.aten.rsqrt %2128 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2129, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2130 = torch.aten.mul.Tensor %2124, %2129 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2130, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_3081 = torch.constant.none
    %none_3082 = torch.constant.none
    %int6_3083 = torch.constant.int 6
    %cpu_3084 = torch.constant.device "cpu"
    %int0_3085 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2130, %none_3081, %none_3082, %int6_3083, %cpu_3084, %int0_3085 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3086 = torch.constant.int 5
    %2131 = torch.prims.convert_element_type %2130, %int5_3086 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2131, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %2132 = torch.aten.mul.Tensor %76, %2131 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2132, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_3087 = torch.constant.none
    %none_3088 = torch.constant.none
    %int6_3089 = torch.constant.int 6
    %cpu_3090 = torch.constant.device "cpu"
    %int0_3091 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2132, %none_3087, %none_3088, %int6_3089, %cpu_3090, %int0_3091 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3092 = torch.constant.int 5
    %2133 = torch.prims.convert_element_type %2132, %int5_3092 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2133, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_3093 = torch.constant.int 2
    %2134 = torch.aten.view.dtype %77, %int2_3093 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %2135 = torch.aten.detach %2134 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_3094 = torch.constant.int -1
    %int17_3095 = torch.constant.int 17
    %2136 = torch.prim.ListConstruct %int-1_3094, %int17_3095 : (!torch.int, !torch.int) -> !torch.list<int>
    %2137 = torch.aten.view %2135, %2136 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_3096 = torch.constant.int 2048
    %int-1_3097 = torch.constant.int -1
    %int17_3098 = torch.constant.int 17
    %2138 = torch.prim.ListConstruct %int2048_3096, %int-1_3097, %int17_3098 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2139 = torch.aten.view %2137, %2138 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_3099 = torch.constant.int 2
    %int0_3100 = torch.constant.int 0
    %int1_3101 = torch.constant.int 1
    %int1_3102 = torch.constant.int 1
    %2140 = torch.aten.slice.Tensor %2139, %int2_3099, %int0_3100, %int1_3101, %int1_3102 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_3103 = torch.constant.int 5
    %2141 = torch.aten.view.dtype %2140, %int5_3103 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %2142 = torch.aten.detach %2141 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_3104 = torch.constant.int 2
    %int1_3105 = torch.constant.int 1
    %int9223372036854775807_3106 = torch.constant.int 9223372036854775807
    %int1_3107 = torch.constant.int 1
    %2143 = torch.aten.slice.Tensor %2139, %int2_3104, %int1_3105, %int9223372036854775807_3106, %int1_3107 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_3108 = torch.constant.int 1
    %2144 = torch.aten.view.dtype %2143, %int1_3108 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %2145 = torch.aten.detach %2144 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %2146 = torch_c.to_builtin_tensor %2133 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_3109 = tensor.cast %2146 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2147 = torch_c.to_builtin_tensor %2142 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %2148 = torch_c.to_builtin_tensor %2145 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %2149 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_3109, %2147, %2148) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_3110 = tensor.cast %2149 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %2150 = torch_c.from_builtin_tensor %cast_3110 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2150, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_3111 = torch.constant.int 2
    %2151 = torch.aten.view.dtype %78, %int2_3111 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %2152 = torch.aten.detach %2151 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_3112 = torch.constant.int -1
    %int17_3113 = torch.constant.int 17
    %2153 = torch.prim.ListConstruct %int-1_3112, %int17_3113 : (!torch.int, !torch.int) -> !torch.list<int>
    %2154 = torch.aten.view %2152, %2153 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_3114 = torch.constant.int 256
    %int-1_3115 = torch.constant.int -1
    %int17_3116 = torch.constant.int 17
    %2155 = torch.prim.ListConstruct %int256_3114, %int-1_3115, %int17_3116 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2156 = torch.aten.view %2154, %2155 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_3117 = torch.constant.int 2
    %int0_3118 = torch.constant.int 0
    %int1_3119 = torch.constant.int 1
    %int1_3120 = torch.constant.int 1
    %2157 = torch.aten.slice.Tensor %2156, %int2_3117, %int0_3118, %int1_3119, %int1_3120 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_3121 = torch.constant.int 5
    %2158 = torch.aten.view.dtype %2157, %int5_3121 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %2159 = torch.aten.detach %2158 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_3122 = torch.constant.int 2
    %int1_3123 = torch.constant.int 1
    %int9223372036854775807_3124 = torch.constant.int 9223372036854775807
    %int1_3125 = torch.constant.int 1
    %2160 = torch.aten.slice.Tensor %2156, %int2_3122, %int1_3123, %int9223372036854775807_3124, %int1_3125 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_3126 = torch.constant.int 1
    %2161 = torch.aten.view.dtype %2160, %int1_3126 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %2162 = torch.aten.detach %2161 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %2163 = torch_c.to_builtin_tensor %2133 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_3127 = tensor.cast %2163 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2164 = torch_c.to_builtin_tensor %2159 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %2165 = torch_c.to_builtin_tensor %2162 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %2166 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_3127, %2164, %2165) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_3128 = tensor.cast %2166 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %2167 = torch_c.from_builtin_tensor %cast_3128 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %2167, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_3129 = torch.constant.int 2
    %2168 = torch.aten.view.dtype %79, %int2_3129 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %2169 = torch.aten.detach %2168 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_3130 = torch.constant.int -1
    %int17_3131 = torch.constant.int 17
    %2170 = torch.prim.ListConstruct %int-1_3130, %int17_3131 : (!torch.int, !torch.int) -> !torch.list<int>
    %2171 = torch.aten.view %2169, %2170 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_3132 = torch.constant.int 256
    %int-1_3133 = torch.constant.int -1
    %int17_3134 = torch.constant.int 17
    %2172 = torch.prim.ListConstruct %int256_3132, %int-1_3133, %int17_3134 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2173 = torch.aten.view %2171, %2172 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_3135 = torch.constant.int 2
    %int0_3136 = torch.constant.int 0
    %int1_3137 = torch.constant.int 1
    %int1_3138 = torch.constant.int 1
    %2174 = torch.aten.slice.Tensor %2173, %int2_3135, %int0_3136, %int1_3137, %int1_3138 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_3139 = torch.constant.int 5
    %2175 = torch.aten.view.dtype %2174, %int5_3139 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %2176 = torch.aten.detach %2175 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_3140 = torch.constant.int 2
    %int1_3141 = torch.constant.int 1
    %int9223372036854775807_3142 = torch.constant.int 9223372036854775807
    %int1_3143 = torch.constant.int 1
    %2177 = torch.aten.slice.Tensor %2173, %int2_3140, %int1_3141, %int9223372036854775807_3142, %int1_3143 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_3144 = torch.constant.int 1
    %2178 = torch.aten.view.dtype %2177, %int1_3144 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %2179 = torch.aten.detach %2178 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %2180 = torch_c.to_builtin_tensor %2133 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_3145 = tensor.cast %2180 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2181 = torch_c.to_builtin_tensor %2176 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %2182 = torch_c.to_builtin_tensor %2179 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %2183 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_3145, %2181, %2182) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_3146 = tensor.cast %2183 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %2184 = torch_c.from_builtin_tensor %cast_3146 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %2184, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_3147 = torch.constant.int 4
    %int32_3148 = torch.constant.int 32
    %int64_3149 = torch.constant.int 64
    %2185 = torch.prim.ListConstruct %int4_3147, %273, %int32_3148, %int64_3149 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2186 = torch.aten.view %2150, %2185 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2186, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_3150 = torch.constant.int 4
    %int4_3151 = torch.constant.int 4
    %int64_3152 = torch.constant.int 64
    %2187 = torch.prim.ListConstruct %int4_3150, %273, %int4_3151, %int64_3152 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2188 = torch.aten.view %2167, %2187 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %2188, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_3153 = torch.constant.int 4
    %int4_3154 = torch.constant.int 4
    %int64_3155 = torch.constant.int 64
    %2189 = torch.prim.ListConstruct %int4_3153, %273, %int4_3154, %int64_3155 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2190 = torch.aten.view %2184, %2189 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %2190, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_3156 = torch.constant.int 0
    %none_3157 = torch.constant.none
    %none_3158 = torch.constant.none
    %cpu_3159 = torch.constant.device "cpu"
    %false_3160 = torch.constant.bool false
    %2191 = torch.aten.arange.start %int0_3156, %273, %none_3157, %none_3158, %cpu_3159, %false_3160 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2191, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_3161 = torch.constant.int 0
    %2192 = torch.aten.unsqueeze %2191, %int0_3161 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %2192, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_3162 = torch.constant.int 0
    %int64_3163 = torch.constant.int 64
    %int2_3164 = torch.constant.int 2
    %none_3165 = torch.constant.none
    %none_3166 = torch.constant.none
    %cpu_3167 = torch.constant.device "cpu"
    %false_3168 = torch.constant.bool false
    %2193 = torch.aten.arange.start_step %int0_3162, %int64_3163, %int2_3164, %none_3165, %none_3166, %cpu_3167, %false_3168 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_3169 = torch.constant.none
    %none_3170 = torch.constant.none
    %int4_3171 = torch.constant.int 4
    %cpu_3172 = torch.constant.device "cpu"
    %int0_3173 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2193, %none_3169, %none_3170, %int4_3171, %cpu_3172, %int0_3173 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3174 = torch.constant.int 6
    %2194 = torch.prims.convert_element_type %2193, %int6_3174 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_3175 = torch.constant.int 64
    %2195 = torch.aten.div.Scalar %2194, %int64_3175 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_3176 = torch.constant.float 1.000000e+04
    %2196 = torch.aten.pow.Scalar %float1.000000e04_3176, %2195 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %2197 = torch.aten.reciprocal %2196 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_3177 = torch.constant.float 1.000000e+00
    %2198 = torch.aten.mul.Scalar %2197, %float1.000000e00_3177 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_3178 = torch.constant.none
    %2199 = torch.aten.clone %73, %none_3178 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_3179 = torch.constant.int 0
    %2200 = torch.aten.unsqueeze %2198, %int0_3179 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_3180 = torch.constant.int 2
    %2201 = torch.aten.unsqueeze %2200, %int2_3180 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_3181 = torch.constant.none
    %none_3182 = torch.constant.none
    %int6_3183 = torch.constant.int 6
    %cpu_3184 = torch.constant.device "cpu"
    %int0_3185 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2201, %none_3181, %none_3182, %int6_3183, %cpu_3184, %int0_3185 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_3186 = torch.constant.int 1
    %int-1_3187 = torch.constant.int -1
    %int1_3188 = torch.constant.int 1
    %2202 = torch.prim.ListConstruct %int1_3186, %int-1_3187, %int1_3188 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3189 = torch.constant.bool false
    %2203 = torch.aten.expand %2201, %2202, %false_3189 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_3190 = torch.constant.int 1
    %2204 = torch.aten.unsqueeze %2192, %int1_3190 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %2204, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_3191 = torch.constant.none
    %none_3192 = torch.constant.none
    %int4_3193 = torch.constant.int 4
    %cpu_3194 = torch.constant.device "cpu"
    %int0_3195 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2204, %none_3191, %none_3192, %int4_3193, %cpu_3194, %int0_3195 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3196 = torch.constant.int 6
    %2205 = torch.prims.convert_element_type %2204, %int6_3196 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %2205, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %2206 = torch.aten.matmul %2203, %2205 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %2206, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_3197 = torch.constant.int 1
    %int2_3198 = torch.constant.int 2
    %2207 = torch.aten.transpose.int %2206, %int1_3197, %int2_3198 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2207, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %2208 = torch.aten.cos %2207 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2208, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %2209 = torch.aten.mul.Tensor %2208, %2199 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2209, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_3199 = torch.constant.none
    %none_3200 = torch.constant.none
    %int6_3201 = torch.constant.int 6
    %cpu_3202 = torch.constant.device "cpu"
    %int0_3203 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2209, %none_3199, %none_3200, %int6_3201, %cpu_3202, %int0_3203 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3204 = torch.constant.int 5
    %2210 = torch.prims.convert_element_type %2209, %int5_3204 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %2210, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %2211 = torch.aten.sin %2207 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2211, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %2212 = torch.aten.mul.Tensor %2211, %2199 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2212, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_3205 = torch.constant.none
    %none_3206 = torch.constant.none
    %int6_3207 = torch.constant.int 6
    %cpu_3208 = torch.constant.device "cpu"
    %int0_3209 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2212, %none_3205, %none_3206, %int6_3207, %cpu_3208, %int0_3209 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3210 = torch.constant.int 5
    %2213 = torch.prims.convert_element_type %2212, %int5_3210 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %2213, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_3211 = torch.constant.int 2
    %2214 = torch.aten.unsqueeze %2210, %int2_3211 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %2214, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_3212 = torch.constant.int 2
    %2215 = torch.aten.unsqueeze %2213, %int2_3212 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %2215, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_3213 = torch.constant.none
    %none_3214 = torch.constant.none
    %int5_3215 = torch.constant.int 5
    %cpu_3216 = torch.constant.device "cpu"
    %int0_3217 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2214, %none_3213, %none_3214, %int5_3215, %cpu_3216, %int0_3217 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_3218 = torch.constant.none
    %none_3219 = torch.constant.none
    %int5_3220 = torch.constant.int 5
    %cpu_3221 = torch.constant.device "cpu"
    %int0_3222 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2215, %none_3218, %none_3219, %int5_3220, %cpu_3221, %int0_3222 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_3223 = torch.constant.none
    %none_3224 = torch.constant.none
    %int5_3225 = torch.constant.int 5
    %cpu_3226 = torch.constant.device "cpu"
    %int0_3227 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2186, %none_3223, %none_3224, %int5_3225, %cpu_3226, %int0_3227 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_3228 = torch.constant.int 3
    %int0_3229 = torch.constant.int 0
    %int64_3230 = torch.constant.int 64
    %int2_3231 = torch.constant.int 2
    %2216 = torch.aten.slice.Tensor %2186, %int3_3228, %int0_3229, %int64_3230, %int2_3231 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2216, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_3232 = torch.constant.int 3
    %int1_3233 = torch.constant.int 1
    %int64_3234 = torch.constant.int 64
    %int2_3235 = torch.constant.int 2
    %2217 = torch.aten.slice.Tensor %2186, %int3_3232, %int1_3233, %int64_3234, %int2_3235 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2217, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %2218 = torch.aten.mul.Tensor %2216, %2214 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2218, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %2219 = torch.aten.mul.Tensor %2217, %2215 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2219, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_3236 = torch.constant.int 1
    %2220 = torch.aten.sub.Tensor %2218, %2219, %int1_3236 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2220, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %2221 = torch.aten.mul.Tensor %2217, %2214 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2221, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %2222 = torch.aten.mul.Tensor %2216, %2215 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2222, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_3237 = torch.constant.int 1
    %2223 = torch.aten.add.Tensor %2221, %2222, %int1_3237 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2223, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %2224 = torch_c.to_builtin_tensor %2220 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_3238 = tensor.cast %2224 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %2225 = torch_c.to_builtin_tensor %2223 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_3239 = tensor.cast %2225 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %2226 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_3238, %cast_3239) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_3240 = tensor.cast %2226 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %2227 = torch_c.from_builtin_tensor %cast_3240 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %2227, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_3241 = torch.constant.int 4
    %int32_3242 = torch.constant.int 32
    %int64_3243 = torch.constant.int 64
    %2228 = torch.prim.ListConstruct %int4_3241, %273, %int32_3242, %int64_3243 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2229 = torch.aten.view %2227, %2228 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2229, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_3244 = torch.constant.none
    %none_3245 = torch.constant.none
    %int5_3246 = torch.constant.int 5
    %cpu_3247 = torch.constant.device "cpu"
    %int0_3248 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2229, %none_3244, %none_3245, %int5_3246, %cpu_3247, %int0_3248 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_3249 = torch.constant.int 0
    %none_3250 = torch.constant.none
    %none_3251 = torch.constant.none
    %cpu_3252 = torch.constant.device "cpu"
    %false_3253 = torch.constant.bool false
    %2230 = torch.aten.arange.start %int0_3249, %273, %none_3250, %none_3251, %cpu_3252, %false_3253 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2230, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_3254 = torch.constant.int 0
    %2231 = torch.aten.unsqueeze %2230, %int0_3254 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %2231, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_3255 = torch.constant.int 0
    %int64_3256 = torch.constant.int 64
    %int2_3257 = torch.constant.int 2
    %none_3258 = torch.constant.none
    %none_3259 = torch.constant.none
    %cpu_3260 = torch.constant.device "cpu"
    %false_3261 = torch.constant.bool false
    %2232 = torch.aten.arange.start_step %int0_3255, %int64_3256, %int2_3257, %none_3258, %none_3259, %cpu_3260, %false_3261 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_3262 = torch.constant.none
    %none_3263 = torch.constant.none
    %int4_3264 = torch.constant.int 4
    %cpu_3265 = torch.constant.device "cpu"
    %int0_3266 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2232, %none_3262, %none_3263, %int4_3264, %cpu_3265, %int0_3266 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3267 = torch.constant.int 6
    %2233 = torch.prims.convert_element_type %2232, %int6_3267 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_3268 = torch.constant.int 64
    %2234 = torch.aten.div.Scalar %2233, %int64_3268 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_3269 = torch.constant.float 1.000000e+04
    %2235 = torch.aten.pow.Scalar %float1.000000e04_3269, %2234 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %2236 = torch.aten.reciprocal %2235 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_3270 = torch.constant.float 1.000000e+00
    %2237 = torch.aten.mul.Scalar %2236, %float1.000000e00_3270 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_3271 = torch.constant.none
    %2238 = torch.aten.clone %74, %none_3271 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_3272 = torch.constant.int 0
    %2239 = torch.aten.unsqueeze %2237, %int0_3272 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_3273 = torch.constant.int 2
    %2240 = torch.aten.unsqueeze %2239, %int2_3273 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_3274 = torch.constant.none
    %none_3275 = torch.constant.none
    %int6_3276 = torch.constant.int 6
    %cpu_3277 = torch.constant.device "cpu"
    %int0_3278 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2240, %none_3274, %none_3275, %int6_3276, %cpu_3277, %int0_3278 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_3279 = torch.constant.int 1
    %int-1_3280 = torch.constant.int -1
    %int1_3281 = torch.constant.int 1
    %2241 = torch.prim.ListConstruct %int1_3279, %int-1_3280, %int1_3281 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3282 = torch.constant.bool false
    %2242 = torch.aten.expand %2240, %2241, %false_3282 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_3283 = torch.constant.int 1
    %2243 = torch.aten.unsqueeze %2231, %int1_3283 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %2243, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_3284 = torch.constant.none
    %none_3285 = torch.constant.none
    %int4_3286 = torch.constant.int 4
    %cpu_3287 = torch.constant.device "cpu"
    %int0_3288 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2243, %none_3284, %none_3285, %int4_3286, %cpu_3287, %int0_3288 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3289 = torch.constant.int 6
    %2244 = torch.prims.convert_element_type %2243, %int6_3289 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %2244, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %2245 = torch.aten.matmul %2242, %2244 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %2245, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_3290 = torch.constant.int 1
    %int2_3291 = torch.constant.int 2
    %2246 = torch.aten.transpose.int %2245, %int1_3290, %int2_3291 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2246, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %2247 = torch.aten.cos %2246 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2247, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %2248 = torch.aten.mul.Tensor %2247, %2238 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2248, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_3292 = torch.constant.none
    %none_3293 = torch.constant.none
    %int6_3294 = torch.constant.int 6
    %cpu_3295 = torch.constant.device "cpu"
    %int0_3296 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2248, %none_3292, %none_3293, %int6_3294, %cpu_3295, %int0_3296 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3297 = torch.constant.int 5
    %2249 = torch.prims.convert_element_type %2248, %int5_3297 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %2249, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %2250 = torch.aten.sin %2246 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2250, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %2251 = torch.aten.mul.Tensor %2250, %2238 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2251, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_3298 = torch.constant.none
    %none_3299 = torch.constant.none
    %int6_3300 = torch.constant.int 6
    %cpu_3301 = torch.constant.device "cpu"
    %int0_3302 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2251, %none_3298, %none_3299, %int6_3300, %cpu_3301, %int0_3302 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3303 = torch.constant.int 5
    %2252 = torch.prims.convert_element_type %2251, %int5_3303 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %2252, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_3304 = torch.constant.int 2
    %2253 = torch.aten.unsqueeze %2249, %int2_3304 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %2253, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_3305 = torch.constant.int 2
    %2254 = torch.aten.unsqueeze %2252, %int2_3305 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %2254, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_3306 = torch.constant.none
    %none_3307 = torch.constant.none
    %int5_3308 = torch.constant.int 5
    %cpu_3309 = torch.constant.device "cpu"
    %int0_3310 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2253, %none_3306, %none_3307, %int5_3308, %cpu_3309, %int0_3310 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_3311 = torch.constant.none
    %none_3312 = torch.constant.none
    %int5_3313 = torch.constant.int 5
    %cpu_3314 = torch.constant.device "cpu"
    %int0_3315 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2254, %none_3311, %none_3312, %int5_3313, %cpu_3314, %int0_3315 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_3316 = torch.constant.none
    %none_3317 = torch.constant.none
    %int5_3318 = torch.constant.int 5
    %cpu_3319 = torch.constant.device "cpu"
    %int0_3320 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2188, %none_3316, %none_3317, %int5_3318, %cpu_3319, %int0_3320 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_3321 = torch.constant.int 3
    %int0_3322 = torch.constant.int 0
    %int64_3323 = torch.constant.int 64
    %int2_3324 = torch.constant.int 2
    %2255 = torch.aten.slice.Tensor %2188, %int3_3321, %int0_3322, %int64_3323, %int2_3324 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2255, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_3325 = torch.constant.int 3
    %int1_3326 = torch.constant.int 1
    %int64_3327 = torch.constant.int 64
    %int2_3328 = torch.constant.int 2
    %2256 = torch.aten.slice.Tensor %2188, %int3_3325, %int1_3326, %int64_3327, %int2_3328 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2256, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %2257 = torch.aten.mul.Tensor %2255, %2253 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2257, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %2258 = torch.aten.mul.Tensor %2256, %2254 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2258, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_3329 = torch.constant.int 1
    %2259 = torch.aten.sub.Tensor %2257, %2258, %int1_3329 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2259, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %2260 = torch.aten.mul.Tensor %2256, %2253 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2260, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %2261 = torch.aten.mul.Tensor %2255, %2254 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2261, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_3330 = torch.constant.int 1
    %2262 = torch.aten.add.Tensor %2260, %2261, %int1_3330 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2262, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %2263 = torch_c.to_builtin_tensor %2259 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_3331 = tensor.cast %2263 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %2264 = torch_c.to_builtin_tensor %2262 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_3332 = tensor.cast %2264 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %2265 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_3331, %cast_3332) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_3333 = tensor.cast %2265 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %2266 = torch_c.from_builtin_tensor %cast_3333 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %2266, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_3334 = torch.constant.int 4
    %int4_3335 = torch.constant.int 4
    %int64_3336 = torch.constant.int 64
    %2267 = torch.prim.ListConstruct %int4_3334, %273, %int4_3335, %int64_3336 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2268 = torch.aten.view %2266, %2267 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %2268, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_3337 = torch.constant.none
    %none_3338 = torch.constant.none
    %int5_3339 = torch.constant.int 5
    %cpu_3340 = torch.constant.device "cpu"
    %int0_3341 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2268, %none_3337, %none_3338, %int5_3339, %cpu_3340, %int0_3341 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_3342 = torch.constant.int 22
    %2269 = torch.aten.mul.Scalar %arg2, %int22_3342 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2269, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int6_3343 = torch.constant.int 6
    %int1_3344 = torch.constant.int 1
    %2270 = torch.aten.add.Scalar %2269, %int6_3343, %int1_3344 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2270, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_3345 = torch.constant.int 2
    %2271 = torch.aten.mul.Scalar %2270, %int2_3345 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2271, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_3346 = torch.constant.int 0
    %int1_3347 = torch.constant.int 1
    %2272 = torch.aten.add.Scalar %2271, %int0_3346, %int1_3347 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2272, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %2273 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %2274 = torch.aten.view %2272, %2273 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2274, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_3348 = torch.constant.int 4
    %int32_3349 = torch.constant.int 32
    %int4_3350 = torch.constant.int 4
    %int64_3351 = torch.constant.int 64
    %2275 = torch.prim.ListConstruct %int4_3348, %271, %int32_3349, %int4_3350, %int64_3351 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2276 = torch.aten.view %2268, %2275 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %2276, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_3352 = torch.constant.int 32
    %int4_3353 = torch.constant.int 4
    %int64_3354 = torch.constant.int 64
    %2277 = torch.prim.ListConstruct %446, %int32_3352, %int4_3353, %int64_3354 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2278 = torch.aten.view %2276, %2277 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %2278, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_3355 = torch.constant.int 1
    %int2_3356 = torch.constant.int 2
    %2279 = torch.aten.transpose.int %2278, %int1_3355, %int2_3356 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2279, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_3357 = torch.constant.none
    %none_3358 = torch.constant.none
    %int5_3359 = torch.constant.int 5
    %cpu_3360 = torch.constant.device "cpu"
    %int0_3361 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2279, %none_3357, %none_3358, %int5_3359, %cpu_3360, %int0_3361 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_3362 = torch.constant.int 22
    %int2_3363 = torch.constant.int 2
    %int4_3364 = torch.constant.int 4
    %int32_3365 = torch.constant.int 32
    %int64_3366 = torch.constant.int 64
    %2280 = torch.prim.ListConstruct %272, %int22_3362, %int2_3363, %int4_3364, %int32_3365, %int64_3366 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2281 = torch.aten.view %2005, %2280 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2281, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_3367 = torch.constant.int 4
    %int32_3368 = torch.constant.int 32
    %int64_3369 = torch.constant.int 64
    %2282 = torch.prim.ListConstruct %439, %int4_3367, %int32_3368, %int64_3369 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2283 = torch.aten.view %2281, %2282 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2283, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %2284 = torch.prim.ListConstruct %2274 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_3370 = torch.constant.bool false
    %2285 = torch.aten.index_put %2283, %2284, %2279, %false_3370 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2285, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_3371 = torch.constant.int 22
    %int2_3372 = torch.constant.int 2
    %int4_3373 = torch.constant.int 4
    %int32_3374 = torch.constant.int 32
    %int64_3375 = torch.constant.int 64
    %2286 = torch.prim.ListConstruct %272, %int22_3371, %int2_3372, %int4_3373, %int32_3374, %int64_3375 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2287 = torch.aten.view %2285, %2286 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2287, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_3376 = torch.constant.int 360448
    %2288 = torch.prim.ListConstruct %272, %int360448_3376 : (!torch.int, !torch.int) -> !torch.list<int>
    %2289 = torch.aten.view %2287, %2288 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %2289, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_3377 = torch.constant.int 22
    %int2_3378 = torch.constant.int 2
    %int4_3379 = torch.constant.int 4
    %int32_3380 = torch.constant.int 32
    %int64_3381 = torch.constant.int 64
    %2290 = torch.prim.ListConstruct %272, %int22_3377, %int2_3378, %int4_3379, %int32_3380, %int64_3381 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2291 = torch.aten.view %2289, %2290 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2291, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_3382 = torch.constant.int 4
    %int32_3383 = torch.constant.int 32
    %int64_3384 = torch.constant.int 64
    %2292 = torch.prim.ListConstruct %439, %int4_3382, %int32_3383, %int64_3384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2293 = torch.aten.view %2291, %2292 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2293, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_3385 = torch.constant.int 22
    %2294 = torch.aten.mul.Scalar %arg2, %int22_3385 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2294, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int6_3386 = torch.constant.int 6
    %int1_3387 = torch.constant.int 1
    %2295 = torch.aten.add.Scalar %2294, %int6_3386, %int1_3387 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2295, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_3388 = torch.constant.int 2
    %2296 = torch.aten.mul.Scalar %2295, %int2_3388 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2296, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_3389 = torch.constant.int 1
    %int1_3390 = torch.constant.int 1
    %2297 = torch.aten.add.Scalar %2296, %int1_3389, %int1_3390 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2297, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %2298 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %2299 = torch.aten.view %2297, %2298 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2299, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_3391 = torch.constant.int 4
    %int32_3392 = torch.constant.int 32
    %int4_3393 = torch.constant.int 4
    %int64_3394 = torch.constant.int 64
    %2300 = torch.prim.ListConstruct %int4_3391, %271, %int32_3392, %int4_3393, %int64_3394 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2301 = torch.aten.view %2190, %2300 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %2301, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_3395 = torch.constant.int 32
    %int4_3396 = torch.constant.int 4
    %int64_3397 = torch.constant.int 64
    %2302 = torch.prim.ListConstruct %446, %int32_3395, %int4_3396, %int64_3397 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2303 = torch.aten.view %2301, %2302 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %2303, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_3398 = torch.constant.int 1
    %int2_3399 = torch.constant.int 2
    %2304 = torch.aten.transpose.int %2303, %int1_3398, %int2_3399 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2304, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_3400 = torch.constant.none
    %none_3401 = torch.constant.none
    %int5_3402 = torch.constant.int 5
    %cpu_3403 = torch.constant.device "cpu"
    %int0_3404 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2304, %none_3400, %none_3401, %int5_3402, %cpu_3403, %int0_3404 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %2305 = torch.prim.ListConstruct %2299 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_3405 = torch.constant.bool false
    %2306 = torch.aten.index_put %2293, %2305, %2304, %false_3405 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2306, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_3406 = torch.constant.int 22
    %int2_3407 = torch.constant.int 2
    %int4_3408 = torch.constant.int 4
    %int32_3409 = torch.constant.int 32
    %int64_3410 = torch.constant.int 64
    %2307 = torch.prim.ListConstruct %272, %int22_3406, %int2_3407, %int4_3408, %int32_3409, %int64_3410 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2308 = torch.aten.view %2306, %2307 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2308, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_3411 = torch.constant.int 360448
    %2309 = torch.prim.ListConstruct %272, %int360448_3411 : (!torch.int, !torch.int) -> !torch.list<int>
    %2310 = torch.aten.view %2308, %2309 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %2310, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_3412 = torch.constant.int 0
    %int1_3413 = torch.constant.int 1
    %none_3414 = torch.constant.none
    %none_3415 = torch.constant.none
    %cpu_3416 = torch.constant.device "cpu"
    %false_3417 = torch.constant.bool false
    %2311 = torch.aten.arange.start_step %int0_3412, %273, %int1_3413, %none_3414, %none_3415, %cpu_3416, %false_3417 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2311, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_3418 = torch.constant.int -1
    %2312 = torch.aten.unsqueeze %arg1, %int-1_3418 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %2313 = torch.aten.ge.Tensor %2311, %2312 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %2313, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_3419 = torch.constant.none
    %none_3420 = torch.constant.none
    %cpu_3421 = torch.constant.device "cpu"
    %false_3422 = torch.constant.bool false
    %2314 = torch.aten.arange %273, %none_3419, %none_3420, %cpu_3421, %false_3422 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2314, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_3423 = torch.constant.int 0
    %2315 = torch.aten.unsqueeze %2314, %int0_3423 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %2315, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_3424 = torch.constant.int 1
    %2316 = torch.aten.unsqueeze %2315, %int1_3424 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %2316, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_3425 = torch.constant.int 2
    %2317 = torch.aten.unsqueeze %2316, %int2_3425 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %2317, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_3426 = torch.constant.none
    %none_3427 = torch.constant.none
    %cpu_3428 = torch.constant.device "cpu"
    %false_3429 = torch.constant.bool false
    %2318 = torch.aten.arange %273, %none_3426, %none_3427, %cpu_3428, %false_3429 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2318, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_3430 = torch.constant.int 0
    %2319 = torch.aten.unsqueeze %2318, %int0_3430 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %2319, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_3431 = torch.constant.int 1
    %2320 = torch.aten.unsqueeze %2319, %int1_3431 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %2320, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_3432 = torch.constant.int 3
    %2321 = torch.aten.unsqueeze %2320, %int3_3432 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %2321, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %2322 = torch.aten.gt.Tensor %2317, %2321 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %2322, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_3433 = torch.constant.int 1
    %2323 = torch.aten.unsqueeze %2313, %int1_3433 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %2323, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_3434 = torch.constant.int 2
    %2324 = torch.aten.unsqueeze %2323, %int2_3434 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %2324, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %2325 = torch.aten.logical_or %2322, %2324 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %2325, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_3435 = torch.constant.none
    %2326 = torch.aten.clone %75, %none_3435 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_3436 = torch.constant.int 0
    %2327 = torch.aten.where.ScalarOther %2325, %2326, %int0_3436 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %2327, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_3437 = torch.constant.none
    %none_3438 = torch.constant.none
    %int5_3439 = torch.constant.int 5
    %cpu_3440 = torch.constant.device "cpu"
    %int0_3441 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2327, %none_3437, %none_3438, %int5_3439, %cpu_3440, %int0_3441 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_3442 = torch.constant.int -2
    %2328 = torch.aten.unsqueeze %2268, %int-2_3442 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %2328, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_3443 = torch.constant.int 4
    %int4_3444 = torch.constant.int 4
    %int8_3445 = torch.constant.int 8
    %int64_3446 = torch.constant.int 64
    %2329 = torch.prim.ListConstruct %int4_3443, %273, %int4_3444, %int8_3445, %int64_3446 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3447 = torch.constant.bool false
    %2330 = torch.aten.expand %2328, %2329, %false_3447 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2330, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_3448 = torch.constant.int 0
    %2331 = torch.aten.clone %2330, %int0_3448 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2331, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_3449 = torch.constant.int 4
    %int32_3450 = torch.constant.int 32
    %int64_3451 = torch.constant.int 64
    %2332 = torch.prim.ListConstruct %int4_3449, %273, %int32_3450, %int64_3451 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2333 = torch.aten._unsafe_view %2331, %2332 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2333, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_3452 = torch.constant.int -2
    %2334 = torch.aten.unsqueeze %2190, %int-2_3452 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %2334, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_3453 = torch.constant.int 4
    %int4_3454 = torch.constant.int 4
    %int8_3455 = torch.constant.int 8
    %int64_3456 = torch.constant.int 64
    %2335 = torch.prim.ListConstruct %int4_3453, %273, %int4_3454, %int8_3455, %int64_3456 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3457 = torch.constant.bool false
    %2336 = torch.aten.expand %2334, %2335, %false_3457 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2336, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_3458 = torch.constant.int 0
    %2337 = torch.aten.clone %2336, %int0_3458 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2337, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_3459 = torch.constant.int 4
    %int32_3460 = torch.constant.int 32
    %int64_3461 = torch.constant.int 64
    %2338 = torch.prim.ListConstruct %int4_3459, %273, %int32_3460, %int64_3461 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2339 = torch.aten._unsafe_view %2337, %2338 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2339, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_3462 = torch.constant.int 1
    %int2_3463 = torch.constant.int 2
    %2340 = torch.aten.transpose.int %2229, %int1_3462, %int2_3463 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2340, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_3464 = torch.constant.int 1
    %int2_3465 = torch.constant.int 2
    %2341 = torch.aten.transpose.int %2333, %int1_3464, %int2_3465 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2341, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_3466 = torch.constant.int 1
    %int2_3467 = torch.constant.int 2
    %2342 = torch.aten.transpose.int %2339, %int1_3466, %int2_3467 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2342, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_3468 = torch.constant.float 0.000000e+00
    %false_3469 = torch.constant.bool false
    %none_3470 = torch.constant.none
    %false_3471 = torch.constant.bool false
    %2343 = torch.aten.scaled_dot_product_attention %2340, %2341, %2342, %2327, %float0.000000e00_3468, %false_3469, %none_3470, %false_3471 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2343, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_3472 = torch.constant.int 1
    %int2_3473 = torch.constant.int 2
    %2344 = torch.aten.transpose.int %2343, %int1_3472, %int2_3473 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2344, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_3474 = torch.constant.int 4
    %int2048_3475 = torch.constant.int 2048
    %2345 = torch.prim.ListConstruct %int4_3474, %273, %int2048_3475 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2346 = torch.aten.view %2344, %2345 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2346, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_3476 = torch.constant.int 2
    %2347 = torch.aten.view.dtype %80, %int2_3476 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %2348 = torch.aten.detach %2347 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_3477 = torch.constant.int -1
    %int17_3478 = torch.constant.int 17
    %2349 = torch.prim.ListConstruct %int-1_3477, %int17_3478 : (!torch.int, !torch.int) -> !torch.list<int>
    %2350 = torch.aten.view %2348, %2349 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_3479 = torch.constant.int 2048
    %int-1_3480 = torch.constant.int -1
    %int17_3481 = torch.constant.int 17
    %2351 = torch.prim.ListConstruct %int2048_3479, %int-1_3480, %int17_3481 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2352 = torch.aten.view %2350, %2351 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_3482 = torch.constant.int 2
    %int0_3483 = torch.constant.int 0
    %int1_3484 = torch.constant.int 1
    %int1_3485 = torch.constant.int 1
    %2353 = torch.aten.slice.Tensor %2352, %int2_3482, %int0_3483, %int1_3484, %int1_3485 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_3486 = torch.constant.int 5
    %2354 = torch.aten.view.dtype %2353, %int5_3486 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %2355 = torch.aten.detach %2354 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_3487 = torch.constant.int 2
    %int1_3488 = torch.constant.int 1
    %int9223372036854775807_3489 = torch.constant.int 9223372036854775807
    %int1_3490 = torch.constant.int 1
    %2356 = torch.aten.slice.Tensor %2352, %int2_3487, %int1_3488, %int9223372036854775807_3489, %int1_3490 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_3491 = torch.constant.int 1
    %2357 = torch.aten.view.dtype %2356, %int1_3491 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %2358 = torch.aten.detach %2357 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %2359 = torch_c.to_builtin_tensor %2346 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_3492 = tensor.cast %2359 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2360 = torch_c.to_builtin_tensor %2355 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %2361 = torch_c.to_builtin_tensor %2358 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %2362 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_3492, %2360, %2361) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_3493 = tensor.cast %2362 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %2363 = torch_c.from_builtin_tensor %cast_3493 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2363, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_3494 = torch.constant.none
    %none_3495 = torch.constant.none
    %int5_3496 = torch.constant.int 5
    %cpu_3497 = torch.constant.device "cpu"
    %int0_3498 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2363, %none_3494, %none_3495, %int5_3496, %cpu_3497, %int0_3498 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_3499 = torch.constant.int 1
    %2364 = torch.aten.add.Tensor %2123, %2363, %int1_3499 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2364, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_3500 = torch.constant.none
    %none_3501 = torch.constant.none
    %int5_3502 = torch.constant.int 5
    %cpu_3503 = torch.constant.device "cpu"
    %int0_3504 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2364, %none_3500, %none_3501, %int5_3502, %cpu_3503, %int0_3504 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3505 = torch.constant.int 6
    %2365 = torch.prims.convert_element_type %2364, %int6_3505 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2365, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_3506 = torch.constant.int 2
    %2366 = torch.aten.pow.Tensor_Scalar %2365, %int2_3506 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2366, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_3507 = torch.constant.int -1
    %2367 = torch.prim.ListConstruct %int-1_3507 : (!torch.int) -> !torch.list<int>
    %true_3508 = torch.constant.bool true
    %none_3509 = torch.constant.none
    %2368 = torch.aten.mean.dim %2366, %2367, %true_3508, %none_3509 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2368, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_3510 = torch.constant.float 9.9999997473787516E-6
    %int1_3511 = torch.constant.int 1
    %2369 = torch.aten.add.Scalar %2368, %float9.999990e-06_3510, %int1_3511 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2369, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2370 = torch.aten.rsqrt %2369 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2370, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2371 = torch.aten.mul.Tensor %2365, %2370 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2371, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_3512 = torch.constant.none
    %none_3513 = torch.constant.none
    %int6_3514 = torch.constant.int 6
    %cpu_3515 = torch.constant.device "cpu"
    %int0_3516 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2371, %none_3512, %none_3513, %int6_3514, %cpu_3515, %int0_3516 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3517 = torch.constant.int 5
    %2372 = torch.prims.convert_element_type %2371, %int5_3517 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2372, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %2373 = torch.aten.mul.Tensor %81, %2372 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2373, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_3518 = torch.constant.none
    %none_3519 = torch.constant.none
    %int6_3520 = torch.constant.int 6
    %cpu_3521 = torch.constant.device "cpu"
    %int0_3522 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2373, %none_3518, %none_3519, %int6_3520, %cpu_3521, %int0_3522 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3523 = torch.constant.int 5
    %2374 = torch.prims.convert_element_type %2373, %int5_3523 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2374, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_3524 = torch.constant.int 2
    %2375 = torch.aten.view.dtype %82, %int2_3524 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %2376 = torch.aten.detach %2375 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_3525 = torch.constant.int -1
    %int17_3526 = torch.constant.int 17
    %2377 = torch.prim.ListConstruct %int-1_3525, %int17_3526 : (!torch.int, !torch.int) -> !torch.list<int>
    %2378 = torch.aten.view %2376, %2377 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_3527 = torch.constant.int 5632
    %int-1_3528 = torch.constant.int -1
    %int17_3529 = torch.constant.int 17
    %2379 = torch.prim.ListConstruct %int5632_3527, %int-1_3528, %int17_3529 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2380 = torch.aten.view %2378, %2379 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_3530 = torch.constant.int 2
    %int0_3531 = torch.constant.int 0
    %int1_3532 = torch.constant.int 1
    %int1_3533 = torch.constant.int 1
    %2381 = torch.aten.slice.Tensor %2380, %int2_3530, %int0_3531, %int1_3532, %int1_3533 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_3534 = torch.constant.int 5
    %2382 = torch.aten.view.dtype %2381, %int5_3534 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %2383 = torch.aten.detach %2382 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_3535 = torch.constant.int 2
    %int1_3536 = torch.constant.int 1
    %int9223372036854775807_3537 = torch.constant.int 9223372036854775807
    %int1_3538 = torch.constant.int 1
    %2384 = torch.aten.slice.Tensor %2380, %int2_3535, %int1_3536, %int9223372036854775807_3537, %int1_3538 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_3539 = torch.constant.int 1
    %2385 = torch.aten.view.dtype %2384, %int1_3539 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %2386 = torch.aten.detach %2385 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %2387 = torch_c.to_builtin_tensor %2374 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_3540 = tensor.cast %2387 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2388 = torch_c.to_builtin_tensor %2383 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %2389 = torch_c.to_builtin_tensor %2386 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %2390 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_3540, %2388, %2389) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_3541 = tensor.cast %2390 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %2391 = torch_c.from_builtin_tensor %cast_3541 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %2391, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %2392 = torch.aten.silu %2391 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %2392, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_3542 = torch.constant.int 2
    %2393 = torch.aten.view.dtype %83, %int2_3542 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %2394 = torch.aten.detach %2393 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_3543 = torch.constant.int -1
    %int17_3544 = torch.constant.int 17
    %2395 = torch.prim.ListConstruct %int-1_3543, %int17_3544 : (!torch.int, !torch.int) -> !torch.list<int>
    %2396 = torch.aten.view %2394, %2395 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_3545 = torch.constant.int 5632
    %int-1_3546 = torch.constant.int -1
    %int17_3547 = torch.constant.int 17
    %2397 = torch.prim.ListConstruct %int5632_3545, %int-1_3546, %int17_3547 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2398 = torch.aten.view %2396, %2397 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_3548 = torch.constant.int 2
    %int0_3549 = torch.constant.int 0
    %int1_3550 = torch.constant.int 1
    %int1_3551 = torch.constant.int 1
    %2399 = torch.aten.slice.Tensor %2398, %int2_3548, %int0_3549, %int1_3550, %int1_3551 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_3552 = torch.constant.int 5
    %2400 = torch.aten.view.dtype %2399, %int5_3552 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %2401 = torch.aten.detach %2400 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_3553 = torch.constant.int 2
    %int1_3554 = torch.constant.int 1
    %int9223372036854775807_3555 = torch.constant.int 9223372036854775807
    %int1_3556 = torch.constant.int 1
    %2402 = torch.aten.slice.Tensor %2398, %int2_3553, %int1_3554, %int9223372036854775807_3555, %int1_3556 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_3557 = torch.constant.int 1
    %2403 = torch.aten.view.dtype %2402, %int1_3557 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %2404 = torch.aten.detach %2403 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %2405 = torch_c.to_builtin_tensor %2374 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_3558 = tensor.cast %2405 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2406 = torch_c.to_builtin_tensor %2401 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %2407 = torch_c.to_builtin_tensor %2404 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %2408 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_3558, %2406, %2407) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_3559 = tensor.cast %2408 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %2409 = torch_c.from_builtin_tensor %cast_3559 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %2409, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %2410 = torch.aten.mul.Tensor %2392, %2409 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %2410, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_3560 = torch.constant.int 2
    %2411 = torch.aten.view.dtype %84, %int2_3560 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %2412 = torch.aten.detach %2411 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_3561 = torch.constant.int -1
    %int17_3562 = torch.constant.int 17
    %2413 = torch.prim.ListConstruct %int-1_3561, %int17_3562 : (!torch.int, !torch.int) -> !torch.list<int>
    %2414 = torch.aten.view %2412, %2413 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_3563 = torch.constant.int 2048
    %int-1_3564 = torch.constant.int -1
    %int17_3565 = torch.constant.int 17
    %2415 = torch.prim.ListConstruct %int2048_3563, %int-1_3564, %int17_3565 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2416 = torch.aten.view %2414, %2415 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_3566 = torch.constant.int 2
    %int0_3567 = torch.constant.int 0
    %int1_3568 = torch.constant.int 1
    %int1_3569 = torch.constant.int 1
    %2417 = torch.aten.slice.Tensor %2416, %int2_3566, %int0_3567, %int1_3568, %int1_3569 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_3570 = torch.constant.int 5
    %2418 = torch.aten.view.dtype %2417, %int5_3570 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %2419 = torch.aten.detach %2418 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_3571 = torch.constant.int 2
    %int1_3572 = torch.constant.int 1
    %int9223372036854775807_3573 = torch.constant.int 9223372036854775807
    %int1_3574 = torch.constant.int 1
    %2420 = torch.aten.slice.Tensor %2416, %int2_3571, %int1_3572, %int9223372036854775807_3573, %int1_3574 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_3575 = torch.constant.int 1
    %2421 = torch.aten.view.dtype %2420, %int1_3575 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %2422 = torch.aten.detach %2421 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %2423 = torch_c.to_builtin_tensor %2410 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_3576 = tensor.cast %2423 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %2424 = torch_c.to_builtin_tensor %2419 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %2425 = torch_c.to_builtin_tensor %2422 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %2426 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_3576, %2424, %2425) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_3577 = tensor.cast %2426 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %2427 = torch_c.from_builtin_tensor %cast_3577 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2427, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_3578 = torch.constant.int 1
    %2428 = torch.aten.add.Tensor %2364, %2427, %int1_3578 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2428, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_3579 = torch.constant.none
    %none_3580 = torch.constant.none
    %int5_3581 = torch.constant.int 5
    %cpu_3582 = torch.constant.device "cpu"
    %int0_3583 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2428, %none_3579, %none_3580, %int5_3581, %cpu_3582, %int0_3583 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3584 = torch.constant.int 6
    %2429 = torch.prims.convert_element_type %2428, %int6_3584 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2429, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_3585 = torch.constant.int 2
    %2430 = torch.aten.pow.Tensor_Scalar %2429, %int2_3585 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2430, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_3586 = torch.constant.int -1
    %2431 = torch.prim.ListConstruct %int-1_3586 : (!torch.int) -> !torch.list<int>
    %true_3587 = torch.constant.bool true
    %none_3588 = torch.constant.none
    %2432 = torch.aten.mean.dim %2430, %2431, %true_3587, %none_3588 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2432, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_3589 = torch.constant.float 9.9999997473787516E-6
    %int1_3590 = torch.constant.int 1
    %2433 = torch.aten.add.Scalar %2432, %float9.999990e-06_3589, %int1_3590 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2433, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2434 = torch.aten.rsqrt %2433 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2434, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2435 = torch.aten.mul.Tensor %2429, %2434 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2435, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_3591 = torch.constant.none
    %none_3592 = torch.constant.none
    %int6_3593 = torch.constant.int 6
    %cpu_3594 = torch.constant.device "cpu"
    %int0_3595 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2435, %none_3591, %none_3592, %int6_3593, %cpu_3594, %int0_3595 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3596 = torch.constant.int 5
    %2436 = torch.prims.convert_element_type %2435, %int5_3596 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2436, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %2437 = torch.aten.mul.Tensor %88, %2436 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2437, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_3597 = torch.constant.none
    %none_3598 = torch.constant.none
    %int6_3599 = torch.constant.int 6
    %cpu_3600 = torch.constant.device "cpu"
    %int0_3601 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2437, %none_3597, %none_3598, %int6_3599, %cpu_3600, %int0_3601 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3602 = torch.constant.int 5
    %2438 = torch.prims.convert_element_type %2437, %int5_3602 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2438, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_3603 = torch.constant.int 2
    %2439 = torch.aten.view.dtype %89, %int2_3603 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %2440 = torch.aten.detach %2439 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_3604 = torch.constant.int -1
    %int17_3605 = torch.constant.int 17
    %2441 = torch.prim.ListConstruct %int-1_3604, %int17_3605 : (!torch.int, !torch.int) -> !torch.list<int>
    %2442 = torch.aten.view %2440, %2441 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_3606 = torch.constant.int 2048
    %int-1_3607 = torch.constant.int -1
    %int17_3608 = torch.constant.int 17
    %2443 = torch.prim.ListConstruct %int2048_3606, %int-1_3607, %int17_3608 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2444 = torch.aten.view %2442, %2443 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_3609 = torch.constant.int 2
    %int0_3610 = torch.constant.int 0
    %int1_3611 = torch.constant.int 1
    %int1_3612 = torch.constant.int 1
    %2445 = torch.aten.slice.Tensor %2444, %int2_3609, %int0_3610, %int1_3611, %int1_3612 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_3613 = torch.constant.int 5
    %2446 = torch.aten.view.dtype %2445, %int5_3613 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %2447 = torch.aten.detach %2446 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_3614 = torch.constant.int 2
    %int1_3615 = torch.constant.int 1
    %int9223372036854775807_3616 = torch.constant.int 9223372036854775807
    %int1_3617 = torch.constant.int 1
    %2448 = torch.aten.slice.Tensor %2444, %int2_3614, %int1_3615, %int9223372036854775807_3616, %int1_3617 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_3618 = torch.constant.int 1
    %2449 = torch.aten.view.dtype %2448, %int1_3618 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %2450 = torch.aten.detach %2449 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %2451 = torch_c.to_builtin_tensor %2438 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_3619 = tensor.cast %2451 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2452 = torch_c.to_builtin_tensor %2447 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %2453 = torch_c.to_builtin_tensor %2450 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %2454 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_3619, %2452, %2453) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_3620 = tensor.cast %2454 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %2455 = torch_c.from_builtin_tensor %cast_3620 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2455, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_3621 = torch.constant.int 2
    %2456 = torch.aten.view.dtype %90, %int2_3621 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %2457 = torch.aten.detach %2456 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_3622 = torch.constant.int -1
    %int17_3623 = torch.constant.int 17
    %2458 = torch.prim.ListConstruct %int-1_3622, %int17_3623 : (!torch.int, !torch.int) -> !torch.list<int>
    %2459 = torch.aten.view %2457, %2458 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_3624 = torch.constant.int 256
    %int-1_3625 = torch.constant.int -1
    %int17_3626 = torch.constant.int 17
    %2460 = torch.prim.ListConstruct %int256_3624, %int-1_3625, %int17_3626 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2461 = torch.aten.view %2459, %2460 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_3627 = torch.constant.int 2
    %int0_3628 = torch.constant.int 0
    %int1_3629 = torch.constant.int 1
    %int1_3630 = torch.constant.int 1
    %2462 = torch.aten.slice.Tensor %2461, %int2_3627, %int0_3628, %int1_3629, %int1_3630 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_3631 = torch.constant.int 5
    %2463 = torch.aten.view.dtype %2462, %int5_3631 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %2464 = torch.aten.detach %2463 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_3632 = torch.constant.int 2
    %int1_3633 = torch.constant.int 1
    %int9223372036854775807_3634 = torch.constant.int 9223372036854775807
    %int1_3635 = torch.constant.int 1
    %2465 = torch.aten.slice.Tensor %2461, %int2_3632, %int1_3633, %int9223372036854775807_3634, %int1_3635 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_3636 = torch.constant.int 1
    %2466 = torch.aten.view.dtype %2465, %int1_3636 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %2467 = torch.aten.detach %2466 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %2468 = torch_c.to_builtin_tensor %2438 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_3637 = tensor.cast %2468 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2469 = torch_c.to_builtin_tensor %2464 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %2470 = torch_c.to_builtin_tensor %2467 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %2471 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_3637, %2469, %2470) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_3638 = tensor.cast %2471 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %2472 = torch_c.from_builtin_tensor %cast_3638 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %2472, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_3639 = torch.constant.int 2
    %2473 = torch.aten.view.dtype %91, %int2_3639 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %2474 = torch.aten.detach %2473 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_3640 = torch.constant.int -1
    %int17_3641 = torch.constant.int 17
    %2475 = torch.prim.ListConstruct %int-1_3640, %int17_3641 : (!torch.int, !torch.int) -> !torch.list<int>
    %2476 = torch.aten.view %2474, %2475 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_3642 = torch.constant.int 256
    %int-1_3643 = torch.constant.int -1
    %int17_3644 = torch.constant.int 17
    %2477 = torch.prim.ListConstruct %int256_3642, %int-1_3643, %int17_3644 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2478 = torch.aten.view %2476, %2477 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_3645 = torch.constant.int 2
    %int0_3646 = torch.constant.int 0
    %int1_3647 = torch.constant.int 1
    %int1_3648 = torch.constant.int 1
    %2479 = torch.aten.slice.Tensor %2478, %int2_3645, %int0_3646, %int1_3647, %int1_3648 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_3649 = torch.constant.int 5
    %2480 = torch.aten.view.dtype %2479, %int5_3649 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %2481 = torch.aten.detach %2480 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_3650 = torch.constant.int 2
    %int1_3651 = torch.constant.int 1
    %int9223372036854775807_3652 = torch.constant.int 9223372036854775807
    %int1_3653 = torch.constant.int 1
    %2482 = torch.aten.slice.Tensor %2478, %int2_3650, %int1_3651, %int9223372036854775807_3652, %int1_3653 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_3654 = torch.constant.int 1
    %2483 = torch.aten.view.dtype %2482, %int1_3654 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %2484 = torch.aten.detach %2483 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %2485 = torch_c.to_builtin_tensor %2438 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_3655 = tensor.cast %2485 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2486 = torch_c.to_builtin_tensor %2481 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %2487 = torch_c.to_builtin_tensor %2484 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %2488 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_3655, %2486, %2487) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_3656 = tensor.cast %2488 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %2489 = torch_c.from_builtin_tensor %cast_3656 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %2489, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_3657 = torch.constant.int 4
    %int32_3658 = torch.constant.int 32
    %int64_3659 = torch.constant.int 64
    %2490 = torch.prim.ListConstruct %int4_3657, %273, %int32_3658, %int64_3659 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2491 = torch.aten.view %2455, %2490 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2491, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_3660 = torch.constant.int 4
    %int4_3661 = torch.constant.int 4
    %int64_3662 = torch.constant.int 64
    %2492 = torch.prim.ListConstruct %int4_3660, %273, %int4_3661, %int64_3662 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2493 = torch.aten.view %2472, %2492 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %2493, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_3663 = torch.constant.int 4
    %int4_3664 = torch.constant.int 4
    %int64_3665 = torch.constant.int 64
    %2494 = torch.prim.ListConstruct %int4_3663, %273, %int4_3664, %int64_3665 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2495 = torch.aten.view %2489, %2494 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %2495, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_3666 = torch.constant.int 0
    %none_3667 = torch.constant.none
    %none_3668 = torch.constant.none
    %cpu_3669 = torch.constant.device "cpu"
    %false_3670 = torch.constant.bool false
    %2496 = torch.aten.arange.start %int0_3666, %273, %none_3667, %none_3668, %cpu_3669, %false_3670 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2496, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_3671 = torch.constant.int 0
    %2497 = torch.aten.unsqueeze %2496, %int0_3671 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %2497, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_3672 = torch.constant.int 0
    %int64_3673 = torch.constant.int 64
    %int2_3674 = torch.constant.int 2
    %none_3675 = torch.constant.none
    %none_3676 = torch.constant.none
    %cpu_3677 = torch.constant.device "cpu"
    %false_3678 = torch.constant.bool false
    %2498 = torch.aten.arange.start_step %int0_3672, %int64_3673, %int2_3674, %none_3675, %none_3676, %cpu_3677, %false_3678 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_3679 = torch.constant.none
    %none_3680 = torch.constant.none
    %int4_3681 = torch.constant.int 4
    %cpu_3682 = torch.constant.device "cpu"
    %int0_3683 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2498, %none_3679, %none_3680, %int4_3681, %cpu_3682, %int0_3683 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3684 = torch.constant.int 6
    %2499 = torch.prims.convert_element_type %2498, %int6_3684 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_3685 = torch.constant.int 64
    %2500 = torch.aten.div.Scalar %2499, %int64_3685 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_3686 = torch.constant.float 1.000000e+04
    %2501 = torch.aten.pow.Scalar %float1.000000e04_3686, %2500 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %2502 = torch.aten.reciprocal %2501 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_3687 = torch.constant.float 1.000000e+00
    %2503 = torch.aten.mul.Scalar %2502, %float1.000000e00_3687 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_3688 = torch.constant.none
    %2504 = torch.aten.clone %85, %none_3688 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_3689 = torch.constant.int 0
    %2505 = torch.aten.unsqueeze %2503, %int0_3689 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_3690 = torch.constant.int 2
    %2506 = torch.aten.unsqueeze %2505, %int2_3690 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_3691 = torch.constant.none
    %none_3692 = torch.constant.none
    %int6_3693 = torch.constant.int 6
    %cpu_3694 = torch.constant.device "cpu"
    %int0_3695 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2506, %none_3691, %none_3692, %int6_3693, %cpu_3694, %int0_3695 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_3696 = torch.constant.int 1
    %int-1_3697 = torch.constant.int -1
    %int1_3698 = torch.constant.int 1
    %2507 = torch.prim.ListConstruct %int1_3696, %int-1_3697, %int1_3698 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3699 = torch.constant.bool false
    %2508 = torch.aten.expand %2506, %2507, %false_3699 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_3700 = torch.constant.int 1
    %2509 = torch.aten.unsqueeze %2497, %int1_3700 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %2509, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_3701 = torch.constant.none
    %none_3702 = torch.constant.none
    %int4_3703 = torch.constant.int 4
    %cpu_3704 = torch.constant.device "cpu"
    %int0_3705 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2509, %none_3701, %none_3702, %int4_3703, %cpu_3704, %int0_3705 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3706 = torch.constant.int 6
    %2510 = torch.prims.convert_element_type %2509, %int6_3706 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %2510, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %2511 = torch.aten.matmul %2508, %2510 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %2511, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_3707 = torch.constant.int 1
    %int2_3708 = torch.constant.int 2
    %2512 = torch.aten.transpose.int %2511, %int1_3707, %int2_3708 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2512, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %2513 = torch.aten.cos %2512 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2513, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %2514 = torch.aten.mul.Tensor %2513, %2504 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2514, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_3709 = torch.constant.none
    %none_3710 = torch.constant.none
    %int6_3711 = torch.constant.int 6
    %cpu_3712 = torch.constant.device "cpu"
    %int0_3713 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2514, %none_3709, %none_3710, %int6_3711, %cpu_3712, %int0_3713 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3714 = torch.constant.int 5
    %2515 = torch.prims.convert_element_type %2514, %int5_3714 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %2515, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %2516 = torch.aten.sin %2512 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2516, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %2517 = torch.aten.mul.Tensor %2516, %2504 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2517, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_3715 = torch.constant.none
    %none_3716 = torch.constant.none
    %int6_3717 = torch.constant.int 6
    %cpu_3718 = torch.constant.device "cpu"
    %int0_3719 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2517, %none_3715, %none_3716, %int6_3717, %cpu_3718, %int0_3719 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3720 = torch.constant.int 5
    %2518 = torch.prims.convert_element_type %2517, %int5_3720 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %2518, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_3721 = torch.constant.int 2
    %2519 = torch.aten.unsqueeze %2515, %int2_3721 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %2519, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_3722 = torch.constant.int 2
    %2520 = torch.aten.unsqueeze %2518, %int2_3722 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %2520, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_3723 = torch.constant.none
    %none_3724 = torch.constant.none
    %int5_3725 = torch.constant.int 5
    %cpu_3726 = torch.constant.device "cpu"
    %int0_3727 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2519, %none_3723, %none_3724, %int5_3725, %cpu_3726, %int0_3727 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_3728 = torch.constant.none
    %none_3729 = torch.constant.none
    %int5_3730 = torch.constant.int 5
    %cpu_3731 = torch.constant.device "cpu"
    %int0_3732 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2520, %none_3728, %none_3729, %int5_3730, %cpu_3731, %int0_3732 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_3733 = torch.constant.none
    %none_3734 = torch.constant.none
    %int5_3735 = torch.constant.int 5
    %cpu_3736 = torch.constant.device "cpu"
    %int0_3737 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2491, %none_3733, %none_3734, %int5_3735, %cpu_3736, %int0_3737 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_3738 = torch.constant.int 3
    %int0_3739 = torch.constant.int 0
    %int64_3740 = torch.constant.int 64
    %int2_3741 = torch.constant.int 2
    %2521 = torch.aten.slice.Tensor %2491, %int3_3738, %int0_3739, %int64_3740, %int2_3741 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2521, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_3742 = torch.constant.int 3
    %int1_3743 = torch.constant.int 1
    %int64_3744 = torch.constant.int 64
    %int2_3745 = torch.constant.int 2
    %2522 = torch.aten.slice.Tensor %2491, %int3_3742, %int1_3743, %int64_3744, %int2_3745 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2522, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %2523 = torch.aten.mul.Tensor %2521, %2519 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2523, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %2524 = torch.aten.mul.Tensor %2522, %2520 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2524, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_3746 = torch.constant.int 1
    %2525 = torch.aten.sub.Tensor %2523, %2524, %int1_3746 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2525, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %2526 = torch.aten.mul.Tensor %2522, %2519 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2526, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %2527 = torch.aten.mul.Tensor %2521, %2520 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2527, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_3747 = torch.constant.int 1
    %2528 = torch.aten.add.Tensor %2526, %2527, %int1_3747 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2528, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %2529 = torch_c.to_builtin_tensor %2525 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_3748 = tensor.cast %2529 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %2530 = torch_c.to_builtin_tensor %2528 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_3749 = tensor.cast %2530 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %2531 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_3748, %cast_3749) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_3750 = tensor.cast %2531 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %2532 = torch_c.from_builtin_tensor %cast_3750 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %2532, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_3751 = torch.constant.int 4
    %int32_3752 = torch.constant.int 32
    %int64_3753 = torch.constant.int 64
    %2533 = torch.prim.ListConstruct %int4_3751, %273, %int32_3752, %int64_3753 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2534 = torch.aten.view %2532, %2533 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2534, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_3754 = torch.constant.none
    %none_3755 = torch.constant.none
    %int5_3756 = torch.constant.int 5
    %cpu_3757 = torch.constant.device "cpu"
    %int0_3758 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2534, %none_3754, %none_3755, %int5_3756, %cpu_3757, %int0_3758 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_3759 = torch.constant.int 0
    %none_3760 = torch.constant.none
    %none_3761 = torch.constant.none
    %cpu_3762 = torch.constant.device "cpu"
    %false_3763 = torch.constant.bool false
    %2535 = torch.aten.arange.start %int0_3759, %273, %none_3760, %none_3761, %cpu_3762, %false_3763 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2535, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_3764 = torch.constant.int 0
    %2536 = torch.aten.unsqueeze %2535, %int0_3764 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %2536, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_3765 = torch.constant.int 0
    %int64_3766 = torch.constant.int 64
    %int2_3767 = torch.constant.int 2
    %none_3768 = torch.constant.none
    %none_3769 = torch.constant.none
    %cpu_3770 = torch.constant.device "cpu"
    %false_3771 = torch.constant.bool false
    %2537 = torch.aten.arange.start_step %int0_3765, %int64_3766, %int2_3767, %none_3768, %none_3769, %cpu_3770, %false_3771 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_3772 = torch.constant.none
    %none_3773 = torch.constant.none
    %int4_3774 = torch.constant.int 4
    %cpu_3775 = torch.constant.device "cpu"
    %int0_3776 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2537, %none_3772, %none_3773, %int4_3774, %cpu_3775, %int0_3776 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3777 = torch.constant.int 6
    %2538 = torch.prims.convert_element_type %2537, %int6_3777 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_3778 = torch.constant.int 64
    %2539 = torch.aten.div.Scalar %2538, %int64_3778 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_3779 = torch.constant.float 1.000000e+04
    %2540 = torch.aten.pow.Scalar %float1.000000e04_3779, %2539 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %2541 = torch.aten.reciprocal %2540 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_3780 = torch.constant.float 1.000000e+00
    %2542 = torch.aten.mul.Scalar %2541, %float1.000000e00_3780 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_3781 = torch.constant.none
    %2543 = torch.aten.clone %86, %none_3781 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_3782 = torch.constant.int 0
    %2544 = torch.aten.unsqueeze %2542, %int0_3782 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_3783 = torch.constant.int 2
    %2545 = torch.aten.unsqueeze %2544, %int2_3783 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_3784 = torch.constant.none
    %none_3785 = torch.constant.none
    %int6_3786 = torch.constant.int 6
    %cpu_3787 = torch.constant.device "cpu"
    %int0_3788 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2545, %none_3784, %none_3785, %int6_3786, %cpu_3787, %int0_3788 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_3789 = torch.constant.int 1
    %int-1_3790 = torch.constant.int -1
    %int1_3791 = torch.constant.int 1
    %2546 = torch.prim.ListConstruct %int1_3789, %int-1_3790, %int1_3791 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3792 = torch.constant.bool false
    %2547 = torch.aten.expand %2545, %2546, %false_3792 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_3793 = torch.constant.int 1
    %2548 = torch.aten.unsqueeze %2536, %int1_3793 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %2548, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_3794 = torch.constant.none
    %none_3795 = torch.constant.none
    %int4_3796 = torch.constant.int 4
    %cpu_3797 = torch.constant.device "cpu"
    %int0_3798 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2548, %none_3794, %none_3795, %int4_3796, %cpu_3797, %int0_3798 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3799 = torch.constant.int 6
    %2549 = torch.prims.convert_element_type %2548, %int6_3799 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %2549, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %2550 = torch.aten.matmul %2547, %2549 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %2550, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_3800 = torch.constant.int 1
    %int2_3801 = torch.constant.int 2
    %2551 = torch.aten.transpose.int %2550, %int1_3800, %int2_3801 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2551, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %2552 = torch.aten.cos %2551 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2552, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %2553 = torch.aten.mul.Tensor %2552, %2543 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2553, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_3802 = torch.constant.none
    %none_3803 = torch.constant.none
    %int6_3804 = torch.constant.int 6
    %cpu_3805 = torch.constant.device "cpu"
    %int0_3806 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2553, %none_3802, %none_3803, %int6_3804, %cpu_3805, %int0_3806 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3807 = torch.constant.int 5
    %2554 = torch.prims.convert_element_type %2553, %int5_3807 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %2554, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %2555 = torch.aten.sin %2551 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2555, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %2556 = torch.aten.mul.Tensor %2555, %2543 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2556, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_3808 = torch.constant.none
    %none_3809 = torch.constant.none
    %int6_3810 = torch.constant.int 6
    %cpu_3811 = torch.constant.device "cpu"
    %int0_3812 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2556, %none_3808, %none_3809, %int6_3810, %cpu_3811, %int0_3812 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3813 = torch.constant.int 5
    %2557 = torch.prims.convert_element_type %2556, %int5_3813 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %2557, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_3814 = torch.constant.int 2
    %2558 = torch.aten.unsqueeze %2554, %int2_3814 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %2558, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_3815 = torch.constant.int 2
    %2559 = torch.aten.unsqueeze %2557, %int2_3815 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %2559, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_3816 = torch.constant.none
    %none_3817 = torch.constant.none
    %int5_3818 = torch.constant.int 5
    %cpu_3819 = torch.constant.device "cpu"
    %int0_3820 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2558, %none_3816, %none_3817, %int5_3818, %cpu_3819, %int0_3820 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_3821 = torch.constant.none
    %none_3822 = torch.constant.none
    %int5_3823 = torch.constant.int 5
    %cpu_3824 = torch.constant.device "cpu"
    %int0_3825 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2559, %none_3821, %none_3822, %int5_3823, %cpu_3824, %int0_3825 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_3826 = torch.constant.none
    %none_3827 = torch.constant.none
    %int5_3828 = torch.constant.int 5
    %cpu_3829 = torch.constant.device "cpu"
    %int0_3830 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2493, %none_3826, %none_3827, %int5_3828, %cpu_3829, %int0_3830 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_3831 = torch.constant.int 3
    %int0_3832 = torch.constant.int 0
    %int64_3833 = torch.constant.int 64
    %int2_3834 = torch.constant.int 2
    %2560 = torch.aten.slice.Tensor %2493, %int3_3831, %int0_3832, %int64_3833, %int2_3834 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2560, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_3835 = torch.constant.int 3
    %int1_3836 = torch.constant.int 1
    %int64_3837 = torch.constant.int 64
    %int2_3838 = torch.constant.int 2
    %2561 = torch.aten.slice.Tensor %2493, %int3_3835, %int1_3836, %int64_3837, %int2_3838 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2561, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %2562 = torch.aten.mul.Tensor %2560, %2558 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2562, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %2563 = torch.aten.mul.Tensor %2561, %2559 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2563, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_3839 = torch.constant.int 1
    %2564 = torch.aten.sub.Tensor %2562, %2563, %int1_3839 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2564, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %2565 = torch.aten.mul.Tensor %2561, %2558 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2565, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %2566 = torch.aten.mul.Tensor %2560, %2559 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2566, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_3840 = torch.constant.int 1
    %2567 = torch.aten.add.Tensor %2565, %2566, %int1_3840 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2567, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %2568 = torch_c.to_builtin_tensor %2564 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_3841 = tensor.cast %2568 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %2569 = torch_c.to_builtin_tensor %2567 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_3842 = tensor.cast %2569 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %2570 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_3841, %cast_3842) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_3843 = tensor.cast %2570 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %2571 = torch_c.from_builtin_tensor %cast_3843 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %2571, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_3844 = torch.constant.int 4
    %int4_3845 = torch.constant.int 4
    %int64_3846 = torch.constant.int 64
    %2572 = torch.prim.ListConstruct %int4_3844, %273, %int4_3845, %int64_3846 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2573 = torch.aten.view %2571, %2572 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %2573, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_3847 = torch.constant.none
    %none_3848 = torch.constant.none
    %int5_3849 = torch.constant.int 5
    %cpu_3850 = torch.constant.device "cpu"
    %int0_3851 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2573, %none_3847, %none_3848, %int5_3849, %cpu_3850, %int0_3851 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_3852 = torch.constant.int 22
    %2574 = torch.aten.mul.Scalar %arg2, %int22_3852 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2574, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int7 = torch.constant.int 7
    %int1_3853 = torch.constant.int 1
    %2575 = torch.aten.add.Scalar %2574, %int7, %int1_3853 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2575, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_3854 = torch.constant.int 2
    %2576 = torch.aten.mul.Scalar %2575, %int2_3854 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2576, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_3855 = torch.constant.int 0
    %int1_3856 = torch.constant.int 1
    %2577 = torch.aten.add.Scalar %2576, %int0_3855, %int1_3856 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2577, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %2578 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %2579 = torch.aten.view %2577, %2578 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2579, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_3857 = torch.constant.int 4
    %int32_3858 = torch.constant.int 32
    %int4_3859 = torch.constant.int 4
    %int64_3860 = torch.constant.int 64
    %2580 = torch.prim.ListConstruct %int4_3857, %271, %int32_3858, %int4_3859, %int64_3860 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2581 = torch.aten.view %2573, %2580 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %2581, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_3861 = torch.constant.int 32
    %int4_3862 = torch.constant.int 4
    %int64_3863 = torch.constant.int 64
    %2582 = torch.prim.ListConstruct %446, %int32_3861, %int4_3862, %int64_3863 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2583 = torch.aten.view %2581, %2582 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %2583, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_3864 = torch.constant.int 1
    %int2_3865 = torch.constant.int 2
    %2584 = torch.aten.transpose.int %2583, %int1_3864, %int2_3865 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2584, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_3866 = torch.constant.none
    %none_3867 = torch.constant.none
    %int5_3868 = torch.constant.int 5
    %cpu_3869 = torch.constant.device "cpu"
    %int0_3870 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2584, %none_3866, %none_3867, %int5_3868, %cpu_3869, %int0_3870 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_3871 = torch.constant.int 22
    %int2_3872 = torch.constant.int 2
    %int4_3873 = torch.constant.int 4
    %int32_3874 = torch.constant.int 32
    %int64_3875 = torch.constant.int 64
    %2585 = torch.prim.ListConstruct %272, %int22_3871, %int2_3872, %int4_3873, %int32_3874, %int64_3875 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2586 = torch.aten.view %2310, %2585 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2586, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_3876 = torch.constant.int 4
    %int32_3877 = torch.constant.int 32
    %int64_3878 = torch.constant.int 64
    %2587 = torch.prim.ListConstruct %439, %int4_3876, %int32_3877, %int64_3878 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2588 = torch.aten.view %2586, %2587 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2588, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %2589 = torch.prim.ListConstruct %2579 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_3879 = torch.constant.bool false
    %2590 = torch.aten.index_put %2588, %2589, %2584, %false_3879 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2590, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_3880 = torch.constant.int 22
    %int2_3881 = torch.constant.int 2
    %int4_3882 = torch.constant.int 4
    %int32_3883 = torch.constant.int 32
    %int64_3884 = torch.constant.int 64
    %2591 = torch.prim.ListConstruct %272, %int22_3880, %int2_3881, %int4_3882, %int32_3883, %int64_3884 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2592 = torch.aten.view %2590, %2591 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2592, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_3885 = torch.constant.int 360448
    %2593 = torch.prim.ListConstruct %272, %int360448_3885 : (!torch.int, !torch.int) -> !torch.list<int>
    %2594 = torch.aten.view %2592, %2593 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %2594, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_3886 = torch.constant.int 22
    %int2_3887 = torch.constant.int 2
    %int4_3888 = torch.constant.int 4
    %int32_3889 = torch.constant.int 32
    %int64_3890 = torch.constant.int 64
    %2595 = torch.prim.ListConstruct %272, %int22_3886, %int2_3887, %int4_3888, %int32_3889, %int64_3890 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2596 = torch.aten.view %2594, %2595 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2596, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_3891 = torch.constant.int 4
    %int32_3892 = torch.constant.int 32
    %int64_3893 = torch.constant.int 64
    %2597 = torch.prim.ListConstruct %439, %int4_3891, %int32_3892, %int64_3893 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2598 = torch.aten.view %2596, %2597 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2598, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_3894 = torch.constant.int 22
    %2599 = torch.aten.mul.Scalar %arg2, %int22_3894 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2599, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int7_3895 = torch.constant.int 7
    %int1_3896 = torch.constant.int 1
    %2600 = torch.aten.add.Scalar %2599, %int7_3895, %int1_3896 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2600, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_3897 = torch.constant.int 2
    %2601 = torch.aten.mul.Scalar %2600, %int2_3897 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2601, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_3898 = torch.constant.int 1
    %int1_3899 = torch.constant.int 1
    %2602 = torch.aten.add.Scalar %2601, %int1_3898, %int1_3899 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2602, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %2603 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %2604 = torch.aten.view %2602, %2603 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2604, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_3900 = torch.constant.int 4
    %int32_3901 = torch.constant.int 32
    %int4_3902 = torch.constant.int 4
    %int64_3903 = torch.constant.int 64
    %2605 = torch.prim.ListConstruct %int4_3900, %271, %int32_3901, %int4_3902, %int64_3903 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2606 = torch.aten.view %2495, %2605 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %2606, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_3904 = torch.constant.int 32
    %int4_3905 = torch.constant.int 4
    %int64_3906 = torch.constant.int 64
    %2607 = torch.prim.ListConstruct %446, %int32_3904, %int4_3905, %int64_3906 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2608 = torch.aten.view %2606, %2607 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %2608, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_3907 = torch.constant.int 1
    %int2_3908 = torch.constant.int 2
    %2609 = torch.aten.transpose.int %2608, %int1_3907, %int2_3908 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2609, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_3909 = torch.constant.none
    %none_3910 = torch.constant.none
    %int5_3911 = torch.constant.int 5
    %cpu_3912 = torch.constant.device "cpu"
    %int0_3913 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2609, %none_3909, %none_3910, %int5_3911, %cpu_3912, %int0_3913 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %2610 = torch.prim.ListConstruct %2604 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_3914 = torch.constant.bool false
    %2611 = torch.aten.index_put %2598, %2610, %2609, %false_3914 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2611, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_3915 = torch.constant.int 22
    %int2_3916 = torch.constant.int 2
    %int4_3917 = torch.constant.int 4
    %int32_3918 = torch.constant.int 32
    %int64_3919 = torch.constant.int 64
    %2612 = torch.prim.ListConstruct %272, %int22_3915, %int2_3916, %int4_3917, %int32_3918, %int64_3919 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2613 = torch.aten.view %2611, %2612 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2613, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_3920 = torch.constant.int 360448
    %2614 = torch.prim.ListConstruct %272, %int360448_3920 : (!torch.int, !torch.int) -> !torch.list<int>
    %2615 = torch.aten.view %2613, %2614 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %2615, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_3921 = torch.constant.int 0
    %int1_3922 = torch.constant.int 1
    %none_3923 = torch.constant.none
    %none_3924 = torch.constant.none
    %cpu_3925 = torch.constant.device "cpu"
    %false_3926 = torch.constant.bool false
    %2616 = torch.aten.arange.start_step %int0_3921, %273, %int1_3922, %none_3923, %none_3924, %cpu_3925, %false_3926 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2616, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_3927 = torch.constant.int -1
    %2617 = torch.aten.unsqueeze %arg1, %int-1_3927 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %2618 = torch.aten.ge.Tensor %2616, %2617 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %2618, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_3928 = torch.constant.none
    %none_3929 = torch.constant.none
    %cpu_3930 = torch.constant.device "cpu"
    %false_3931 = torch.constant.bool false
    %2619 = torch.aten.arange %273, %none_3928, %none_3929, %cpu_3930, %false_3931 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2619, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_3932 = torch.constant.int 0
    %2620 = torch.aten.unsqueeze %2619, %int0_3932 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %2620, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_3933 = torch.constant.int 1
    %2621 = torch.aten.unsqueeze %2620, %int1_3933 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %2621, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_3934 = torch.constant.int 2
    %2622 = torch.aten.unsqueeze %2621, %int2_3934 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %2622, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_3935 = torch.constant.none
    %none_3936 = torch.constant.none
    %cpu_3937 = torch.constant.device "cpu"
    %false_3938 = torch.constant.bool false
    %2623 = torch.aten.arange %273, %none_3935, %none_3936, %cpu_3937, %false_3938 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2623, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_3939 = torch.constant.int 0
    %2624 = torch.aten.unsqueeze %2623, %int0_3939 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %2624, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_3940 = torch.constant.int 1
    %2625 = torch.aten.unsqueeze %2624, %int1_3940 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %2625, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_3941 = torch.constant.int 3
    %2626 = torch.aten.unsqueeze %2625, %int3_3941 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %2626, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %2627 = torch.aten.gt.Tensor %2622, %2626 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %2627, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_3942 = torch.constant.int 1
    %2628 = torch.aten.unsqueeze %2618, %int1_3942 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %2628, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_3943 = torch.constant.int 2
    %2629 = torch.aten.unsqueeze %2628, %int2_3943 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %2629, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %2630 = torch.aten.logical_or %2627, %2629 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %2630, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_3944 = torch.constant.none
    %2631 = torch.aten.clone %87, %none_3944 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_3945 = torch.constant.int 0
    %2632 = torch.aten.where.ScalarOther %2630, %2631, %int0_3945 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %2632, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_3946 = torch.constant.none
    %none_3947 = torch.constant.none
    %int5_3948 = torch.constant.int 5
    %cpu_3949 = torch.constant.device "cpu"
    %int0_3950 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2632, %none_3946, %none_3947, %int5_3948, %cpu_3949, %int0_3950 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_3951 = torch.constant.int -2
    %2633 = torch.aten.unsqueeze %2573, %int-2_3951 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %2633, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_3952 = torch.constant.int 4
    %int4_3953 = torch.constant.int 4
    %int8_3954 = torch.constant.int 8
    %int64_3955 = torch.constant.int 64
    %2634 = torch.prim.ListConstruct %int4_3952, %273, %int4_3953, %int8_3954, %int64_3955 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3956 = torch.constant.bool false
    %2635 = torch.aten.expand %2633, %2634, %false_3956 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2635, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_3957 = torch.constant.int 0
    %2636 = torch.aten.clone %2635, %int0_3957 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2636, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_3958 = torch.constant.int 4
    %int32_3959 = torch.constant.int 32
    %int64_3960 = torch.constant.int 64
    %2637 = torch.prim.ListConstruct %int4_3958, %273, %int32_3959, %int64_3960 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2638 = torch.aten._unsafe_view %2636, %2637 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2638, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_3961 = torch.constant.int -2
    %2639 = torch.aten.unsqueeze %2495, %int-2_3961 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %2639, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_3962 = torch.constant.int 4
    %int4_3963 = torch.constant.int 4
    %int8_3964 = torch.constant.int 8
    %int64_3965 = torch.constant.int 64
    %2640 = torch.prim.ListConstruct %int4_3962, %273, %int4_3963, %int8_3964, %int64_3965 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3966 = torch.constant.bool false
    %2641 = torch.aten.expand %2639, %2640, %false_3966 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2641, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_3967 = torch.constant.int 0
    %2642 = torch.aten.clone %2641, %int0_3967 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2642, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_3968 = torch.constant.int 4
    %int32_3969 = torch.constant.int 32
    %int64_3970 = torch.constant.int 64
    %2643 = torch.prim.ListConstruct %int4_3968, %273, %int32_3969, %int64_3970 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2644 = torch.aten._unsafe_view %2642, %2643 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2644, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_3971 = torch.constant.int 1
    %int2_3972 = torch.constant.int 2
    %2645 = torch.aten.transpose.int %2534, %int1_3971, %int2_3972 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2645, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_3973 = torch.constant.int 1
    %int2_3974 = torch.constant.int 2
    %2646 = torch.aten.transpose.int %2638, %int1_3973, %int2_3974 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2646, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_3975 = torch.constant.int 1
    %int2_3976 = torch.constant.int 2
    %2647 = torch.aten.transpose.int %2644, %int1_3975, %int2_3976 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2647, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_3977 = torch.constant.float 0.000000e+00
    %false_3978 = torch.constant.bool false
    %none_3979 = torch.constant.none
    %false_3980 = torch.constant.bool false
    %2648 = torch.aten.scaled_dot_product_attention %2645, %2646, %2647, %2632, %float0.000000e00_3977, %false_3978, %none_3979, %false_3980 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2648, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_3981 = torch.constant.int 1
    %int2_3982 = torch.constant.int 2
    %2649 = torch.aten.transpose.int %2648, %int1_3981, %int2_3982 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2649, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_3983 = torch.constant.int 4
    %int2048_3984 = torch.constant.int 2048
    %2650 = torch.prim.ListConstruct %int4_3983, %273, %int2048_3984 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2651 = torch.aten.view %2649, %2650 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2651, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_3985 = torch.constant.int 2
    %2652 = torch.aten.view.dtype %92, %int2_3985 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %2653 = torch.aten.detach %2652 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_3986 = torch.constant.int -1
    %int17_3987 = torch.constant.int 17
    %2654 = torch.prim.ListConstruct %int-1_3986, %int17_3987 : (!torch.int, !torch.int) -> !torch.list<int>
    %2655 = torch.aten.view %2653, %2654 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_3988 = torch.constant.int 2048
    %int-1_3989 = torch.constant.int -1
    %int17_3990 = torch.constant.int 17
    %2656 = torch.prim.ListConstruct %int2048_3988, %int-1_3989, %int17_3990 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2657 = torch.aten.view %2655, %2656 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_3991 = torch.constant.int 2
    %int0_3992 = torch.constant.int 0
    %int1_3993 = torch.constant.int 1
    %int1_3994 = torch.constant.int 1
    %2658 = torch.aten.slice.Tensor %2657, %int2_3991, %int0_3992, %int1_3993, %int1_3994 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_3995 = torch.constant.int 5
    %2659 = torch.aten.view.dtype %2658, %int5_3995 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %2660 = torch.aten.detach %2659 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_3996 = torch.constant.int 2
    %int1_3997 = torch.constant.int 1
    %int9223372036854775807_3998 = torch.constant.int 9223372036854775807
    %int1_3999 = torch.constant.int 1
    %2661 = torch.aten.slice.Tensor %2657, %int2_3996, %int1_3997, %int9223372036854775807_3998, %int1_3999 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_4000 = torch.constant.int 1
    %2662 = torch.aten.view.dtype %2661, %int1_4000 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %2663 = torch.aten.detach %2662 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %2664 = torch_c.to_builtin_tensor %2651 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_4001 = tensor.cast %2664 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2665 = torch_c.to_builtin_tensor %2660 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %2666 = torch_c.to_builtin_tensor %2663 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %2667 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_4001, %2665, %2666) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_4002 = tensor.cast %2667 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %2668 = torch_c.from_builtin_tensor %cast_4002 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2668, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_4003 = torch.constant.none
    %none_4004 = torch.constant.none
    %int5_4005 = torch.constant.int 5
    %cpu_4006 = torch.constant.device "cpu"
    %int0_4007 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2668, %none_4003, %none_4004, %int5_4005, %cpu_4006, %int0_4007 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_4008 = torch.constant.int 1
    %2669 = torch.aten.add.Tensor %2428, %2668, %int1_4008 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2669, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_4009 = torch.constant.none
    %none_4010 = torch.constant.none
    %int5_4011 = torch.constant.int 5
    %cpu_4012 = torch.constant.device "cpu"
    %int0_4013 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2669, %none_4009, %none_4010, %int5_4011, %cpu_4012, %int0_4013 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4014 = torch.constant.int 6
    %2670 = torch.prims.convert_element_type %2669, %int6_4014 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2670, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_4015 = torch.constant.int 2
    %2671 = torch.aten.pow.Tensor_Scalar %2670, %int2_4015 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2671, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_4016 = torch.constant.int -1
    %2672 = torch.prim.ListConstruct %int-1_4016 : (!torch.int) -> !torch.list<int>
    %true_4017 = torch.constant.bool true
    %none_4018 = torch.constant.none
    %2673 = torch.aten.mean.dim %2671, %2672, %true_4017, %none_4018 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2673, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_4019 = torch.constant.float 9.9999997473787516E-6
    %int1_4020 = torch.constant.int 1
    %2674 = torch.aten.add.Scalar %2673, %float9.999990e-06_4019, %int1_4020 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2674, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2675 = torch.aten.rsqrt %2674 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2675, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2676 = torch.aten.mul.Tensor %2670, %2675 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2676, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_4021 = torch.constant.none
    %none_4022 = torch.constant.none
    %int6_4023 = torch.constant.int 6
    %cpu_4024 = torch.constant.device "cpu"
    %int0_4025 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2676, %none_4021, %none_4022, %int6_4023, %cpu_4024, %int0_4025 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4026 = torch.constant.int 5
    %2677 = torch.prims.convert_element_type %2676, %int5_4026 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2677, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %2678 = torch.aten.mul.Tensor %93, %2677 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2678, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_4027 = torch.constant.none
    %none_4028 = torch.constant.none
    %int6_4029 = torch.constant.int 6
    %cpu_4030 = torch.constant.device "cpu"
    %int0_4031 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2678, %none_4027, %none_4028, %int6_4029, %cpu_4030, %int0_4031 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4032 = torch.constant.int 5
    %2679 = torch.prims.convert_element_type %2678, %int5_4032 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2679, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_4033 = torch.constant.int 2
    %2680 = torch.aten.view.dtype %94, %int2_4033 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %2681 = torch.aten.detach %2680 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_4034 = torch.constant.int -1
    %int17_4035 = torch.constant.int 17
    %2682 = torch.prim.ListConstruct %int-1_4034, %int17_4035 : (!torch.int, !torch.int) -> !torch.list<int>
    %2683 = torch.aten.view %2681, %2682 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_4036 = torch.constant.int 5632
    %int-1_4037 = torch.constant.int -1
    %int17_4038 = torch.constant.int 17
    %2684 = torch.prim.ListConstruct %int5632_4036, %int-1_4037, %int17_4038 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2685 = torch.aten.view %2683, %2684 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_4039 = torch.constant.int 2
    %int0_4040 = torch.constant.int 0
    %int1_4041 = torch.constant.int 1
    %int1_4042 = torch.constant.int 1
    %2686 = torch.aten.slice.Tensor %2685, %int2_4039, %int0_4040, %int1_4041, %int1_4042 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_4043 = torch.constant.int 5
    %2687 = torch.aten.view.dtype %2686, %int5_4043 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %2688 = torch.aten.detach %2687 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_4044 = torch.constant.int 2
    %int1_4045 = torch.constant.int 1
    %int9223372036854775807_4046 = torch.constant.int 9223372036854775807
    %int1_4047 = torch.constant.int 1
    %2689 = torch.aten.slice.Tensor %2685, %int2_4044, %int1_4045, %int9223372036854775807_4046, %int1_4047 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_4048 = torch.constant.int 1
    %2690 = torch.aten.view.dtype %2689, %int1_4048 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %2691 = torch.aten.detach %2690 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %2692 = torch_c.to_builtin_tensor %2679 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_4049 = tensor.cast %2692 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2693 = torch_c.to_builtin_tensor %2688 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %2694 = torch_c.to_builtin_tensor %2691 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %2695 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_4049, %2693, %2694) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_4050 = tensor.cast %2695 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %2696 = torch_c.from_builtin_tensor %cast_4050 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %2696, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %2697 = torch.aten.silu %2696 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %2697, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_4051 = torch.constant.int 2
    %2698 = torch.aten.view.dtype %95, %int2_4051 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %2699 = torch.aten.detach %2698 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_4052 = torch.constant.int -1
    %int17_4053 = torch.constant.int 17
    %2700 = torch.prim.ListConstruct %int-1_4052, %int17_4053 : (!torch.int, !torch.int) -> !torch.list<int>
    %2701 = torch.aten.view %2699, %2700 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_4054 = torch.constant.int 5632
    %int-1_4055 = torch.constant.int -1
    %int17_4056 = torch.constant.int 17
    %2702 = torch.prim.ListConstruct %int5632_4054, %int-1_4055, %int17_4056 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2703 = torch.aten.view %2701, %2702 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_4057 = torch.constant.int 2
    %int0_4058 = torch.constant.int 0
    %int1_4059 = torch.constant.int 1
    %int1_4060 = torch.constant.int 1
    %2704 = torch.aten.slice.Tensor %2703, %int2_4057, %int0_4058, %int1_4059, %int1_4060 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_4061 = torch.constant.int 5
    %2705 = torch.aten.view.dtype %2704, %int5_4061 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %2706 = torch.aten.detach %2705 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_4062 = torch.constant.int 2
    %int1_4063 = torch.constant.int 1
    %int9223372036854775807_4064 = torch.constant.int 9223372036854775807
    %int1_4065 = torch.constant.int 1
    %2707 = torch.aten.slice.Tensor %2703, %int2_4062, %int1_4063, %int9223372036854775807_4064, %int1_4065 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_4066 = torch.constant.int 1
    %2708 = torch.aten.view.dtype %2707, %int1_4066 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %2709 = torch.aten.detach %2708 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %2710 = torch_c.to_builtin_tensor %2679 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_4067 = tensor.cast %2710 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2711 = torch_c.to_builtin_tensor %2706 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %2712 = torch_c.to_builtin_tensor %2709 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %2713 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_4067, %2711, %2712) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_4068 = tensor.cast %2713 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %2714 = torch_c.from_builtin_tensor %cast_4068 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %2714, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %2715 = torch.aten.mul.Tensor %2697, %2714 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %2715, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_4069 = torch.constant.int 2
    %2716 = torch.aten.view.dtype %96, %int2_4069 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %2717 = torch.aten.detach %2716 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_4070 = torch.constant.int -1
    %int17_4071 = torch.constant.int 17
    %2718 = torch.prim.ListConstruct %int-1_4070, %int17_4071 : (!torch.int, !torch.int) -> !torch.list<int>
    %2719 = torch.aten.view %2717, %2718 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_4072 = torch.constant.int 2048
    %int-1_4073 = torch.constant.int -1
    %int17_4074 = torch.constant.int 17
    %2720 = torch.prim.ListConstruct %int2048_4072, %int-1_4073, %int17_4074 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2721 = torch.aten.view %2719, %2720 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_4075 = torch.constant.int 2
    %int0_4076 = torch.constant.int 0
    %int1_4077 = torch.constant.int 1
    %int1_4078 = torch.constant.int 1
    %2722 = torch.aten.slice.Tensor %2721, %int2_4075, %int0_4076, %int1_4077, %int1_4078 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_4079 = torch.constant.int 5
    %2723 = torch.aten.view.dtype %2722, %int5_4079 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %2724 = torch.aten.detach %2723 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_4080 = torch.constant.int 2
    %int1_4081 = torch.constant.int 1
    %int9223372036854775807_4082 = torch.constant.int 9223372036854775807
    %int1_4083 = torch.constant.int 1
    %2725 = torch.aten.slice.Tensor %2721, %int2_4080, %int1_4081, %int9223372036854775807_4082, %int1_4083 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_4084 = torch.constant.int 1
    %2726 = torch.aten.view.dtype %2725, %int1_4084 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %2727 = torch.aten.detach %2726 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %2728 = torch_c.to_builtin_tensor %2715 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_4085 = tensor.cast %2728 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %2729 = torch_c.to_builtin_tensor %2724 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %2730 = torch_c.to_builtin_tensor %2727 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %2731 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_4085, %2729, %2730) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_4086 = tensor.cast %2731 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %2732 = torch_c.from_builtin_tensor %cast_4086 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2732, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_4087 = torch.constant.int 1
    %2733 = torch.aten.add.Tensor %2669, %2732, %int1_4087 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2733, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_4088 = torch.constant.none
    %none_4089 = torch.constant.none
    %int5_4090 = torch.constant.int 5
    %cpu_4091 = torch.constant.device "cpu"
    %int0_4092 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2733, %none_4088, %none_4089, %int5_4090, %cpu_4091, %int0_4092 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4093 = torch.constant.int 6
    %2734 = torch.prims.convert_element_type %2733, %int6_4093 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2734, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_4094 = torch.constant.int 2
    %2735 = torch.aten.pow.Tensor_Scalar %2734, %int2_4094 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2735, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_4095 = torch.constant.int -1
    %2736 = torch.prim.ListConstruct %int-1_4095 : (!torch.int) -> !torch.list<int>
    %true_4096 = torch.constant.bool true
    %none_4097 = torch.constant.none
    %2737 = torch.aten.mean.dim %2735, %2736, %true_4096, %none_4097 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2737, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_4098 = torch.constant.float 9.9999997473787516E-6
    %int1_4099 = torch.constant.int 1
    %2738 = torch.aten.add.Scalar %2737, %float9.999990e-06_4098, %int1_4099 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2738, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2739 = torch.aten.rsqrt %2738 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2739, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2740 = torch.aten.mul.Tensor %2734, %2739 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2740, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_4100 = torch.constant.none
    %none_4101 = torch.constant.none
    %int6_4102 = torch.constant.int 6
    %cpu_4103 = torch.constant.device "cpu"
    %int0_4104 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2740, %none_4100, %none_4101, %int6_4102, %cpu_4103, %int0_4104 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4105 = torch.constant.int 5
    %2741 = torch.prims.convert_element_type %2740, %int5_4105 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2741, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %2742 = torch.aten.mul.Tensor %100, %2741 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2742, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_4106 = torch.constant.none
    %none_4107 = torch.constant.none
    %int6_4108 = torch.constant.int 6
    %cpu_4109 = torch.constant.device "cpu"
    %int0_4110 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2742, %none_4106, %none_4107, %int6_4108, %cpu_4109, %int0_4110 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4111 = torch.constant.int 5
    %2743 = torch.prims.convert_element_type %2742, %int5_4111 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2743, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_4112 = torch.constant.int 2
    %2744 = torch.aten.view.dtype %101, %int2_4112 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %2745 = torch.aten.detach %2744 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_4113 = torch.constant.int -1
    %int17_4114 = torch.constant.int 17
    %2746 = torch.prim.ListConstruct %int-1_4113, %int17_4114 : (!torch.int, !torch.int) -> !torch.list<int>
    %2747 = torch.aten.view %2745, %2746 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_4115 = torch.constant.int 2048
    %int-1_4116 = torch.constant.int -1
    %int17_4117 = torch.constant.int 17
    %2748 = torch.prim.ListConstruct %int2048_4115, %int-1_4116, %int17_4117 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2749 = torch.aten.view %2747, %2748 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_4118 = torch.constant.int 2
    %int0_4119 = torch.constant.int 0
    %int1_4120 = torch.constant.int 1
    %int1_4121 = torch.constant.int 1
    %2750 = torch.aten.slice.Tensor %2749, %int2_4118, %int0_4119, %int1_4120, %int1_4121 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_4122 = torch.constant.int 5
    %2751 = torch.aten.view.dtype %2750, %int5_4122 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %2752 = torch.aten.detach %2751 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_4123 = torch.constant.int 2
    %int1_4124 = torch.constant.int 1
    %int9223372036854775807_4125 = torch.constant.int 9223372036854775807
    %int1_4126 = torch.constant.int 1
    %2753 = torch.aten.slice.Tensor %2749, %int2_4123, %int1_4124, %int9223372036854775807_4125, %int1_4126 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_4127 = torch.constant.int 1
    %2754 = torch.aten.view.dtype %2753, %int1_4127 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %2755 = torch.aten.detach %2754 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %2756 = torch_c.to_builtin_tensor %2743 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_4128 = tensor.cast %2756 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2757 = torch_c.to_builtin_tensor %2752 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %2758 = torch_c.to_builtin_tensor %2755 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %2759 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_4128, %2757, %2758) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_4129 = tensor.cast %2759 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %2760 = torch_c.from_builtin_tensor %cast_4129 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2760, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_4130 = torch.constant.int 2
    %2761 = torch.aten.view.dtype %102, %int2_4130 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %2762 = torch.aten.detach %2761 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_4131 = torch.constant.int -1
    %int17_4132 = torch.constant.int 17
    %2763 = torch.prim.ListConstruct %int-1_4131, %int17_4132 : (!torch.int, !torch.int) -> !torch.list<int>
    %2764 = torch.aten.view %2762, %2763 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_4133 = torch.constant.int 256
    %int-1_4134 = torch.constant.int -1
    %int17_4135 = torch.constant.int 17
    %2765 = torch.prim.ListConstruct %int256_4133, %int-1_4134, %int17_4135 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2766 = torch.aten.view %2764, %2765 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_4136 = torch.constant.int 2
    %int0_4137 = torch.constant.int 0
    %int1_4138 = torch.constant.int 1
    %int1_4139 = torch.constant.int 1
    %2767 = torch.aten.slice.Tensor %2766, %int2_4136, %int0_4137, %int1_4138, %int1_4139 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_4140 = torch.constant.int 5
    %2768 = torch.aten.view.dtype %2767, %int5_4140 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %2769 = torch.aten.detach %2768 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_4141 = torch.constant.int 2
    %int1_4142 = torch.constant.int 1
    %int9223372036854775807_4143 = torch.constant.int 9223372036854775807
    %int1_4144 = torch.constant.int 1
    %2770 = torch.aten.slice.Tensor %2766, %int2_4141, %int1_4142, %int9223372036854775807_4143, %int1_4144 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_4145 = torch.constant.int 1
    %2771 = torch.aten.view.dtype %2770, %int1_4145 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %2772 = torch.aten.detach %2771 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %2773 = torch_c.to_builtin_tensor %2743 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_4146 = tensor.cast %2773 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2774 = torch_c.to_builtin_tensor %2769 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %2775 = torch_c.to_builtin_tensor %2772 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %2776 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_4146, %2774, %2775) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_4147 = tensor.cast %2776 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %2777 = torch_c.from_builtin_tensor %cast_4147 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %2777, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_4148 = torch.constant.int 2
    %2778 = torch.aten.view.dtype %103, %int2_4148 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %2779 = torch.aten.detach %2778 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_4149 = torch.constant.int -1
    %int17_4150 = torch.constant.int 17
    %2780 = torch.prim.ListConstruct %int-1_4149, %int17_4150 : (!torch.int, !torch.int) -> !torch.list<int>
    %2781 = torch.aten.view %2779, %2780 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_4151 = torch.constant.int 256
    %int-1_4152 = torch.constant.int -1
    %int17_4153 = torch.constant.int 17
    %2782 = torch.prim.ListConstruct %int256_4151, %int-1_4152, %int17_4153 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2783 = torch.aten.view %2781, %2782 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_4154 = torch.constant.int 2
    %int0_4155 = torch.constant.int 0
    %int1_4156 = torch.constant.int 1
    %int1_4157 = torch.constant.int 1
    %2784 = torch.aten.slice.Tensor %2783, %int2_4154, %int0_4155, %int1_4156, %int1_4157 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_4158 = torch.constant.int 5
    %2785 = torch.aten.view.dtype %2784, %int5_4158 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %2786 = torch.aten.detach %2785 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_4159 = torch.constant.int 2
    %int1_4160 = torch.constant.int 1
    %int9223372036854775807_4161 = torch.constant.int 9223372036854775807
    %int1_4162 = torch.constant.int 1
    %2787 = torch.aten.slice.Tensor %2783, %int2_4159, %int1_4160, %int9223372036854775807_4161, %int1_4162 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_4163 = torch.constant.int 1
    %2788 = torch.aten.view.dtype %2787, %int1_4163 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %2789 = torch.aten.detach %2788 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %2790 = torch_c.to_builtin_tensor %2743 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_4164 = tensor.cast %2790 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2791 = torch_c.to_builtin_tensor %2786 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %2792 = torch_c.to_builtin_tensor %2789 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %2793 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_4164, %2791, %2792) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_4165 = tensor.cast %2793 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %2794 = torch_c.from_builtin_tensor %cast_4165 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %2794, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_4166 = torch.constant.int 4
    %int32_4167 = torch.constant.int 32
    %int64_4168 = torch.constant.int 64
    %2795 = torch.prim.ListConstruct %int4_4166, %273, %int32_4167, %int64_4168 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2796 = torch.aten.view %2760, %2795 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2796, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_4169 = torch.constant.int 4
    %int4_4170 = torch.constant.int 4
    %int64_4171 = torch.constant.int 64
    %2797 = torch.prim.ListConstruct %int4_4169, %273, %int4_4170, %int64_4171 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2798 = torch.aten.view %2777, %2797 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %2798, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_4172 = torch.constant.int 4
    %int4_4173 = torch.constant.int 4
    %int64_4174 = torch.constant.int 64
    %2799 = torch.prim.ListConstruct %int4_4172, %273, %int4_4173, %int64_4174 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2800 = torch.aten.view %2794, %2799 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %2800, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_4175 = torch.constant.int 0
    %none_4176 = torch.constant.none
    %none_4177 = torch.constant.none
    %cpu_4178 = torch.constant.device "cpu"
    %false_4179 = torch.constant.bool false
    %2801 = torch.aten.arange.start %int0_4175, %273, %none_4176, %none_4177, %cpu_4178, %false_4179 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2801, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_4180 = torch.constant.int 0
    %2802 = torch.aten.unsqueeze %2801, %int0_4180 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %2802, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_4181 = torch.constant.int 0
    %int64_4182 = torch.constant.int 64
    %int2_4183 = torch.constant.int 2
    %none_4184 = torch.constant.none
    %none_4185 = torch.constant.none
    %cpu_4186 = torch.constant.device "cpu"
    %false_4187 = torch.constant.bool false
    %2803 = torch.aten.arange.start_step %int0_4181, %int64_4182, %int2_4183, %none_4184, %none_4185, %cpu_4186, %false_4187 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_4188 = torch.constant.none
    %none_4189 = torch.constant.none
    %int4_4190 = torch.constant.int 4
    %cpu_4191 = torch.constant.device "cpu"
    %int0_4192 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2803, %none_4188, %none_4189, %int4_4190, %cpu_4191, %int0_4192 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4193 = torch.constant.int 6
    %2804 = torch.prims.convert_element_type %2803, %int6_4193 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_4194 = torch.constant.int 64
    %2805 = torch.aten.div.Scalar %2804, %int64_4194 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_4195 = torch.constant.float 1.000000e+04
    %2806 = torch.aten.pow.Scalar %float1.000000e04_4195, %2805 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %2807 = torch.aten.reciprocal %2806 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_4196 = torch.constant.float 1.000000e+00
    %2808 = torch.aten.mul.Scalar %2807, %float1.000000e00_4196 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_4197 = torch.constant.none
    %2809 = torch.aten.clone %97, %none_4197 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_4198 = torch.constant.int 0
    %2810 = torch.aten.unsqueeze %2808, %int0_4198 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_4199 = torch.constant.int 2
    %2811 = torch.aten.unsqueeze %2810, %int2_4199 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_4200 = torch.constant.none
    %none_4201 = torch.constant.none
    %int6_4202 = torch.constant.int 6
    %cpu_4203 = torch.constant.device "cpu"
    %int0_4204 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2811, %none_4200, %none_4201, %int6_4202, %cpu_4203, %int0_4204 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_4205 = torch.constant.int 1
    %int-1_4206 = torch.constant.int -1
    %int1_4207 = torch.constant.int 1
    %2812 = torch.prim.ListConstruct %int1_4205, %int-1_4206, %int1_4207 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4208 = torch.constant.bool false
    %2813 = torch.aten.expand %2811, %2812, %false_4208 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_4209 = torch.constant.int 1
    %2814 = torch.aten.unsqueeze %2802, %int1_4209 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %2814, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_4210 = torch.constant.none
    %none_4211 = torch.constant.none
    %int4_4212 = torch.constant.int 4
    %cpu_4213 = torch.constant.device "cpu"
    %int0_4214 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2814, %none_4210, %none_4211, %int4_4212, %cpu_4213, %int0_4214 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4215 = torch.constant.int 6
    %2815 = torch.prims.convert_element_type %2814, %int6_4215 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %2815, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %2816 = torch.aten.matmul %2813, %2815 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %2816, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_4216 = torch.constant.int 1
    %int2_4217 = torch.constant.int 2
    %2817 = torch.aten.transpose.int %2816, %int1_4216, %int2_4217 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2817, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %2818 = torch.aten.cos %2817 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2818, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %2819 = torch.aten.mul.Tensor %2818, %2809 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2819, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_4218 = torch.constant.none
    %none_4219 = torch.constant.none
    %int6_4220 = torch.constant.int 6
    %cpu_4221 = torch.constant.device "cpu"
    %int0_4222 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2819, %none_4218, %none_4219, %int6_4220, %cpu_4221, %int0_4222 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4223 = torch.constant.int 5
    %2820 = torch.prims.convert_element_type %2819, %int5_4223 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %2820, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %2821 = torch.aten.sin %2817 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2821, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %2822 = torch.aten.mul.Tensor %2821, %2809 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2822, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_4224 = torch.constant.none
    %none_4225 = torch.constant.none
    %int6_4226 = torch.constant.int 6
    %cpu_4227 = torch.constant.device "cpu"
    %int0_4228 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2822, %none_4224, %none_4225, %int6_4226, %cpu_4227, %int0_4228 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4229 = torch.constant.int 5
    %2823 = torch.prims.convert_element_type %2822, %int5_4229 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %2823, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_4230 = torch.constant.int 2
    %2824 = torch.aten.unsqueeze %2820, %int2_4230 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %2824, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_4231 = torch.constant.int 2
    %2825 = torch.aten.unsqueeze %2823, %int2_4231 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %2825, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_4232 = torch.constant.none
    %none_4233 = torch.constant.none
    %int5_4234 = torch.constant.int 5
    %cpu_4235 = torch.constant.device "cpu"
    %int0_4236 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2824, %none_4232, %none_4233, %int5_4234, %cpu_4235, %int0_4236 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_4237 = torch.constant.none
    %none_4238 = torch.constant.none
    %int5_4239 = torch.constant.int 5
    %cpu_4240 = torch.constant.device "cpu"
    %int0_4241 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2825, %none_4237, %none_4238, %int5_4239, %cpu_4240, %int0_4241 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_4242 = torch.constant.none
    %none_4243 = torch.constant.none
    %int5_4244 = torch.constant.int 5
    %cpu_4245 = torch.constant.device "cpu"
    %int0_4246 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2796, %none_4242, %none_4243, %int5_4244, %cpu_4245, %int0_4246 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_4247 = torch.constant.int 3
    %int0_4248 = torch.constant.int 0
    %int64_4249 = torch.constant.int 64
    %int2_4250 = torch.constant.int 2
    %2826 = torch.aten.slice.Tensor %2796, %int3_4247, %int0_4248, %int64_4249, %int2_4250 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2826, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_4251 = torch.constant.int 3
    %int1_4252 = torch.constant.int 1
    %int64_4253 = torch.constant.int 64
    %int2_4254 = torch.constant.int 2
    %2827 = torch.aten.slice.Tensor %2796, %int3_4251, %int1_4252, %int64_4253, %int2_4254 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2827, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %2828 = torch.aten.mul.Tensor %2826, %2824 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2828, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %2829 = torch.aten.mul.Tensor %2827, %2825 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2829, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_4255 = torch.constant.int 1
    %2830 = torch.aten.sub.Tensor %2828, %2829, %int1_4255 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2830, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %2831 = torch.aten.mul.Tensor %2827, %2824 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2831, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %2832 = torch.aten.mul.Tensor %2826, %2825 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2832, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_4256 = torch.constant.int 1
    %2833 = torch.aten.add.Tensor %2831, %2832, %int1_4256 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %2833, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %2834 = torch_c.to_builtin_tensor %2830 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_4257 = tensor.cast %2834 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %2835 = torch_c.to_builtin_tensor %2833 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_4258 = tensor.cast %2835 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %2836 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_4257, %cast_4258) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_4259 = tensor.cast %2836 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %2837 = torch_c.from_builtin_tensor %cast_4259 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %2837, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_4260 = torch.constant.int 4
    %int32_4261 = torch.constant.int 32
    %int64_4262 = torch.constant.int 64
    %2838 = torch.prim.ListConstruct %int4_4260, %273, %int32_4261, %int64_4262 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2839 = torch.aten.view %2837, %2838 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2839, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_4263 = torch.constant.none
    %none_4264 = torch.constant.none
    %int5_4265 = torch.constant.int 5
    %cpu_4266 = torch.constant.device "cpu"
    %int0_4267 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2839, %none_4263, %none_4264, %int5_4265, %cpu_4266, %int0_4267 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_4268 = torch.constant.int 0
    %none_4269 = torch.constant.none
    %none_4270 = torch.constant.none
    %cpu_4271 = torch.constant.device "cpu"
    %false_4272 = torch.constant.bool false
    %2840 = torch.aten.arange.start %int0_4268, %273, %none_4269, %none_4270, %cpu_4271, %false_4272 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2840, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_4273 = torch.constant.int 0
    %2841 = torch.aten.unsqueeze %2840, %int0_4273 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %2841, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_4274 = torch.constant.int 0
    %int64_4275 = torch.constant.int 64
    %int2_4276 = torch.constant.int 2
    %none_4277 = torch.constant.none
    %none_4278 = torch.constant.none
    %cpu_4279 = torch.constant.device "cpu"
    %false_4280 = torch.constant.bool false
    %2842 = torch.aten.arange.start_step %int0_4274, %int64_4275, %int2_4276, %none_4277, %none_4278, %cpu_4279, %false_4280 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_4281 = torch.constant.none
    %none_4282 = torch.constant.none
    %int4_4283 = torch.constant.int 4
    %cpu_4284 = torch.constant.device "cpu"
    %int0_4285 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2842, %none_4281, %none_4282, %int4_4283, %cpu_4284, %int0_4285 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4286 = torch.constant.int 6
    %2843 = torch.prims.convert_element_type %2842, %int6_4286 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_4287 = torch.constant.int 64
    %2844 = torch.aten.div.Scalar %2843, %int64_4287 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_4288 = torch.constant.float 1.000000e+04
    %2845 = torch.aten.pow.Scalar %float1.000000e04_4288, %2844 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %2846 = torch.aten.reciprocal %2845 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_4289 = torch.constant.float 1.000000e+00
    %2847 = torch.aten.mul.Scalar %2846, %float1.000000e00_4289 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_4290 = torch.constant.none
    %2848 = torch.aten.clone %98, %none_4290 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_4291 = torch.constant.int 0
    %2849 = torch.aten.unsqueeze %2847, %int0_4291 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_4292 = torch.constant.int 2
    %2850 = torch.aten.unsqueeze %2849, %int2_4292 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_4293 = torch.constant.none
    %none_4294 = torch.constant.none
    %int6_4295 = torch.constant.int 6
    %cpu_4296 = torch.constant.device "cpu"
    %int0_4297 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2850, %none_4293, %none_4294, %int6_4295, %cpu_4296, %int0_4297 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_4298 = torch.constant.int 1
    %int-1_4299 = torch.constant.int -1
    %int1_4300 = torch.constant.int 1
    %2851 = torch.prim.ListConstruct %int1_4298, %int-1_4299, %int1_4300 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4301 = torch.constant.bool false
    %2852 = torch.aten.expand %2850, %2851, %false_4301 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_4302 = torch.constant.int 1
    %2853 = torch.aten.unsqueeze %2841, %int1_4302 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %2853, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_4303 = torch.constant.none
    %none_4304 = torch.constant.none
    %int4_4305 = torch.constant.int 4
    %cpu_4306 = torch.constant.device "cpu"
    %int0_4307 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2853, %none_4303, %none_4304, %int4_4305, %cpu_4306, %int0_4307 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4308 = torch.constant.int 6
    %2854 = torch.prims.convert_element_type %2853, %int6_4308 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %2854, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %2855 = torch.aten.matmul %2852, %2854 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %2855, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_4309 = torch.constant.int 1
    %int2_4310 = torch.constant.int 2
    %2856 = torch.aten.transpose.int %2855, %int1_4309, %int2_4310 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2856, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %2857 = torch.aten.cos %2856 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2857, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %2858 = torch.aten.mul.Tensor %2857, %2848 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2858, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_4311 = torch.constant.none
    %none_4312 = torch.constant.none
    %int6_4313 = torch.constant.int 6
    %cpu_4314 = torch.constant.device "cpu"
    %int0_4315 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2858, %none_4311, %none_4312, %int6_4313, %cpu_4314, %int0_4315 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4316 = torch.constant.int 5
    %2859 = torch.prims.convert_element_type %2858, %int5_4316 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %2859, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %2860 = torch.aten.sin %2856 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2860, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %2861 = torch.aten.mul.Tensor %2860, %2848 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %2861, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_4317 = torch.constant.none
    %none_4318 = torch.constant.none
    %int6_4319 = torch.constant.int 6
    %cpu_4320 = torch.constant.device "cpu"
    %int0_4321 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2861, %none_4317, %none_4318, %int6_4319, %cpu_4320, %int0_4321 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4322 = torch.constant.int 5
    %2862 = torch.prims.convert_element_type %2861, %int5_4322 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %2862, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_4323 = torch.constant.int 2
    %2863 = torch.aten.unsqueeze %2859, %int2_4323 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %2863, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_4324 = torch.constant.int 2
    %2864 = torch.aten.unsqueeze %2862, %int2_4324 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %2864, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_4325 = torch.constant.none
    %none_4326 = torch.constant.none
    %int5_4327 = torch.constant.int 5
    %cpu_4328 = torch.constant.device "cpu"
    %int0_4329 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2863, %none_4325, %none_4326, %int5_4327, %cpu_4328, %int0_4329 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_4330 = torch.constant.none
    %none_4331 = torch.constant.none
    %int5_4332 = torch.constant.int 5
    %cpu_4333 = torch.constant.device "cpu"
    %int0_4334 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2864, %none_4330, %none_4331, %int5_4332, %cpu_4333, %int0_4334 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_4335 = torch.constant.none
    %none_4336 = torch.constant.none
    %int5_4337 = torch.constant.int 5
    %cpu_4338 = torch.constant.device "cpu"
    %int0_4339 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2798, %none_4335, %none_4336, %int5_4337, %cpu_4338, %int0_4339 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_4340 = torch.constant.int 3
    %int0_4341 = torch.constant.int 0
    %int64_4342 = torch.constant.int 64
    %int2_4343 = torch.constant.int 2
    %2865 = torch.aten.slice.Tensor %2798, %int3_4340, %int0_4341, %int64_4342, %int2_4343 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2865, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_4344 = torch.constant.int 3
    %int1_4345 = torch.constant.int 1
    %int64_4346 = torch.constant.int 64
    %int2_4347 = torch.constant.int 2
    %2866 = torch.aten.slice.Tensor %2798, %int3_4344, %int1_4345, %int64_4346, %int2_4347 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2866, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %2867 = torch.aten.mul.Tensor %2865, %2863 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2867, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %2868 = torch.aten.mul.Tensor %2866, %2864 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2868, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_4348 = torch.constant.int 1
    %2869 = torch.aten.sub.Tensor %2867, %2868, %int1_4348 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2869, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %2870 = torch.aten.mul.Tensor %2866, %2863 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2870, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %2871 = torch.aten.mul.Tensor %2865, %2864 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2871, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_4349 = torch.constant.int 1
    %2872 = torch.aten.add.Tensor %2870, %2871, %int1_4349 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %2872, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %2873 = torch_c.to_builtin_tensor %2869 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_4350 = tensor.cast %2873 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %2874 = torch_c.to_builtin_tensor %2872 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_4351 = tensor.cast %2874 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %2875 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_4350, %cast_4351) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_4352 = tensor.cast %2875 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %2876 = torch_c.from_builtin_tensor %cast_4352 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %2876, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_4353 = torch.constant.int 4
    %int4_4354 = torch.constant.int 4
    %int64_4355 = torch.constant.int 64
    %2877 = torch.prim.ListConstruct %int4_4353, %273, %int4_4354, %int64_4355 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2878 = torch.aten.view %2876, %2877 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %2878, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_4356 = torch.constant.none
    %none_4357 = torch.constant.none
    %int5_4358 = torch.constant.int 5
    %cpu_4359 = torch.constant.device "cpu"
    %int0_4360 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2878, %none_4356, %none_4357, %int5_4358, %cpu_4359, %int0_4360 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_4361 = torch.constant.int 22
    %2879 = torch.aten.mul.Scalar %arg2, %int22_4361 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2879, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int8_4362 = torch.constant.int 8
    %int1_4363 = torch.constant.int 1
    %2880 = torch.aten.add.Scalar %2879, %int8_4362, %int1_4363 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2880, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_4364 = torch.constant.int 2
    %2881 = torch.aten.mul.Scalar %2880, %int2_4364 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2881, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_4365 = torch.constant.int 0
    %int1_4366 = torch.constant.int 1
    %2882 = torch.aten.add.Scalar %2881, %int0_4365, %int1_4366 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2882, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %2883 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %2884 = torch.aten.view %2882, %2883 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2884, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_4367 = torch.constant.int 4
    %int32_4368 = torch.constant.int 32
    %int4_4369 = torch.constant.int 4
    %int64_4370 = torch.constant.int 64
    %2885 = torch.prim.ListConstruct %int4_4367, %271, %int32_4368, %int4_4369, %int64_4370 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2886 = torch.aten.view %2878, %2885 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %2886, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_4371 = torch.constant.int 32
    %int4_4372 = torch.constant.int 4
    %int64_4373 = torch.constant.int 64
    %2887 = torch.prim.ListConstruct %446, %int32_4371, %int4_4372, %int64_4373 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2888 = torch.aten.view %2886, %2887 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %2888, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_4374 = torch.constant.int 1
    %int2_4375 = torch.constant.int 2
    %2889 = torch.aten.transpose.int %2888, %int1_4374, %int2_4375 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2889, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_4376 = torch.constant.none
    %none_4377 = torch.constant.none
    %int5_4378 = torch.constant.int 5
    %cpu_4379 = torch.constant.device "cpu"
    %int0_4380 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2889, %none_4376, %none_4377, %int5_4378, %cpu_4379, %int0_4380 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_4381 = torch.constant.int 22
    %int2_4382 = torch.constant.int 2
    %int4_4383 = torch.constant.int 4
    %int32_4384 = torch.constant.int 32
    %int64_4385 = torch.constant.int 64
    %2890 = torch.prim.ListConstruct %272, %int22_4381, %int2_4382, %int4_4383, %int32_4384, %int64_4385 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2891 = torch.aten.view %2615, %2890 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2891, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_4386 = torch.constant.int 4
    %int32_4387 = torch.constant.int 32
    %int64_4388 = torch.constant.int 64
    %2892 = torch.prim.ListConstruct %439, %int4_4386, %int32_4387, %int64_4388 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2893 = torch.aten.view %2891, %2892 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2893, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %2894 = torch.prim.ListConstruct %2884 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_4389 = torch.constant.bool false
    %2895 = torch.aten.index_put %2893, %2894, %2889, %false_4389 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2895, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_4390 = torch.constant.int 22
    %int2_4391 = torch.constant.int 2
    %int4_4392 = torch.constant.int 4
    %int32_4393 = torch.constant.int 32
    %int64_4394 = torch.constant.int 64
    %2896 = torch.prim.ListConstruct %272, %int22_4390, %int2_4391, %int4_4392, %int32_4393, %int64_4394 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2897 = torch.aten.view %2895, %2896 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2897, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_4395 = torch.constant.int 360448
    %2898 = torch.prim.ListConstruct %272, %int360448_4395 : (!torch.int, !torch.int) -> !torch.list<int>
    %2899 = torch.aten.view %2897, %2898 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %2899, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_4396 = torch.constant.int 22
    %int2_4397 = torch.constant.int 2
    %int4_4398 = torch.constant.int 4
    %int32_4399 = torch.constant.int 32
    %int64_4400 = torch.constant.int 64
    %2900 = torch.prim.ListConstruct %272, %int22_4396, %int2_4397, %int4_4398, %int32_4399, %int64_4400 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2901 = torch.aten.view %2899, %2900 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2901, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_4401 = torch.constant.int 4
    %int32_4402 = torch.constant.int 32
    %int64_4403 = torch.constant.int 64
    %2902 = torch.prim.ListConstruct %439, %int4_4401, %int32_4402, %int64_4403 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2903 = torch.aten.view %2901, %2902 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2903, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_4404 = torch.constant.int 22
    %2904 = torch.aten.mul.Scalar %arg2, %int22_4404 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2904, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int8_4405 = torch.constant.int 8
    %int1_4406 = torch.constant.int 1
    %2905 = torch.aten.add.Scalar %2904, %int8_4405, %int1_4406 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2905, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_4407 = torch.constant.int 2
    %2906 = torch.aten.mul.Scalar %2905, %int2_4407 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2906, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_4408 = torch.constant.int 1
    %int1_4409 = torch.constant.int 1
    %2907 = torch.aten.add.Scalar %2906, %int1_4408, %int1_4409 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2907, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %2908 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %2909 = torch.aten.view %2907, %2908 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2909, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_4410 = torch.constant.int 4
    %int32_4411 = torch.constant.int 32
    %int4_4412 = torch.constant.int 4
    %int64_4413 = torch.constant.int 64
    %2910 = torch.prim.ListConstruct %int4_4410, %271, %int32_4411, %int4_4412, %int64_4413 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2911 = torch.aten.view %2800, %2910 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %2911, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_4414 = torch.constant.int 32
    %int4_4415 = torch.constant.int 4
    %int64_4416 = torch.constant.int 64
    %2912 = torch.prim.ListConstruct %446, %int32_4414, %int4_4415, %int64_4416 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2913 = torch.aten.view %2911, %2912 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %2913, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_4417 = torch.constant.int 1
    %int2_4418 = torch.constant.int 2
    %2914 = torch.aten.transpose.int %2913, %int1_4417, %int2_4418 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2914, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_4419 = torch.constant.none
    %none_4420 = torch.constant.none
    %int5_4421 = torch.constant.int 5
    %cpu_4422 = torch.constant.device "cpu"
    %int0_4423 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2914, %none_4419, %none_4420, %int5_4421, %cpu_4422, %int0_4423 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %2915 = torch.prim.ListConstruct %2909 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_4424 = torch.constant.bool false
    %2916 = torch.aten.index_put %2903, %2915, %2914, %false_4424 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %2916, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_4425 = torch.constant.int 22
    %int2_4426 = torch.constant.int 2
    %int4_4427 = torch.constant.int 4
    %int32_4428 = torch.constant.int 32
    %int64_4429 = torch.constant.int 64
    %2917 = torch.prim.ListConstruct %272, %int22_4425, %int2_4426, %int4_4427, %int32_4428, %int64_4429 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2918 = torch.aten.view %2916, %2917 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2918, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_4430 = torch.constant.int 360448
    %2919 = torch.prim.ListConstruct %272, %int360448_4430 : (!torch.int, !torch.int) -> !torch.list<int>
    %2920 = torch.aten.view %2918, %2919 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %2920, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_4431 = torch.constant.int 0
    %int1_4432 = torch.constant.int 1
    %none_4433 = torch.constant.none
    %none_4434 = torch.constant.none
    %cpu_4435 = torch.constant.device "cpu"
    %false_4436 = torch.constant.bool false
    %2921 = torch.aten.arange.start_step %int0_4431, %273, %int1_4432, %none_4433, %none_4434, %cpu_4435, %false_4436 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2921, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_4437 = torch.constant.int -1
    %2922 = torch.aten.unsqueeze %arg1, %int-1_4437 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %2923 = torch.aten.ge.Tensor %2921, %2922 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %2923, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_4438 = torch.constant.none
    %none_4439 = torch.constant.none
    %cpu_4440 = torch.constant.device "cpu"
    %false_4441 = torch.constant.bool false
    %2924 = torch.aten.arange %273, %none_4438, %none_4439, %cpu_4440, %false_4441 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2924, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_4442 = torch.constant.int 0
    %2925 = torch.aten.unsqueeze %2924, %int0_4442 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %2925, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_4443 = torch.constant.int 1
    %2926 = torch.aten.unsqueeze %2925, %int1_4443 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %2926, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_4444 = torch.constant.int 2
    %2927 = torch.aten.unsqueeze %2926, %int2_4444 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %2927, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_4445 = torch.constant.none
    %none_4446 = torch.constant.none
    %cpu_4447 = torch.constant.device "cpu"
    %false_4448 = torch.constant.bool false
    %2928 = torch.aten.arange %273, %none_4445, %none_4446, %cpu_4447, %false_4448 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2928, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_4449 = torch.constant.int 0
    %2929 = torch.aten.unsqueeze %2928, %int0_4449 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %2929, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_4450 = torch.constant.int 1
    %2930 = torch.aten.unsqueeze %2929, %int1_4450 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %2930, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_4451 = torch.constant.int 3
    %2931 = torch.aten.unsqueeze %2930, %int3_4451 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %2931, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %2932 = torch.aten.gt.Tensor %2927, %2931 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %2932, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_4452 = torch.constant.int 1
    %2933 = torch.aten.unsqueeze %2923, %int1_4452 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %2933, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_4453 = torch.constant.int 2
    %2934 = torch.aten.unsqueeze %2933, %int2_4453 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %2934, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %2935 = torch.aten.logical_or %2932, %2934 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %2935, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_4454 = torch.constant.none
    %2936 = torch.aten.clone %99, %none_4454 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_4455 = torch.constant.int 0
    %2937 = torch.aten.where.ScalarOther %2935, %2936, %int0_4455 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %2937, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_4456 = torch.constant.none
    %none_4457 = torch.constant.none
    %int5_4458 = torch.constant.int 5
    %cpu_4459 = torch.constant.device "cpu"
    %int0_4460 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2937, %none_4456, %none_4457, %int5_4458, %cpu_4459, %int0_4460 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_4461 = torch.constant.int -2
    %2938 = torch.aten.unsqueeze %2878, %int-2_4461 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %2938, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_4462 = torch.constant.int 4
    %int4_4463 = torch.constant.int 4
    %int8_4464 = torch.constant.int 8
    %int64_4465 = torch.constant.int 64
    %2939 = torch.prim.ListConstruct %int4_4462, %273, %int4_4463, %int8_4464, %int64_4465 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4466 = torch.constant.bool false
    %2940 = torch.aten.expand %2938, %2939, %false_4466 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2940, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_4467 = torch.constant.int 0
    %2941 = torch.aten.clone %2940, %int0_4467 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2941, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_4468 = torch.constant.int 4
    %int32_4469 = torch.constant.int 32
    %int64_4470 = torch.constant.int 64
    %2942 = torch.prim.ListConstruct %int4_4468, %273, %int32_4469, %int64_4470 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2943 = torch.aten._unsafe_view %2941, %2942 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2943, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_4471 = torch.constant.int -2
    %2944 = torch.aten.unsqueeze %2800, %int-2_4471 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %2944, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_4472 = torch.constant.int 4
    %int4_4473 = torch.constant.int 4
    %int8_4474 = torch.constant.int 8
    %int64_4475 = torch.constant.int 64
    %2945 = torch.prim.ListConstruct %int4_4472, %273, %int4_4473, %int8_4474, %int64_4475 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4476 = torch.constant.bool false
    %2946 = torch.aten.expand %2944, %2945, %false_4476 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2946, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_4477 = torch.constant.int 0
    %2947 = torch.aten.clone %2946, %int0_4477 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2947, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_4478 = torch.constant.int 4
    %int32_4479 = torch.constant.int 32
    %int64_4480 = torch.constant.int 64
    %2948 = torch.prim.ListConstruct %int4_4478, %273, %int32_4479, %int64_4480 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2949 = torch.aten._unsafe_view %2947, %2948 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2949, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_4481 = torch.constant.int 1
    %int2_4482 = torch.constant.int 2
    %2950 = torch.aten.transpose.int %2839, %int1_4481, %int2_4482 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2950, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_4483 = torch.constant.int 1
    %int2_4484 = torch.constant.int 2
    %2951 = torch.aten.transpose.int %2943, %int1_4483, %int2_4484 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2951, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_4485 = torch.constant.int 1
    %int2_4486 = torch.constant.int 2
    %2952 = torch.aten.transpose.int %2949, %int1_4485, %int2_4486 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2952, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_4487 = torch.constant.float 0.000000e+00
    %false_4488 = torch.constant.bool false
    %none_4489 = torch.constant.none
    %false_4490 = torch.constant.bool false
    %2953 = torch.aten.scaled_dot_product_attention %2950, %2951, %2952, %2937, %float0.000000e00_4487, %false_4488, %none_4489, %false_4490 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2953, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_4491 = torch.constant.int 1
    %int2_4492 = torch.constant.int 2
    %2954 = torch.aten.transpose.int %2953, %int1_4491, %int2_4492 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2954, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_4493 = torch.constant.int 4
    %int2048_4494 = torch.constant.int 2048
    %2955 = torch.prim.ListConstruct %int4_4493, %273, %int2048_4494 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2956 = torch.aten.view %2954, %2955 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2956, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_4495 = torch.constant.int 2
    %2957 = torch.aten.view.dtype %104, %int2_4495 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %2958 = torch.aten.detach %2957 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_4496 = torch.constant.int -1
    %int17_4497 = torch.constant.int 17
    %2959 = torch.prim.ListConstruct %int-1_4496, %int17_4497 : (!torch.int, !torch.int) -> !torch.list<int>
    %2960 = torch.aten.view %2958, %2959 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_4498 = torch.constant.int 2048
    %int-1_4499 = torch.constant.int -1
    %int17_4500 = torch.constant.int 17
    %2961 = torch.prim.ListConstruct %int2048_4498, %int-1_4499, %int17_4500 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2962 = torch.aten.view %2960, %2961 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_4501 = torch.constant.int 2
    %int0_4502 = torch.constant.int 0
    %int1_4503 = torch.constant.int 1
    %int1_4504 = torch.constant.int 1
    %2963 = torch.aten.slice.Tensor %2962, %int2_4501, %int0_4502, %int1_4503, %int1_4504 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_4505 = torch.constant.int 5
    %2964 = torch.aten.view.dtype %2963, %int5_4505 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %2965 = torch.aten.detach %2964 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_4506 = torch.constant.int 2
    %int1_4507 = torch.constant.int 1
    %int9223372036854775807_4508 = torch.constant.int 9223372036854775807
    %int1_4509 = torch.constant.int 1
    %2966 = torch.aten.slice.Tensor %2962, %int2_4506, %int1_4507, %int9223372036854775807_4508, %int1_4509 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_4510 = torch.constant.int 1
    %2967 = torch.aten.view.dtype %2966, %int1_4510 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %2968 = torch.aten.detach %2967 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %2969 = torch_c.to_builtin_tensor %2956 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_4511 = tensor.cast %2969 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2970 = torch_c.to_builtin_tensor %2965 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %2971 = torch_c.to_builtin_tensor %2968 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %2972 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_4511, %2970, %2971) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_4512 = tensor.cast %2972 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %2973 = torch_c.from_builtin_tensor %cast_4512 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2973, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_4513 = torch.constant.none
    %none_4514 = torch.constant.none
    %int5_4515 = torch.constant.int 5
    %cpu_4516 = torch.constant.device "cpu"
    %int0_4517 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2973, %none_4513, %none_4514, %int5_4515, %cpu_4516, %int0_4517 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_4518 = torch.constant.int 1
    %2974 = torch.aten.add.Tensor %2733, %2973, %int1_4518 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2974, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_4519 = torch.constant.none
    %none_4520 = torch.constant.none
    %int5_4521 = torch.constant.int 5
    %cpu_4522 = torch.constant.device "cpu"
    %int0_4523 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2974, %none_4519, %none_4520, %int5_4521, %cpu_4522, %int0_4523 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4524 = torch.constant.int 6
    %2975 = torch.prims.convert_element_type %2974, %int6_4524 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2975, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_4525 = torch.constant.int 2
    %2976 = torch.aten.pow.Tensor_Scalar %2975, %int2_4525 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2976, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_4526 = torch.constant.int -1
    %2977 = torch.prim.ListConstruct %int-1_4526 : (!torch.int) -> !torch.list<int>
    %true_4527 = torch.constant.bool true
    %none_4528 = torch.constant.none
    %2978 = torch.aten.mean.dim %2976, %2977, %true_4527, %none_4528 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2978, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_4529 = torch.constant.float 9.9999997473787516E-6
    %int1_4530 = torch.constant.int 1
    %2979 = torch.aten.add.Scalar %2978, %float9.999990e-06_4529, %int1_4530 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2979, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2980 = torch.aten.rsqrt %2979 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2980, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2981 = torch.aten.mul.Tensor %2975, %2980 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2981, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_4531 = torch.constant.none
    %none_4532 = torch.constant.none
    %int6_4533 = torch.constant.int 6
    %cpu_4534 = torch.constant.device "cpu"
    %int0_4535 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2981, %none_4531, %none_4532, %int6_4533, %cpu_4534, %int0_4535 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4536 = torch.constant.int 5
    %2982 = torch.prims.convert_element_type %2981, %int5_4536 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2982, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %2983 = torch.aten.mul.Tensor %105, %2982 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %2983, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_4537 = torch.constant.none
    %none_4538 = torch.constant.none
    %int6_4539 = torch.constant.int 6
    %cpu_4540 = torch.constant.device "cpu"
    %int0_4541 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2983, %none_4537, %none_4538, %int6_4539, %cpu_4540, %int0_4541 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4542 = torch.constant.int 5
    %2984 = torch.prims.convert_element_type %2983, %int5_4542 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %2984, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_4543 = torch.constant.int 2
    %2985 = torch.aten.view.dtype %106, %int2_4543 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %2986 = torch.aten.detach %2985 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_4544 = torch.constant.int -1
    %int17_4545 = torch.constant.int 17
    %2987 = torch.prim.ListConstruct %int-1_4544, %int17_4545 : (!torch.int, !torch.int) -> !torch.list<int>
    %2988 = torch.aten.view %2986, %2987 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_4546 = torch.constant.int 5632
    %int-1_4547 = torch.constant.int -1
    %int17_4548 = torch.constant.int 17
    %2989 = torch.prim.ListConstruct %int5632_4546, %int-1_4547, %int17_4548 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2990 = torch.aten.view %2988, %2989 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_4549 = torch.constant.int 2
    %int0_4550 = torch.constant.int 0
    %int1_4551 = torch.constant.int 1
    %int1_4552 = torch.constant.int 1
    %2991 = torch.aten.slice.Tensor %2990, %int2_4549, %int0_4550, %int1_4551, %int1_4552 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_4553 = torch.constant.int 5
    %2992 = torch.aten.view.dtype %2991, %int5_4553 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %2993 = torch.aten.detach %2992 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_4554 = torch.constant.int 2
    %int1_4555 = torch.constant.int 1
    %int9223372036854775807_4556 = torch.constant.int 9223372036854775807
    %int1_4557 = torch.constant.int 1
    %2994 = torch.aten.slice.Tensor %2990, %int2_4554, %int1_4555, %int9223372036854775807_4556, %int1_4557 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_4558 = torch.constant.int 1
    %2995 = torch.aten.view.dtype %2994, %int1_4558 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %2996 = torch.aten.detach %2995 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %2997 = torch_c.to_builtin_tensor %2984 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_4559 = tensor.cast %2997 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %2998 = torch_c.to_builtin_tensor %2993 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %2999 = torch_c.to_builtin_tensor %2996 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %3000 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_4559, %2998, %2999) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_4560 = tensor.cast %3000 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %3001 = torch_c.from_builtin_tensor %cast_4560 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %3001, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %3002 = torch.aten.silu %3001 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %3002, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_4561 = torch.constant.int 2
    %3003 = torch.aten.view.dtype %107, %int2_4561 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %3004 = torch.aten.detach %3003 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_4562 = torch.constant.int -1
    %int17_4563 = torch.constant.int 17
    %3005 = torch.prim.ListConstruct %int-1_4562, %int17_4563 : (!torch.int, !torch.int) -> !torch.list<int>
    %3006 = torch.aten.view %3004, %3005 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_4564 = torch.constant.int 5632
    %int-1_4565 = torch.constant.int -1
    %int17_4566 = torch.constant.int 17
    %3007 = torch.prim.ListConstruct %int5632_4564, %int-1_4565, %int17_4566 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3008 = torch.aten.view %3006, %3007 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_4567 = torch.constant.int 2
    %int0_4568 = torch.constant.int 0
    %int1_4569 = torch.constant.int 1
    %int1_4570 = torch.constant.int 1
    %3009 = torch.aten.slice.Tensor %3008, %int2_4567, %int0_4568, %int1_4569, %int1_4570 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_4571 = torch.constant.int 5
    %3010 = torch.aten.view.dtype %3009, %int5_4571 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %3011 = torch.aten.detach %3010 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_4572 = torch.constant.int 2
    %int1_4573 = torch.constant.int 1
    %int9223372036854775807_4574 = torch.constant.int 9223372036854775807
    %int1_4575 = torch.constant.int 1
    %3012 = torch.aten.slice.Tensor %3008, %int2_4572, %int1_4573, %int9223372036854775807_4574, %int1_4575 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_4576 = torch.constant.int 1
    %3013 = torch.aten.view.dtype %3012, %int1_4576 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %3014 = torch.aten.detach %3013 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %3015 = torch_c.to_builtin_tensor %2984 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_4577 = tensor.cast %3015 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3016 = torch_c.to_builtin_tensor %3011 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %3017 = torch_c.to_builtin_tensor %3014 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %3018 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_4577, %3016, %3017) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_4578 = tensor.cast %3018 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %3019 = torch_c.from_builtin_tensor %cast_4578 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %3019, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %3020 = torch.aten.mul.Tensor %3002, %3019 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %3020, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_4579 = torch.constant.int 2
    %3021 = torch.aten.view.dtype %108, %int2_4579 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %3022 = torch.aten.detach %3021 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_4580 = torch.constant.int -1
    %int17_4581 = torch.constant.int 17
    %3023 = torch.prim.ListConstruct %int-1_4580, %int17_4581 : (!torch.int, !torch.int) -> !torch.list<int>
    %3024 = torch.aten.view %3022, %3023 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_4582 = torch.constant.int 2048
    %int-1_4583 = torch.constant.int -1
    %int17_4584 = torch.constant.int 17
    %3025 = torch.prim.ListConstruct %int2048_4582, %int-1_4583, %int17_4584 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3026 = torch.aten.view %3024, %3025 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_4585 = torch.constant.int 2
    %int0_4586 = torch.constant.int 0
    %int1_4587 = torch.constant.int 1
    %int1_4588 = torch.constant.int 1
    %3027 = torch.aten.slice.Tensor %3026, %int2_4585, %int0_4586, %int1_4587, %int1_4588 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_4589 = torch.constant.int 5
    %3028 = torch.aten.view.dtype %3027, %int5_4589 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %3029 = torch.aten.detach %3028 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_4590 = torch.constant.int 2
    %int1_4591 = torch.constant.int 1
    %int9223372036854775807_4592 = torch.constant.int 9223372036854775807
    %int1_4593 = torch.constant.int 1
    %3030 = torch.aten.slice.Tensor %3026, %int2_4590, %int1_4591, %int9223372036854775807_4592, %int1_4593 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_4594 = torch.constant.int 1
    %3031 = torch.aten.view.dtype %3030, %int1_4594 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %3032 = torch.aten.detach %3031 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %3033 = torch_c.to_builtin_tensor %3020 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_4595 = tensor.cast %3033 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %3034 = torch_c.to_builtin_tensor %3029 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %3035 = torch_c.to_builtin_tensor %3032 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %3036 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_4595, %3034, %3035) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_4596 = tensor.cast %3036 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %3037 = torch_c.from_builtin_tensor %cast_4596 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3037, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_4597 = torch.constant.int 1
    %3038 = torch.aten.add.Tensor %2974, %3037, %int1_4597 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3038, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_4598 = torch.constant.none
    %none_4599 = torch.constant.none
    %int5_4600 = torch.constant.int 5
    %cpu_4601 = torch.constant.device "cpu"
    %int0_4602 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3038, %none_4598, %none_4599, %int5_4600, %cpu_4601, %int0_4602 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4603 = torch.constant.int 6
    %3039 = torch.prims.convert_element_type %3038, %int6_4603 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3039, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_4604 = torch.constant.int 2
    %3040 = torch.aten.pow.Tensor_Scalar %3039, %int2_4604 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3040, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_4605 = torch.constant.int -1
    %3041 = torch.prim.ListConstruct %int-1_4605 : (!torch.int) -> !torch.list<int>
    %true_4606 = torch.constant.bool true
    %none_4607 = torch.constant.none
    %3042 = torch.aten.mean.dim %3040, %3041, %true_4606, %none_4607 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3042, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_4608 = torch.constant.float 9.9999997473787516E-6
    %int1_4609 = torch.constant.int 1
    %3043 = torch.aten.add.Scalar %3042, %float9.999990e-06_4608, %int1_4609 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3043, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3044 = torch.aten.rsqrt %3043 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3044, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3045 = torch.aten.mul.Tensor %3039, %3044 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3045, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_4610 = torch.constant.none
    %none_4611 = torch.constant.none
    %int6_4612 = torch.constant.int 6
    %cpu_4613 = torch.constant.device "cpu"
    %int0_4614 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3045, %none_4610, %none_4611, %int6_4612, %cpu_4613, %int0_4614 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4615 = torch.constant.int 5
    %3046 = torch.prims.convert_element_type %3045, %int5_4615 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3046, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %3047 = torch.aten.mul.Tensor %112, %3046 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3047, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_4616 = torch.constant.none
    %none_4617 = torch.constant.none
    %int6_4618 = torch.constant.int 6
    %cpu_4619 = torch.constant.device "cpu"
    %int0_4620 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3047, %none_4616, %none_4617, %int6_4618, %cpu_4619, %int0_4620 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4621 = torch.constant.int 5
    %3048 = torch.prims.convert_element_type %3047, %int5_4621 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3048, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_4622 = torch.constant.int 2
    %3049 = torch.aten.view.dtype %113, %int2_4622 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %3050 = torch.aten.detach %3049 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_4623 = torch.constant.int -1
    %int17_4624 = torch.constant.int 17
    %3051 = torch.prim.ListConstruct %int-1_4623, %int17_4624 : (!torch.int, !torch.int) -> !torch.list<int>
    %3052 = torch.aten.view %3050, %3051 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_4625 = torch.constant.int 2048
    %int-1_4626 = torch.constant.int -1
    %int17_4627 = torch.constant.int 17
    %3053 = torch.prim.ListConstruct %int2048_4625, %int-1_4626, %int17_4627 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3054 = torch.aten.view %3052, %3053 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_4628 = torch.constant.int 2
    %int0_4629 = torch.constant.int 0
    %int1_4630 = torch.constant.int 1
    %int1_4631 = torch.constant.int 1
    %3055 = torch.aten.slice.Tensor %3054, %int2_4628, %int0_4629, %int1_4630, %int1_4631 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_4632 = torch.constant.int 5
    %3056 = torch.aten.view.dtype %3055, %int5_4632 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %3057 = torch.aten.detach %3056 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_4633 = torch.constant.int 2
    %int1_4634 = torch.constant.int 1
    %int9223372036854775807_4635 = torch.constant.int 9223372036854775807
    %int1_4636 = torch.constant.int 1
    %3058 = torch.aten.slice.Tensor %3054, %int2_4633, %int1_4634, %int9223372036854775807_4635, %int1_4636 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_4637 = torch.constant.int 1
    %3059 = torch.aten.view.dtype %3058, %int1_4637 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %3060 = torch.aten.detach %3059 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %3061 = torch_c.to_builtin_tensor %3048 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_4638 = tensor.cast %3061 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3062 = torch_c.to_builtin_tensor %3057 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %3063 = torch_c.to_builtin_tensor %3060 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %3064 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_4638, %3062, %3063) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_4639 = tensor.cast %3064 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %3065 = torch_c.from_builtin_tensor %cast_4639 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3065, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_4640 = torch.constant.int 2
    %3066 = torch.aten.view.dtype %114, %int2_4640 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %3067 = torch.aten.detach %3066 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_4641 = torch.constant.int -1
    %int17_4642 = torch.constant.int 17
    %3068 = torch.prim.ListConstruct %int-1_4641, %int17_4642 : (!torch.int, !torch.int) -> !torch.list<int>
    %3069 = torch.aten.view %3067, %3068 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_4643 = torch.constant.int 256
    %int-1_4644 = torch.constant.int -1
    %int17_4645 = torch.constant.int 17
    %3070 = torch.prim.ListConstruct %int256_4643, %int-1_4644, %int17_4645 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3071 = torch.aten.view %3069, %3070 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_4646 = torch.constant.int 2
    %int0_4647 = torch.constant.int 0
    %int1_4648 = torch.constant.int 1
    %int1_4649 = torch.constant.int 1
    %3072 = torch.aten.slice.Tensor %3071, %int2_4646, %int0_4647, %int1_4648, %int1_4649 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_4650 = torch.constant.int 5
    %3073 = torch.aten.view.dtype %3072, %int5_4650 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %3074 = torch.aten.detach %3073 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_4651 = torch.constant.int 2
    %int1_4652 = torch.constant.int 1
    %int9223372036854775807_4653 = torch.constant.int 9223372036854775807
    %int1_4654 = torch.constant.int 1
    %3075 = torch.aten.slice.Tensor %3071, %int2_4651, %int1_4652, %int9223372036854775807_4653, %int1_4654 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_4655 = torch.constant.int 1
    %3076 = torch.aten.view.dtype %3075, %int1_4655 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %3077 = torch.aten.detach %3076 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %3078 = torch_c.to_builtin_tensor %3048 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_4656 = tensor.cast %3078 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3079 = torch_c.to_builtin_tensor %3074 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %3080 = torch_c.to_builtin_tensor %3077 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %3081 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_4656, %3079, %3080) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_4657 = tensor.cast %3081 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %3082 = torch_c.from_builtin_tensor %cast_4657 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %3082, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_4658 = torch.constant.int 2
    %3083 = torch.aten.view.dtype %115, %int2_4658 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %3084 = torch.aten.detach %3083 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_4659 = torch.constant.int -1
    %int17_4660 = torch.constant.int 17
    %3085 = torch.prim.ListConstruct %int-1_4659, %int17_4660 : (!torch.int, !torch.int) -> !torch.list<int>
    %3086 = torch.aten.view %3084, %3085 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_4661 = torch.constant.int 256
    %int-1_4662 = torch.constant.int -1
    %int17_4663 = torch.constant.int 17
    %3087 = torch.prim.ListConstruct %int256_4661, %int-1_4662, %int17_4663 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3088 = torch.aten.view %3086, %3087 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_4664 = torch.constant.int 2
    %int0_4665 = torch.constant.int 0
    %int1_4666 = torch.constant.int 1
    %int1_4667 = torch.constant.int 1
    %3089 = torch.aten.slice.Tensor %3088, %int2_4664, %int0_4665, %int1_4666, %int1_4667 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_4668 = torch.constant.int 5
    %3090 = torch.aten.view.dtype %3089, %int5_4668 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %3091 = torch.aten.detach %3090 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_4669 = torch.constant.int 2
    %int1_4670 = torch.constant.int 1
    %int9223372036854775807_4671 = torch.constant.int 9223372036854775807
    %int1_4672 = torch.constant.int 1
    %3092 = torch.aten.slice.Tensor %3088, %int2_4669, %int1_4670, %int9223372036854775807_4671, %int1_4672 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_4673 = torch.constant.int 1
    %3093 = torch.aten.view.dtype %3092, %int1_4673 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %3094 = torch.aten.detach %3093 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %3095 = torch_c.to_builtin_tensor %3048 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_4674 = tensor.cast %3095 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3096 = torch_c.to_builtin_tensor %3091 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %3097 = torch_c.to_builtin_tensor %3094 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %3098 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_4674, %3096, %3097) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_4675 = tensor.cast %3098 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %3099 = torch_c.from_builtin_tensor %cast_4675 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %3099, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_4676 = torch.constant.int 4
    %int32_4677 = torch.constant.int 32
    %int64_4678 = torch.constant.int 64
    %3100 = torch.prim.ListConstruct %int4_4676, %273, %int32_4677, %int64_4678 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3101 = torch.aten.view %3065, %3100 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3101, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_4679 = torch.constant.int 4
    %int4_4680 = torch.constant.int 4
    %int64_4681 = torch.constant.int 64
    %3102 = torch.prim.ListConstruct %int4_4679, %273, %int4_4680, %int64_4681 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3103 = torch.aten.view %3082, %3102 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %3103, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_4682 = torch.constant.int 4
    %int4_4683 = torch.constant.int 4
    %int64_4684 = torch.constant.int 64
    %3104 = torch.prim.ListConstruct %int4_4682, %273, %int4_4683, %int64_4684 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3105 = torch.aten.view %3099, %3104 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %3105, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_4685 = torch.constant.int 0
    %none_4686 = torch.constant.none
    %none_4687 = torch.constant.none
    %cpu_4688 = torch.constant.device "cpu"
    %false_4689 = torch.constant.bool false
    %3106 = torch.aten.arange.start %int0_4685, %273, %none_4686, %none_4687, %cpu_4688, %false_4689 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3106, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_4690 = torch.constant.int 0
    %3107 = torch.aten.unsqueeze %3106, %int0_4690 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %3107, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_4691 = torch.constant.int 0
    %int64_4692 = torch.constant.int 64
    %int2_4693 = torch.constant.int 2
    %none_4694 = torch.constant.none
    %none_4695 = torch.constant.none
    %cpu_4696 = torch.constant.device "cpu"
    %false_4697 = torch.constant.bool false
    %3108 = torch.aten.arange.start_step %int0_4691, %int64_4692, %int2_4693, %none_4694, %none_4695, %cpu_4696, %false_4697 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_4698 = torch.constant.none
    %none_4699 = torch.constant.none
    %int4_4700 = torch.constant.int 4
    %cpu_4701 = torch.constant.device "cpu"
    %int0_4702 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3108, %none_4698, %none_4699, %int4_4700, %cpu_4701, %int0_4702 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4703 = torch.constant.int 6
    %3109 = torch.prims.convert_element_type %3108, %int6_4703 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_4704 = torch.constant.int 64
    %3110 = torch.aten.div.Scalar %3109, %int64_4704 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_4705 = torch.constant.float 1.000000e+04
    %3111 = torch.aten.pow.Scalar %float1.000000e04_4705, %3110 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %3112 = torch.aten.reciprocal %3111 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_4706 = torch.constant.float 1.000000e+00
    %3113 = torch.aten.mul.Scalar %3112, %float1.000000e00_4706 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_4707 = torch.constant.none
    %3114 = torch.aten.clone %109, %none_4707 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_4708 = torch.constant.int 0
    %3115 = torch.aten.unsqueeze %3113, %int0_4708 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_4709 = torch.constant.int 2
    %3116 = torch.aten.unsqueeze %3115, %int2_4709 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_4710 = torch.constant.none
    %none_4711 = torch.constant.none
    %int6_4712 = torch.constant.int 6
    %cpu_4713 = torch.constant.device "cpu"
    %int0_4714 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3116, %none_4710, %none_4711, %int6_4712, %cpu_4713, %int0_4714 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_4715 = torch.constant.int 1
    %int-1_4716 = torch.constant.int -1
    %int1_4717 = torch.constant.int 1
    %3117 = torch.prim.ListConstruct %int1_4715, %int-1_4716, %int1_4717 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4718 = torch.constant.bool false
    %3118 = torch.aten.expand %3116, %3117, %false_4718 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_4719 = torch.constant.int 1
    %3119 = torch.aten.unsqueeze %3107, %int1_4719 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %3119, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_4720 = torch.constant.none
    %none_4721 = torch.constant.none
    %int4_4722 = torch.constant.int 4
    %cpu_4723 = torch.constant.device "cpu"
    %int0_4724 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3119, %none_4720, %none_4721, %int4_4722, %cpu_4723, %int0_4724 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4725 = torch.constant.int 6
    %3120 = torch.prims.convert_element_type %3119, %int6_4725 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %3120, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %3121 = torch.aten.matmul %3118, %3120 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %3121, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_4726 = torch.constant.int 1
    %int2_4727 = torch.constant.int 2
    %3122 = torch.aten.transpose.int %3121, %int1_4726, %int2_4727 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3122, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %3123 = torch.aten.cos %3122 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3123, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %3124 = torch.aten.mul.Tensor %3123, %3114 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3124, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_4728 = torch.constant.none
    %none_4729 = torch.constant.none
    %int6_4730 = torch.constant.int 6
    %cpu_4731 = torch.constant.device "cpu"
    %int0_4732 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3124, %none_4728, %none_4729, %int6_4730, %cpu_4731, %int0_4732 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4733 = torch.constant.int 5
    %3125 = torch.prims.convert_element_type %3124, %int5_4733 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %3125, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %3126 = torch.aten.sin %3122 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3126, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %3127 = torch.aten.mul.Tensor %3126, %3114 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3127, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_4734 = torch.constant.none
    %none_4735 = torch.constant.none
    %int6_4736 = torch.constant.int 6
    %cpu_4737 = torch.constant.device "cpu"
    %int0_4738 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3127, %none_4734, %none_4735, %int6_4736, %cpu_4737, %int0_4738 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4739 = torch.constant.int 5
    %3128 = torch.prims.convert_element_type %3127, %int5_4739 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %3128, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_4740 = torch.constant.int 2
    %3129 = torch.aten.unsqueeze %3125, %int2_4740 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %3129, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_4741 = torch.constant.int 2
    %3130 = torch.aten.unsqueeze %3128, %int2_4741 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %3130, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_4742 = torch.constant.none
    %none_4743 = torch.constant.none
    %int5_4744 = torch.constant.int 5
    %cpu_4745 = torch.constant.device "cpu"
    %int0_4746 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3129, %none_4742, %none_4743, %int5_4744, %cpu_4745, %int0_4746 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_4747 = torch.constant.none
    %none_4748 = torch.constant.none
    %int5_4749 = torch.constant.int 5
    %cpu_4750 = torch.constant.device "cpu"
    %int0_4751 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3130, %none_4747, %none_4748, %int5_4749, %cpu_4750, %int0_4751 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_4752 = torch.constant.none
    %none_4753 = torch.constant.none
    %int5_4754 = torch.constant.int 5
    %cpu_4755 = torch.constant.device "cpu"
    %int0_4756 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3101, %none_4752, %none_4753, %int5_4754, %cpu_4755, %int0_4756 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_4757 = torch.constant.int 3
    %int0_4758 = torch.constant.int 0
    %int64_4759 = torch.constant.int 64
    %int2_4760 = torch.constant.int 2
    %3131 = torch.aten.slice.Tensor %3101, %int3_4757, %int0_4758, %int64_4759, %int2_4760 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3131, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_4761 = torch.constant.int 3
    %int1_4762 = torch.constant.int 1
    %int64_4763 = torch.constant.int 64
    %int2_4764 = torch.constant.int 2
    %3132 = torch.aten.slice.Tensor %3101, %int3_4761, %int1_4762, %int64_4763, %int2_4764 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3132, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %3133 = torch.aten.mul.Tensor %3131, %3129 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3133, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %3134 = torch.aten.mul.Tensor %3132, %3130 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3134, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_4765 = torch.constant.int 1
    %3135 = torch.aten.sub.Tensor %3133, %3134, %int1_4765 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3135, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %3136 = torch.aten.mul.Tensor %3132, %3129 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3136, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %3137 = torch.aten.mul.Tensor %3131, %3130 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3137, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_4766 = torch.constant.int 1
    %3138 = torch.aten.add.Tensor %3136, %3137, %int1_4766 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3138, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %3139 = torch_c.to_builtin_tensor %3135 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_4767 = tensor.cast %3139 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %3140 = torch_c.to_builtin_tensor %3138 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_4768 = tensor.cast %3140 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %3141 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_4767, %cast_4768) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_4769 = tensor.cast %3141 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %3142 = torch_c.from_builtin_tensor %cast_4769 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %3142, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_4770 = torch.constant.int 4
    %int32_4771 = torch.constant.int 32
    %int64_4772 = torch.constant.int 64
    %3143 = torch.prim.ListConstruct %int4_4770, %273, %int32_4771, %int64_4772 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3144 = torch.aten.view %3142, %3143 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3144, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_4773 = torch.constant.none
    %none_4774 = torch.constant.none
    %int5_4775 = torch.constant.int 5
    %cpu_4776 = torch.constant.device "cpu"
    %int0_4777 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3144, %none_4773, %none_4774, %int5_4775, %cpu_4776, %int0_4777 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_4778 = torch.constant.int 0
    %none_4779 = torch.constant.none
    %none_4780 = torch.constant.none
    %cpu_4781 = torch.constant.device "cpu"
    %false_4782 = torch.constant.bool false
    %3145 = torch.aten.arange.start %int0_4778, %273, %none_4779, %none_4780, %cpu_4781, %false_4782 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3145, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_4783 = torch.constant.int 0
    %3146 = torch.aten.unsqueeze %3145, %int0_4783 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %3146, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_4784 = torch.constant.int 0
    %int64_4785 = torch.constant.int 64
    %int2_4786 = torch.constant.int 2
    %none_4787 = torch.constant.none
    %none_4788 = torch.constant.none
    %cpu_4789 = torch.constant.device "cpu"
    %false_4790 = torch.constant.bool false
    %3147 = torch.aten.arange.start_step %int0_4784, %int64_4785, %int2_4786, %none_4787, %none_4788, %cpu_4789, %false_4790 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_4791 = torch.constant.none
    %none_4792 = torch.constant.none
    %int4_4793 = torch.constant.int 4
    %cpu_4794 = torch.constant.device "cpu"
    %int0_4795 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3147, %none_4791, %none_4792, %int4_4793, %cpu_4794, %int0_4795 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4796 = torch.constant.int 6
    %3148 = torch.prims.convert_element_type %3147, %int6_4796 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_4797 = torch.constant.int 64
    %3149 = torch.aten.div.Scalar %3148, %int64_4797 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_4798 = torch.constant.float 1.000000e+04
    %3150 = torch.aten.pow.Scalar %float1.000000e04_4798, %3149 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %3151 = torch.aten.reciprocal %3150 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_4799 = torch.constant.float 1.000000e+00
    %3152 = torch.aten.mul.Scalar %3151, %float1.000000e00_4799 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_4800 = torch.constant.none
    %3153 = torch.aten.clone %110, %none_4800 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_4801 = torch.constant.int 0
    %3154 = torch.aten.unsqueeze %3152, %int0_4801 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_4802 = torch.constant.int 2
    %3155 = torch.aten.unsqueeze %3154, %int2_4802 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_4803 = torch.constant.none
    %none_4804 = torch.constant.none
    %int6_4805 = torch.constant.int 6
    %cpu_4806 = torch.constant.device "cpu"
    %int0_4807 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3155, %none_4803, %none_4804, %int6_4805, %cpu_4806, %int0_4807 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_4808 = torch.constant.int 1
    %int-1_4809 = torch.constant.int -1
    %int1_4810 = torch.constant.int 1
    %3156 = torch.prim.ListConstruct %int1_4808, %int-1_4809, %int1_4810 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4811 = torch.constant.bool false
    %3157 = torch.aten.expand %3155, %3156, %false_4811 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_4812 = torch.constant.int 1
    %3158 = torch.aten.unsqueeze %3146, %int1_4812 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %3158, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_4813 = torch.constant.none
    %none_4814 = torch.constant.none
    %int4_4815 = torch.constant.int 4
    %cpu_4816 = torch.constant.device "cpu"
    %int0_4817 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3158, %none_4813, %none_4814, %int4_4815, %cpu_4816, %int0_4817 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4818 = torch.constant.int 6
    %3159 = torch.prims.convert_element_type %3158, %int6_4818 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %3159, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %3160 = torch.aten.matmul %3157, %3159 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %3160, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_4819 = torch.constant.int 1
    %int2_4820 = torch.constant.int 2
    %3161 = torch.aten.transpose.int %3160, %int1_4819, %int2_4820 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3161, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %3162 = torch.aten.cos %3161 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3162, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %3163 = torch.aten.mul.Tensor %3162, %3153 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3163, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_4821 = torch.constant.none
    %none_4822 = torch.constant.none
    %int6_4823 = torch.constant.int 6
    %cpu_4824 = torch.constant.device "cpu"
    %int0_4825 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3163, %none_4821, %none_4822, %int6_4823, %cpu_4824, %int0_4825 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4826 = torch.constant.int 5
    %3164 = torch.prims.convert_element_type %3163, %int5_4826 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %3164, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %3165 = torch.aten.sin %3161 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3165, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %3166 = torch.aten.mul.Tensor %3165, %3153 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3166, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_4827 = torch.constant.none
    %none_4828 = torch.constant.none
    %int6_4829 = torch.constant.int 6
    %cpu_4830 = torch.constant.device "cpu"
    %int0_4831 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3166, %none_4827, %none_4828, %int6_4829, %cpu_4830, %int0_4831 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4832 = torch.constant.int 5
    %3167 = torch.prims.convert_element_type %3166, %int5_4832 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %3167, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_4833 = torch.constant.int 2
    %3168 = torch.aten.unsqueeze %3164, %int2_4833 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %3168, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_4834 = torch.constant.int 2
    %3169 = torch.aten.unsqueeze %3167, %int2_4834 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %3169, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_4835 = torch.constant.none
    %none_4836 = torch.constant.none
    %int5_4837 = torch.constant.int 5
    %cpu_4838 = torch.constant.device "cpu"
    %int0_4839 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3168, %none_4835, %none_4836, %int5_4837, %cpu_4838, %int0_4839 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_4840 = torch.constant.none
    %none_4841 = torch.constant.none
    %int5_4842 = torch.constant.int 5
    %cpu_4843 = torch.constant.device "cpu"
    %int0_4844 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3169, %none_4840, %none_4841, %int5_4842, %cpu_4843, %int0_4844 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_4845 = torch.constant.none
    %none_4846 = torch.constant.none
    %int5_4847 = torch.constant.int 5
    %cpu_4848 = torch.constant.device "cpu"
    %int0_4849 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3103, %none_4845, %none_4846, %int5_4847, %cpu_4848, %int0_4849 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_4850 = torch.constant.int 3
    %int0_4851 = torch.constant.int 0
    %int64_4852 = torch.constant.int 64
    %int2_4853 = torch.constant.int 2
    %3170 = torch.aten.slice.Tensor %3103, %int3_4850, %int0_4851, %int64_4852, %int2_4853 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3170, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_4854 = torch.constant.int 3
    %int1_4855 = torch.constant.int 1
    %int64_4856 = torch.constant.int 64
    %int2_4857 = torch.constant.int 2
    %3171 = torch.aten.slice.Tensor %3103, %int3_4854, %int1_4855, %int64_4856, %int2_4857 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3171, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %3172 = torch.aten.mul.Tensor %3170, %3168 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3172, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %3173 = torch.aten.mul.Tensor %3171, %3169 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3173, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_4858 = torch.constant.int 1
    %3174 = torch.aten.sub.Tensor %3172, %3173, %int1_4858 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3174, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %3175 = torch.aten.mul.Tensor %3171, %3168 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3175, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %3176 = torch.aten.mul.Tensor %3170, %3169 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3176, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_4859 = torch.constant.int 1
    %3177 = torch.aten.add.Tensor %3175, %3176, %int1_4859 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3177, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %3178 = torch_c.to_builtin_tensor %3174 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_4860 = tensor.cast %3178 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %3179 = torch_c.to_builtin_tensor %3177 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_4861 = tensor.cast %3179 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %3180 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_4860, %cast_4861) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_4862 = tensor.cast %3180 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %3181 = torch_c.from_builtin_tensor %cast_4862 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %3181, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_4863 = torch.constant.int 4
    %int4_4864 = torch.constant.int 4
    %int64_4865 = torch.constant.int 64
    %3182 = torch.prim.ListConstruct %int4_4863, %273, %int4_4864, %int64_4865 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3183 = torch.aten.view %3181, %3182 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %3183, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_4866 = torch.constant.none
    %none_4867 = torch.constant.none
    %int5_4868 = torch.constant.int 5
    %cpu_4869 = torch.constant.device "cpu"
    %int0_4870 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3183, %none_4866, %none_4867, %int5_4868, %cpu_4869, %int0_4870 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_4871 = torch.constant.int 22
    %3184 = torch.aten.mul.Scalar %arg2, %int22_4871 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3184, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int9 = torch.constant.int 9
    %int1_4872 = torch.constant.int 1
    %3185 = torch.aten.add.Scalar %3184, %int9, %int1_4872 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3185, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_4873 = torch.constant.int 2
    %3186 = torch.aten.mul.Scalar %3185, %int2_4873 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3186, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_4874 = torch.constant.int 0
    %int1_4875 = torch.constant.int 1
    %3187 = torch.aten.add.Scalar %3186, %int0_4874, %int1_4875 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3187, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %3188 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %3189 = torch.aten.view %3187, %3188 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3189, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_4876 = torch.constant.int 4
    %int32_4877 = torch.constant.int 32
    %int4_4878 = torch.constant.int 4
    %int64_4879 = torch.constant.int 64
    %3190 = torch.prim.ListConstruct %int4_4876, %271, %int32_4877, %int4_4878, %int64_4879 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3191 = torch.aten.view %3183, %3190 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %3191, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_4880 = torch.constant.int 32
    %int4_4881 = torch.constant.int 4
    %int64_4882 = torch.constant.int 64
    %3192 = torch.prim.ListConstruct %446, %int32_4880, %int4_4881, %int64_4882 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3193 = torch.aten.view %3191, %3192 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %3193, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_4883 = torch.constant.int 1
    %int2_4884 = torch.constant.int 2
    %3194 = torch.aten.transpose.int %3193, %int1_4883, %int2_4884 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %3194, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_4885 = torch.constant.none
    %none_4886 = torch.constant.none
    %int5_4887 = torch.constant.int 5
    %cpu_4888 = torch.constant.device "cpu"
    %int0_4889 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3194, %none_4885, %none_4886, %int5_4887, %cpu_4888, %int0_4889 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_4890 = torch.constant.int 22
    %int2_4891 = torch.constant.int 2
    %int4_4892 = torch.constant.int 4
    %int32_4893 = torch.constant.int 32
    %int64_4894 = torch.constant.int 64
    %3195 = torch.prim.ListConstruct %272, %int22_4890, %int2_4891, %int4_4892, %int32_4893, %int64_4894 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3196 = torch.aten.view %2920, %3195 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3196, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_4895 = torch.constant.int 4
    %int32_4896 = torch.constant.int 32
    %int64_4897 = torch.constant.int 64
    %3197 = torch.prim.ListConstruct %439, %int4_4895, %int32_4896, %int64_4897 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3198 = torch.aten.view %3196, %3197 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %3198, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %3199 = torch.prim.ListConstruct %3189 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_4898 = torch.constant.bool false
    %3200 = torch.aten.index_put %3198, %3199, %3194, %false_4898 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %3200, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_4899 = torch.constant.int 22
    %int2_4900 = torch.constant.int 2
    %int4_4901 = torch.constant.int 4
    %int32_4902 = torch.constant.int 32
    %int64_4903 = torch.constant.int 64
    %3201 = torch.prim.ListConstruct %272, %int22_4899, %int2_4900, %int4_4901, %int32_4902, %int64_4903 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3202 = torch.aten.view %3200, %3201 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3202, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_4904 = torch.constant.int 360448
    %3203 = torch.prim.ListConstruct %272, %int360448_4904 : (!torch.int, !torch.int) -> !torch.list<int>
    %3204 = torch.aten.view %3202, %3203 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %3204, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_4905 = torch.constant.int 22
    %int2_4906 = torch.constant.int 2
    %int4_4907 = torch.constant.int 4
    %int32_4908 = torch.constant.int 32
    %int64_4909 = torch.constant.int 64
    %3205 = torch.prim.ListConstruct %272, %int22_4905, %int2_4906, %int4_4907, %int32_4908, %int64_4909 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3206 = torch.aten.view %3204, %3205 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3206, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_4910 = torch.constant.int 4
    %int32_4911 = torch.constant.int 32
    %int64_4912 = torch.constant.int 64
    %3207 = torch.prim.ListConstruct %439, %int4_4910, %int32_4911, %int64_4912 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3208 = torch.aten.view %3206, %3207 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %3208, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_4913 = torch.constant.int 22
    %3209 = torch.aten.mul.Scalar %arg2, %int22_4913 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3209, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int9_4914 = torch.constant.int 9
    %int1_4915 = torch.constant.int 1
    %3210 = torch.aten.add.Scalar %3209, %int9_4914, %int1_4915 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3210, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_4916 = torch.constant.int 2
    %3211 = torch.aten.mul.Scalar %3210, %int2_4916 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3211, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_4917 = torch.constant.int 1
    %int1_4918 = torch.constant.int 1
    %3212 = torch.aten.add.Scalar %3211, %int1_4917, %int1_4918 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3212, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %3213 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %3214 = torch.aten.view %3212, %3213 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3214, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_4919 = torch.constant.int 4
    %int32_4920 = torch.constant.int 32
    %int4_4921 = torch.constant.int 4
    %int64_4922 = torch.constant.int 64
    %3215 = torch.prim.ListConstruct %int4_4919, %271, %int32_4920, %int4_4921, %int64_4922 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3216 = torch.aten.view %3105, %3215 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %3216, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_4923 = torch.constant.int 32
    %int4_4924 = torch.constant.int 4
    %int64_4925 = torch.constant.int 64
    %3217 = torch.prim.ListConstruct %446, %int32_4923, %int4_4924, %int64_4925 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3218 = torch.aten.view %3216, %3217 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %3218, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_4926 = torch.constant.int 1
    %int2_4927 = torch.constant.int 2
    %3219 = torch.aten.transpose.int %3218, %int1_4926, %int2_4927 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %3219, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_4928 = torch.constant.none
    %none_4929 = torch.constant.none
    %int5_4930 = torch.constant.int 5
    %cpu_4931 = torch.constant.device "cpu"
    %int0_4932 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3219, %none_4928, %none_4929, %int5_4930, %cpu_4931, %int0_4932 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %3220 = torch.prim.ListConstruct %3214 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_4933 = torch.constant.bool false
    %3221 = torch.aten.index_put %3208, %3220, %3219, %false_4933 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %3221, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_4934 = torch.constant.int 22
    %int2_4935 = torch.constant.int 2
    %int4_4936 = torch.constant.int 4
    %int32_4937 = torch.constant.int 32
    %int64_4938 = torch.constant.int 64
    %3222 = torch.prim.ListConstruct %272, %int22_4934, %int2_4935, %int4_4936, %int32_4937, %int64_4938 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3223 = torch.aten.view %3221, %3222 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3223, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_4939 = torch.constant.int 360448
    %3224 = torch.prim.ListConstruct %272, %int360448_4939 : (!torch.int, !torch.int) -> !torch.list<int>
    %3225 = torch.aten.view %3223, %3224 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %3225, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_4940 = torch.constant.int 0
    %int1_4941 = torch.constant.int 1
    %none_4942 = torch.constant.none
    %none_4943 = torch.constant.none
    %cpu_4944 = torch.constant.device "cpu"
    %false_4945 = torch.constant.bool false
    %3226 = torch.aten.arange.start_step %int0_4940, %273, %int1_4941, %none_4942, %none_4943, %cpu_4944, %false_4945 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3226, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_4946 = torch.constant.int -1
    %3227 = torch.aten.unsqueeze %arg1, %int-1_4946 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %3228 = torch.aten.ge.Tensor %3226, %3227 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %3228, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_4947 = torch.constant.none
    %none_4948 = torch.constant.none
    %cpu_4949 = torch.constant.device "cpu"
    %false_4950 = torch.constant.bool false
    %3229 = torch.aten.arange %273, %none_4947, %none_4948, %cpu_4949, %false_4950 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3229, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_4951 = torch.constant.int 0
    %3230 = torch.aten.unsqueeze %3229, %int0_4951 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %3230, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_4952 = torch.constant.int 1
    %3231 = torch.aten.unsqueeze %3230, %int1_4952 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %3231, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_4953 = torch.constant.int 2
    %3232 = torch.aten.unsqueeze %3231, %int2_4953 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %3232, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_4954 = torch.constant.none
    %none_4955 = torch.constant.none
    %cpu_4956 = torch.constant.device "cpu"
    %false_4957 = torch.constant.bool false
    %3233 = torch.aten.arange %273, %none_4954, %none_4955, %cpu_4956, %false_4957 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3233, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_4958 = torch.constant.int 0
    %3234 = torch.aten.unsqueeze %3233, %int0_4958 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %3234, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_4959 = torch.constant.int 1
    %3235 = torch.aten.unsqueeze %3234, %int1_4959 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %3235, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_4960 = torch.constant.int 3
    %3236 = torch.aten.unsqueeze %3235, %int3_4960 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %3236, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %3237 = torch.aten.gt.Tensor %3232, %3236 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %3237, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_4961 = torch.constant.int 1
    %3238 = torch.aten.unsqueeze %3228, %int1_4961 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %3238, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_4962 = torch.constant.int 2
    %3239 = torch.aten.unsqueeze %3238, %int2_4962 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %3239, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %3240 = torch.aten.logical_or %3237, %3239 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %3240, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_4963 = torch.constant.none
    %3241 = torch.aten.clone %111, %none_4963 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_4964 = torch.constant.int 0
    %3242 = torch.aten.where.ScalarOther %3240, %3241, %int0_4964 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %3242, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_4965 = torch.constant.none
    %none_4966 = torch.constant.none
    %int5_4967 = torch.constant.int 5
    %cpu_4968 = torch.constant.device "cpu"
    %int0_4969 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3242, %none_4965, %none_4966, %int5_4967, %cpu_4968, %int0_4969 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_4970 = torch.constant.int -2
    %3243 = torch.aten.unsqueeze %3183, %int-2_4970 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %3243, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_4971 = torch.constant.int 4
    %int4_4972 = torch.constant.int 4
    %int8_4973 = torch.constant.int 8
    %int64_4974 = torch.constant.int 64
    %3244 = torch.prim.ListConstruct %int4_4971, %273, %int4_4972, %int8_4973, %int64_4974 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4975 = torch.constant.bool false
    %3245 = torch.aten.expand %3243, %3244, %false_4975 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3245, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_4976 = torch.constant.int 0
    %3246 = torch.aten.clone %3245, %int0_4976 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3246, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_4977 = torch.constant.int 4
    %int32_4978 = torch.constant.int 32
    %int64_4979 = torch.constant.int 64
    %3247 = torch.prim.ListConstruct %int4_4977, %273, %int32_4978, %int64_4979 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3248 = torch.aten._unsafe_view %3246, %3247 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3248, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_4980 = torch.constant.int -2
    %3249 = torch.aten.unsqueeze %3105, %int-2_4980 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %3249, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_4981 = torch.constant.int 4
    %int4_4982 = torch.constant.int 4
    %int8_4983 = torch.constant.int 8
    %int64_4984 = torch.constant.int 64
    %3250 = torch.prim.ListConstruct %int4_4981, %273, %int4_4982, %int8_4983, %int64_4984 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4985 = torch.constant.bool false
    %3251 = torch.aten.expand %3249, %3250, %false_4985 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3251, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_4986 = torch.constant.int 0
    %3252 = torch.aten.clone %3251, %int0_4986 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3252, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_4987 = torch.constant.int 4
    %int32_4988 = torch.constant.int 32
    %int64_4989 = torch.constant.int 64
    %3253 = torch.prim.ListConstruct %int4_4987, %273, %int32_4988, %int64_4989 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3254 = torch.aten._unsafe_view %3252, %3253 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3254, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_4990 = torch.constant.int 1
    %int2_4991 = torch.constant.int 2
    %3255 = torch.aten.transpose.int %3144, %int1_4990, %int2_4991 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %3255, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_4992 = torch.constant.int 1
    %int2_4993 = torch.constant.int 2
    %3256 = torch.aten.transpose.int %3248, %int1_4992, %int2_4993 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %3256, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_4994 = torch.constant.int 1
    %int2_4995 = torch.constant.int 2
    %3257 = torch.aten.transpose.int %3254, %int1_4994, %int2_4995 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %3257, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_4996 = torch.constant.float 0.000000e+00
    %false_4997 = torch.constant.bool false
    %none_4998 = torch.constant.none
    %false_4999 = torch.constant.bool false
    %3258 = torch.aten.scaled_dot_product_attention %3255, %3256, %3257, %3242, %float0.000000e00_4996, %false_4997, %none_4998, %false_4999 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %3258, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_5000 = torch.constant.int 1
    %int2_5001 = torch.constant.int 2
    %3259 = torch.aten.transpose.int %3258, %int1_5000, %int2_5001 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3259, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_5002 = torch.constant.int 4
    %int2048_5003 = torch.constant.int 2048
    %3260 = torch.prim.ListConstruct %int4_5002, %273, %int2048_5003 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3261 = torch.aten.view %3259, %3260 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3261, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_5004 = torch.constant.int 2
    %3262 = torch.aten.view.dtype %116, %int2_5004 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %3263 = torch.aten.detach %3262 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_5005 = torch.constant.int -1
    %int17_5006 = torch.constant.int 17
    %3264 = torch.prim.ListConstruct %int-1_5005, %int17_5006 : (!torch.int, !torch.int) -> !torch.list<int>
    %3265 = torch.aten.view %3263, %3264 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_5007 = torch.constant.int 2048
    %int-1_5008 = torch.constant.int -1
    %int17_5009 = torch.constant.int 17
    %3266 = torch.prim.ListConstruct %int2048_5007, %int-1_5008, %int17_5009 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3267 = torch.aten.view %3265, %3266 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_5010 = torch.constant.int 2
    %int0_5011 = torch.constant.int 0
    %int1_5012 = torch.constant.int 1
    %int1_5013 = torch.constant.int 1
    %3268 = torch.aten.slice.Tensor %3267, %int2_5010, %int0_5011, %int1_5012, %int1_5013 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_5014 = torch.constant.int 5
    %3269 = torch.aten.view.dtype %3268, %int5_5014 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %3270 = torch.aten.detach %3269 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_5015 = torch.constant.int 2
    %int1_5016 = torch.constant.int 1
    %int9223372036854775807_5017 = torch.constant.int 9223372036854775807
    %int1_5018 = torch.constant.int 1
    %3271 = torch.aten.slice.Tensor %3267, %int2_5015, %int1_5016, %int9223372036854775807_5017, %int1_5018 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_5019 = torch.constant.int 1
    %3272 = torch.aten.view.dtype %3271, %int1_5019 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %3273 = torch.aten.detach %3272 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %3274 = torch_c.to_builtin_tensor %3261 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_5020 = tensor.cast %3274 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3275 = torch_c.to_builtin_tensor %3270 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %3276 = torch_c.to_builtin_tensor %3273 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %3277 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_5020, %3275, %3276) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_5021 = tensor.cast %3277 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %3278 = torch_c.from_builtin_tensor %cast_5021 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3278, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_5022 = torch.constant.none
    %none_5023 = torch.constant.none
    %int5_5024 = torch.constant.int 5
    %cpu_5025 = torch.constant.device "cpu"
    %int0_5026 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3278, %none_5022, %none_5023, %int5_5024, %cpu_5025, %int0_5026 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_5027 = torch.constant.int 1
    %3279 = torch.aten.add.Tensor %3038, %3278, %int1_5027 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3279, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_5028 = torch.constant.none
    %none_5029 = torch.constant.none
    %int5_5030 = torch.constant.int 5
    %cpu_5031 = torch.constant.device "cpu"
    %int0_5032 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3279, %none_5028, %none_5029, %int5_5030, %cpu_5031, %int0_5032 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5033 = torch.constant.int 6
    %3280 = torch.prims.convert_element_type %3279, %int6_5033 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3280, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_5034 = torch.constant.int 2
    %3281 = torch.aten.pow.Tensor_Scalar %3280, %int2_5034 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3281, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_5035 = torch.constant.int -1
    %3282 = torch.prim.ListConstruct %int-1_5035 : (!torch.int) -> !torch.list<int>
    %true_5036 = torch.constant.bool true
    %none_5037 = torch.constant.none
    %3283 = torch.aten.mean.dim %3281, %3282, %true_5036, %none_5037 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3283, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_5038 = torch.constant.float 9.9999997473787516E-6
    %int1_5039 = torch.constant.int 1
    %3284 = torch.aten.add.Scalar %3283, %float9.999990e-06_5038, %int1_5039 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3284, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3285 = torch.aten.rsqrt %3284 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3285, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3286 = torch.aten.mul.Tensor %3280, %3285 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3286, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_5040 = torch.constant.none
    %none_5041 = torch.constant.none
    %int6_5042 = torch.constant.int 6
    %cpu_5043 = torch.constant.device "cpu"
    %int0_5044 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3286, %none_5040, %none_5041, %int6_5042, %cpu_5043, %int0_5044 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5045 = torch.constant.int 5
    %3287 = torch.prims.convert_element_type %3286, %int5_5045 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3287, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %3288 = torch.aten.mul.Tensor %117, %3287 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3288, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_5046 = torch.constant.none
    %none_5047 = torch.constant.none
    %int6_5048 = torch.constant.int 6
    %cpu_5049 = torch.constant.device "cpu"
    %int0_5050 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3288, %none_5046, %none_5047, %int6_5048, %cpu_5049, %int0_5050 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5051 = torch.constant.int 5
    %3289 = torch.prims.convert_element_type %3288, %int5_5051 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3289, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_5052 = torch.constant.int 2
    %3290 = torch.aten.view.dtype %118, %int2_5052 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %3291 = torch.aten.detach %3290 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_5053 = torch.constant.int -1
    %int17_5054 = torch.constant.int 17
    %3292 = torch.prim.ListConstruct %int-1_5053, %int17_5054 : (!torch.int, !torch.int) -> !torch.list<int>
    %3293 = torch.aten.view %3291, %3292 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_5055 = torch.constant.int 5632
    %int-1_5056 = torch.constant.int -1
    %int17_5057 = torch.constant.int 17
    %3294 = torch.prim.ListConstruct %int5632_5055, %int-1_5056, %int17_5057 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3295 = torch.aten.view %3293, %3294 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_5058 = torch.constant.int 2
    %int0_5059 = torch.constant.int 0
    %int1_5060 = torch.constant.int 1
    %int1_5061 = torch.constant.int 1
    %3296 = torch.aten.slice.Tensor %3295, %int2_5058, %int0_5059, %int1_5060, %int1_5061 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_5062 = torch.constant.int 5
    %3297 = torch.aten.view.dtype %3296, %int5_5062 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %3298 = torch.aten.detach %3297 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_5063 = torch.constant.int 2
    %int1_5064 = torch.constant.int 1
    %int9223372036854775807_5065 = torch.constant.int 9223372036854775807
    %int1_5066 = torch.constant.int 1
    %3299 = torch.aten.slice.Tensor %3295, %int2_5063, %int1_5064, %int9223372036854775807_5065, %int1_5066 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_5067 = torch.constant.int 1
    %3300 = torch.aten.view.dtype %3299, %int1_5067 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %3301 = torch.aten.detach %3300 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %3302 = torch_c.to_builtin_tensor %3289 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_5068 = tensor.cast %3302 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3303 = torch_c.to_builtin_tensor %3298 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %3304 = torch_c.to_builtin_tensor %3301 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %3305 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_5068, %3303, %3304) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_5069 = tensor.cast %3305 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %3306 = torch_c.from_builtin_tensor %cast_5069 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %3306, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %3307 = torch.aten.silu %3306 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %3307, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_5070 = torch.constant.int 2
    %3308 = torch.aten.view.dtype %119, %int2_5070 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %3309 = torch.aten.detach %3308 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_5071 = torch.constant.int -1
    %int17_5072 = torch.constant.int 17
    %3310 = torch.prim.ListConstruct %int-1_5071, %int17_5072 : (!torch.int, !torch.int) -> !torch.list<int>
    %3311 = torch.aten.view %3309, %3310 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_5073 = torch.constant.int 5632
    %int-1_5074 = torch.constant.int -1
    %int17_5075 = torch.constant.int 17
    %3312 = torch.prim.ListConstruct %int5632_5073, %int-1_5074, %int17_5075 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3313 = torch.aten.view %3311, %3312 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_5076 = torch.constant.int 2
    %int0_5077 = torch.constant.int 0
    %int1_5078 = torch.constant.int 1
    %int1_5079 = torch.constant.int 1
    %3314 = torch.aten.slice.Tensor %3313, %int2_5076, %int0_5077, %int1_5078, %int1_5079 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_5080 = torch.constant.int 5
    %3315 = torch.aten.view.dtype %3314, %int5_5080 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %3316 = torch.aten.detach %3315 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_5081 = torch.constant.int 2
    %int1_5082 = torch.constant.int 1
    %int9223372036854775807_5083 = torch.constant.int 9223372036854775807
    %int1_5084 = torch.constant.int 1
    %3317 = torch.aten.slice.Tensor %3313, %int2_5081, %int1_5082, %int9223372036854775807_5083, %int1_5084 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_5085 = torch.constant.int 1
    %3318 = torch.aten.view.dtype %3317, %int1_5085 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %3319 = torch.aten.detach %3318 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %3320 = torch_c.to_builtin_tensor %3289 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_5086 = tensor.cast %3320 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3321 = torch_c.to_builtin_tensor %3316 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %3322 = torch_c.to_builtin_tensor %3319 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %3323 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_5086, %3321, %3322) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_5087 = tensor.cast %3323 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %3324 = torch_c.from_builtin_tensor %cast_5087 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %3324, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %3325 = torch.aten.mul.Tensor %3307, %3324 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %3325, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_5088 = torch.constant.int 2
    %3326 = torch.aten.view.dtype %120, %int2_5088 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %3327 = torch.aten.detach %3326 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_5089 = torch.constant.int -1
    %int17_5090 = torch.constant.int 17
    %3328 = torch.prim.ListConstruct %int-1_5089, %int17_5090 : (!torch.int, !torch.int) -> !torch.list<int>
    %3329 = torch.aten.view %3327, %3328 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_5091 = torch.constant.int 2048
    %int-1_5092 = torch.constant.int -1
    %int17_5093 = torch.constant.int 17
    %3330 = torch.prim.ListConstruct %int2048_5091, %int-1_5092, %int17_5093 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3331 = torch.aten.view %3329, %3330 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_5094 = torch.constant.int 2
    %int0_5095 = torch.constant.int 0
    %int1_5096 = torch.constant.int 1
    %int1_5097 = torch.constant.int 1
    %3332 = torch.aten.slice.Tensor %3331, %int2_5094, %int0_5095, %int1_5096, %int1_5097 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_5098 = torch.constant.int 5
    %3333 = torch.aten.view.dtype %3332, %int5_5098 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %3334 = torch.aten.detach %3333 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_5099 = torch.constant.int 2
    %int1_5100 = torch.constant.int 1
    %int9223372036854775807_5101 = torch.constant.int 9223372036854775807
    %int1_5102 = torch.constant.int 1
    %3335 = torch.aten.slice.Tensor %3331, %int2_5099, %int1_5100, %int9223372036854775807_5101, %int1_5102 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_5103 = torch.constant.int 1
    %3336 = torch.aten.view.dtype %3335, %int1_5103 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %3337 = torch.aten.detach %3336 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %3338 = torch_c.to_builtin_tensor %3325 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_5104 = tensor.cast %3338 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %3339 = torch_c.to_builtin_tensor %3334 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %3340 = torch_c.to_builtin_tensor %3337 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %3341 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_5104, %3339, %3340) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_5105 = tensor.cast %3341 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %3342 = torch_c.from_builtin_tensor %cast_5105 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3342, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_5106 = torch.constant.int 1
    %3343 = torch.aten.add.Tensor %3279, %3342, %int1_5106 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3343, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_5107 = torch.constant.none
    %none_5108 = torch.constant.none
    %int5_5109 = torch.constant.int 5
    %cpu_5110 = torch.constant.device "cpu"
    %int0_5111 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3343, %none_5107, %none_5108, %int5_5109, %cpu_5110, %int0_5111 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5112 = torch.constant.int 6
    %3344 = torch.prims.convert_element_type %3343, %int6_5112 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3344, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_5113 = torch.constant.int 2
    %3345 = torch.aten.pow.Tensor_Scalar %3344, %int2_5113 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3345, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_5114 = torch.constant.int -1
    %3346 = torch.prim.ListConstruct %int-1_5114 : (!torch.int) -> !torch.list<int>
    %true_5115 = torch.constant.bool true
    %none_5116 = torch.constant.none
    %3347 = torch.aten.mean.dim %3345, %3346, %true_5115, %none_5116 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3347, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_5117 = torch.constant.float 9.9999997473787516E-6
    %int1_5118 = torch.constant.int 1
    %3348 = torch.aten.add.Scalar %3347, %float9.999990e-06_5117, %int1_5118 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3348, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3349 = torch.aten.rsqrt %3348 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3349, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3350 = torch.aten.mul.Tensor %3344, %3349 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3350, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_5119 = torch.constant.none
    %none_5120 = torch.constant.none
    %int6_5121 = torch.constant.int 6
    %cpu_5122 = torch.constant.device "cpu"
    %int0_5123 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3350, %none_5119, %none_5120, %int6_5121, %cpu_5122, %int0_5123 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5124 = torch.constant.int 5
    %3351 = torch.prims.convert_element_type %3350, %int5_5124 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3351, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %3352 = torch.aten.mul.Tensor %124, %3351 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3352, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_5125 = torch.constant.none
    %none_5126 = torch.constant.none
    %int6_5127 = torch.constant.int 6
    %cpu_5128 = torch.constant.device "cpu"
    %int0_5129 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3352, %none_5125, %none_5126, %int6_5127, %cpu_5128, %int0_5129 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5130 = torch.constant.int 5
    %3353 = torch.prims.convert_element_type %3352, %int5_5130 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3353, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_5131 = torch.constant.int 2
    %3354 = torch.aten.view.dtype %125, %int2_5131 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %3355 = torch.aten.detach %3354 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_5132 = torch.constant.int -1
    %int17_5133 = torch.constant.int 17
    %3356 = torch.prim.ListConstruct %int-1_5132, %int17_5133 : (!torch.int, !torch.int) -> !torch.list<int>
    %3357 = torch.aten.view %3355, %3356 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_5134 = torch.constant.int 2048
    %int-1_5135 = torch.constant.int -1
    %int17_5136 = torch.constant.int 17
    %3358 = torch.prim.ListConstruct %int2048_5134, %int-1_5135, %int17_5136 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3359 = torch.aten.view %3357, %3358 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_5137 = torch.constant.int 2
    %int0_5138 = torch.constant.int 0
    %int1_5139 = torch.constant.int 1
    %int1_5140 = torch.constant.int 1
    %3360 = torch.aten.slice.Tensor %3359, %int2_5137, %int0_5138, %int1_5139, %int1_5140 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_5141 = torch.constant.int 5
    %3361 = torch.aten.view.dtype %3360, %int5_5141 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %3362 = torch.aten.detach %3361 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_5142 = torch.constant.int 2
    %int1_5143 = torch.constant.int 1
    %int9223372036854775807_5144 = torch.constant.int 9223372036854775807
    %int1_5145 = torch.constant.int 1
    %3363 = torch.aten.slice.Tensor %3359, %int2_5142, %int1_5143, %int9223372036854775807_5144, %int1_5145 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_5146 = torch.constant.int 1
    %3364 = torch.aten.view.dtype %3363, %int1_5146 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %3365 = torch.aten.detach %3364 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %3366 = torch_c.to_builtin_tensor %3353 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_5147 = tensor.cast %3366 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3367 = torch_c.to_builtin_tensor %3362 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %3368 = torch_c.to_builtin_tensor %3365 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %3369 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_5147, %3367, %3368) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_5148 = tensor.cast %3369 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %3370 = torch_c.from_builtin_tensor %cast_5148 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3370, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_5149 = torch.constant.int 2
    %3371 = torch.aten.view.dtype %126, %int2_5149 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %3372 = torch.aten.detach %3371 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_5150 = torch.constant.int -1
    %int17_5151 = torch.constant.int 17
    %3373 = torch.prim.ListConstruct %int-1_5150, %int17_5151 : (!torch.int, !torch.int) -> !torch.list<int>
    %3374 = torch.aten.view %3372, %3373 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_5152 = torch.constant.int 256
    %int-1_5153 = torch.constant.int -1
    %int17_5154 = torch.constant.int 17
    %3375 = torch.prim.ListConstruct %int256_5152, %int-1_5153, %int17_5154 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3376 = torch.aten.view %3374, %3375 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_5155 = torch.constant.int 2
    %int0_5156 = torch.constant.int 0
    %int1_5157 = torch.constant.int 1
    %int1_5158 = torch.constant.int 1
    %3377 = torch.aten.slice.Tensor %3376, %int2_5155, %int0_5156, %int1_5157, %int1_5158 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_5159 = torch.constant.int 5
    %3378 = torch.aten.view.dtype %3377, %int5_5159 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %3379 = torch.aten.detach %3378 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_5160 = torch.constant.int 2
    %int1_5161 = torch.constant.int 1
    %int9223372036854775807_5162 = torch.constant.int 9223372036854775807
    %int1_5163 = torch.constant.int 1
    %3380 = torch.aten.slice.Tensor %3376, %int2_5160, %int1_5161, %int9223372036854775807_5162, %int1_5163 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_5164 = torch.constant.int 1
    %3381 = torch.aten.view.dtype %3380, %int1_5164 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %3382 = torch.aten.detach %3381 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %3383 = torch_c.to_builtin_tensor %3353 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_5165 = tensor.cast %3383 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3384 = torch_c.to_builtin_tensor %3379 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %3385 = torch_c.to_builtin_tensor %3382 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %3386 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_5165, %3384, %3385) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_5166 = tensor.cast %3386 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %3387 = torch_c.from_builtin_tensor %cast_5166 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %3387, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_5167 = torch.constant.int 2
    %3388 = torch.aten.view.dtype %127, %int2_5167 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %3389 = torch.aten.detach %3388 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_5168 = torch.constant.int -1
    %int17_5169 = torch.constant.int 17
    %3390 = torch.prim.ListConstruct %int-1_5168, %int17_5169 : (!torch.int, !torch.int) -> !torch.list<int>
    %3391 = torch.aten.view %3389, %3390 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_5170 = torch.constant.int 256
    %int-1_5171 = torch.constant.int -1
    %int17_5172 = torch.constant.int 17
    %3392 = torch.prim.ListConstruct %int256_5170, %int-1_5171, %int17_5172 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3393 = torch.aten.view %3391, %3392 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_5173 = torch.constant.int 2
    %int0_5174 = torch.constant.int 0
    %int1_5175 = torch.constant.int 1
    %int1_5176 = torch.constant.int 1
    %3394 = torch.aten.slice.Tensor %3393, %int2_5173, %int0_5174, %int1_5175, %int1_5176 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_5177 = torch.constant.int 5
    %3395 = torch.aten.view.dtype %3394, %int5_5177 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %3396 = torch.aten.detach %3395 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_5178 = torch.constant.int 2
    %int1_5179 = torch.constant.int 1
    %int9223372036854775807_5180 = torch.constant.int 9223372036854775807
    %int1_5181 = torch.constant.int 1
    %3397 = torch.aten.slice.Tensor %3393, %int2_5178, %int1_5179, %int9223372036854775807_5180, %int1_5181 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_5182 = torch.constant.int 1
    %3398 = torch.aten.view.dtype %3397, %int1_5182 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %3399 = torch.aten.detach %3398 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %3400 = torch_c.to_builtin_tensor %3353 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_5183 = tensor.cast %3400 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3401 = torch_c.to_builtin_tensor %3396 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %3402 = torch_c.to_builtin_tensor %3399 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %3403 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_5183, %3401, %3402) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_5184 = tensor.cast %3403 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %3404 = torch_c.from_builtin_tensor %cast_5184 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %3404, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_5185 = torch.constant.int 4
    %int32_5186 = torch.constant.int 32
    %int64_5187 = torch.constant.int 64
    %3405 = torch.prim.ListConstruct %int4_5185, %273, %int32_5186, %int64_5187 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3406 = torch.aten.view %3370, %3405 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3406, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_5188 = torch.constant.int 4
    %int4_5189 = torch.constant.int 4
    %int64_5190 = torch.constant.int 64
    %3407 = torch.prim.ListConstruct %int4_5188, %273, %int4_5189, %int64_5190 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3408 = torch.aten.view %3387, %3407 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %3408, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_5191 = torch.constant.int 4
    %int4_5192 = torch.constant.int 4
    %int64_5193 = torch.constant.int 64
    %3409 = torch.prim.ListConstruct %int4_5191, %273, %int4_5192, %int64_5193 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3410 = torch.aten.view %3404, %3409 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %3410, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_5194 = torch.constant.int 0
    %none_5195 = torch.constant.none
    %none_5196 = torch.constant.none
    %cpu_5197 = torch.constant.device "cpu"
    %false_5198 = torch.constant.bool false
    %3411 = torch.aten.arange.start %int0_5194, %273, %none_5195, %none_5196, %cpu_5197, %false_5198 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3411, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_5199 = torch.constant.int 0
    %3412 = torch.aten.unsqueeze %3411, %int0_5199 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %3412, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_5200 = torch.constant.int 0
    %int64_5201 = torch.constant.int 64
    %int2_5202 = torch.constant.int 2
    %none_5203 = torch.constant.none
    %none_5204 = torch.constant.none
    %cpu_5205 = torch.constant.device "cpu"
    %false_5206 = torch.constant.bool false
    %3413 = torch.aten.arange.start_step %int0_5200, %int64_5201, %int2_5202, %none_5203, %none_5204, %cpu_5205, %false_5206 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_5207 = torch.constant.none
    %none_5208 = torch.constant.none
    %int4_5209 = torch.constant.int 4
    %cpu_5210 = torch.constant.device "cpu"
    %int0_5211 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3413, %none_5207, %none_5208, %int4_5209, %cpu_5210, %int0_5211 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5212 = torch.constant.int 6
    %3414 = torch.prims.convert_element_type %3413, %int6_5212 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_5213 = torch.constant.int 64
    %3415 = torch.aten.div.Scalar %3414, %int64_5213 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_5214 = torch.constant.float 1.000000e+04
    %3416 = torch.aten.pow.Scalar %float1.000000e04_5214, %3415 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %3417 = torch.aten.reciprocal %3416 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_5215 = torch.constant.float 1.000000e+00
    %3418 = torch.aten.mul.Scalar %3417, %float1.000000e00_5215 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_5216 = torch.constant.none
    %3419 = torch.aten.clone %121, %none_5216 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_5217 = torch.constant.int 0
    %3420 = torch.aten.unsqueeze %3418, %int0_5217 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_5218 = torch.constant.int 2
    %3421 = torch.aten.unsqueeze %3420, %int2_5218 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_5219 = torch.constant.none
    %none_5220 = torch.constant.none
    %int6_5221 = torch.constant.int 6
    %cpu_5222 = torch.constant.device "cpu"
    %int0_5223 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3421, %none_5219, %none_5220, %int6_5221, %cpu_5222, %int0_5223 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_5224 = torch.constant.int 1
    %int-1_5225 = torch.constant.int -1
    %int1_5226 = torch.constant.int 1
    %3422 = torch.prim.ListConstruct %int1_5224, %int-1_5225, %int1_5226 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5227 = torch.constant.bool false
    %3423 = torch.aten.expand %3421, %3422, %false_5227 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_5228 = torch.constant.int 1
    %3424 = torch.aten.unsqueeze %3412, %int1_5228 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %3424, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_5229 = torch.constant.none
    %none_5230 = torch.constant.none
    %int4_5231 = torch.constant.int 4
    %cpu_5232 = torch.constant.device "cpu"
    %int0_5233 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3424, %none_5229, %none_5230, %int4_5231, %cpu_5232, %int0_5233 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5234 = torch.constant.int 6
    %3425 = torch.prims.convert_element_type %3424, %int6_5234 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %3425, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %3426 = torch.aten.matmul %3423, %3425 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %3426, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_5235 = torch.constant.int 1
    %int2_5236 = torch.constant.int 2
    %3427 = torch.aten.transpose.int %3426, %int1_5235, %int2_5236 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3427, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %3428 = torch.aten.cos %3427 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3428, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %3429 = torch.aten.mul.Tensor %3428, %3419 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3429, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_5237 = torch.constant.none
    %none_5238 = torch.constant.none
    %int6_5239 = torch.constant.int 6
    %cpu_5240 = torch.constant.device "cpu"
    %int0_5241 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3429, %none_5237, %none_5238, %int6_5239, %cpu_5240, %int0_5241 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5242 = torch.constant.int 5
    %3430 = torch.prims.convert_element_type %3429, %int5_5242 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %3430, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %3431 = torch.aten.sin %3427 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3431, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %3432 = torch.aten.mul.Tensor %3431, %3419 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3432, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_5243 = torch.constant.none
    %none_5244 = torch.constant.none
    %int6_5245 = torch.constant.int 6
    %cpu_5246 = torch.constant.device "cpu"
    %int0_5247 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3432, %none_5243, %none_5244, %int6_5245, %cpu_5246, %int0_5247 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5248 = torch.constant.int 5
    %3433 = torch.prims.convert_element_type %3432, %int5_5248 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %3433, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_5249 = torch.constant.int 2
    %3434 = torch.aten.unsqueeze %3430, %int2_5249 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %3434, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_5250 = torch.constant.int 2
    %3435 = torch.aten.unsqueeze %3433, %int2_5250 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %3435, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_5251 = torch.constant.none
    %none_5252 = torch.constant.none
    %int5_5253 = torch.constant.int 5
    %cpu_5254 = torch.constant.device "cpu"
    %int0_5255 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3434, %none_5251, %none_5252, %int5_5253, %cpu_5254, %int0_5255 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_5256 = torch.constant.none
    %none_5257 = torch.constant.none
    %int5_5258 = torch.constant.int 5
    %cpu_5259 = torch.constant.device "cpu"
    %int0_5260 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3435, %none_5256, %none_5257, %int5_5258, %cpu_5259, %int0_5260 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_5261 = torch.constant.none
    %none_5262 = torch.constant.none
    %int5_5263 = torch.constant.int 5
    %cpu_5264 = torch.constant.device "cpu"
    %int0_5265 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3406, %none_5261, %none_5262, %int5_5263, %cpu_5264, %int0_5265 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_5266 = torch.constant.int 3
    %int0_5267 = torch.constant.int 0
    %int64_5268 = torch.constant.int 64
    %int2_5269 = torch.constant.int 2
    %3436 = torch.aten.slice.Tensor %3406, %int3_5266, %int0_5267, %int64_5268, %int2_5269 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3436, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_5270 = torch.constant.int 3
    %int1_5271 = torch.constant.int 1
    %int64_5272 = torch.constant.int 64
    %int2_5273 = torch.constant.int 2
    %3437 = torch.aten.slice.Tensor %3406, %int3_5270, %int1_5271, %int64_5272, %int2_5273 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3437, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %3438 = torch.aten.mul.Tensor %3436, %3434 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3438, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %3439 = torch.aten.mul.Tensor %3437, %3435 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3439, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_5274 = torch.constant.int 1
    %3440 = torch.aten.sub.Tensor %3438, %3439, %int1_5274 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3440, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %3441 = torch.aten.mul.Tensor %3437, %3434 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3441, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %3442 = torch.aten.mul.Tensor %3436, %3435 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3442, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_5275 = torch.constant.int 1
    %3443 = torch.aten.add.Tensor %3441, %3442, %int1_5275 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3443, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %3444 = torch_c.to_builtin_tensor %3440 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_5276 = tensor.cast %3444 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %3445 = torch_c.to_builtin_tensor %3443 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_5277 = tensor.cast %3445 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %3446 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_5276, %cast_5277) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_5278 = tensor.cast %3446 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %3447 = torch_c.from_builtin_tensor %cast_5278 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %3447, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_5279 = torch.constant.int 4
    %int32_5280 = torch.constant.int 32
    %int64_5281 = torch.constant.int 64
    %3448 = torch.prim.ListConstruct %int4_5279, %273, %int32_5280, %int64_5281 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3449 = torch.aten.view %3447, %3448 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3449, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_5282 = torch.constant.none
    %none_5283 = torch.constant.none
    %int5_5284 = torch.constant.int 5
    %cpu_5285 = torch.constant.device "cpu"
    %int0_5286 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3449, %none_5282, %none_5283, %int5_5284, %cpu_5285, %int0_5286 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_5287 = torch.constant.int 0
    %none_5288 = torch.constant.none
    %none_5289 = torch.constant.none
    %cpu_5290 = torch.constant.device "cpu"
    %false_5291 = torch.constant.bool false
    %3450 = torch.aten.arange.start %int0_5287, %273, %none_5288, %none_5289, %cpu_5290, %false_5291 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3450, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_5292 = torch.constant.int 0
    %3451 = torch.aten.unsqueeze %3450, %int0_5292 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %3451, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_5293 = torch.constant.int 0
    %int64_5294 = torch.constant.int 64
    %int2_5295 = torch.constant.int 2
    %none_5296 = torch.constant.none
    %none_5297 = torch.constant.none
    %cpu_5298 = torch.constant.device "cpu"
    %false_5299 = torch.constant.bool false
    %3452 = torch.aten.arange.start_step %int0_5293, %int64_5294, %int2_5295, %none_5296, %none_5297, %cpu_5298, %false_5299 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_5300 = torch.constant.none
    %none_5301 = torch.constant.none
    %int4_5302 = torch.constant.int 4
    %cpu_5303 = torch.constant.device "cpu"
    %int0_5304 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3452, %none_5300, %none_5301, %int4_5302, %cpu_5303, %int0_5304 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5305 = torch.constant.int 6
    %3453 = torch.prims.convert_element_type %3452, %int6_5305 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_5306 = torch.constant.int 64
    %3454 = torch.aten.div.Scalar %3453, %int64_5306 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_5307 = torch.constant.float 1.000000e+04
    %3455 = torch.aten.pow.Scalar %float1.000000e04_5307, %3454 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %3456 = torch.aten.reciprocal %3455 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_5308 = torch.constant.float 1.000000e+00
    %3457 = torch.aten.mul.Scalar %3456, %float1.000000e00_5308 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_5309 = torch.constant.none
    %3458 = torch.aten.clone %122, %none_5309 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_5310 = torch.constant.int 0
    %3459 = torch.aten.unsqueeze %3457, %int0_5310 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_5311 = torch.constant.int 2
    %3460 = torch.aten.unsqueeze %3459, %int2_5311 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_5312 = torch.constant.none
    %none_5313 = torch.constant.none
    %int6_5314 = torch.constant.int 6
    %cpu_5315 = torch.constant.device "cpu"
    %int0_5316 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3460, %none_5312, %none_5313, %int6_5314, %cpu_5315, %int0_5316 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_5317 = torch.constant.int 1
    %int-1_5318 = torch.constant.int -1
    %int1_5319 = torch.constant.int 1
    %3461 = torch.prim.ListConstruct %int1_5317, %int-1_5318, %int1_5319 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5320 = torch.constant.bool false
    %3462 = torch.aten.expand %3460, %3461, %false_5320 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_5321 = torch.constant.int 1
    %3463 = torch.aten.unsqueeze %3451, %int1_5321 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %3463, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_5322 = torch.constant.none
    %none_5323 = torch.constant.none
    %int4_5324 = torch.constant.int 4
    %cpu_5325 = torch.constant.device "cpu"
    %int0_5326 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3463, %none_5322, %none_5323, %int4_5324, %cpu_5325, %int0_5326 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5327 = torch.constant.int 6
    %3464 = torch.prims.convert_element_type %3463, %int6_5327 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %3464, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %3465 = torch.aten.matmul %3462, %3464 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %3465, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_5328 = torch.constant.int 1
    %int2_5329 = torch.constant.int 2
    %3466 = torch.aten.transpose.int %3465, %int1_5328, %int2_5329 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3466, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %3467 = torch.aten.cos %3466 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3467, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %3468 = torch.aten.mul.Tensor %3467, %3458 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3468, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_5330 = torch.constant.none
    %none_5331 = torch.constant.none
    %int6_5332 = torch.constant.int 6
    %cpu_5333 = torch.constant.device "cpu"
    %int0_5334 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3468, %none_5330, %none_5331, %int6_5332, %cpu_5333, %int0_5334 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5335 = torch.constant.int 5
    %3469 = torch.prims.convert_element_type %3468, %int5_5335 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %3469, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %3470 = torch.aten.sin %3466 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3470, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %3471 = torch.aten.mul.Tensor %3470, %3458 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3471, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_5336 = torch.constant.none
    %none_5337 = torch.constant.none
    %int6_5338 = torch.constant.int 6
    %cpu_5339 = torch.constant.device "cpu"
    %int0_5340 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3471, %none_5336, %none_5337, %int6_5338, %cpu_5339, %int0_5340 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5341 = torch.constant.int 5
    %3472 = torch.prims.convert_element_type %3471, %int5_5341 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %3472, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_5342 = torch.constant.int 2
    %3473 = torch.aten.unsqueeze %3469, %int2_5342 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %3473, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_5343 = torch.constant.int 2
    %3474 = torch.aten.unsqueeze %3472, %int2_5343 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %3474, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_5344 = torch.constant.none
    %none_5345 = torch.constant.none
    %int5_5346 = torch.constant.int 5
    %cpu_5347 = torch.constant.device "cpu"
    %int0_5348 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3473, %none_5344, %none_5345, %int5_5346, %cpu_5347, %int0_5348 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_5349 = torch.constant.none
    %none_5350 = torch.constant.none
    %int5_5351 = torch.constant.int 5
    %cpu_5352 = torch.constant.device "cpu"
    %int0_5353 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3474, %none_5349, %none_5350, %int5_5351, %cpu_5352, %int0_5353 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_5354 = torch.constant.none
    %none_5355 = torch.constant.none
    %int5_5356 = torch.constant.int 5
    %cpu_5357 = torch.constant.device "cpu"
    %int0_5358 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3408, %none_5354, %none_5355, %int5_5356, %cpu_5357, %int0_5358 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_5359 = torch.constant.int 3
    %int0_5360 = torch.constant.int 0
    %int64_5361 = torch.constant.int 64
    %int2_5362 = torch.constant.int 2
    %3475 = torch.aten.slice.Tensor %3408, %int3_5359, %int0_5360, %int64_5361, %int2_5362 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3475, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_5363 = torch.constant.int 3
    %int1_5364 = torch.constant.int 1
    %int64_5365 = torch.constant.int 64
    %int2_5366 = torch.constant.int 2
    %3476 = torch.aten.slice.Tensor %3408, %int3_5363, %int1_5364, %int64_5365, %int2_5366 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3476, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %3477 = torch.aten.mul.Tensor %3475, %3473 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3477, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %3478 = torch.aten.mul.Tensor %3476, %3474 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3478, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_5367 = torch.constant.int 1
    %3479 = torch.aten.sub.Tensor %3477, %3478, %int1_5367 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3479, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %3480 = torch.aten.mul.Tensor %3476, %3473 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3480, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %3481 = torch.aten.mul.Tensor %3475, %3474 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3481, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_5368 = torch.constant.int 1
    %3482 = torch.aten.add.Tensor %3480, %3481, %int1_5368 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3482, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %3483 = torch_c.to_builtin_tensor %3479 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_5369 = tensor.cast %3483 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %3484 = torch_c.to_builtin_tensor %3482 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_5370 = tensor.cast %3484 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %3485 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_5369, %cast_5370) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_5371 = tensor.cast %3485 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %3486 = torch_c.from_builtin_tensor %cast_5371 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %3486, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_5372 = torch.constant.int 4
    %int4_5373 = torch.constant.int 4
    %int64_5374 = torch.constant.int 64
    %3487 = torch.prim.ListConstruct %int4_5372, %273, %int4_5373, %int64_5374 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3488 = torch.aten.view %3486, %3487 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %3488, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_5375 = torch.constant.none
    %none_5376 = torch.constant.none
    %int5_5377 = torch.constant.int 5
    %cpu_5378 = torch.constant.device "cpu"
    %int0_5379 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3488, %none_5375, %none_5376, %int5_5377, %cpu_5378, %int0_5379 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_5380 = torch.constant.int 22
    %3489 = torch.aten.mul.Scalar %arg2, %int22_5380 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3489, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int10 = torch.constant.int 10
    %int1_5381 = torch.constant.int 1
    %3490 = torch.aten.add.Scalar %3489, %int10, %int1_5381 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3490, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_5382 = torch.constant.int 2
    %3491 = torch.aten.mul.Scalar %3490, %int2_5382 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3491, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_5383 = torch.constant.int 0
    %int1_5384 = torch.constant.int 1
    %3492 = torch.aten.add.Scalar %3491, %int0_5383, %int1_5384 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3492, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %3493 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %3494 = torch.aten.view %3492, %3493 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3494, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_5385 = torch.constant.int 4
    %int32_5386 = torch.constant.int 32
    %int4_5387 = torch.constant.int 4
    %int64_5388 = torch.constant.int 64
    %3495 = torch.prim.ListConstruct %int4_5385, %271, %int32_5386, %int4_5387, %int64_5388 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3496 = torch.aten.view %3488, %3495 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %3496, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_5389 = torch.constant.int 32
    %int4_5390 = torch.constant.int 4
    %int64_5391 = torch.constant.int 64
    %3497 = torch.prim.ListConstruct %446, %int32_5389, %int4_5390, %int64_5391 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3498 = torch.aten.view %3496, %3497 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %3498, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_5392 = torch.constant.int 1
    %int2_5393 = torch.constant.int 2
    %3499 = torch.aten.transpose.int %3498, %int1_5392, %int2_5393 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %3499, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_5394 = torch.constant.none
    %none_5395 = torch.constant.none
    %int5_5396 = torch.constant.int 5
    %cpu_5397 = torch.constant.device "cpu"
    %int0_5398 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3499, %none_5394, %none_5395, %int5_5396, %cpu_5397, %int0_5398 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_5399 = torch.constant.int 22
    %int2_5400 = torch.constant.int 2
    %int4_5401 = torch.constant.int 4
    %int32_5402 = torch.constant.int 32
    %int64_5403 = torch.constant.int 64
    %3500 = torch.prim.ListConstruct %272, %int22_5399, %int2_5400, %int4_5401, %int32_5402, %int64_5403 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3501 = torch.aten.view %3225, %3500 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3501, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_5404 = torch.constant.int 4
    %int32_5405 = torch.constant.int 32
    %int64_5406 = torch.constant.int 64
    %3502 = torch.prim.ListConstruct %439, %int4_5404, %int32_5405, %int64_5406 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3503 = torch.aten.view %3501, %3502 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %3503, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %3504 = torch.prim.ListConstruct %3494 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_5407 = torch.constant.bool false
    %3505 = torch.aten.index_put %3503, %3504, %3499, %false_5407 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %3505, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_5408 = torch.constant.int 22
    %int2_5409 = torch.constant.int 2
    %int4_5410 = torch.constant.int 4
    %int32_5411 = torch.constant.int 32
    %int64_5412 = torch.constant.int 64
    %3506 = torch.prim.ListConstruct %272, %int22_5408, %int2_5409, %int4_5410, %int32_5411, %int64_5412 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3507 = torch.aten.view %3505, %3506 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3507, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_5413 = torch.constant.int 360448
    %3508 = torch.prim.ListConstruct %272, %int360448_5413 : (!torch.int, !torch.int) -> !torch.list<int>
    %3509 = torch.aten.view %3507, %3508 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %3509, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_5414 = torch.constant.int 22
    %int2_5415 = torch.constant.int 2
    %int4_5416 = torch.constant.int 4
    %int32_5417 = torch.constant.int 32
    %int64_5418 = torch.constant.int 64
    %3510 = torch.prim.ListConstruct %272, %int22_5414, %int2_5415, %int4_5416, %int32_5417, %int64_5418 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3511 = torch.aten.view %3509, %3510 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3511, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_5419 = torch.constant.int 4
    %int32_5420 = torch.constant.int 32
    %int64_5421 = torch.constant.int 64
    %3512 = torch.prim.ListConstruct %439, %int4_5419, %int32_5420, %int64_5421 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3513 = torch.aten.view %3511, %3512 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %3513, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_5422 = torch.constant.int 22
    %3514 = torch.aten.mul.Scalar %arg2, %int22_5422 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3514, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int10_5423 = torch.constant.int 10
    %int1_5424 = torch.constant.int 1
    %3515 = torch.aten.add.Scalar %3514, %int10_5423, %int1_5424 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3515, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_5425 = torch.constant.int 2
    %3516 = torch.aten.mul.Scalar %3515, %int2_5425 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3516, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_5426 = torch.constant.int 1
    %int1_5427 = torch.constant.int 1
    %3517 = torch.aten.add.Scalar %3516, %int1_5426, %int1_5427 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3517, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %3518 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %3519 = torch.aten.view %3517, %3518 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3519, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_5428 = torch.constant.int 4
    %int32_5429 = torch.constant.int 32
    %int4_5430 = torch.constant.int 4
    %int64_5431 = torch.constant.int 64
    %3520 = torch.prim.ListConstruct %int4_5428, %271, %int32_5429, %int4_5430, %int64_5431 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3521 = torch.aten.view %3410, %3520 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %3521, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_5432 = torch.constant.int 32
    %int4_5433 = torch.constant.int 4
    %int64_5434 = torch.constant.int 64
    %3522 = torch.prim.ListConstruct %446, %int32_5432, %int4_5433, %int64_5434 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3523 = torch.aten.view %3521, %3522 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %3523, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_5435 = torch.constant.int 1
    %int2_5436 = torch.constant.int 2
    %3524 = torch.aten.transpose.int %3523, %int1_5435, %int2_5436 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %3524, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_5437 = torch.constant.none
    %none_5438 = torch.constant.none
    %int5_5439 = torch.constant.int 5
    %cpu_5440 = torch.constant.device "cpu"
    %int0_5441 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3524, %none_5437, %none_5438, %int5_5439, %cpu_5440, %int0_5441 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %3525 = torch.prim.ListConstruct %3519 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_5442 = torch.constant.bool false
    %3526 = torch.aten.index_put %3513, %3525, %3524, %false_5442 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %3526, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_5443 = torch.constant.int 22
    %int2_5444 = torch.constant.int 2
    %int4_5445 = torch.constant.int 4
    %int32_5446 = torch.constant.int 32
    %int64_5447 = torch.constant.int 64
    %3527 = torch.prim.ListConstruct %272, %int22_5443, %int2_5444, %int4_5445, %int32_5446, %int64_5447 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3528 = torch.aten.view %3526, %3527 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3528, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_5448 = torch.constant.int 360448
    %3529 = torch.prim.ListConstruct %272, %int360448_5448 : (!torch.int, !torch.int) -> !torch.list<int>
    %3530 = torch.aten.view %3528, %3529 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %3530, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_5449 = torch.constant.int 0
    %int1_5450 = torch.constant.int 1
    %none_5451 = torch.constant.none
    %none_5452 = torch.constant.none
    %cpu_5453 = torch.constant.device "cpu"
    %false_5454 = torch.constant.bool false
    %3531 = torch.aten.arange.start_step %int0_5449, %273, %int1_5450, %none_5451, %none_5452, %cpu_5453, %false_5454 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3531, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_5455 = torch.constant.int -1
    %3532 = torch.aten.unsqueeze %arg1, %int-1_5455 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %3533 = torch.aten.ge.Tensor %3531, %3532 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %3533, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_5456 = torch.constant.none
    %none_5457 = torch.constant.none
    %cpu_5458 = torch.constant.device "cpu"
    %false_5459 = torch.constant.bool false
    %3534 = torch.aten.arange %273, %none_5456, %none_5457, %cpu_5458, %false_5459 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3534, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_5460 = torch.constant.int 0
    %3535 = torch.aten.unsqueeze %3534, %int0_5460 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %3535, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_5461 = torch.constant.int 1
    %3536 = torch.aten.unsqueeze %3535, %int1_5461 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %3536, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_5462 = torch.constant.int 2
    %3537 = torch.aten.unsqueeze %3536, %int2_5462 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %3537, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_5463 = torch.constant.none
    %none_5464 = torch.constant.none
    %cpu_5465 = torch.constant.device "cpu"
    %false_5466 = torch.constant.bool false
    %3538 = torch.aten.arange %273, %none_5463, %none_5464, %cpu_5465, %false_5466 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3538, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_5467 = torch.constant.int 0
    %3539 = torch.aten.unsqueeze %3538, %int0_5467 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %3539, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_5468 = torch.constant.int 1
    %3540 = torch.aten.unsqueeze %3539, %int1_5468 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %3540, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_5469 = torch.constant.int 3
    %3541 = torch.aten.unsqueeze %3540, %int3_5469 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %3541, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %3542 = torch.aten.gt.Tensor %3537, %3541 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %3542, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_5470 = torch.constant.int 1
    %3543 = torch.aten.unsqueeze %3533, %int1_5470 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %3543, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_5471 = torch.constant.int 2
    %3544 = torch.aten.unsqueeze %3543, %int2_5471 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %3544, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %3545 = torch.aten.logical_or %3542, %3544 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %3545, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_5472 = torch.constant.none
    %3546 = torch.aten.clone %123, %none_5472 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_5473 = torch.constant.int 0
    %3547 = torch.aten.where.ScalarOther %3545, %3546, %int0_5473 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %3547, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_5474 = torch.constant.none
    %none_5475 = torch.constant.none
    %int5_5476 = torch.constant.int 5
    %cpu_5477 = torch.constant.device "cpu"
    %int0_5478 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3547, %none_5474, %none_5475, %int5_5476, %cpu_5477, %int0_5478 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_5479 = torch.constant.int -2
    %3548 = torch.aten.unsqueeze %3488, %int-2_5479 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %3548, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_5480 = torch.constant.int 4
    %int4_5481 = torch.constant.int 4
    %int8_5482 = torch.constant.int 8
    %int64_5483 = torch.constant.int 64
    %3549 = torch.prim.ListConstruct %int4_5480, %273, %int4_5481, %int8_5482, %int64_5483 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5484 = torch.constant.bool false
    %3550 = torch.aten.expand %3548, %3549, %false_5484 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3550, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_5485 = torch.constant.int 0
    %3551 = torch.aten.clone %3550, %int0_5485 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3551, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_5486 = torch.constant.int 4
    %int32_5487 = torch.constant.int 32
    %int64_5488 = torch.constant.int 64
    %3552 = torch.prim.ListConstruct %int4_5486, %273, %int32_5487, %int64_5488 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3553 = torch.aten._unsafe_view %3551, %3552 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3553, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_5489 = torch.constant.int -2
    %3554 = torch.aten.unsqueeze %3410, %int-2_5489 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %3554, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_5490 = torch.constant.int 4
    %int4_5491 = torch.constant.int 4
    %int8_5492 = torch.constant.int 8
    %int64_5493 = torch.constant.int 64
    %3555 = torch.prim.ListConstruct %int4_5490, %273, %int4_5491, %int8_5492, %int64_5493 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5494 = torch.constant.bool false
    %3556 = torch.aten.expand %3554, %3555, %false_5494 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3556, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_5495 = torch.constant.int 0
    %3557 = torch.aten.clone %3556, %int0_5495 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3557, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_5496 = torch.constant.int 4
    %int32_5497 = torch.constant.int 32
    %int64_5498 = torch.constant.int 64
    %3558 = torch.prim.ListConstruct %int4_5496, %273, %int32_5497, %int64_5498 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3559 = torch.aten._unsafe_view %3557, %3558 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3559, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_5499 = torch.constant.int 1
    %int2_5500 = torch.constant.int 2
    %3560 = torch.aten.transpose.int %3449, %int1_5499, %int2_5500 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %3560, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_5501 = torch.constant.int 1
    %int2_5502 = torch.constant.int 2
    %3561 = torch.aten.transpose.int %3553, %int1_5501, %int2_5502 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %3561, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_5503 = torch.constant.int 1
    %int2_5504 = torch.constant.int 2
    %3562 = torch.aten.transpose.int %3559, %int1_5503, %int2_5504 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %3562, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_5505 = torch.constant.float 0.000000e+00
    %false_5506 = torch.constant.bool false
    %none_5507 = torch.constant.none
    %false_5508 = torch.constant.bool false
    %3563 = torch.aten.scaled_dot_product_attention %3560, %3561, %3562, %3547, %float0.000000e00_5505, %false_5506, %none_5507, %false_5508 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %3563, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_5509 = torch.constant.int 1
    %int2_5510 = torch.constant.int 2
    %3564 = torch.aten.transpose.int %3563, %int1_5509, %int2_5510 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3564, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_5511 = torch.constant.int 4
    %int2048_5512 = torch.constant.int 2048
    %3565 = torch.prim.ListConstruct %int4_5511, %273, %int2048_5512 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3566 = torch.aten.view %3564, %3565 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3566, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_5513 = torch.constant.int 2
    %3567 = torch.aten.view.dtype %128, %int2_5513 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %3568 = torch.aten.detach %3567 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_5514 = torch.constant.int -1
    %int17_5515 = torch.constant.int 17
    %3569 = torch.prim.ListConstruct %int-1_5514, %int17_5515 : (!torch.int, !torch.int) -> !torch.list<int>
    %3570 = torch.aten.view %3568, %3569 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_5516 = torch.constant.int 2048
    %int-1_5517 = torch.constant.int -1
    %int17_5518 = torch.constant.int 17
    %3571 = torch.prim.ListConstruct %int2048_5516, %int-1_5517, %int17_5518 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3572 = torch.aten.view %3570, %3571 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_5519 = torch.constant.int 2
    %int0_5520 = torch.constant.int 0
    %int1_5521 = torch.constant.int 1
    %int1_5522 = torch.constant.int 1
    %3573 = torch.aten.slice.Tensor %3572, %int2_5519, %int0_5520, %int1_5521, %int1_5522 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_5523 = torch.constant.int 5
    %3574 = torch.aten.view.dtype %3573, %int5_5523 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %3575 = torch.aten.detach %3574 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_5524 = torch.constant.int 2
    %int1_5525 = torch.constant.int 1
    %int9223372036854775807_5526 = torch.constant.int 9223372036854775807
    %int1_5527 = torch.constant.int 1
    %3576 = torch.aten.slice.Tensor %3572, %int2_5524, %int1_5525, %int9223372036854775807_5526, %int1_5527 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_5528 = torch.constant.int 1
    %3577 = torch.aten.view.dtype %3576, %int1_5528 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %3578 = torch.aten.detach %3577 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %3579 = torch_c.to_builtin_tensor %3566 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_5529 = tensor.cast %3579 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3580 = torch_c.to_builtin_tensor %3575 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %3581 = torch_c.to_builtin_tensor %3578 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %3582 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_5529, %3580, %3581) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_5530 = tensor.cast %3582 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %3583 = torch_c.from_builtin_tensor %cast_5530 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3583, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_5531 = torch.constant.none
    %none_5532 = torch.constant.none
    %int5_5533 = torch.constant.int 5
    %cpu_5534 = torch.constant.device "cpu"
    %int0_5535 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3583, %none_5531, %none_5532, %int5_5533, %cpu_5534, %int0_5535 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_5536 = torch.constant.int 1
    %3584 = torch.aten.add.Tensor %3343, %3583, %int1_5536 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3584, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_5537 = torch.constant.none
    %none_5538 = torch.constant.none
    %int5_5539 = torch.constant.int 5
    %cpu_5540 = torch.constant.device "cpu"
    %int0_5541 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3584, %none_5537, %none_5538, %int5_5539, %cpu_5540, %int0_5541 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5542 = torch.constant.int 6
    %3585 = torch.prims.convert_element_type %3584, %int6_5542 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3585, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_5543 = torch.constant.int 2
    %3586 = torch.aten.pow.Tensor_Scalar %3585, %int2_5543 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3586, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_5544 = torch.constant.int -1
    %3587 = torch.prim.ListConstruct %int-1_5544 : (!torch.int) -> !torch.list<int>
    %true_5545 = torch.constant.bool true
    %none_5546 = torch.constant.none
    %3588 = torch.aten.mean.dim %3586, %3587, %true_5545, %none_5546 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3588, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_5547 = torch.constant.float 9.9999997473787516E-6
    %int1_5548 = torch.constant.int 1
    %3589 = torch.aten.add.Scalar %3588, %float9.999990e-06_5547, %int1_5548 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3589, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3590 = torch.aten.rsqrt %3589 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3590, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3591 = torch.aten.mul.Tensor %3585, %3590 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3591, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_5549 = torch.constant.none
    %none_5550 = torch.constant.none
    %int6_5551 = torch.constant.int 6
    %cpu_5552 = torch.constant.device "cpu"
    %int0_5553 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3591, %none_5549, %none_5550, %int6_5551, %cpu_5552, %int0_5553 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5554 = torch.constant.int 5
    %3592 = torch.prims.convert_element_type %3591, %int5_5554 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3592, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %3593 = torch.aten.mul.Tensor %129, %3592 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3593, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_5555 = torch.constant.none
    %none_5556 = torch.constant.none
    %int6_5557 = torch.constant.int 6
    %cpu_5558 = torch.constant.device "cpu"
    %int0_5559 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3593, %none_5555, %none_5556, %int6_5557, %cpu_5558, %int0_5559 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5560 = torch.constant.int 5
    %3594 = torch.prims.convert_element_type %3593, %int5_5560 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3594, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_5561 = torch.constant.int 2
    %3595 = torch.aten.view.dtype %130, %int2_5561 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %3596 = torch.aten.detach %3595 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_5562 = torch.constant.int -1
    %int17_5563 = torch.constant.int 17
    %3597 = torch.prim.ListConstruct %int-1_5562, %int17_5563 : (!torch.int, !torch.int) -> !torch.list<int>
    %3598 = torch.aten.view %3596, %3597 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_5564 = torch.constant.int 5632
    %int-1_5565 = torch.constant.int -1
    %int17_5566 = torch.constant.int 17
    %3599 = torch.prim.ListConstruct %int5632_5564, %int-1_5565, %int17_5566 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3600 = torch.aten.view %3598, %3599 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_5567 = torch.constant.int 2
    %int0_5568 = torch.constant.int 0
    %int1_5569 = torch.constant.int 1
    %int1_5570 = torch.constant.int 1
    %3601 = torch.aten.slice.Tensor %3600, %int2_5567, %int0_5568, %int1_5569, %int1_5570 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_5571 = torch.constant.int 5
    %3602 = torch.aten.view.dtype %3601, %int5_5571 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %3603 = torch.aten.detach %3602 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_5572 = torch.constant.int 2
    %int1_5573 = torch.constant.int 1
    %int9223372036854775807_5574 = torch.constant.int 9223372036854775807
    %int1_5575 = torch.constant.int 1
    %3604 = torch.aten.slice.Tensor %3600, %int2_5572, %int1_5573, %int9223372036854775807_5574, %int1_5575 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_5576 = torch.constant.int 1
    %3605 = torch.aten.view.dtype %3604, %int1_5576 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %3606 = torch.aten.detach %3605 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %3607 = torch_c.to_builtin_tensor %3594 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_5577 = tensor.cast %3607 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3608 = torch_c.to_builtin_tensor %3603 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %3609 = torch_c.to_builtin_tensor %3606 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %3610 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_5577, %3608, %3609) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_5578 = tensor.cast %3610 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %3611 = torch_c.from_builtin_tensor %cast_5578 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %3611, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %3612 = torch.aten.silu %3611 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %3612, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_5579 = torch.constant.int 2
    %3613 = torch.aten.view.dtype %131, %int2_5579 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %3614 = torch.aten.detach %3613 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_5580 = torch.constant.int -1
    %int17_5581 = torch.constant.int 17
    %3615 = torch.prim.ListConstruct %int-1_5580, %int17_5581 : (!torch.int, !torch.int) -> !torch.list<int>
    %3616 = torch.aten.view %3614, %3615 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_5582 = torch.constant.int 5632
    %int-1_5583 = torch.constant.int -1
    %int17_5584 = torch.constant.int 17
    %3617 = torch.prim.ListConstruct %int5632_5582, %int-1_5583, %int17_5584 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3618 = torch.aten.view %3616, %3617 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_5585 = torch.constant.int 2
    %int0_5586 = torch.constant.int 0
    %int1_5587 = torch.constant.int 1
    %int1_5588 = torch.constant.int 1
    %3619 = torch.aten.slice.Tensor %3618, %int2_5585, %int0_5586, %int1_5587, %int1_5588 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_5589 = torch.constant.int 5
    %3620 = torch.aten.view.dtype %3619, %int5_5589 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %3621 = torch.aten.detach %3620 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_5590 = torch.constant.int 2
    %int1_5591 = torch.constant.int 1
    %int9223372036854775807_5592 = torch.constant.int 9223372036854775807
    %int1_5593 = torch.constant.int 1
    %3622 = torch.aten.slice.Tensor %3618, %int2_5590, %int1_5591, %int9223372036854775807_5592, %int1_5593 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_5594 = torch.constant.int 1
    %3623 = torch.aten.view.dtype %3622, %int1_5594 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %3624 = torch.aten.detach %3623 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %3625 = torch_c.to_builtin_tensor %3594 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_5595 = tensor.cast %3625 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3626 = torch_c.to_builtin_tensor %3621 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %3627 = torch_c.to_builtin_tensor %3624 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %3628 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_5595, %3626, %3627) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_5596 = tensor.cast %3628 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %3629 = torch_c.from_builtin_tensor %cast_5596 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %3629, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %3630 = torch.aten.mul.Tensor %3612, %3629 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %3630, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_5597 = torch.constant.int 2
    %3631 = torch.aten.view.dtype %132, %int2_5597 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %3632 = torch.aten.detach %3631 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_5598 = torch.constant.int -1
    %int17_5599 = torch.constant.int 17
    %3633 = torch.prim.ListConstruct %int-1_5598, %int17_5599 : (!torch.int, !torch.int) -> !torch.list<int>
    %3634 = torch.aten.view %3632, %3633 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_5600 = torch.constant.int 2048
    %int-1_5601 = torch.constant.int -1
    %int17_5602 = torch.constant.int 17
    %3635 = torch.prim.ListConstruct %int2048_5600, %int-1_5601, %int17_5602 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3636 = torch.aten.view %3634, %3635 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_5603 = torch.constant.int 2
    %int0_5604 = torch.constant.int 0
    %int1_5605 = torch.constant.int 1
    %int1_5606 = torch.constant.int 1
    %3637 = torch.aten.slice.Tensor %3636, %int2_5603, %int0_5604, %int1_5605, %int1_5606 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_5607 = torch.constant.int 5
    %3638 = torch.aten.view.dtype %3637, %int5_5607 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %3639 = torch.aten.detach %3638 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_5608 = torch.constant.int 2
    %int1_5609 = torch.constant.int 1
    %int9223372036854775807_5610 = torch.constant.int 9223372036854775807
    %int1_5611 = torch.constant.int 1
    %3640 = torch.aten.slice.Tensor %3636, %int2_5608, %int1_5609, %int9223372036854775807_5610, %int1_5611 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_5612 = torch.constant.int 1
    %3641 = torch.aten.view.dtype %3640, %int1_5612 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %3642 = torch.aten.detach %3641 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %3643 = torch_c.to_builtin_tensor %3630 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_5613 = tensor.cast %3643 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %3644 = torch_c.to_builtin_tensor %3639 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %3645 = torch_c.to_builtin_tensor %3642 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %3646 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_5613, %3644, %3645) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_5614 = tensor.cast %3646 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %3647 = torch_c.from_builtin_tensor %cast_5614 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3647, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_5615 = torch.constant.int 1
    %3648 = torch.aten.add.Tensor %3584, %3647, %int1_5615 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3648, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_5616 = torch.constant.none
    %none_5617 = torch.constant.none
    %int5_5618 = torch.constant.int 5
    %cpu_5619 = torch.constant.device "cpu"
    %int0_5620 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3648, %none_5616, %none_5617, %int5_5618, %cpu_5619, %int0_5620 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5621 = torch.constant.int 6
    %3649 = torch.prims.convert_element_type %3648, %int6_5621 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3649, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_5622 = torch.constant.int 2
    %3650 = torch.aten.pow.Tensor_Scalar %3649, %int2_5622 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3650, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_5623 = torch.constant.int -1
    %3651 = torch.prim.ListConstruct %int-1_5623 : (!torch.int) -> !torch.list<int>
    %true_5624 = torch.constant.bool true
    %none_5625 = torch.constant.none
    %3652 = torch.aten.mean.dim %3650, %3651, %true_5624, %none_5625 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3652, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_5626 = torch.constant.float 9.9999997473787516E-6
    %int1_5627 = torch.constant.int 1
    %3653 = torch.aten.add.Scalar %3652, %float9.999990e-06_5626, %int1_5627 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3653, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3654 = torch.aten.rsqrt %3653 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3654, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3655 = torch.aten.mul.Tensor %3649, %3654 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3655, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_5628 = torch.constant.none
    %none_5629 = torch.constant.none
    %int6_5630 = torch.constant.int 6
    %cpu_5631 = torch.constant.device "cpu"
    %int0_5632 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3655, %none_5628, %none_5629, %int6_5630, %cpu_5631, %int0_5632 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5633 = torch.constant.int 5
    %3656 = torch.prims.convert_element_type %3655, %int5_5633 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3656, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %3657 = torch.aten.mul.Tensor %136, %3656 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3657, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_5634 = torch.constant.none
    %none_5635 = torch.constant.none
    %int6_5636 = torch.constant.int 6
    %cpu_5637 = torch.constant.device "cpu"
    %int0_5638 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3657, %none_5634, %none_5635, %int6_5636, %cpu_5637, %int0_5638 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5639 = torch.constant.int 5
    %3658 = torch.prims.convert_element_type %3657, %int5_5639 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3658, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_5640 = torch.constant.int 2
    %3659 = torch.aten.view.dtype %137, %int2_5640 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %3660 = torch.aten.detach %3659 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_5641 = torch.constant.int -1
    %int17_5642 = torch.constant.int 17
    %3661 = torch.prim.ListConstruct %int-1_5641, %int17_5642 : (!torch.int, !torch.int) -> !torch.list<int>
    %3662 = torch.aten.view %3660, %3661 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_5643 = torch.constant.int 2048
    %int-1_5644 = torch.constant.int -1
    %int17_5645 = torch.constant.int 17
    %3663 = torch.prim.ListConstruct %int2048_5643, %int-1_5644, %int17_5645 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3664 = torch.aten.view %3662, %3663 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_5646 = torch.constant.int 2
    %int0_5647 = torch.constant.int 0
    %int1_5648 = torch.constant.int 1
    %int1_5649 = torch.constant.int 1
    %3665 = torch.aten.slice.Tensor %3664, %int2_5646, %int0_5647, %int1_5648, %int1_5649 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_5650 = torch.constant.int 5
    %3666 = torch.aten.view.dtype %3665, %int5_5650 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %3667 = torch.aten.detach %3666 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_5651 = torch.constant.int 2
    %int1_5652 = torch.constant.int 1
    %int9223372036854775807_5653 = torch.constant.int 9223372036854775807
    %int1_5654 = torch.constant.int 1
    %3668 = torch.aten.slice.Tensor %3664, %int2_5651, %int1_5652, %int9223372036854775807_5653, %int1_5654 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_5655 = torch.constant.int 1
    %3669 = torch.aten.view.dtype %3668, %int1_5655 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %3670 = torch.aten.detach %3669 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %3671 = torch_c.to_builtin_tensor %3658 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_5656 = tensor.cast %3671 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3672 = torch_c.to_builtin_tensor %3667 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %3673 = torch_c.to_builtin_tensor %3670 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %3674 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_5656, %3672, %3673) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_5657 = tensor.cast %3674 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %3675 = torch_c.from_builtin_tensor %cast_5657 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3675, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_5658 = torch.constant.int 2
    %3676 = torch.aten.view.dtype %138, %int2_5658 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %3677 = torch.aten.detach %3676 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_5659 = torch.constant.int -1
    %int17_5660 = torch.constant.int 17
    %3678 = torch.prim.ListConstruct %int-1_5659, %int17_5660 : (!torch.int, !torch.int) -> !torch.list<int>
    %3679 = torch.aten.view %3677, %3678 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_5661 = torch.constant.int 256
    %int-1_5662 = torch.constant.int -1
    %int17_5663 = torch.constant.int 17
    %3680 = torch.prim.ListConstruct %int256_5661, %int-1_5662, %int17_5663 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3681 = torch.aten.view %3679, %3680 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_5664 = torch.constant.int 2
    %int0_5665 = torch.constant.int 0
    %int1_5666 = torch.constant.int 1
    %int1_5667 = torch.constant.int 1
    %3682 = torch.aten.slice.Tensor %3681, %int2_5664, %int0_5665, %int1_5666, %int1_5667 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_5668 = torch.constant.int 5
    %3683 = torch.aten.view.dtype %3682, %int5_5668 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %3684 = torch.aten.detach %3683 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_5669 = torch.constant.int 2
    %int1_5670 = torch.constant.int 1
    %int9223372036854775807_5671 = torch.constant.int 9223372036854775807
    %int1_5672 = torch.constant.int 1
    %3685 = torch.aten.slice.Tensor %3681, %int2_5669, %int1_5670, %int9223372036854775807_5671, %int1_5672 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_5673 = torch.constant.int 1
    %3686 = torch.aten.view.dtype %3685, %int1_5673 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %3687 = torch.aten.detach %3686 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %3688 = torch_c.to_builtin_tensor %3658 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_5674 = tensor.cast %3688 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3689 = torch_c.to_builtin_tensor %3684 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %3690 = torch_c.to_builtin_tensor %3687 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %3691 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_5674, %3689, %3690) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_5675 = tensor.cast %3691 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %3692 = torch_c.from_builtin_tensor %cast_5675 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %3692, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_5676 = torch.constant.int 2
    %3693 = torch.aten.view.dtype %139, %int2_5676 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %3694 = torch.aten.detach %3693 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_5677 = torch.constant.int -1
    %int17_5678 = torch.constant.int 17
    %3695 = torch.prim.ListConstruct %int-1_5677, %int17_5678 : (!torch.int, !torch.int) -> !torch.list<int>
    %3696 = torch.aten.view %3694, %3695 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_5679 = torch.constant.int 256
    %int-1_5680 = torch.constant.int -1
    %int17_5681 = torch.constant.int 17
    %3697 = torch.prim.ListConstruct %int256_5679, %int-1_5680, %int17_5681 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3698 = torch.aten.view %3696, %3697 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_5682 = torch.constant.int 2
    %int0_5683 = torch.constant.int 0
    %int1_5684 = torch.constant.int 1
    %int1_5685 = torch.constant.int 1
    %3699 = torch.aten.slice.Tensor %3698, %int2_5682, %int0_5683, %int1_5684, %int1_5685 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_5686 = torch.constant.int 5
    %3700 = torch.aten.view.dtype %3699, %int5_5686 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %3701 = torch.aten.detach %3700 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_5687 = torch.constant.int 2
    %int1_5688 = torch.constant.int 1
    %int9223372036854775807_5689 = torch.constant.int 9223372036854775807
    %int1_5690 = torch.constant.int 1
    %3702 = torch.aten.slice.Tensor %3698, %int2_5687, %int1_5688, %int9223372036854775807_5689, %int1_5690 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_5691 = torch.constant.int 1
    %3703 = torch.aten.view.dtype %3702, %int1_5691 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %3704 = torch.aten.detach %3703 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %3705 = torch_c.to_builtin_tensor %3658 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_5692 = tensor.cast %3705 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3706 = torch_c.to_builtin_tensor %3701 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %3707 = torch_c.to_builtin_tensor %3704 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %3708 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_5692, %3706, %3707) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_5693 = tensor.cast %3708 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %3709 = torch_c.from_builtin_tensor %cast_5693 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %3709, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_5694 = torch.constant.int 4
    %int32_5695 = torch.constant.int 32
    %int64_5696 = torch.constant.int 64
    %3710 = torch.prim.ListConstruct %int4_5694, %273, %int32_5695, %int64_5696 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3711 = torch.aten.view %3675, %3710 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3711, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_5697 = torch.constant.int 4
    %int4_5698 = torch.constant.int 4
    %int64_5699 = torch.constant.int 64
    %3712 = torch.prim.ListConstruct %int4_5697, %273, %int4_5698, %int64_5699 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3713 = torch.aten.view %3692, %3712 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %3713, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_5700 = torch.constant.int 4
    %int4_5701 = torch.constant.int 4
    %int64_5702 = torch.constant.int 64
    %3714 = torch.prim.ListConstruct %int4_5700, %273, %int4_5701, %int64_5702 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3715 = torch.aten.view %3709, %3714 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %3715, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_5703 = torch.constant.int 0
    %none_5704 = torch.constant.none
    %none_5705 = torch.constant.none
    %cpu_5706 = torch.constant.device "cpu"
    %false_5707 = torch.constant.bool false
    %3716 = torch.aten.arange.start %int0_5703, %273, %none_5704, %none_5705, %cpu_5706, %false_5707 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3716, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_5708 = torch.constant.int 0
    %3717 = torch.aten.unsqueeze %3716, %int0_5708 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %3717, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_5709 = torch.constant.int 0
    %int64_5710 = torch.constant.int 64
    %int2_5711 = torch.constant.int 2
    %none_5712 = torch.constant.none
    %none_5713 = torch.constant.none
    %cpu_5714 = torch.constant.device "cpu"
    %false_5715 = torch.constant.bool false
    %3718 = torch.aten.arange.start_step %int0_5709, %int64_5710, %int2_5711, %none_5712, %none_5713, %cpu_5714, %false_5715 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_5716 = torch.constant.none
    %none_5717 = torch.constant.none
    %int4_5718 = torch.constant.int 4
    %cpu_5719 = torch.constant.device "cpu"
    %int0_5720 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3718, %none_5716, %none_5717, %int4_5718, %cpu_5719, %int0_5720 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5721 = torch.constant.int 6
    %3719 = torch.prims.convert_element_type %3718, %int6_5721 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_5722 = torch.constant.int 64
    %3720 = torch.aten.div.Scalar %3719, %int64_5722 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_5723 = torch.constant.float 1.000000e+04
    %3721 = torch.aten.pow.Scalar %float1.000000e04_5723, %3720 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %3722 = torch.aten.reciprocal %3721 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_5724 = torch.constant.float 1.000000e+00
    %3723 = torch.aten.mul.Scalar %3722, %float1.000000e00_5724 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_5725 = torch.constant.none
    %3724 = torch.aten.clone %133, %none_5725 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_5726 = torch.constant.int 0
    %3725 = torch.aten.unsqueeze %3723, %int0_5726 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_5727 = torch.constant.int 2
    %3726 = torch.aten.unsqueeze %3725, %int2_5727 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_5728 = torch.constant.none
    %none_5729 = torch.constant.none
    %int6_5730 = torch.constant.int 6
    %cpu_5731 = torch.constant.device "cpu"
    %int0_5732 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3726, %none_5728, %none_5729, %int6_5730, %cpu_5731, %int0_5732 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_5733 = torch.constant.int 1
    %int-1_5734 = torch.constant.int -1
    %int1_5735 = torch.constant.int 1
    %3727 = torch.prim.ListConstruct %int1_5733, %int-1_5734, %int1_5735 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5736 = torch.constant.bool false
    %3728 = torch.aten.expand %3726, %3727, %false_5736 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_5737 = torch.constant.int 1
    %3729 = torch.aten.unsqueeze %3717, %int1_5737 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %3729, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_5738 = torch.constant.none
    %none_5739 = torch.constant.none
    %int4_5740 = torch.constant.int 4
    %cpu_5741 = torch.constant.device "cpu"
    %int0_5742 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3729, %none_5738, %none_5739, %int4_5740, %cpu_5741, %int0_5742 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5743 = torch.constant.int 6
    %3730 = torch.prims.convert_element_type %3729, %int6_5743 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %3730, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %3731 = torch.aten.matmul %3728, %3730 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %3731, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_5744 = torch.constant.int 1
    %int2_5745 = torch.constant.int 2
    %3732 = torch.aten.transpose.int %3731, %int1_5744, %int2_5745 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3732, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %3733 = torch.aten.cos %3732 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3733, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %3734 = torch.aten.mul.Tensor %3733, %3724 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3734, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_5746 = torch.constant.none
    %none_5747 = torch.constant.none
    %int6_5748 = torch.constant.int 6
    %cpu_5749 = torch.constant.device "cpu"
    %int0_5750 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3734, %none_5746, %none_5747, %int6_5748, %cpu_5749, %int0_5750 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5751 = torch.constant.int 5
    %3735 = torch.prims.convert_element_type %3734, %int5_5751 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %3735, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %3736 = torch.aten.sin %3732 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3736, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %3737 = torch.aten.mul.Tensor %3736, %3724 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3737, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_5752 = torch.constant.none
    %none_5753 = torch.constant.none
    %int6_5754 = torch.constant.int 6
    %cpu_5755 = torch.constant.device "cpu"
    %int0_5756 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3737, %none_5752, %none_5753, %int6_5754, %cpu_5755, %int0_5756 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5757 = torch.constant.int 5
    %3738 = torch.prims.convert_element_type %3737, %int5_5757 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %3738, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_5758 = torch.constant.int 2
    %3739 = torch.aten.unsqueeze %3735, %int2_5758 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %3739, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_5759 = torch.constant.int 2
    %3740 = torch.aten.unsqueeze %3738, %int2_5759 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %3740, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_5760 = torch.constant.none
    %none_5761 = torch.constant.none
    %int5_5762 = torch.constant.int 5
    %cpu_5763 = torch.constant.device "cpu"
    %int0_5764 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3739, %none_5760, %none_5761, %int5_5762, %cpu_5763, %int0_5764 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_5765 = torch.constant.none
    %none_5766 = torch.constant.none
    %int5_5767 = torch.constant.int 5
    %cpu_5768 = torch.constant.device "cpu"
    %int0_5769 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3740, %none_5765, %none_5766, %int5_5767, %cpu_5768, %int0_5769 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_5770 = torch.constant.none
    %none_5771 = torch.constant.none
    %int5_5772 = torch.constant.int 5
    %cpu_5773 = torch.constant.device "cpu"
    %int0_5774 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3711, %none_5770, %none_5771, %int5_5772, %cpu_5773, %int0_5774 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_5775 = torch.constant.int 3
    %int0_5776 = torch.constant.int 0
    %int64_5777 = torch.constant.int 64
    %int2_5778 = torch.constant.int 2
    %3741 = torch.aten.slice.Tensor %3711, %int3_5775, %int0_5776, %int64_5777, %int2_5778 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3741, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_5779 = torch.constant.int 3
    %int1_5780 = torch.constant.int 1
    %int64_5781 = torch.constant.int 64
    %int2_5782 = torch.constant.int 2
    %3742 = torch.aten.slice.Tensor %3711, %int3_5779, %int1_5780, %int64_5781, %int2_5782 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3742, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %3743 = torch.aten.mul.Tensor %3741, %3739 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3743, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %3744 = torch.aten.mul.Tensor %3742, %3740 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3744, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_5783 = torch.constant.int 1
    %3745 = torch.aten.sub.Tensor %3743, %3744, %int1_5783 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3745, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %3746 = torch.aten.mul.Tensor %3742, %3739 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3746, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %3747 = torch.aten.mul.Tensor %3741, %3740 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3747, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_5784 = torch.constant.int 1
    %3748 = torch.aten.add.Tensor %3746, %3747, %int1_5784 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %3748, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %3749 = torch_c.to_builtin_tensor %3745 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_5785 = tensor.cast %3749 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %3750 = torch_c.to_builtin_tensor %3748 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_5786 = tensor.cast %3750 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %3751 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_5785, %cast_5786) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_5787 = tensor.cast %3751 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %3752 = torch_c.from_builtin_tensor %cast_5787 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %3752, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_5788 = torch.constant.int 4
    %int32_5789 = torch.constant.int 32
    %int64_5790 = torch.constant.int 64
    %3753 = torch.prim.ListConstruct %int4_5788, %273, %int32_5789, %int64_5790 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3754 = torch.aten.view %3752, %3753 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3754, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_5791 = torch.constant.none
    %none_5792 = torch.constant.none
    %int5_5793 = torch.constant.int 5
    %cpu_5794 = torch.constant.device "cpu"
    %int0_5795 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3754, %none_5791, %none_5792, %int5_5793, %cpu_5794, %int0_5795 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_5796 = torch.constant.int 0
    %none_5797 = torch.constant.none
    %none_5798 = torch.constant.none
    %cpu_5799 = torch.constant.device "cpu"
    %false_5800 = torch.constant.bool false
    %3755 = torch.aten.arange.start %int0_5796, %273, %none_5797, %none_5798, %cpu_5799, %false_5800 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3755, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_5801 = torch.constant.int 0
    %3756 = torch.aten.unsqueeze %3755, %int0_5801 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %3756, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_5802 = torch.constant.int 0
    %int64_5803 = torch.constant.int 64
    %int2_5804 = torch.constant.int 2
    %none_5805 = torch.constant.none
    %none_5806 = torch.constant.none
    %cpu_5807 = torch.constant.device "cpu"
    %false_5808 = torch.constant.bool false
    %3757 = torch.aten.arange.start_step %int0_5802, %int64_5803, %int2_5804, %none_5805, %none_5806, %cpu_5807, %false_5808 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_5809 = torch.constant.none
    %none_5810 = torch.constant.none
    %int4_5811 = torch.constant.int 4
    %cpu_5812 = torch.constant.device "cpu"
    %int0_5813 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3757, %none_5809, %none_5810, %int4_5811, %cpu_5812, %int0_5813 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5814 = torch.constant.int 6
    %3758 = torch.prims.convert_element_type %3757, %int6_5814 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_5815 = torch.constant.int 64
    %3759 = torch.aten.div.Scalar %3758, %int64_5815 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_5816 = torch.constant.float 1.000000e+04
    %3760 = torch.aten.pow.Scalar %float1.000000e04_5816, %3759 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %3761 = torch.aten.reciprocal %3760 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_5817 = torch.constant.float 1.000000e+00
    %3762 = torch.aten.mul.Scalar %3761, %float1.000000e00_5817 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_5818 = torch.constant.none
    %3763 = torch.aten.clone %134, %none_5818 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_5819 = torch.constant.int 0
    %3764 = torch.aten.unsqueeze %3762, %int0_5819 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_5820 = torch.constant.int 2
    %3765 = torch.aten.unsqueeze %3764, %int2_5820 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_5821 = torch.constant.none
    %none_5822 = torch.constant.none
    %int6_5823 = torch.constant.int 6
    %cpu_5824 = torch.constant.device "cpu"
    %int0_5825 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3765, %none_5821, %none_5822, %int6_5823, %cpu_5824, %int0_5825 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_5826 = torch.constant.int 1
    %int-1_5827 = torch.constant.int -1
    %int1_5828 = torch.constant.int 1
    %3766 = torch.prim.ListConstruct %int1_5826, %int-1_5827, %int1_5828 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5829 = torch.constant.bool false
    %3767 = torch.aten.expand %3765, %3766, %false_5829 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_5830 = torch.constant.int 1
    %3768 = torch.aten.unsqueeze %3756, %int1_5830 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %3768, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_5831 = torch.constant.none
    %none_5832 = torch.constant.none
    %int4_5833 = torch.constant.int 4
    %cpu_5834 = torch.constant.device "cpu"
    %int0_5835 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3768, %none_5831, %none_5832, %int4_5833, %cpu_5834, %int0_5835 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5836 = torch.constant.int 6
    %3769 = torch.prims.convert_element_type %3768, %int6_5836 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %3769, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %3770 = torch.aten.matmul %3767, %3769 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %3770, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_5837 = torch.constant.int 1
    %int2_5838 = torch.constant.int 2
    %3771 = torch.aten.transpose.int %3770, %int1_5837, %int2_5838 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3771, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %3772 = torch.aten.cos %3771 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3772, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %3773 = torch.aten.mul.Tensor %3772, %3763 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3773, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_5839 = torch.constant.none
    %none_5840 = torch.constant.none
    %int6_5841 = torch.constant.int 6
    %cpu_5842 = torch.constant.device "cpu"
    %int0_5843 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3773, %none_5839, %none_5840, %int6_5841, %cpu_5842, %int0_5843 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5844 = torch.constant.int 5
    %3774 = torch.prims.convert_element_type %3773, %int5_5844 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %3774, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %3775 = torch.aten.sin %3771 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3775, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %3776 = torch.aten.mul.Tensor %3775, %3763 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %3776, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_5845 = torch.constant.none
    %none_5846 = torch.constant.none
    %int6_5847 = torch.constant.int 6
    %cpu_5848 = torch.constant.device "cpu"
    %int0_5849 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3776, %none_5845, %none_5846, %int6_5847, %cpu_5848, %int0_5849 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5850 = torch.constant.int 5
    %3777 = torch.prims.convert_element_type %3776, %int5_5850 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %3777, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_5851 = torch.constant.int 2
    %3778 = torch.aten.unsqueeze %3774, %int2_5851 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %3778, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_5852 = torch.constant.int 2
    %3779 = torch.aten.unsqueeze %3777, %int2_5852 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %3779, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_5853 = torch.constant.none
    %none_5854 = torch.constant.none
    %int5_5855 = torch.constant.int 5
    %cpu_5856 = torch.constant.device "cpu"
    %int0_5857 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3778, %none_5853, %none_5854, %int5_5855, %cpu_5856, %int0_5857 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_5858 = torch.constant.none
    %none_5859 = torch.constant.none
    %int5_5860 = torch.constant.int 5
    %cpu_5861 = torch.constant.device "cpu"
    %int0_5862 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3779, %none_5858, %none_5859, %int5_5860, %cpu_5861, %int0_5862 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_5863 = torch.constant.none
    %none_5864 = torch.constant.none
    %int5_5865 = torch.constant.int 5
    %cpu_5866 = torch.constant.device "cpu"
    %int0_5867 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3713, %none_5863, %none_5864, %int5_5865, %cpu_5866, %int0_5867 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_5868 = torch.constant.int 3
    %int0_5869 = torch.constant.int 0
    %int64_5870 = torch.constant.int 64
    %int2_5871 = torch.constant.int 2
    %3780 = torch.aten.slice.Tensor %3713, %int3_5868, %int0_5869, %int64_5870, %int2_5871 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3780, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_5872 = torch.constant.int 3
    %int1_5873 = torch.constant.int 1
    %int64_5874 = torch.constant.int 64
    %int2_5875 = torch.constant.int 2
    %3781 = torch.aten.slice.Tensor %3713, %int3_5872, %int1_5873, %int64_5874, %int2_5875 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3781, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %3782 = torch.aten.mul.Tensor %3780, %3778 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3782, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %3783 = torch.aten.mul.Tensor %3781, %3779 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3783, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_5876 = torch.constant.int 1
    %3784 = torch.aten.sub.Tensor %3782, %3783, %int1_5876 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3784, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %3785 = torch.aten.mul.Tensor %3781, %3778 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3785, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %3786 = torch.aten.mul.Tensor %3780, %3779 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3786, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_5877 = torch.constant.int 1
    %3787 = torch.aten.add.Tensor %3785, %3786, %int1_5877 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %3787, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %3788 = torch_c.to_builtin_tensor %3784 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_5878 = tensor.cast %3788 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %3789 = torch_c.to_builtin_tensor %3787 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_5879 = tensor.cast %3789 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %3790 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_5878, %cast_5879) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_5880 = tensor.cast %3790 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %3791 = torch_c.from_builtin_tensor %cast_5880 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %3791, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_5881 = torch.constant.int 4
    %int4_5882 = torch.constant.int 4
    %int64_5883 = torch.constant.int 64
    %3792 = torch.prim.ListConstruct %int4_5881, %273, %int4_5882, %int64_5883 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3793 = torch.aten.view %3791, %3792 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %3793, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_5884 = torch.constant.none
    %none_5885 = torch.constant.none
    %int5_5886 = torch.constant.int 5
    %cpu_5887 = torch.constant.device "cpu"
    %int0_5888 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3793, %none_5884, %none_5885, %int5_5886, %cpu_5887, %int0_5888 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_5889 = torch.constant.int 22
    %3794 = torch.aten.mul.Scalar %arg2, %int22_5889 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3794, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int11 = torch.constant.int 11
    %int1_5890 = torch.constant.int 1
    %3795 = torch.aten.add.Scalar %3794, %int11, %int1_5890 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3795, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_5891 = torch.constant.int 2
    %3796 = torch.aten.mul.Scalar %3795, %int2_5891 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3796, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_5892 = torch.constant.int 0
    %int1_5893 = torch.constant.int 1
    %3797 = torch.aten.add.Scalar %3796, %int0_5892, %int1_5893 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3797, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %3798 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %3799 = torch.aten.view %3797, %3798 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3799, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_5894 = torch.constant.int 4
    %int32_5895 = torch.constant.int 32
    %int4_5896 = torch.constant.int 4
    %int64_5897 = torch.constant.int 64
    %3800 = torch.prim.ListConstruct %int4_5894, %271, %int32_5895, %int4_5896, %int64_5897 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3801 = torch.aten.view %3793, %3800 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %3801, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_5898 = torch.constant.int 32
    %int4_5899 = torch.constant.int 4
    %int64_5900 = torch.constant.int 64
    %3802 = torch.prim.ListConstruct %446, %int32_5898, %int4_5899, %int64_5900 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3803 = torch.aten.view %3801, %3802 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %3803, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_5901 = torch.constant.int 1
    %int2_5902 = torch.constant.int 2
    %3804 = torch.aten.transpose.int %3803, %int1_5901, %int2_5902 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %3804, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_5903 = torch.constant.none
    %none_5904 = torch.constant.none
    %int5_5905 = torch.constant.int 5
    %cpu_5906 = torch.constant.device "cpu"
    %int0_5907 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3804, %none_5903, %none_5904, %int5_5905, %cpu_5906, %int0_5907 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_5908 = torch.constant.int 22
    %int2_5909 = torch.constant.int 2
    %int4_5910 = torch.constant.int 4
    %int32_5911 = torch.constant.int 32
    %int64_5912 = torch.constant.int 64
    %3805 = torch.prim.ListConstruct %272, %int22_5908, %int2_5909, %int4_5910, %int32_5911, %int64_5912 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3806 = torch.aten.view %3530, %3805 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3806, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_5913 = torch.constant.int 4
    %int32_5914 = torch.constant.int 32
    %int64_5915 = torch.constant.int 64
    %3807 = torch.prim.ListConstruct %439, %int4_5913, %int32_5914, %int64_5915 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3808 = torch.aten.view %3806, %3807 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %3808, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %3809 = torch.prim.ListConstruct %3799 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_5916 = torch.constant.bool false
    %3810 = torch.aten.index_put %3808, %3809, %3804, %false_5916 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %3810, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_5917 = torch.constant.int 22
    %int2_5918 = torch.constant.int 2
    %int4_5919 = torch.constant.int 4
    %int32_5920 = torch.constant.int 32
    %int64_5921 = torch.constant.int 64
    %3811 = torch.prim.ListConstruct %272, %int22_5917, %int2_5918, %int4_5919, %int32_5920, %int64_5921 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3812 = torch.aten.view %3810, %3811 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3812, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_5922 = torch.constant.int 360448
    %3813 = torch.prim.ListConstruct %272, %int360448_5922 : (!torch.int, !torch.int) -> !torch.list<int>
    %3814 = torch.aten.view %3812, %3813 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %3814, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_5923 = torch.constant.int 22
    %int2_5924 = torch.constant.int 2
    %int4_5925 = torch.constant.int 4
    %int32_5926 = torch.constant.int 32
    %int64_5927 = torch.constant.int 64
    %3815 = torch.prim.ListConstruct %272, %int22_5923, %int2_5924, %int4_5925, %int32_5926, %int64_5927 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3816 = torch.aten.view %3814, %3815 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3816, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_5928 = torch.constant.int 4
    %int32_5929 = torch.constant.int 32
    %int64_5930 = torch.constant.int 64
    %3817 = torch.prim.ListConstruct %439, %int4_5928, %int32_5929, %int64_5930 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3818 = torch.aten.view %3816, %3817 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %3818, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_5931 = torch.constant.int 22
    %3819 = torch.aten.mul.Scalar %arg2, %int22_5931 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3819, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int11_5932 = torch.constant.int 11
    %int1_5933 = torch.constant.int 1
    %3820 = torch.aten.add.Scalar %3819, %int11_5932, %int1_5933 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3820, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_5934 = torch.constant.int 2
    %3821 = torch.aten.mul.Scalar %3820, %int2_5934 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3821, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_5935 = torch.constant.int 1
    %int1_5936 = torch.constant.int 1
    %3822 = torch.aten.add.Scalar %3821, %int1_5935, %int1_5936 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3822, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %3823 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %3824 = torch.aten.view %3822, %3823 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3824, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_5937 = torch.constant.int 4
    %int32_5938 = torch.constant.int 32
    %int4_5939 = torch.constant.int 4
    %int64_5940 = torch.constant.int 64
    %3825 = torch.prim.ListConstruct %int4_5937, %271, %int32_5938, %int4_5939, %int64_5940 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3826 = torch.aten.view %3715, %3825 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %3826, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_5941 = torch.constant.int 32
    %int4_5942 = torch.constant.int 4
    %int64_5943 = torch.constant.int 64
    %3827 = torch.prim.ListConstruct %446, %int32_5941, %int4_5942, %int64_5943 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3828 = torch.aten.view %3826, %3827 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %3828, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_5944 = torch.constant.int 1
    %int2_5945 = torch.constant.int 2
    %3829 = torch.aten.transpose.int %3828, %int1_5944, %int2_5945 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %3829, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_5946 = torch.constant.none
    %none_5947 = torch.constant.none
    %int5_5948 = torch.constant.int 5
    %cpu_5949 = torch.constant.device "cpu"
    %int0_5950 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3829, %none_5946, %none_5947, %int5_5948, %cpu_5949, %int0_5950 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %3830 = torch.prim.ListConstruct %3824 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_5951 = torch.constant.bool false
    %3831 = torch.aten.index_put %3818, %3830, %3829, %false_5951 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %3831, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_5952 = torch.constant.int 22
    %int2_5953 = torch.constant.int 2
    %int4_5954 = torch.constant.int 4
    %int32_5955 = torch.constant.int 32
    %int64_5956 = torch.constant.int 64
    %3832 = torch.prim.ListConstruct %272, %int22_5952, %int2_5953, %int4_5954, %int32_5955, %int64_5956 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3833 = torch.aten.view %3831, %3832 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3833, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_5957 = torch.constant.int 360448
    %3834 = torch.prim.ListConstruct %272, %int360448_5957 : (!torch.int, !torch.int) -> !torch.list<int>
    %3835 = torch.aten.view %3833, %3834 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %3835, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_5958 = torch.constant.int 0
    %int1_5959 = torch.constant.int 1
    %none_5960 = torch.constant.none
    %none_5961 = torch.constant.none
    %cpu_5962 = torch.constant.device "cpu"
    %false_5963 = torch.constant.bool false
    %3836 = torch.aten.arange.start_step %int0_5958, %273, %int1_5959, %none_5960, %none_5961, %cpu_5962, %false_5963 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3836, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_5964 = torch.constant.int -1
    %3837 = torch.aten.unsqueeze %arg1, %int-1_5964 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %3838 = torch.aten.ge.Tensor %3836, %3837 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %3838, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_5965 = torch.constant.none
    %none_5966 = torch.constant.none
    %cpu_5967 = torch.constant.device "cpu"
    %false_5968 = torch.constant.bool false
    %3839 = torch.aten.arange %273, %none_5965, %none_5966, %cpu_5967, %false_5968 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3839, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_5969 = torch.constant.int 0
    %3840 = torch.aten.unsqueeze %3839, %int0_5969 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %3840, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_5970 = torch.constant.int 1
    %3841 = torch.aten.unsqueeze %3840, %int1_5970 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %3841, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_5971 = torch.constant.int 2
    %3842 = torch.aten.unsqueeze %3841, %int2_5971 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %3842, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_5972 = torch.constant.none
    %none_5973 = torch.constant.none
    %cpu_5974 = torch.constant.device "cpu"
    %false_5975 = torch.constant.bool false
    %3843 = torch.aten.arange %273, %none_5972, %none_5973, %cpu_5974, %false_5975 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3843, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_5976 = torch.constant.int 0
    %3844 = torch.aten.unsqueeze %3843, %int0_5976 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %3844, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_5977 = torch.constant.int 1
    %3845 = torch.aten.unsqueeze %3844, %int1_5977 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %3845, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_5978 = torch.constant.int 3
    %3846 = torch.aten.unsqueeze %3845, %int3_5978 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %3846, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %3847 = torch.aten.gt.Tensor %3842, %3846 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %3847, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_5979 = torch.constant.int 1
    %3848 = torch.aten.unsqueeze %3838, %int1_5979 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %3848, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_5980 = torch.constant.int 2
    %3849 = torch.aten.unsqueeze %3848, %int2_5980 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %3849, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %3850 = torch.aten.logical_or %3847, %3849 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %3850, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_5981 = torch.constant.none
    %3851 = torch.aten.clone %135, %none_5981 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_5982 = torch.constant.int 0
    %3852 = torch.aten.where.ScalarOther %3850, %3851, %int0_5982 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %3852, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_5983 = torch.constant.none
    %none_5984 = torch.constant.none
    %int5_5985 = torch.constant.int 5
    %cpu_5986 = torch.constant.device "cpu"
    %int0_5987 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3852, %none_5983, %none_5984, %int5_5985, %cpu_5986, %int0_5987 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_5988 = torch.constant.int -2
    %3853 = torch.aten.unsqueeze %3793, %int-2_5988 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %3853, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_5989 = torch.constant.int 4
    %int4_5990 = torch.constant.int 4
    %int8_5991 = torch.constant.int 8
    %int64_5992 = torch.constant.int 64
    %3854 = torch.prim.ListConstruct %int4_5989, %273, %int4_5990, %int8_5991, %int64_5992 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5993 = torch.constant.bool false
    %3855 = torch.aten.expand %3853, %3854, %false_5993 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3855, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_5994 = torch.constant.int 0
    %3856 = torch.aten.clone %3855, %int0_5994 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3856, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_5995 = torch.constant.int 4
    %int32_5996 = torch.constant.int 32
    %int64_5997 = torch.constant.int 64
    %3857 = torch.prim.ListConstruct %int4_5995, %273, %int32_5996, %int64_5997 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3858 = torch.aten._unsafe_view %3856, %3857 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3858, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_5998 = torch.constant.int -2
    %3859 = torch.aten.unsqueeze %3715, %int-2_5998 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %3859, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_5999 = torch.constant.int 4
    %int4_6000 = torch.constant.int 4
    %int8_6001 = torch.constant.int 8
    %int64_6002 = torch.constant.int 64
    %3860 = torch.prim.ListConstruct %int4_5999, %273, %int4_6000, %int8_6001, %int64_6002 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6003 = torch.constant.bool false
    %3861 = torch.aten.expand %3859, %3860, %false_6003 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3861, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_6004 = torch.constant.int 0
    %3862 = torch.aten.clone %3861, %int0_6004 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3862, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_6005 = torch.constant.int 4
    %int32_6006 = torch.constant.int 32
    %int64_6007 = torch.constant.int 64
    %3863 = torch.prim.ListConstruct %int4_6005, %273, %int32_6006, %int64_6007 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3864 = torch.aten._unsafe_view %3862, %3863 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3864, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_6008 = torch.constant.int 1
    %int2_6009 = torch.constant.int 2
    %3865 = torch.aten.transpose.int %3754, %int1_6008, %int2_6009 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %3865, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_6010 = torch.constant.int 1
    %int2_6011 = torch.constant.int 2
    %3866 = torch.aten.transpose.int %3858, %int1_6010, %int2_6011 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %3866, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_6012 = torch.constant.int 1
    %int2_6013 = torch.constant.int 2
    %3867 = torch.aten.transpose.int %3864, %int1_6012, %int2_6013 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %3867, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_6014 = torch.constant.float 0.000000e+00
    %false_6015 = torch.constant.bool false
    %none_6016 = torch.constant.none
    %false_6017 = torch.constant.bool false
    %3868 = torch.aten.scaled_dot_product_attention %3865, %3866, %3867, %3852, %float0.000000e00_6014, %false_6015, %none_6016, %false_6017 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %3868, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_6018 = torch.constant.int 1
    %int2_6019 = torch.constant.int 2
    %3869 = torch.aten.transpose.int %3868, %int1_6018, %int2_6019 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3869, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_6020 = torch.constant.int 4
    %int2048_6021 = torch.constant.int 2048
    %3870 = torch.prim.ListConstruct %int4_6020, %273, %int2048_6021 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3871 = torch.aten.view %3869, %3870 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3871, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_6022 = torch.constant.int 2
    %3872 = torch.aten.view.dtype %140, %int2_6022 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %3873 = torch.aten.detach %3872 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_6023 = torch.constant.int -1
    %int17_6024 = torch.constant.int 17
    %3874 = torch.prim.ListConstruct %int-1_6023, %int17_6024 : (!torch.int, !torch.int) -> !torch.list<int>
    %3875 = torch.aten.view %3873, %3874 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_6025 = torch.constant.int 2048
    %int-1_6026 = torch.constant.int -1
    %int17_6027 = torch.constant.int 17
    %3876 = torch.prim.ListConstruct %int2048_6025, %int-1_6026, %int17_6027 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3877 = torch.aten.view %3875, %3876 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_6028 = torch.constant.int 2
    %int0_6029 = torch.constant.int 0
    %int1_6030 = torch.constant.int 1
    %int1_6031 = torch.constant.int 1
    %3878 = torch.aten.slice.Tensor %3877, %int2_6028, %int0_6029, %int1_6030, %int1_6031 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_6032 = torch.constant.int 5
    %3879 = torch.aten.view.dtype %3878, %int5_6032 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %3880 = torch.aten.detach %3879 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_6033 = torch.constant.int 2
    %int1_6034 = torch.constant.int 1
    %int9223372036854775807_6035 = torch.constant.int 9223372036854775807
    %int1_6036 = torch.constant.int 1
    %3881 = torch.aten.slice.Tensor %3877, %int2_6033, %int1_6034, %int9223372036854775807_6035, %int1_6036 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_6037 = torch.constant.int 1
    %3882 = torch.aten.view.dtype %3881, %int1_6037 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %3883 = torch.aten.detach %3882 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %3884 = torch_c.to_builtin_tensor %3871 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_6038 = tensor.cast %3884 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3885 = torch_c.to_builtin_tensor %3880 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %3886 = torch_c.to_builtin_tensor %3883 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %3887 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_6038, %3885, %3886) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_6039 = tensor.cast %3887 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %3888 = torch_c.from_builtin_tensor %cast_6039 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3888, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_6040 = torch.constant.none
    %none_6041 = torch.constant.none
    %int5_6042 = torch.constant.int 5
    %cpu_6043 = torch.constant.device "cpu"
    %int0_6044 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3888, %none_6040, %none_6041, %int5_6042, %cpu_6043, %int0_6044 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_6045 = torch.constant.int 1
    %3889 = torch.aten.add.Tensor %3648, %3888, %int1_6045 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3889, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_6046 = torch.constant.none
    %none_6047 = torch.constant.none
    %int5_6048 = torch.constant.int 5
    %cpu_6049 = torch.constant.device "cpu"
    %int0_6050 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3889, %none_6046, %none_6047, %int5_6048, %cpu_6049, %int0_6050 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6051 = torch.constant.int 6
    %3890 = torch.prims.convert_element_type %3889, %int6_6051 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3890, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_6052 = torch.constant.int 2
    %3891 = torch.aten.pow.Tensor_Scalar %3890, %int2_6052 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3891, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_6053 = torch.constant.int -1
    %3892 = torch.prim.ListConstruct %int-1_6053 : (!torch.int) -> !torch.list<int>
    %true_6054 = torch.constant.bool true
    %none_6055 = torch.constant.none
    %3893 = torch.aten.mean.dim %3891, %3892, %true_6054, %none_6055 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3893, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_6056 = torch.constant.float 9.9999997473787516E-6
    %int1_6057 = torch.constant.int 1
    %3894 = torch.aten.add.Scalar %3893, %float9.999990e-06_6056, %int1_6057 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3894, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3895 = torch.aten.rsqrt %3894 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3895, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3896 = torch.aten.mul.Tensor %3890, %3895 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3896, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_6058 = torch.constant.none
    %none_6059 = torch.constant.none
    %int6_6060 = torch.constant.int 6
    %cpu_6061 = torch.constant.device "cpu"
    %int0_6062 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3896, %none_6058, %none_6059, %int6_6060, %cpu_6061, %int0_6062 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6063 = torch.constant.int 5
    %3897 = torch.prims.convert_element_type %3896, %int5_6063 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3897, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %3898 = torch.aten.mul.Tensor %141, %3897 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3898, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_6064 = torch.constant.none
    %none_6065 = torch.constant.none
    %int6_6066 = torch.constant.int 6
    %cpu_6067 = torch.constant.device "cpu"
    %int0_6068 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3898, %none_6064, %none_6065, %int6_6066, %cpu_6067, %int0_6068 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6069 = torch.constant.int 5
    %3899 = torch.prims.convert_element_type %3898, %int5_6069 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3899, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_6070 = torch.constant.int 2
    %3900 = torch.aten.view.dtype %142, %int2_6070 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %3901 = torch.aten.detach %3900 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_6071 = torch.constant.int -1
    %int17_6072 = torch.constant.int 17
    %3902 = torch.prim.ListConstruct %int-1_6071, %int17_6072 : (!torch.int, !torch.int) -> !torch.list<int>
    %3903 = torch.aten.view %3901, %3902 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_6073 = torch.constant.int 5632
    %int-1_6074 = torch.constant.int -1
    %int17_6075 = torch.constant.int 17
    %3904 = torch.prim.ListConstruct %int5632_6073, %int-1_6074, %int17_6075 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3905 = torch.aten.view %3903, %3904 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_6076 = torch.constant.int 2
    %int0_6077 = torch.constant.int 0
    %int1_6078 = torch.constant.int 1
    %int1_6079 = torch.constant.int 1
    %3906 = torch.aten.slice.Tensor %3905, %int2_6076, %int0_6077, %int1_6078, %int1_6079 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_6080 = torch.constant.int 5
    %3907 = torch.aten.view.dtype %3906, %int5_6080 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %3908 = torch.aten.detach %3907 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_6081 = torch.constant.int 2
    %int1_6082 = torch.constant.int 1
    %int9223372036854775807_6083 = torch.constant.int 9223372036854775807
    %int1_6084 = torch.constant.int 1
    %3909 = torch.aten.slice.Tensor %3905, %int2_6081, %int1_6082, %int9223372036854775807_6083, %int1_6084 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_6085 = torch.constant.int 1
    %3910 = torch.aten.view.dtype %3909, %int1_6085 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %3911 = torch.aten.detach %3910 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %3912 = torch_c.to_builtin_tensor %3899 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_6086 = tensor.cast %3912 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3913 = torch_c.to_builtin_tensor %3908 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %3914 = torch_c.to_builtin_tensor %3911 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %3915 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_6086, %3913, %3914) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_6087 = tensor.cast %3915 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %3916 = torch_c.from_builtin_tensor %cast_6087 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %3916, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %3917 = torch.aten.silu %3916 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %3917, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_6088 = torch.constant.int 2
    %3918 = torch.aten.view.dtype %143, %int2_6088 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %3919 = torch.aten.detach %3918 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_6089 = torch.constant.int -1
    %int17_6090 = torch.constant.int 17
    %3920 = torch.prim.ListConstruct %int-1_6089, %int17_6090 : (!torch.int, !torch.int) -> !torch.list<int>
    %3921 = torch.aten.view %3919, %3920 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_6091 = torch.constant.int 5632
    %int-1_6092 = torch.constant.int -1
    %int17_6093 = torch.constant.int 17
    %3922 = torch.prim.ListConstruct %int5632_6091, %int-1_6092, %int17_6093 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3923 = torch.aten.view %3921, %3922 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_6094 = torch.constant.int 2
    %int0_6095 = torch.constant.int 0
    %int1_6096 = torch.constant.int 1
    %int1_6097 = torch.constant.int 1
    %3924 = torch.aten.slice.Tensor %3923, %int2_6094, %int0_6095, %int1_6096, %int1_6097 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_6098 = torch.constant.int 5
    %3925 = torch.aten.view.dtype %3924, %int5_6098 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %3926 = torch.aten.detach %3925 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_6099 = torch.constant.int 2
    %int1_6100 = torch.constant.int 1
    %int9223372036854775807_6101 = torch.constant.int 9223372036854775807
    %int1_6102 = torch.constant.int 1
    %3927 = torch.aten.slice.Tensor %3923, %int2_6099, %int1_6100, %int9223372036854775807_6101, %int1_6102 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_6103 = torch.constant.int 1
    %3928 = torch.aten.view.dtype %3927, %int1_6103 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %3929 = torch.aten.detach %3928 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %3930 = torch_c.to_builtin_tensor %3899 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_6104 = tensor.cast %3930 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3931 = torch_c.to_builtin_tensor %3926 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %3932 = torch_c.to_builtin_tensor %3929 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %3933 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_6104, %3931, %3932) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_6105 = tensor.cast %3933 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %3934 = torch_c.from_builtin_tensor %cast_6105 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %3934, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %3935 = torch.aten.mul.Tensor %3917, %3934 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %3935, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_6106 = torch.constant.int 2
    %3936 = torch.aten.view.dtype %144, %int2_6106 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %3937 = torch.aten.detach %3936 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_6107 = torch.constant.int -1
    %int17_6108 = torch.constant.int 17
    %3938 = torch.prim.ListConstruct %int-1_6107, %int17_6108 : (!torch.int, !torch.int) -> !torch.list<int>
    %3939 = torch.aten.view %3937, %3938 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_6109 = torch.constant.int 2048
    %int-1_6110 = torch.constant.int -1
    %int17_6111 = torch.constant.int 17
    %3940 = torch.prim.ListConstruct %int2048_6109, %int-1_6110, %int17_6111 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3941 = torch.aten.view %3939, %3940 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_6112 = torch.constant.int 2
    %int0_6113 = torch.constant.int 0
    %int1_6114 = torch.constant.int 1
    %int1_6115 = torch.constant.int 1
    %3942 = torch.aten.slice.Tensor %3941, %int2_6112, %int0_6113, %int1_6114, %int1_6115 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_6116 = torch.constant.int 5
    %3943 = torch.aten.view.dtype %3942, %int5_6116 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %3944 = torch.aten.detach %3943 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_6117 = torch.constant.int 2
    %int1_6118 = torch.constant.int 1
    %int9223372036854775807_6119 = torch.constant.int 9223372036854775807
    %int1_6120 = torch.constant.int 1
    %3945 = torch.aten.slice.Tensor %3941, %int2_6117, %int1_6118, %int9223372036854775807_6119, %int1_6120 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_6121 = torch.constant.int 1
    %3946 = torch.aten.view.dtype %3945, %int1_6121 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %3947 = torch.aten.detach %3946 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %3948 = torch_c.to_builtin_tensor %3935 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_6122 = tensor.cast %3948 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %3949 = torch_c.to_builtin_tensor %3944 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %3950 = torch_c.to_builtin_tensor %3947 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %3951 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_6122, %3949, %3950) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_6123 = tensor.cast %3951 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %3952 = torch_c.from_builtin_tensor %cast_6123 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3952, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_6124 = torch.constant.int 1
    %3953 = torch.aten.add.Tensor %3889, %3952, %int1_6124 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3953, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_6125 = torch.constant.none
    %none_6126 = torch.constant.none
    %int5_6127 = torch.constant.int 5
    %cpu_6128 = torch.constant.device "cpu"
    %int0_6129 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3953, %none_6125, %none_6126, %int5_6127, %cpu_6128, %int0_6129 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6130 = torch.constant.int 6
    %3954 = torch.prims.convert_element_type %3953, %int6_6130 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3954, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_6131 = torch.constant.int 2
    %3955 = torch.aten.pow.Tensor_Scalar %3954, %int2_6131 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3955, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_6132 = torch.constant.int -1
    %3956 = torch.prim.ListConstruct %int-1_6132 : (!torch.int) -> !torch.list<int>
    %true_6133 = torch.constant.bool true
    %none_6134 = torch.constant.none
    %3957 = torch.aten.mean.dim %3955, %3956, %true_6133, %none_6134 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3957, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_6135 = torch.constant.float 9.9999997473787516E-6
    %int1_6136 = torch.constant.int 1
    %3958 = torch.aten.add.Scalar %3957, %float9.999990e-06_6135, %int1_6136 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3958, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3959 = torch.aten.rsqrt %3958 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3959, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3960 = torch.aten.mul.Tensor %3954, %3959 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3960, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_6137 = torch.constant.none
    %none_6138 = torch.constant.none
    %int6_6139 = torch.constant.int 6
    %cpu_6140 = torch.constant.device "cpu"
    %int0_6141 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3960, %none_6137, %none_6138, %int6_6139, %cpu_6140, %int0_6141 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6142 = torch.constant.int 5
    %3961 = torch.prims.convert_element_type %3960, %int5_6142 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3961, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %3962 = torch.aten.mul.Tensor %148, %3961 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %3962, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_6143 = torch.constant.none
    %none_6144 = torch.constant.none
    %int6_6145 = torch.constant.int 6
    %cpu_6146 = torch.constant.device "cpu"
    %int0_6147 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3962, %none_6143, %none_6144, %int6_6145, %cpu_6146, %int0_6147 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6148 = torch.constant.int 5
    %3963 = torch.prims.convert_element_type %3962, %int5_6148 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3963, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_6149 = torch.constant.int 2
    %3964 = torch.aten.view.dtype %149, %int2_6149 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %3965 = torch.aten.detach %3964 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_6150 = torch.constant.int -1
    %int17_6151 = torch.constant.int 17
    %3966 = torch.prim.ListConstruct %int-1_6150, %int17_6151 : (!torch.int, !torch.int) -> !torch.list<int>
    %3967 = torch.aten.view %3965, %3966 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_6152 = torch.constant.int 2048
    %int-1_6153 = torch.constant.int -1
    %int17_6154 = torch.constant.int 17
    %3968 = torch.prim.ListConstruct %int2048_6152, %int-1_6153, %int17_6154 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3969 = torch.aten.view %3967, %3968 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_6155 = torch.constant.int 2
    %int0_6156 = torch.constant.int 0
    %int1_6157 = torch.constant.int 1
    %int1_6158 = torch.constant.int 1
    %3970 = torch.aten.slice.Tensor %3969, %int2_6155, %int0_6156, %int1_6157, %int1_6158 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_6159 = torch.constant.int 5
    %3971 = torch.aten.view.dtype %3970, %int5_6159 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %3972 = torch.aten.detach %3971 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_6160 = torch.constant.int 2
    %int1_6161 = torch.constant.int 1
    %int9223372036854775807_6162 = torch.constant.int 9223372036854775807
    %int1_6163 = torch.constant.int 1
    %3973 = torch.aten.slice.Tensor %3969, %int2_6160, %int1_6161, %int9223372036854775807_6162, %int1_6163 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_6164 = torch.constant.int 1
    %3974 = torch.aten.view.dtype %3973, %int1_6164 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %3975 = torch.aten.detach %3974 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %3976 = torch_c.to_builtin_tensor %3963 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_6165 = tensor.cast %3976 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3977 = torch_c.to_builtin_tensor %3972 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %3978 = torch_c.to_builtin_tensor %3975 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %3979 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_6165, %3977, %3978) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_6166 = tensor.cast %3979 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %3980 = torch_c.from_builtin_tensor %cast_6166 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %3980, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_6167 = torch.constant.int 2
    %3981 = torch.aten.view.dtype %150, %int2_6167 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %3982 = torch.aten.detach %3981 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_6168 = torch.constant.int -1
    %int17_6169 = torch.constant.int 17
    %3983 = torch.prim.ListConstruct %int-1_6168, %int17_6169 : (!torch.int, !torch.int) -> !torch.list<int>
    %3984 = torch.aten.view %3982, %3983 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_6170 = torch.constant.int 256
    %int-1_6171 = torch.constant.int -1
    %int17_6172 = torch.constant.int 17
    %3985 = torch.prim.ListConstruct %int256_6170, %int-1_6171, %int17_6172 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3986 = torch.aten.view %3984, %3985 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_6173 = torch.constant.int 2
    %int0_6174 = torch.constant.int 0
    %int1_6175 = torch.constant.int 1
    %int1_6176 = torch.constant.int 1
    %3987 = torch.aten.slice.Tensor %3986, %int2_6173, %int0_6174, %int1_6175, %int1_6176 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_6177 = torch.constant.int 5
    %3988 = torch.aten.view.dtype %3987, %int5_6177 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %3989 = torch.aten.detach %3988 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_6178 = torch.constant.int 2
    %int1_6179 = torch.constant.int 1
    %int9223372036854775807_6180 = torch.constant.int 9223372036854775807
    %int1_6181 = torch.constant.int 1
    %3990 = torch.aten.slice.Tensor %3986, %int2_6178, %int1_6179, %int9223372036854775807_6180, %int1_6181 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_6182 = torch.constant.int 1
    %3991 = torch.aten.view.dtype %3990, %int1_6182 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %3992 = torch.aten.detach %3991 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %3993 = torch_c.to_builtin_tensor %3963 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_6183 = tensor.cast %3993 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %3994 = torch_c.to_builtin_tensor %3989 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %3995 = torch_c.to_builtin_tensor %3992 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %3996 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_6183, %3994, %3995) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_6184 = tensor.cast %3996 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %3997 = torch_c.from_builtin_tensor %cast_6184 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %3997, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_6185 = torch.constant.int 2
    %3998 = torch.aten.view.dtype %151, %int2_6185 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %3999 = torch.aten.detach %3998 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_6186 = torch.constant.int -1
    %int17_6187 = torch.constant.int 17
    %4000 = torch.prim.ListConstruct %int-1_6186, %int17_6187 : (!torch.int, !torch.int) -> !torch.list<int>
    %4001 = torch.aten.view %3999, %4000 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_6188 = torch.constant.int 256
    %int-1_6189 = torch.constant.int -1
    %int17_6190 = torch.constant.int 17
    %4002 = torch.prim.ListConstruct %int256_6188, %int-1_6189, %int17_6190 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4003 = torch.aten.view %4001, %4002 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_6191 = torch.constant.int 2
    %int0_6192 = torch.constant.int 0
    %int1_6193 = torch.constant.int 1
    %int1_6194 = torch.constant.int 1
    %4004 = torch.aten.slice.Tensor %4003, %int2_6191, %int0_6192, %int1_6193, %int1_6194 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_6195 = torch.constant.int 5
    %4005 = torch.aten.view.dtype %4004, %int5_6195 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %4006 = torch.aten.detach %4005 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_6196 = torch.constant.int 2
    %int1_6197 = torch.constant.int 1
    %int9223372036854775807_6198 = torch.constant.int 9223372036854775807
    %int1_6199 = torch.constant.int 1
    %4007 = torch.aten.slice.Tensor %4003, %int2_6196, %int1_6197, %int9223372036854775807_6198, %int1_6199 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_6200 = torch.constant.int 1
    %4008 = torch.aten.view.dtype %4007, %int1_6200 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %4009 = torch.aten.detach %4008 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %4010 = torch_c.to_builtin_tensor %3963 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_6201 = tensor.cast %4010 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4011 = torch_c.to_builtin_tensor %4006 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %4012 = torch_c.to_builtin_tensor %4009 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %4013 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_6201, %4011, %4012) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_6202 = tensor.cast %4013 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %4014 = torch_c.from_builtin_tensor %cast_6202 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %4014, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_6203 = torch.constant.int 4
    %int32_6204 = torch.constant.int 32
    %int64_6205 = torch.constant.int 64
    %4015 = torch.prim.ListConstruct %int4_6203, %273, %int32_6204, %int64_6205 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4016 = torch.aten.view %3980, %4015 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4016, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_6206 = torch.constant.int 4
    %int4_6207 = torch.constant.int 4
    %int64_6208 = torch.constant.int 64
    %4017 = torch.prim.ListConstruct %int4_6206, %273, %int4_6207, %int64_6208 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4018 = torch.aten.view %3997, %4017 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %4018, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_6209 = torch.constant.int 4
    %int4_6210 = torch.constant.int 4
    %int64_6211 = torch.constant.int 64
    %4019 = torch.prim.ListConstruct %int4_6209, %273, %int4_6210, %int64_6211 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4020 = torch.aten.view %4014, %4019 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %4020, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_6212 = torch.constant.int 0
    %none_6213 = torch.constant.none
    %none_6214 = torch.constant.none
    %cpu_6215 = torch.constant.device "cpu"
    %false_6216 = torch.constant.bool false
    %4021 = torch.aten.arange.start %int0_6212, %273, %none_6213, %none_6214, %cpu_6215, %false_6216 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4021, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_6217 = torch.constant.int 0
    %4022 = torch.aten.unsqueeze %4021, %int0_6217 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %4022, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_6218 = torch.constant.int 0
    %int64_6219 = torch.constant.int 64
    %int2_6220 = torch.constant.int 2
    %none_6221 = torch.constant.none
    %none_6222 = torch.constant.none
    %cpu_6223 = torch.constant.device "cpu"
    %false_6224 = torch.constant.bool false
    %4023 = torch.aten.arange.start_step %int0_6218, %int64_6219, %int2_6220, %none_6221, %none_6222, %cpu_6223, %false_6224 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_6225 = torch.constant.none
    %none_6226 = torch.constant.none
    %int4_6227 = torch.constant.int 4
    %cpu_6228 = torch.constant.device "cpu"
    %int0_6229 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4023, %none_6225, %none_6226, %int4_6227, %cpu_6228, %int0_6229 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6230 = torch.constant.int 6
    %4024 = torch.prims.convert_element_type %4023, %int6_6230 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_6231 = torch.constant.int 64
    %4025 = torch.aten.div.Scalar %4024, %int64_6231 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_6232 = torch.constant.float 1.000000e+04
    %4026 = torch.aten.pow.Scalar %float1.000000e04_6232, %4025 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %4027 = torch.aten.reciprocal %4026 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_6233 = torch.constant.float 1.000000e+00
    %4028 = torch.aten.mul.Scalar %4027, %float1.000000e00_6233 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_6234 = torch.constant.none
    %4029 = torch.aten.clone %145, %none_6234 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_6235 = torch.constant.int 0
    %4030 = torch.aten.unsqueeze %4028, %int0_6235 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_6236 = torch.constant.int 2
    %4031 = torch.aten.unsqueeze %4030, %int2_6236 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_6237 = torch.constant.none
    %none_6238 = torch.constant.none
    %int6_6239 = torch.constant.int 6
    %cpu_6240 = torch.constant.device "cpu"
    %int0_6241 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4031, %none_6237, %none_6238, %int6_6239, %cpu_6240, %int0_6241 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_6242 = torch.constant.int 1
    %int-1_6243 = torch.constant.int -1
    %int1_6244 = torch.constant.int 1
    %4032 = torch.prim.ListConstruct %int1_6242, %int-1_6243, %int1_6244 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6245 = torch.constant.bool false
    %4033 = torch.aten.expand %4031, %4032, %false_6245 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_6246 = torch.constant.int 1
    %4034 = torch.aten.unsqueeze %4022, %int1_6246 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %4034, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_6247 = torch.constant.none
    %none_6248 = torch.constant.none
    %int4_6249 = torch.constant.int 4
    %cpu_6250 = torch.constant.device "cpu"
    %int0_6251 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4034, %none_6247, %none_6248, %int4_6249, %cpu_6250, %int0_6251 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6252 = torch.constant.int 6
    %4035 = torch.prims.convert_element_type %4034, %int6_6252 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %4035, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %4036 = torch.aten.matmul %4033, %4035 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %4036, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_6253 = torch.constant.int 1
    %int2_6254 = torch.constant.int 2
    %4037 = torch.aten.transpose.int %4036, %int1_6253, %int2_6254 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4037, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4038 = torch.aten.cos %4037 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4038, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4039 = torch.aten.mul.Tensor %4038, %4029 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4039, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_6255 = torch.constant.none
    %none_6256 = torch.constant.none
    %int6_6257 = torch.constant.int 6
    %cpu_6258 = torch.constant.device "cpu"
    %int0_6259 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4039, %none_6255, %none_6256, %int6_6257, %cpu_6258, %int0_6259 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6260 = torch.constant.int 5
    %4040 = torch.prims.convert_element_type %4039, %int5_6260 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %4040, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %4041 = torch.aten.sin %4037 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4041, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4042 = torch.aten.mul.Tensor %4041, %4029 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4042, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_6261 = torch.constant.none
    %none_6262 = torch.constant.none
    %int6_6263 = torch.constant.int 6
    %cpu_6264 = torch.constant.device "cpu"
    %int0_6265 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4042, %none_6261, %none_6262, %int6_6263, %cpu_6264, %int0_6265 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6266 = torch.constant.int 5
    %4043 = torch.prims.convert_element_type %4042, %int5_6266 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %4043, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_6267 = torch.constant.int 2
    %4044 = torch.aten.unsqueeze %4040, %int2_6267 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %4044, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_6268 = torch.constant.int 2
    %4045 = torch.aten.unsqueeze %4043, %int2_6268 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %4045, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_6269 = torch.constant.none
    %none_6270 = torch.constant.none
    %int5_6271 = torch.constant.int 5
    %cpu_6272 = torch.constant.device "cpu"
    %int0_6273 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4044, %none_6269, %none_6270, %int5_6271, %cpu_6272, %int0_6273 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_6274 = torch.constant.none
    %none_6275 = torch.constant.none
    %int5_6276 = torch.constant.int 5
    %cpu_6277 = torch.constant.device "cpu"
    %int0_6278 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4045, %none_6274, %none_6275, %int5_6276, %cpu_6277, %int0_6278 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_6279 = torch.constant.none
    %none_6280 = torch.constant.none
    %int5_6281 = torch.constant.int 5
    %cpu_6282 = torch.constant.device "cpu"
    %int0_6283 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4016, %none_6279, %none_6280, %int5_6281, %cpu_6282, %int0_6283 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_6284 = torch.constant.int 3
    %int0_6285 = torch.constant.int 0
    %int64_6286 = torch.constant.int 64
    %int2_6287 = torch.constant.int 2
    %4046 = torch.aten.slice.Tensor %4016, %int3_6284, %int0_6285, %int64_6286, %int2_6287 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4046, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_6288 = torch.constant.int 3
    %int1_6289 = torch.constant.int 1
    %int64_6290 = torch.constant.int 64
    %int2_6291 = torch.constant.int 2
    %4047 = torch.aten.slice.Tensor %4016, %int3_6288, %int1_6289, %int64_6290, %int2_6291 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4047, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4048 = torch.aten.mul.Tensor %4046, %4044 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4048, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4049 = torch.aten.mul.Tensor %4047, %4045 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4049, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_6292 = torch.constant.int 1
    %4050 = torch.aten.sub.Tensor %4048, %4049, %int1_6292 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4050, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4051 = torch.aten.mul.Tensor %4047, %4044 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4051, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4052 = torch.aten.mul.Tensor %4046, %4045 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4052, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_6293 = torch.constant.int 1
    %4053 = torch.aten.add.Tensor %4051, %4052, %int1_6293 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4053, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4054 = torch_c.to_builtin_tensor %4050 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_6294 = tensor.cast %4054 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %4055 = torch_c.to_builtin_tensor %4053 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_6295 = tensor.cast %4055 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %4056 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_6294, %cast_6295) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_6296 = tensor.cast %4056 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %4057 = torch_c.from_builtin_tensor %cast_6296 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %4057, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_6297 = torch.constant.int 4
    %int32_6298 = torch.constant.int 32
    %int64_6299 = torch.constant.int 64
    %4058 = torch.prim.ListConstruct %int4_6297, %273, %int32_6298, %int64_6299 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4059 = torch.aten.view %4057, %4058 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4059, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_6300 = torch.constant.none
    %none_6301 = torch.constant.none
    %int5_6302 = torch.constant.int 5
    %cpu_6303 = torch.constant.device "cpu"
    %int0_6304 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4059, %none_6300, %none_6301, %int5_6302, %cpu_6303, %int0_6304 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_6305 = torch.constant.int 0
    %none_6306 = torch.constant.none
    %none_6307 = torch.constant.none
    %cpu_6308 = torch.constant.device "cpu"
    %false_6309 = torch.constant.bool false
    %4060 = torch.aten.arange.start %int0_6305, %273, %none_6306, %none_6307, %cpu_6308, %false_6309 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4060, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_6310 = torch.constant.int 0
    %4061 = torch.aten.unsqueeze %4060, %int0_6310 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %4061, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_6311 = torch.constant.int 0
    %int64_6312 = torch.constant.int 64
    %int2_6313 = torch.constant.int 2
    %none_6314 = torch.constant.none
    %none_6315 = torch.constant.none
    %cpu_6316 = torch.constant.device "cpu"
    %false_6317 = torch.constant.bool false
    %4062 = torch.aten.arange.start_step %int0_6311, %int64_6312, %int2_6313, %none_6314, %none_6315, %cpu_6316, %false_6317 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_6318 = torch.constant.none
    %none_6319 = torch.constant.none
    %int4_6320 = torch.constant.int 4
    %cpu_6321 = torch.constant.device "cpu"
    %int0_6322 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4062, %none_6318, %none_6319, %int4_6320, %cpu_6321, %int0_6322 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6323 = torch.constant.int 6
    %4063 = torch.prims.convert_element_type %4062, %int6_6323 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_6324 = torch.constant.int 64
    %4064 = torch.aten.div.Scalar %4063, %int64_6324 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_6325 = torch.constant.float 1.000000e+04
    %4065 = torch.aten.pow.Scalar %float1.000000e04_6325, %4064 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %4066 = torch.aten.reciprocal %4065 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_6326 = torch.constant.float 1.000000e+00
    %4067 = torch.aten.mul.Scalar %4066, %float1.000000e00_6326 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_6327 = torch.constant.none
    %4068 = torch.aten.clone %146, %none_6327 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_6328 = torch.constant.int 0
    %4069 = torch.aten.unsqueeze %4067, %int0_6328 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_6329 = torch.constant.int 2
    %4070 = torch.aten.unsqueeze %4069, %int2_6329 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_6330 = torch.constant.none
    %none_6331 = torch.constant.none
    %int6_6332 = torch.constant.int 6
    %cpu_6333 = torch.constant.device "cpu"
    %int0_6334 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4070, %none_6330, %none_6331, %int6_6332, %cpu_6333, %int0_6334 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_6335 = torch.constant.int 1
    %int-1_6336 = torch.constant.int -1
    %int1_6337 = torch.constant.int 1
    %4071 = torch.prim.ListConstruct %int1_6335, %int-1_6336, %int1_6337 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6338 = torch.constant.bool false
    %4072 = torch.aten.expand %4070, %4071, %false_6338 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_6339 = torch.constant.int 1
    %4073 = torch.aten.unsqueeze %4061, %int1_6339 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %4073, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_6340 = torch.constant.none
    %none_6341 = torch.constant.none
    %int4_6342 = torch.constant.int 4
    %cpu_6343 = torch.constant.device "cpu"
    %int0_6344 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4073, %none_6340, %none_6341, %int4_6342, %cpu_6343, %int0_6344 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6345 = torch.constant.int 6
    %4074 = torch.prims.convert_element_type %4073, %int6_6345 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %4074, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %4075 = torch.aten.matmul %4072, %4074 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %4075, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_6346 = torch.constant.int 1
    %int2_6347 = torch.constant.int 2
    %4076 = torch.aten.transpose.int %4075, %int1_6346, %int2_6347 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4076, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4077 = torch.aten.cos %4076 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4077, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4078 = torch.aten.mul.Tensor %4077, %4068 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4078, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_6348 = torch.constant.none
    %none_6349 = torch.constant.none
    %int6_6350 = torch.constant.int 6
    %cpu_6351 = torch.constant.device "cpu"
    %int0_6352 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4078, %none_6348, %none_6349, %int6_6350, %cpu_6351, %int0_6352 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6353 = torch.constant.int 5
    %4079 = torch.prims.convert_element_type %4078, %int5_6353 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %4079, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %4080 = torch.aten.sin %4076 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4080, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4081 = torch.aten.mul.Tensor %4080, %4068 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4081, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_6354 = torch.constant.none
    %none_6355 = torch.constant.none
    %int6_6356 = torch.constant.int 6
    %cpu_6357 = torch.constant.device "cpu"
    %int0_6358 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4081, %none_6354, %none_6355, %int6_6356, %cpu_6357, %int0_6358 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6359 = torch.constant.int 5
    %4082 = torch.prims.convert_element_type %4081, %int5_6359 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %4082, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_6360 = torch.constant.int 2
    %4083 = torch.aten.unsqueeze %4079, %int2_6360 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %4083, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_6361 = torch.constant.int 2
    %4084 = torch.aten.unsqueeze %4082, %int2_6361 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %4084, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_6362 = torch.constant.none
    %none_6363 = torch.constant.none
    %int5_6364 = torch.constant.int 5
    %cpu_6365 = torch.constant.device "cpu"
    %int0_6366 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4083, %none_6362, %none_6363, %int5_6364, %cpu_6365, %int0_6366 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_6367 = torch.constant.none
    %none_6368 = torch.constant.none
    %int5_6369 = torch.constant.int 5
    %cpu_6370 = torch.constant.device "cpu"
    %int0_6371 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4084, %none_6367, %none_6368, %int5_6369, %cpu_6370, %int0_6371 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_6372 = torch.constant.none
    %none_6373 = torch.constant.none
    %int5_6374 = torch.constant.int 5
    %cpu_6375 = torch.constant.device "cpu"
    %int0_6376 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4018, %none_6372, %none_6373, %int5_6374, %cpu_6375, %int0_6376 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_6377 = torch.constant.int 3
    %int0_6378 = torch.constant.int 0
    %int64_6379 = torch.constant.int 64
    %int2_6380 = torch.constant.int 2
    %4085 = torch.aten.slice.Tensor %4018, %int3_6377, %int0_6378, %int64_6379, %int2_6380 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4085, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_6381 = torch.constant.int 3
    %int1_6382 = torch.constant.int 1
    %int64_6383 = torch.constant.int 64
    %int2_6384 = torch.constant.int 2
    %4086 = torch.aten.slice.Tensor %4018, %int3_6381, %int1_6382, %int64_6383, %int2_6384 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4086, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %4087 = torch.aten.mul.Tensor %4085, %4083 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4087, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %4088 = torch.aten.mul.Tensor %4086, %4084 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4088, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_6385 = torch.constant.int 1
    %4089 = torch.aten.sub.Tensor %4087, %4088, %int1_6385 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4089, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %4090 = torch.aten.mul.Tensor %4086, %4083 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4090, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %4091 = torch.aten.mul.Tensor %4085, %4084 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4091, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_6386 = torch.constant.int 1
    %4092 = torch.aten.add.Tensor %4090, %4091, %int1_6386 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4092, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %4093 = torch_c.to_builtin_tensor %4089 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_6387 = tensor.cast %4093 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %4094 = torch_c.to_builtin_tensor %4092 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_6388 = tensor.cast %4094 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %4095 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_6387, %cast_6388) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_6389 = tensor.cast %4095 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %4096 = torch_c.from_builtin_tensor %cast_6389 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %4096, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_6390 = torch.constant.int 4
    %int4_6391 = torch.constant.int 4
    %int64_6392 = torch.constant.int 64
    %4097 = torch.prim.ListConstruct %int4_6390, %273, %int4_6391, %int64_6392 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4098 = torch.aten.view %4096, %4097 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %4098, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_6393 = torch.constant.none
    %none_6394 = torch.constant.none
    %int5_6395 = torch.constant.int 5
    %cpu_6396 = torch.constant.device "cpu"
    %int0_6397 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4098, %none_6393, %none_6394, %int5_6395, %cpu_6396, %int0_6397 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_6398 = torch.constant.int 22
    %4099 = torch.aten.mul.Scalar %arg2, %int22_6398 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4099, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int12 = torch.constant.int 12
    %int1_6399 = torch.constant.int 1
    %4100 = torch.aten.add.Scalar %4099, %int12, %int1_6399 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4100, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_6400 = torch.constant.int 2
    %4101 = torch.aten.mul.Scalar %4100, %int2_6400 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4101, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_6401 = torch.constant.int 0
    %int1_6402 = torch.constant.int 1
    %4102 = torch.aten.add.Scalar %4101, %int0_6401, %int1_6402 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4102, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %4103 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %4104 = torch.aten.view %4102, %4103 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4104, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_6403 = torch.constant.int 4
    %int32_6404 = torch.constant.int 32
    %int4_6405 = torch.constant.int 4
    %int64_6406 = torch.constant.int 64
    %4105 = torch.prim.ListConstruct %int4_6403, %271, %int32_6404, %int4_6405, %int64_6406 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4106 = torch.aten.view %4098, %4105 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %4106, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_6407 = torch.constant.int 32
    %int4_6408 = torch.constant.int 4
    %int64_6409 = torch.constant.int 64
    %4107 = torch.prim.ListConstruct %446, %int32_6407, %int4_6408, %int64_6409 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4108 = torch.aten.view %4106, %4107 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %4108, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_6410 = torch.constant.int 1
    %int2_6411 = torch.constant.int 2
    %4109 = torch.aten.transpose.int %4108, %int1_6410, %int2_6411 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %4109, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_6412 = torch.constant.none
    %none_6413 = torch.constant.none
    %int5_6414 = torch.constant.int 5
    %cpu_6415 = torch.constant.device "cpu"
    %int0_6416 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4109, %none_6412, %none_6413, %int5_6414, %cpu_6415, %int0_6416 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_6417 = torch.constant.int 22
    %int2_6418 = torch.constant.int 2
    %int4_6419 = torch.constant.int 4
    %int32_6420 = torch.constant.int 32
    %int64_6421 = torch.constant.int 64
    %4110 = torch.prim.ListConstruct %272, %int22_6417, %int2_6418, %int4_6419, %int32_6420, %int64_6421 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4111 = torch.aten.view %3835, %4110 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4111, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_6422 = torch.constant.int 4
    %int32_6423 = torch.constant.int 32
    %int64_6424 = torch.constant.int 64
    %4112 = torch.prim.ListConstruct %439, %int4_6422, %int32_6423, %int64_6424 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4113 = torch.aten.view %4111, %4112 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %4113, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %4114 = torch.prim.ListConstruct %4104 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_6425 = torch.constant.bool false
    %4115 = torch.aten.index_put %4113, %4114, %4109, %false_6425 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %4115, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_6426 = torch.constant.int 22
    %int2_6427 = torch.constant.int 2
    %int4_6428 = torch.constant.int 4
    %int32_6429 = torch.constant.int 32
    %int64_6430 = torch.constant.int 64
    %4116 = torch.prim.ListConstruct %272, %int22_6426, %int2_6427, %int4_6428, %int32_6429, %int64_6430 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4117 = torch.aten.view %4115, %4116 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4117, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_6431 = torch.constant.int 360448
    %4118 = torch.prim.ListConstruct %272, %int360448_6431 : (!torch.int, !torch.int) -> !torch.list<int>
    %4119 = torch.aten.view %4117, %4118 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %4119, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_6432 = torch.constant.int 22
    %int2_6433 = torch.constant.int 2
    %int4_6434 = torch.constant.int 4
    %int32_6435 = torch.constant.int 32
    %int64_6436 = torch.constant.int 64
    %4120 = torch.prim.ListConstruct %272, %int22_6432, %int2_6433, %int4_6434, %int32_6435, %int64_6436 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4121 = torch.aten.view %4119, %4120 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4121, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_6437 = torch.constant.int 4
    %int32_6438 = torch.constant.int 32
    %int64_6439 = torch.constant.int 64
    %4122 = torch.prim.ListConstruct %439, %int4_6437, %int32_6438, %int64_6439 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4123 = torch.aten.view %4121, %4122 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %4123, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_6440 = torch.constant.int 22
    %4124 = torch.aten.mul.Scalar %arg2, %int22_6440 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4124, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int12_6441 = torch.constant.int 12
    %int1_6442 = torch.constant.int 1
    %4125 = torch.aten.add.Scalar %4124, %int12_6441, %int1_6442 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4125, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_6443 = torch.constant.int 2
    %4126 = torch.aten.mul.Scalar %4125, %int2_6443 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4126, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_6444 = torch.constant.int 1
    %int1_6445 = torch.constant.int 1
    %4127 = torch.aten.add.Scalar %4126, %int1_6444, %int1_6445 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4127, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %4128 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %4129 = torch.aten.view %4127, %4128 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4129, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_6446 = torch.constant.int 4
    %int32_6447 = torch.constant.int 32
    %int4_6448 = torch.constant.int 4
    %int64_6449 = torch.constant.int 64
    %4130 = torch.prim.ListConstruct %int4_6446, %271, %int32_6447, %int4_6448, %int64_6449 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4131 = torch.aten.view %4020, %4130 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %4131, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_6450 = torch.constant.int 32
    %int4_6451 = torch.constant.int 4
    %int64_6452 = torch.constant.int 64
    %4132 = torch.prim.ListConstruct %446, %int32_6450, %int4_6451, %int64_6452 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4133 = torch.aten.view %4131, %4132 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %4133, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_6453 = torch.constant.int 1
    %int2_6454 = torch.constant.int 2
    %4134 = torch.aten.transpose.int %4133, %int1_6453, %int2_6454 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %4134, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_6455 = torch.constant.none
    %none_6456 = torch.constant.none
    %int5_6457 = torch.constant.int 5
    %cpu_6458 = torch.constant.device "cpu"
    %int0_6459 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4134, %none_6455, %none_6456, %int5_6457, %cpu_6458, %int0_6459 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %4135 = torch.prim.ListConstruct %4129 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_6460 = torch.constant.bool false
    %4136 = torch.aten.index_put %4123, %4135, %4134, %false_6460 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %4136, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_6461 = torch.constant.int 22
    %int2_6462 = torch.constant.int 2
    %int4_6463 = torch.constant.int 4
    %int32_6464 = torch.constant.int 32
    %int64_6465 = torch.constant.int 64
    %4137 = torch.prim.ListConstruct %272, %int22_6461, %int2_6462, %int4_6463, %int32_6464, %int64_6465 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4138 = torch.aten.view %4136, %4137 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4138, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_6466 = torch.constant.int 360448
    %4139 = torch.prim.ListConstruct %272, %int360448_6466 : (!torch.int, !torch.int) -> !torch.list<int>
    %4140 = torch.aten.view %4138, %4139 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %4140, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_6467 = torch.constant.int 0
    %int1_6468 = torch.constant.int 1
    %none_6469 = torch.constant.none
    %none_6470 = torch.constant.none
    %cpu_6471 = torch.constant.device "cpu"
    %false_6472 = torch.constant.bool false
    %4141 = torch.aten.arange.start_step %int0_6467, %273, %int1_6468, %none_6469, %none_6470, %cpu_6471, %false_6472 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4141, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_6473 = torch.constant.int -1
    %4142 = torch.aten.unsqueeze %arg1, %int-1_6473 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %4143 = torch.aten.ge.Tensor %4141, %4142 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %4143, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_6474 = torch.constant.none
    %none_6475 = torch.constant.none
    %cpu_6476 = torch.constant.device "cpu"
    %false_6477 = torch.constant.bool false
    %4144 = torch.aten.arange %273, %none_6474, %none_6475, %cpu_6476, %false_6477 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4144, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_6478 = torch.constant.int 0
    %4145 = torch.aten.unsqueeze %4144, %int0_6478 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %4145, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_6479 = torch.constant.int 1
    %4146 = torch.aten.unsqueeze %4145, %int1_6479 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %4146, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_6480 = torch.constant.int 2
    %4147 = torch.aten.unsqueeze %4146, %int2_6480 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %4147, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_6481 = torch.constant.none
    %none_6482 = torch.constant.none
    %cpu_6483 = torch.constant.device "cpu"
    %false_6484 = torch.constant.bool false
    %4148 = torch.aten.arange %273, %none_6481, %none_6482, %cpu_6483, %false_6484 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4148, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_6485 = torch.constant.int 0
    %4149 = torch.aten.unsqueeze %4148, %int0_6485 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %4149, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_6486 = torch.constant.int 1
    %4150 = torch.aten.unsqueeze %4149, %int1_6486 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %4150, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_6487 = torch.constant.int 3
    %4151 = torch.aten.unsqueeze %4150, %int3_6487 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %4151, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %4152 = torch.aten.gt.Tensor %4147, %4151 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %4152, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_6488 = torch.constant.int 1
    %4153 = torch.aten.unsqueeze %4143, %int1_6488 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %4153, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_6489 = torch.constant.int 2
    %4154 = torch.aten.unsqueeze %4153, %int2_6489 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %4154, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %4155 = torch.aten.logical_or %4152, %4154 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %4155, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_6490 = torch.constant.none
    %4156 = torch.aten.clone %147, %none_6490 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_6491 = torch.constant.int 0
    %4157 = torch.aten.where.ScalarOther %4155, %4156, %int0_6491 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %4157, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_6492 = torch.constant.none
    %none_6493 = torch.constant.none
    %int5_6494 = torch.constant.int 5
    %cpu_6495 = torch.constant.device "cpu"
    %int0_6496 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4157, %none_6492, %none_6493, %int5_6494, %cpu_6495, %int0_6496 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_6497 = torch.constant.int -2
    %4158 = torch.aten.unsqueeze %4098, %int-2_6497 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %4158, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_6498 = torch.constant.int 4
    %int4_6499 = torch.constant.int 4
    %int8_6500 = torch.constant.int 8
    %int64_6501 = torch.constant.int 64
    %4159 = torch.prim.ListConstruct %int4_6498, %273, %int4_6499, %int8_6500, %int64_6501 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6502 = torch.constant.bool false
    %4160 = torch.aten.expand %4158, %4159, %false_6502 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4160, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_6503 = torch.constant.int 0
    %4161 = torch.aten.clone %4160, %int0_6503 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4161, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_6504 = torch.constant.int 4
    %int32_6505 = torch.constant.int 32
    %int64_6506 = torch.constant.int 64
    %4162 = torch.prim.ListConstruct %int4_6504, %273, %int32_6505, %int64_6506 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4163 = torch.aten._unsafe_view %4161, %4162 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4163, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_6507 = torch.constant.int -2
    %4164 = torch.aten.unsqueeze %4020, %int-2_6507 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %4164, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_6508 = torch.constant.int 4
    %int4_6509 = torch.constant.int 4
    %int8_6510 = torch.constant.int 8
    %int64_6511 = torch.constant.int 64
    %4165 = torch.prim.ListConstruct %int4_6508, %273, %int4_6509, %int8_6510, %int64_6511 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6512 = torch.constant.bool false
    %4166 = torch.aten.expand %4164, %4165, %false_6512 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4166, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_6513 = torch.constant.int 0
    %4167 = torch.aten.clone %4166, %int0_6513 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4167, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_6514 = torch.constant.int 4
    %int32_6515 = torch.constant.int 32
    %int64_6516 = torch.constant.int 64
    %4168 = torch.prim.ListConstruct %int4_6514, %273, %int32_6515, %int64_6516 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4169 = torch.aten._unsafe_view %4167, %4168 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4169, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_6517 = torch.constant.int 1
    %int2_6518 = torch.constant.int 2
    %4170 = torch.aten.transpose.int %4059, %int1_6517, %int2_6518 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %4170, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_6519 = torch.constant.int 1
    %int2_6520 = torch.constant.int 2
    %4171 = torch.aten.transpose.int %4163, %int1_6519, %int2_6520 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %4171, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_6521 = torch.constant.int 1
    %int2_6522 = torch.constant.int 2
    %4172 = torch.aten.transpose.int %4169, %int1_6521, %int2_6522 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %4172, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_6523 = torch.constant.float 0.000000e+00
    %false_6524 = torch.constant.bool false
    %none_6525 = torch.constant.none
    %false_6526 = torch.constant.bool false
    %4173 = torch.aten.scaled_dot_product_attention %4170, %4171, %4172, %4157, %float0.000000e00_6523, %false_6524, %none_6525, %false_6526 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %4173, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_6527 = torch.constant.int 1
    %int2_6528 = torch.constant.int 2
    %4174 = torch.aten.transpose.int %4173, %int1_6527, %int2_6528 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4174, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_6529 = torch.constant.int 4
    %int2048_6530 = torch.constant.int 2048
    %4175 = torch.prim.ListConstruct %int4_6529, %273, %int2048_6530 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4176 = torch.aten.view %4174, %4175 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4176, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_6531 = torch.constant.int 2
    %4177 = torch.aten.view.dtype %152, %int2_6531 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %4178 = torch.aten.detach %4177 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_6532 = torch.constant.int -1
    %int17_6533 = torch.constant.int 17
    %4179 = torch.prim.ListConstruct %int-1_6532, %int17_6533 : (!torch.int, !torch.int) -> !torch.list<int>
    %4180 = torch.aten.view %4178, %4179 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_6534 = torch.constant.int 2048
    %int-1_6535 = torch.constant.int -1
    %int17_6536 = torch.constant.int 17
    %4181 = torch.prim.ListConstruct %int2048_6534, %int-1_6535, %int17_6536 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4182 = torch.aten.view %4180, %4181 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_6537 = torch.constant.int 2
    %int0_6538 = torch.constant.int 0
    %int1_6539 = torch.constant.int 1
    %int1_6540 = torch.constant.int 1
    %4183 = torch.aten.slice.Tensor %4182, %int2_6537, %int0_6538, %int1_6539, %int1_6540 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_6541 = torch.constant.int 5
    %4184 = torch.aten.view.dtype %4183, %int5_6541 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %4185 = torch.aten.detach %4184 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_6542 = torch.constant.int 2
    %int1_6543 = torch.constant.int 1
    %int9223372036854775807_6544 = torch.constant.int 9223372036854775807
    %int1_6545 = torch.constant.int 1
    %4186 = torch.aten.slice.Tensor %4182, %int2_6542, %int1_6543, %int9223372036854775807_6544, %int1_6545 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_6546 = torch.constant.int 1
    %4187 = torch.aten.view.dtype %4186, %int1_6546 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %4188 = torch.aten.detach %4187 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %4189 = torch_c.to_builtin_tensor %4176 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_6547 = tensor.cast %4189 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4190 = torch_c.to_builtin_tensor %4185 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %4191 = torch_c.to_builtin_tensor %4188 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %4192 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_6547, %4190, %4191) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_6548 = tensor.cast %4192 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %4193 = torch_c.from_builtin_tensor %cast_6548 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4193, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_6549 = torch.constant.none
    %none_6550 = torch.constant.none
    %int5_6551 = torch.constant.int 5
    %cpu_6552 = torch.constant.device "cpu"
    %int0_6553 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4193, %none_6549, %none_6550, %int5_6551, %cpu_6552, %int0_6553 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_6554 = torch.constant.int 1
    %4194 = torch.aten.add.Tensor %3953, %4193, %int1_6554 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4194, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_6555 = torch.constant.none
    %none_6556 = torch.constant.none
    %int5_6557 = torch.constant.int 5
    %cpu_6558 = torch.constant.device "cpu"
    %int0_6559 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4194, %none_6555, %none_6556, %int5_6557, %cpu_6558, %int0_6559 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6560 = torch.constant.int 6
    %4195 = torch.prims.convert_element_type %4194, %int6_6560 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4195, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_6561 = torch.constant.int 2
    %4196 = torch.aten.pow.Tensor_Scalar %4195, %int2_6561 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4196, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_6562 = torch.constant.int -1
    %4197 = torch.prim.ListConstruct %int-1_6562 : (!torch.int) -> !torch.list<int>
    %true_6563 = torch.constant.bool true
    %none_6564 = torch.constant.none
    %4198 = torch.aten.mean.dim %4196, %4197, %true_6563, %none_6564 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4198, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_6565 = torch.constant.float 9.9999997473787516E-6
    %int1_6566 = torch.constant.int 1
    %4199 = torch.aten.add.Scalar %4198, %float9.999990e-06_6565, %int1_6566 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4199, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4200 = torch.aten.rsqrt %4199 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4200, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4201 = torch.aten.mul.Tensor %4195, %4200 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4201, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_6567 = torch.constant.none
    %none_6568 = torch.constant.none
    %int6_6569 = torch.constant.int 6
    %cpu_6570 = torch.constant.device "cpu"
    %int0_6571 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4201, %none_6567, %none_6568, %int6_6569, %cpu_6570, %int0_6571 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6572 = torch.constant.int 5
    %4202 = torch.prims.convert_element_type %4201, %int5_6572 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4202, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %4203 = torch.aten.mul.Tensor %153, %4202 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4203, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_6573 = torch.constant.none
    %none_6574 = torch.constant.none
    %int6_6575 = torch.constant.int 6
    %cpu_6576 = torch.constant.device "cpu"
    %int0_6577 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4203, %none_6573, %none_6574, %int6_6575, %cpu_6576, %int0_6577 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6578 = torch.constant.int 5
    %4204 = torch.prims.convert_element_type %4203, %int5_6578 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4204, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_6579 = torch.constant.int 2
    %4205 = torch.aten.view.dtype %154, %int2_6579 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %4206 = torch.aten.detach %4205 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_6580 = torch.constant.int -1
    %int17_6581 = torch.constant.int 17
    %4207 = torch.prim.ListConstruct %int-1_6580, %int17_6581 : (!torch.int, !torch.int) -> !torch.list<int>
    %4208 = torch.aten.view %4206, %4207 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_6582 = torch.constant.int 5632
    %int-1_6583 = torch.constant.int -1
    %int17_6584 = torch.constant.int 17
    %4209 = torch.prim.ListConstruct %int5632_6582, %int-1_6583, %int17_6584 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4210 = torch.aten.view %4208, %4209 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_6585 = torch.constant.int 2
    %int0_6586 = torch.constant.int 0
    %int1_6587 = torch.constant.int 1
    %int1_6588 = torch.constant.int 1
    %4211 = torch.aten.slice.Tensor %4210, %int2_6585, %int0_6586, %int1_6587, %int1_6588 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_6589 = torch.constant.int 5
    %4212 = torch.aten.view.dtype %4211, %int5_6589 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %4213 = torch.aten.detach %4212 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_6590 = torch.constant.int 2
    %int1_6591 = torch.constant.int 1
    %int9223372036854775807_6592 = torch.constant.int 9223372036854775807
    %int1_6593 = torch.constant.int 1
    %4214 = torch.aten.slice.Tensor %4210, %int2_6590, %int1_6591, %int9223372036854775807_6592, %int1_6593 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_6594 = torch.constant.int 1
    %4215 = torch.aten.view.dtype %4214, %int1_6594 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %4216 = torch.aten.detach %4215 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %4217 = torch_c.to_builtin_tensor %4204 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_6595 = tensor.cast %4217 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4218 = torch_c.to_builtin_tensor %4213 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %4219 = torch_c.to_builtin_tensor %4216 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %4220 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_6595, %4218, %4219) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_6596 = tensor.cast %4220 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %4221 = torch_c.from_builtin_tensor %cast_6596 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %4221, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %4222 = torch.aten.silu %4221 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %4222, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_6597 = torch.constant.int 2
    %4223 = torch.aten.view.dtype %155, %int2_6597 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %4224 = torch.aten.detach %4223 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_6598 = torch.constant.int -1
    %int17_6599 = torch.constant.int 17
    %4225 = torch.prim.ListConstruct %int-1_6598, %int17_6599 : (!torch.int, !torch.int) -> !torch.list<int>
    %4226 = torch.aten.view %4224, %4225 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_6600 = torch.constant.int 5632
    %int-1_6601 = torch.constant.int -1
    %int17_6602 = torch.constant.int 17
    %4227 = torch.prim.ListConstruct %int5632_6600, %int-1_6601, %int17_6602 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4228 = torch.aten.view %4226, %4227 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_6603 = torch.constant.int 2
    %int0_6604 = torch.constant.int 0
    %int1_6605 = torch.constant.int 1
    %int1_6606 = torch.constant.int 1
    %4229 = torch.aten.slice.Tensor %4228, %int2_6603, %int0_6604, %int1_6605, %int1_6606 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_6607 = torch.constant.int 5
    %4230 = torch.aten.view.dtype %4229, %int5_6607 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %4231 = torch.aten.detach %4230 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_6608 = torch.constant.int 2
    %int1_6609 = torch.constant.int 1
    %int9223372036854775807_6610 = torch.constant.int 9223372036854775807
    %int1_6611 = torch.constant.int 1
    %4232 = torch.aten.slice.Tensor %4228, %int2_6608, %int1_6609, %int9223372036854775807_6610, %int1_6611 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_6612 = torch.constant.int 1
    %4233 = torch.aten.view.dtype %4232, %int1_6612 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %4234 = torch.aten.detach %4233 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %4235 = torch_c.to_builtin_tensor %4204 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_6613 = tensor.cast %4235 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4236 = torch_c.to_builtin_tensor %4231 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %4237 = torch_c.to_builtin_tensor %4234 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %4238 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_6613, %4236, %4237) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_6614 = tensor.cast %4238 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %4239 = torch_c.from_builtin_tensor %cast_6614 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %4239, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %4240 = torch.aten.mul.Tensor %4222, %4239 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %4240, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_6615 = torch.constant.int 2
    %4241 = torch.aten.view.dtype %156, %int2_6615 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %4242 = torch.aten.detach %4241 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_6616 = torch.constant.int -1
    %int17_6617 = torch.constant.int 17
    %4243 = torch.prim.ListConstruct %int-1_6616, %int17_6617 : (!torch.int, !torch.int) -> !torch.list<int>
    %4244 = torch.aten.view %4242, %4243 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_6618 = torch.constant.int 2048
    %int-1_6619 = torch.constant.int -1
    %int17_6620 = torch.constant.int 17
    %4245 = torch.prim.ListConstruct %int2048_6618, %int-1_6619, %int17_6620 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4246 = torch.aten.view %4244, %4245 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_6621 = torch.constant.int 2
    %int0_6622 = torch.constant.int 0
    %int1_6623 = torch.constant.int 1
    %int1_6624 = torch.constant.int 1
    %4247 = torch.aten.slice.Tensor %4246, %int2_6621, %int0_6622, %int1_6623, %int1_6624 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_6625 = torch.constant.int 5
    %4248 = torch.aten.view.dtype %4247, %int5_6625 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %4249 = torch.aten.detach %4248 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_6626 = torch.constant.int 2
    %int1_6627 = torch.constant.int 1
    %int9223372036854775807_6628 = torch.constant.int 9223372036854775807
    %int1_6629 = torch.constant.int 1
    %4250 = torch.aten.slice.Tensor %4246, %int2_6626, %int1_6627, %int9223372036854775807_6628, %int1_6629 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_6630 = torch.constant.int 1
    %4251 = torch.aten.view.dtype %4250, %int1_6630 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %4252 = torch.aten.detach %4251 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %4253 = torch_c.to_builtin_tensor %4240 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_6631 = tensor.cast %4253 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %4254 = torch_c.to_builtin_tensor %4249 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %4255 = torch_c.to_builtin_tensor %4252 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %4256 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_6631, %4254, %4255) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_6632 = tensor.cast %4256 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %4257 = torch_c.from_builtin_tensor %cast_6632 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4257, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_6633 = torch.constant.int 1
    %4258 = torch.aten.add.Tensor %4194, %4257, %int1_6633 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4258, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_6634 = torch.constant.none
    %none_6635 = torch.constant.none
    %int5_6636 = torch.constant.int 5
    %cpu_6637 = torch.constant.device "cpu"
    %int0_6638 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4258, %none_6634, %none_6635, %int5_6636, %cpu_6637, %int0_6638 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6639 = torch.constant.int 6
    %4259 = torch.prims.convert_element_type %4258, %int6_6639 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4259, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_6640 = torch.constant.int 2
    %4260 = torch.aten.pow.Tensor_Scalar %4259, %int2_6640 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4260, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_6641 = torch.constant.int -1
    %4261 = torch.prim.ListConstruct %int-1_6641 : (!torch.int) -> !torch.list<int>
    %true_6642 = torch.constant.bool true
    %none_6643 = torch.constant.none
    %4262 = torch.aten.mean.dim %4260, %4261, %true_6642, %none_6643 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4262, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_6644 = torch.constant.float 9.9999997473787516E-6
    %int1_6645 = torch.constant.int 1
    %4263 = torch.aten.add.Scalar %4262, %float9.999990e-06_6644, %int1_6645 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4263, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4264 = torch.aten.rsqrt %4263 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4264, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4265 = torch.aten.mul.Tensor %4259, %4264 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4265, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_6646 = torch.constant.none
    %none_6647 = torch.constant.none
    %int6_6648 = torch.constant.int 6
    %cpu_6649 = torch.constant.device "cpu"
    %int0_6650 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4265, %none_6646, %none_6647, %int6_6648, %cpu_6649, %int0_6650 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6651 = torch.constant.int 5
    %4266 = torch.prims.convert_element_type %4265, %int5_6651 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4266, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %4267 = torch.aten.mul.Tensor %160, %4266 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4267, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_6652 = torch.constant.none
    %none_6653 = torch.constant.none
    %int6_6654 = torch.constant.int 6
    %cpu_6655 = torch.constant.device "cpu"
    %int0_6656 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4267, %none_6652, %none_6653, %int6_6654, %cpu_6655, %int0_6656 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6657 = torch.constant.int 5
    %4268 = torch.prims.convert_element_type %4267, %int5_6657 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4268, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_6658 = torch.constant.int 2
    %4269 = torch.aten.view.dtype %161, %int2_6658 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %4270 = torch.aten.detach %4269 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_6659 = torch.constant.int -1
    %int17_6660 = torch.constant.int 17
    %4271 = torch.prim.ListConstruct %int-1_6659, %int17_6660 : (!torch.int, !torch.int) -> !torch.list<int>
    %4272 = torch.aten.view %4270, %4271 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_6661 = torch.constant.int 2048
    %int-1_6662 = torch.constant.int -1
    %int17_6663 = torch.constant.int 17
    %4273 = torch.prim.ListConstruct %int2048_6661, %int-1_6662, %int17_6663 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4274 = torch.aten.view %4272, %4273 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_6664 = torch.constant.int 2
    %int0_6665 = torch.constant.int 0
    %int1_6666 = torch.constant.int 1
    %int1_6667 = torch.constant.int 1
    %4275 = torch.aten.slice.Tensor %4274, %int2_6664, %int0_6665, %int1_6666, %int1_6667 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_6668 = torch.constant.int 5
    %4276 = torch.aten.view.dtype %4275, %int5_6668 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %4277 = torch.aten.detach %4276 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_6669 = torch.constant.int 2
    %int1_6670 = torch.constant.int 1
    %int9223372036854775807_6671 = torch.constant.int 9223372036854775807
    %int1_6672 = torch.constant.int 1
    %4278 = torch.aten.slice.Tensor %4274, %int2_6669, %int1_6670, %int9223372036854775807_6671, %int1_6672 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_6673 = torch.constant.int 1
    %4279 = torch.aten.view.dtype %4278, %int1_6673 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %4280 = torch.aten.detach %4279 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %4281 = torch_c.to_builtin_tensor %4268 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_6674 = tensor.cast %4281 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4282 = torch_c.to_builtin_tensor %4277 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %4283 = torch_c.to_builtin_tensor %4280 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %4284 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_6674, %4282, %4283) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_6675 = tensor.cast %4284 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %4285 = torch_c.from_builtin_tensor %cast_6675 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4285, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_6676 = torch.constant.int 2
    %4286 = torch.aten.view.dtype %162, %int2_6676 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %4287 = torch.aten.detach %4286 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_6677 = torch.constant.int -1
    %int17_6678 = torch.constant.int 17
    %4288 = torch.prim.ListConstruct %int-1_6677, %int17_6678 : (!torch.int, !torch.int) -> !torch.list<int>
    %4289 = torch.aten.view %4287, %4288 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_6679 = torch.constant.int 256
    %int-1_6680 = torch.constant.int -1
    %int17_6681 = torch.constant.int 17
    %4290 = torch.prim.ListConstruct %int256_6679, %int-1_6680, %int17_6681 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4291 = torch.aten.view %4289, %4290 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_6682 = torch.constant.int 2
    %int0_6683 = torch.constant.int 0
    %int1_6684 = torch.constant.int 1
    %int1_6685 = torch.constant.int 1
    %4292 = torch.aten.slice.Tensor %4291, %int2_6682, %int0_6683, %int1_6684, %int1_6685 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_6686 = torch.constant.int 5
    %4293 = torch.aten.view.dtype %4292, %int5_6686 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %4294 = torch.aten.detach %4293 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_6687 = torch.constant.int 2
    %int1_6688 = torch.constant.int 1
    %int9223372036854775807_6689 = torch.constant.int 9223372036854775807
    %int1_6690 = torch.constant.int 1
    %4295 = torch.aten.slice.Tensor %4291, %int2_6687, %int1_6688, %int9223372036854775807_6689, %int1_6690 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_6691 = torch.constant.int 1
    %4296 = torch.aten.view.dtype %4295, %int1_6691 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %4297 = torch.aten.detach %4296 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %4298 = torch_c.to_builtin_tensor %4268 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_6692 = tensor.cast %4298 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4299 = torch_c.to_builtin_tensor %4294 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %4300 = torch_c.to_builtin_tensor %4297 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %4301 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_6692, %4299, %4300) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_6693 = tensor.cast %4301 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %4302 = torch_c.from_builtin_tensor %cast_6693 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %4302, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_6694 = torch.constant.int 2
    %4303 = torch.aten.view.dtype %163, %int2_6694 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %4304 = torch.aten.detach %4303 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_6695 = torch.constant.int -1
    %int17_6696 = torch.constant.int 17
    %4305 = torch.prim.ListConstruct %int-1_6695, %int17_6696 : (!torch.int, !torch.int) -> !torch.list<int>
    %4306 = torch.aten.view %4304, %4305 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_6697 = torch.constant.int 256
    %int-1_6698 = torch.constant.int -1
    %int17_6699 = torch.constant.int 17
    %4307 = torch.prim.ListConstruct %int256_6697, %int-1_6698, %int17_6699 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4308 = torch.aten.view %4306, %4307 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_6700 = torch.constant.int 2
    %int0_6701 = torch.constant.int 0
    %int1_6702 = torch.constant.int 1
    %int1_6703 = torch.constant.int 1
    %4309 = torch.aten.slice.Tensor %4308, %int2_6700, %int0_6701, %int1_6702, %int1_6703 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_6704 = torch.constant.int 5
    %4310 = torch.aten.view.dtype %4309, %int5_6704 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %4311 = torch.aten.detach %4310 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_6705 = torch.constant.int 2
    %int1_6706 = torch.constant.int 1
    %int9223372036854775807_6707 = torch.constant.int 9223372036854775807
    %int1_6708 = torch.constant.int 1
    %4312 = torch.aten.slice.Tensor %4308, %int2_6705, %int1_6706, %int9223372036854775807_6707, %int1_6708 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_6709 = torch.constant.int 1
    %4313 = torch.aten.view.dtype %4312, %int1_6709 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %4314 = torch.aten.detach %4313 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %4315 = torch_c.to_builtin_tensor %4268 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_6710 = tensor.cast %4315 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4316 = torch_c.to_builtin_tensor %4311 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %4317 = torch_c.to_builtin_tensor %4314 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %4318 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_6710, %4316, %4317) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_6711 = tensor.cast %4318 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %4319 = torch_c.from_builtin_tensor %cast_6711 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %4319, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_6712 = torch.constant.int 4
    %int32_6713 = torch.constant.int 32
    %int64_6714 = torch.constant.int 64
    %4320 = torch.prim.ListConstruct %int4_6712, %273, %int32_6713, %int64_6714 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4321 = torch.aten.view %4285, %4320 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4321, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_6715 = torch.constant.int 4
    %int4_6716 = torch.constant.int 4
    %int64_6717 = torch.constant.int 64
    %4322 = torch.prim.ListConstruct %int4_6715, %273, %int4_6716, %int64_6717 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4323 = torch.aten.view %4302, %4322 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %4323, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_6718 = torch.constant.int 4
    %int4_6719 = torch.constant.int 4
    %int64_6720 = torch.constant.int 64
    %4324 = torch.prim.ListConstruct %int4_6718, %273, %int4_6719, %int64_6720 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4325 = torch.aten.view %4319, %4324 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %4325, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_6721 = torch.constant.int 0
    %none_6722 = torch.constant.none
    %none_6723 = torch.constant.none
    %cpu_6724 = torch.constant.device "cpu"
    %false_6725 = torch.constant.bool false
    %4326 = torch.aten.arange.start %int0_6721, %273, %none_6722, %none_6723, %cpu_6724, %false_6725 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4326, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_6726 = torch.constant.int 0
    %4327 = torch.aten.unsqueeze %4326, %int0_6726 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %4327, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_6727 = torch.constant.int 0
    %int64_6728 = torch.constant.int 64
    %int2_6729 = torch.constant.int 2
    %none_6730 = torch.constant.none
    %none_6731 = torch.constant.none
    %cpu_6732 = torch.constant.device "cpu"
    %false_6733 = torch.constant.bool false
    %4328 = torch.aten.arange.start_step %int0_6727, %int64_6728, %int2_6729, %none_6730, %none_6731, %cpu_6732, %false_6733 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_6734 = torch.constant.none
    %none_6735 = torch.constant.none
    %int4_6736 = torch.constant.int 4
    %cpu_6737 = torch.constant.device "cpu"
    %int0_6738 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4328, %none_6734, %none_6735, %int4_6736, %cpu_6737, %int0_6738 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6739 = torch.constant.int 6
    %4329 = torch.prims.convert_element_type %4328, %int6_6739 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_6740 = torch.constant.int 64
    %4330 = torch.aten.div.Scalar %4329, %int64_6740 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_6741 = torch.constant.float 1.000000e+04
    %4331 = torch.aten.pow.Scalar %float1.000000e04_6741, %4330 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %4332 = torch.aten.reciprocal %4331 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_6742 = torch.constant.float 1.000000e+00
    %4333 = torch.aten.mul.Scalar %4332, %float1.000000e00_6742 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_6743 = torch.constant.none
    %4334 = torch.aten.clone %157, %none_6743 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_6744 = torch.constant.int 0
    %4335 = torch.aten.unsqueeze %4333, %int0_6744 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_6745 = torch.constant.int 2
    %4336 = torch.aten.unsqueeze %4335, %int2_6745 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_6746 = torch.constant.none
    %none_6747 = torch.constant.none
    %int6_6748 = torch.constant.int 6
    %cpu_6749 = torch.constant.device "cpu"
    %int0_6750 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4336, %none_6746, %none_6747, %int6_6748, %cpu_6749, %int0_6750 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_6751 = torch.constant.int 1
    %int-1_6752 = torch.constant.int -1
    %int1_6753 = torch.constant.int 1
    %4337 = torch.prim.ListConstruct %int1_6751, %int-1_6752, %int1_6753 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6754 = torch.constant.bool false
    %4338 = torch.aten.expand %4336, %4337, %false_6754 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_6755 = torch.constant.int 1
    %4339 = torch.aten.unsqueeze %4327, %int1_6755 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %4339, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_6756 = torch.constant.none
    %none_6757 = torch.constant.none
    %int4_6758 = torch.constant.int 4
    %cpu_6759 = torch.constant.device "cpu"
    %int0_6760 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4339, %none_6756, %none_6757, %int4_6758, %cpu_6759, %int0_6760 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6761 = torch.constant.int 6
    %4340 = torch.prims.convert_element_type %4339, %int6_6761 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %4340, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %4341 = torch.aten.matmul %4338, %4340 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %4341, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_6762 = torch.constant.int 1
    %int2_6763 = torch.constant.int 2
    %4342 = torch.aten.transpose.int %4341, %int1_6762, %int2_6763 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4342, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4343 = torch.aten.cos %4342 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4343, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4344 = torch.aten.mul.Tensor %4343, %4334 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4344, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_6764 = torch.constant.none
    %none_6765 = torch.constant.none
    %int6_6766 = torch.constant.int 6
    %cpu_6767 = torch.constant.device "cpu"
    %int0_6768 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4344, %none_6764, %none_6765, %int6_6766, %cpu_6767, %int0_6768 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6769 = torch.constant.int 5
    %4345 = torch.prims.convert_element_type %4344, %int5_6769 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %4345, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %4346 = torch.aten.sin %4342 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4346, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4347 = torch.aten.mul.Tensor %4346, %4334 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4347, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_6770 = torch.constant.none
    %none_6771 = torch.constant.none
    %int6_6772 = torch.constant.int 6
    %cpu_6773 = torch.constant.device "cpu"
    %int0_6774 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4347, %none_6770, %none_6771, %int6_6772, %cpu_6773, %int0_6774 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6775 = torch.constant.int 5
    %4348 = torch.prims.convert_element_type %4347, %int5_6775 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %4348, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_6776 = torch.constant.int 2
    %4349 = torch.aten.unsqueeze %4345, %int2_6776 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %4349, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_6777 = torch.constant.int 2
    %4350 = torch.aten.unsqueeze %4348, %int2_6777 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %4350, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_6778 = torch.constant.none
    %none_6779 = torch.constant.none
    %int5_6780 = torch.constant.int 5
    %cpu_6781 = torch.constant.device "cpu"
    %int0_6782 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4349, %none_6778, %none_6779, %int5_6780, %cpu_6781, %int0_6782 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_6783 = torch.constant.none
    %none_6784 = torch.constant.none
    %int5_6785 = torch.constant.int 5
    %cpu_6786 = torch.constant.device "cpu"
    %int0_6787 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4350, %none_6783, %none_6784, %int5_6785, %cpu_6786, %int0_6787 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_6788 = torch.constant.none
    %none_6789 = torch.constant.none
    %int5_6790 = torch.constant.int 5
    %cpu_6791 = torch.constant.device "cpu"
    %int0_6792 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4321, %none_6788, %none_6789, %int5_6790, %cpu_6791, %int0_6792 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_6793 = torch.constant.int 3
    %int0_6794 = torch.constant.int 0
    %int64_6795 = torch.constant.int 64
    %int2_6796 = torch.constant.int 2
    %4351 = torch.aten.slice.Tensor %4321, %int3_6793, %int0_6794, %int64_6795, %int2_6796 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4351, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_6797 = torch.constant.int 3
    %int1_6798 = torch.constant.int 1
    %int64_6799 = torch.constant.int 64
    %int2_6800 = torch.constant.int 2
    %4352 = torch.aten.slice.Tensor %4321, %int3_6797, %int1_6798, %int64_6799, %int2_6800 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4352, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4353 = torch.aten.mul.Tensor %4351, %4349 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4353, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4354 = torch.aten.mul.Tensor %4352, %4350 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4354, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_6801 = torch.constant.int 1
    %4355 = torch.aten.sub.Tensor %4353, %4354, %int1_6801 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4355, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4356 = torch.aten.mul.Tensor %4352, %4349 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4356, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4357 = torch.aten.mul.Tensor %4351, %4350 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4357, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_6802 = torch.constant.int 1
    %4358 = torch.aten.add.Tensor %4356, %4357, %int1_6802 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4358, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4359 = torch_c.to_builtin_tensor %4355 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_6803 = tensor.cast %4359 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %4360 = torch_c.to_builtin_tensor %4358 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_6804 = tensor.cast %4360 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %4361 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_6803, %cast_6804) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_6805 = tensor.cast %4361 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %4362 = torch_c.from_builtin_tensor %cast_6805 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %4362, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_6806 = torch.constant.int 4
    %int32_6807 = torch.constant.int 32
    %int64_6808 = torch.constant.int 64
    %4363 = torch.prim.ListConstruct %int4_6806, %273, %int32_6807, %int64_6808 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4364 = torch.aten.view %4362, %4363 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4364, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_6809 = torch.constant.none
    %none_6810 = torch.constant.none
    %int5_6811 = torch.constant.int 5
    %cpu_6812 = torch.constant.device "cpu"
    %int0_6813 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4364, %none_6809, %none_6810, %int5_6811, %cpu_6812, %int0_6813 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_6814 = torch.constant.int 0
    %none_6815 = torch.constant.none
    %none_6816 = torch.constant.none
    %cpu_6817 = torch.constant.device "cpu"
    %false_6818 = torch.constant.bool false
    %4365 = torch.aten.arange.start %int0_6814, %273, %none_6815, %none_6816, %cpu_6817, %false_6818 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4365, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_6819 = torch.constant.int 0
    %4366 = torch.aten.unsqueeze %4365, %int0_6819 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %4366, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_6820 = torch.constant.int 0
    %int64_6821 = torch.constant.int 64
    %int2_6822 = torch.constant.int 2
    %none_6823 = torch.constant.none
    %none_6824 = torch.constant.none
    %cpu_6825 = torch.constant.device "cpu"
    %false_6826 = torch.constant.bool false
    %4367 = torch.aten.arange.start_step %int0_6820, %int64_6821, %int2_6822, %none_6823, %none_6824, %cpu_6825, %false_6826 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_6827 = torch.constant.none
    %none_6828 = torch.constant.none
    %int4_6829 = torch.constant.int 4
    %cpu_6830 = torch.constant.device "cpu"
    %int0_6831 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4367, %none_6827, %none_6828, %int4_6829, %cpu_6830, %int0_6831 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6832 = torch.constant.int 6
    %4368 = torch.prims.convert_element_type %4367, %int6_6832 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_6833 = torch.constant.int 64
    %4369 = torch.aten.div.Scalar %4368, %int64_6833 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_6834 = torch.constant.float 1.000000e+04
    %4370 = torch.aten.pow.Scalar %float1.000000e04_6834, %4369 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %4371 = torch.aten.reciprocal %4370 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_6835 = torch.constant.float 1.000000e+00
    %4372 = torch.aten.mul.Scalar %4371, %float1.000000e00_6835 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_6836 = torch.constant.none
    %4373 = torch.aten.clone %158, %none_6836 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_6837 = torch.constant.int 0
    %4374 = torch.aten.unsqueeze %4372, %int0_6837 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_6838 = torch.constant.int 2
    %4375 = torch.aten.unsqueeze %4374, %int2_6838 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_6839 = torch.constant.none
    %none_6840 = torch.constant.none
    %int6_6841 = torch.constant.int 6
    %cpu_6842 = torch.constant.device "cpu"
    %int0_6843 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4375, %none_6839, %none_6840, %int6_6841, %cpu_6842, %int0_6843 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_6844 = torch.constant.int 1
    %int-1_6845 = torch.constant.int -1
    %int1_6846 = torch.constant.int 1
    %4376 = torch.prim.ListConstruct %int1_6844, %int-1_6845, %int1_6846 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6847 = torch.constant.bool false
    %4377 = torch.aten.expand %4375, %4376, %false_6847 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_6848 = torch.constant.int 1
    %4378 = torch.aten.unsqueeze %4366, %int1_6848 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %4378, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_6849 = torch.constant.none
    %none_6850 = torch.constant.none
    %int4_6851 = torch.constant.int 4
    %cpu_6852 = torch.constant.device "cpu"
    %int0_6853 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4378, %none_6849, %none_6850, %int4_6851, %cpu_6852, %int0_6853 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6854 = torch.constant.int 6
    %4379 = torch.prims.convert_element_type %4378, %int6_6854 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %4379, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %4380 = torch.aten.matmul %4377, %4379 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %4380, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_6855 = torch.constant.int 1
    %int2_6856 = torch.constant.int 2
    %4381 = torch.aten.transpose.int %4380, %int1_6855, %int2_6856 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4381, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4382 = torch.aten.cos %4381 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4382, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4383 = torch.aten.mul.Tensor %4382, %4373 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4383, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_6857 = torch.constant.none
    %none_6858 = torch.constant.none
    %int6_6859 = torch.constant.int 6
    %cpu_6860 = torch.constant.device "cpu"
    %int0_6861 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4383, %none_6857, %none_6858, %int6_6859, %cpu_6860, %int0_6861 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6862 = torch.constant.int 5
    %4384 = torch.prims.convert_element_type %4383, %int5_6862 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %4384, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %4385 = torch.aten.sin %4381 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4385, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4386 = torch.aten.mul.Tensor %4385, %4373 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4386, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_6863 = torch.constant.none
    %none_6864 = torch.constant.none
    %int6_6865 = torch.constant.int 6
    %cpu_6866 = torch.constant.device "cpu"
    %int0_6867 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4386, %none_6863, %none_6864, %int6_6865, %cpu_6866, %int0_6867 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6868 = torch.constant.int 5
    %4387 = torch.prims.convert_element_type %4386, %int5_6868 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %4387, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_6869 = torch.constant.int 2
    %4388 = torch.aten.unsqueeze %4384, %int2_6869 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %4388, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_6870 = torch.constant.int 2
    %4389 = torch.aten.unsqueeze %4387, %int2_6870 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %4389, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_6871 = torch.constant.none
    %none_6872 = torch.constant.none
    %int5_6873 = torch.constant.int 5
    %cpu_6874 = torch.constant.device "cpu"
    %int0_6875 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4388, %none_6871, %none_6872, %int5_6873, %cpu_6874, %int0_6875 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_6876 = torch.constant.none
    %none_6877 = torch.constant.none
    %int5_6878 = torch.constant.int 5
    %cpu_6879 = torch.constant.device "cpu"
    %int0_6880 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4389, %none_6876, %none_6877, %int5_6878, %cpu_6879, %int0_6880 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_6881 = torch.constant.none
    %none_6882 = torch.constant.none
    %int5_6883 = torch.constant.int 5
    %cpu_6884 = torch.constant.device "cpu"
    %int0_6885 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4323, %none_6881, %none_6882, %int5_6883, %cpu_6884, %int0_6885 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_6886 = torch.constant.int 3
    %int0_6887 = torch.constant.int 0
    %int64_6888 = torch.constant.int 64
    %int2_6889 = torch.constant.int 2
    %4390 = torch.aten.slice.Tensor %4323, %int3_6886, %int0_6887, %int64_6888, %int2_6889 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4390, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_6890 = torch.constant.int 3
    %int1_6891 = torch.constant.int 1
    %int64_6892 = torch.constant.int 64
    %int2_6893 = torch.constant.int 2
    %4391 = torch.aten.slice.Tensor %4323, %int3_6890, %int1_6891, %int64_6892, %int2_6893 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4391, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %4392 = torch.aten.mul.Tensor %4390, %4388 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4392, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %4393 = torch.aten.mul.Tensor %4391, %4389 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4393, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_6894 = torch.constant.int 1
    %4394 = torch.aten.sub.Tensor %4392, %4393, %int1_6894 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4394, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %4395 = torch.aten.mul.Tensor %4391, %4388 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4395, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %4396 = torch.aten.mul.Tensor %4390, %4389 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4396, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_6895 = torch.constant.int 1
    %4397 = torch.aten.add.Tensor %4395, %4396, %int1_6895 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4397, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %4398 = torch_c.to_builtin_tensor %4394 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_6896 = tensor.cast %4398 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %4399 = torch_c.to_builtin_tensor %4397 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_6897 = tensor.cast %4399 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %4400 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_6896, %cast_6897) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_6898 = tensor.cast %4400 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %4401 = torch_c.from_builtin_tensor %cast_6898 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %4401, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_6899 = torch.constant.int 4
    %int4_6900 = torch.constant.int 4
    %int64_6901 = torch.constant.int 64
    %4402 = torch.prim.ListConstruct %int4_6899, %273, %int4_6900, %int64_6901 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4403 = torch.aten.view %4401, %4402 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %4403, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_6902 = torch.constant.none
    %none_6903 = torch.constant.none
    %int5_6904 = torch.constant.int 5
    %cpu_6905 = torch.constant.device "cpu"
    %int0_6906 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4403, %none_6902, %none_6903, %int5_6904, %cpu_6905, %int0_6906 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_6907 = torch.constant.int 22
    %4404 = torch.aten.mul.Scalar %arg2, %int22_6907 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4404, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int13 = torch.constant.int 13
    %int1_6908 = torch.constant.int 1
    %4405 = torch.aten.add.Scalar %4404, %int13, %int1_6908 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4405, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_6909 = torch.constant.int 2
    %4406 = torch.aten.mul.Scalar %4405, %int2_6909 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4406, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_6910 = torch.constant.int 0
    %int1_6911 = torch.constant.int 1
    %4407 = torch.aten.add.Scalar %4406, %int0_6910, %int1_6911 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4407, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %4408 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %4409 = torch.aten.view %4407, %4408 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4409, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_6912 = torch.constant.int 4
    %int32_6913 = torch.constant.int 32
    %int4_6914 = torch.constant.int 4
    %int64_6915 = torch.constant.int 64
    %4410 = torch.prim.ListConstruct %int4_6912, %271, %int32_6913, %int4_6914, %int64_6915 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4411 = torch.aten.view %4403, %4410 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %4411, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_6916 = torch.constant.int 32
    %int4_6917 = torch.constant.int 4
    %int64_6918 = torch.constant.int 64
    %4412 = torch.prim.ListConstruct %446, %int32_6916, %int4_6917, %int64_6918 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4413 = torch.aten.view %4411, %4412 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %4413, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_6919 = torch.constant.int 1
    %int2_6920 = torch.constant.int 2
    %4414 = torch.aten.transpose.int %4413, %int1_6919, %int2_6920 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %4414, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_6921 = torch.constant.none
    %none_6922 = torch.constant.none
    %int5_6923 = torch.constant.int 5
    %cpu_6924 = torch.constant.device "cpu"
    %int0_6925 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4414, %none_6921, %none_6922, %int5_6923, %cpu_6924, %int0_6925 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_6926 = torch.constant.int 22
    %int2_6927 = torch.constant.int 2
    %int4_6928 = torch.constant.int 4
    %int32_6929 = torch.constant.int 32
    %int64_6930 = torch.constant.int 64
    %4415 = torch.prim.ListConstruct %272, %int22_6926, %int2_6927, %int4_6928, %int32_6929, %int64_6930 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4416 = torch.aten.view %4140, %4415 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4416, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_6931 = torch.constant.int 4
    %int32_6932 = torch.constant.int 32
    %int64_6933 = torch.constant.int 64
    %4417 = torch.prim.ListConstruct %439, %int4_6931, %int32_6932, %int64_6933 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4418 = torch.aten.view %4416, %4417 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %4418, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %4419 = torch.prim.ListConstruct %4409 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_6934 = torch.constant.bool false
    %4420 = torch.aten.index_put %4418, %4419, %4414, %false_6934 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %4420, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_6935 = torch.constant.int 22
    %int2_6936 = torch.constant.int 2
    %int4_6937 = torch.constant.int 4
    %int32_6938 = torch.constant.int 32
    %int64_6939 = torch.constant.int 64
    %4421 = torch.prim.ListConstruct %272, %int22_6935, %int2_6936, %int4_6937, %int32_6938, %int64_6939 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4422 = torch.aten.view %4420, %4421 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4422, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_6940 = torch.constant.int 360448
    %4423 = torch.prim.ListConstruct %272, %int360448_6940 : (!torch.int, !torch.int) -> !torch.list<int>
    %4424 = torch.aten.view %4422, %4423 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %4424, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_6941 = torch.constant.int 22
    %int2_6942 = torch.constant.int 2
    %int4_6943 = torch.constant.int 4
    %int32_6944 = torch.constant.int 32
    %int64_6945 = torch.constant.int 64
    %4425 = torch.prim.ListConstruct %272, %int22_6941, %int2_6942, %int4_6943, %int32_6944, %int64_6945 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4426 = torch.aten.view %4424, %4425 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4426, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_6946 = torch.constant.int 4
    %int32_6947 = torch.constant.int 32
    %int64_6948 = torch.constant.int 64
    %4427 = torch.prim.ListConstruct %439, %int4_6946, %int32_6947, %int64_6948 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4428 = torch.aten.view %4426, %4427 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %4428, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_6949 = torch.constant.int 22
    %4429 = torch.aten.mul.Scalar %arg2, %int22_6949 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4429, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int13_6950 = torch.constant.int 13
    %int1_6951 = torch.constant.int 1
    %4430 = torch.aten.add.Scalar %4429, %int13_6950, %int1_6951 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4430, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_6952 = torch.constant.int 2
    %4431 = torch.aten.mul.Scalar %4430, %int2_6952 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4431, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_6953 = torch.constant.int 1
    %int1_6954 = torch.constant.int 1
    %4432 = torch.aten.add.Scalar %4431, %int1_6953, %int1_6954 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4432, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %4433 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %4434 = torch.aten.view %4432, %4433 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4434, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_6955 = torch.constant.int 4
    %int32_6956 = torch.constant.int 32
    %int4_6957 = torch.constant.int 4
    %int64_6958 = torch.constant.int 64
    %4435 = torch.prim.ListConstruct %int4_6955, %271, %int32_6956, %int4_6957, %int64_6958 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4436 = torch.aten.view %4325, %4435 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %4436, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_6959 = torch.constant.int 32
    %int4_6960 = torch.constant.int 4
    %int64_6961 = torch.constant.int 64
    %4437 = torch.prim.ListConstruct %446, %int32_6959, %int4_6960, %int64_6961 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4438 = torch.aten.view %4436, %4437 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %4438, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_6962 = torch.constant.int 1
    %int2_6963 = torch.constant.int 2
    %4439 = torch.aten.transpose.int %4438, %int1_6962, %int2_6963 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %4439, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_6964 = torch.constant.none
    %none_6965 = torch.constant.none
    %int5_6966 = torch.constant.int 5
    %cpu_6967 = torch.constant.device "cpu"
    %int0_6968 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4439, %none_6964, %none_6965, %int5_6966, %cpu_6967, %int0_6968 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %4440 = torch.prim.ListConstruct %4434 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_6969 = torch.constant.bool false
    %4441 = torch.aten.index_put %4428, %4440, %4439, %false_6969 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %4441, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_6970 = torch.constant.int 22
    %int2_6971 = torch.constant.int 2
    %int4_6972 = torch.constant.int 4
    %int32_6973 = torch.constant.int 32
    %int64_6974 = torch.constant.int 64
    %4442 = torch.prim.ListConstruct %272, %int22_6970, %int2_6971, %int4_6972, %int32_6973, %int64_6974 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4443 = torch.aten.view %4441, %4442 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4443, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_6975 = torch.constant.int 360448
    %4444 = torch.prim.ListConstruct %272, %int360448_6975 : (!torch.int, !torch.int) -> !torch.list<int>
    %4445 = torch.aten.view %4443, %4444 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %4445, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_6976 = torch.constant.int 0
    %int1_6977 = torch.constant.int 1
    %none_6978 = torch.constant.none
    %none_6979 = torch.constant.none
    %cpu_6980 = torch.constant.device "cpu"
    %false_6981 = torch.constant.bool false
    %4446 = torch.aten.arange.start_step %int0_6976, %273, %int1_6977, %none_6978, %none_6979, %cpu_6980, %false_6981 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4446, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_6982 = torch.constant.int -1
    %4447 = torch.aten.unsqueeze %arg1, %int-1_6982 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %4448 = torch.aten.ge.Tensor %4446, %4447 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %4448, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_6983 = torch.constant.none
    %none_6984 = torch.constant.none
    %cpu_6985 = torch.constant.device "cpu"
    %false_6986 = torch.constant.bool false
    %4449 = torch.aten.arange %273, %none_6983, %none_6984, %cpu_6985, %false_6986 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4449, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_6987 = torch.constant.int 0
    %4450 = torch.aten.unsqueeze %4449, %int0_6987 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %4450, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_6988 = torch.constant.int 1
    %4451 = torch.aten.unsqueeze %4450, %int1_6988 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %4451, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_6989 = torch.constant.int 2
    %4452 = torch.aten.unsqueeze %4451, %int2_6989 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %4452, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_6990 = torch.constant.none
    %none_6991 = torch.constant.none
    %cpu_6992 = torch.constant.device "cpu"
    %false_6993 = torch.constant.bool false
    %4453 = torch.aten.arange %273, %none_6990, %none_6991, %cpu_6992, %false_6993 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4453, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_6994 = torch.constant.int 0
    %4454 = torch.aten.unsqueeze %4453, %int0_6994 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %4454, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_6995 = torch.constant.int 1
    %4455 = torch.aten.unsqueeze %4454, %int1_6995 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %4455, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_6996 = torch.constant.int 3
    %4456 = torch.aten.unsqueeze %4455, %int3_6996 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %4456, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %4457 = torch.aten.gt.Tensor %4452, %4456 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %4457, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_6997 = torch.constant.int 1
    %4458 = torch.aten.unsqueeze %4448, %int1_6997 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %4458, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_6998 = torch.constant.int 2
    %4459 = torch.aten.unsqueeze %4458, %int2_6998 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %4459, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %4460 = torch.aten.logical_or %4457, %4459 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %4460, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_6999 = torch.constant.none
    %4461 = torch.aten.clone %159, %none_6999 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_7000 = torch.constant.int 0
    %4462 = torch.aten.where.ScalarOther %4460, %4461, %int0_7000 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %4462, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_7001 = torch.constant.none
    %none_7002 = torch.constant.none
    %int5_7003 = torch.constant.int 5
    %cpu_7004 = torch.constant.device "cpu"
    %int0_7005 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4462, %none_7001, %none_7002, %int5_7003, %cpu_7004, %int0_7005 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_7006 = torch.constant.int -2
    %4463 = torch.aten.unsqueeze %4403, %int-2_7006 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %4463, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_7007 = torch.constant.int 4
    %int4_7008 = torch.constant.int 4
    %int8_7009 = torch.constant.int 8
    %int64_7010 = torch.constant.int 64
    %4464 = torch.prim.ListConstruct %int4_7007, %273, %int4_7008, %int8_7009, %int64_7010 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7011 = torch.constant.bool false
    %4465 = torch.aten.expand %4463, %4464, %false_7011 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4465, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_7012 = torch.constant.int 0
    %4466 = torch.aten.clone %4465, %int0_7012 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4466, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_7013 = torch.constant.int 4
    %int32_7014 = torch.constant.int 32
    %int64_7015 = torch.constant.int 64
    %4467 = torch.prim.ListConstruct %int4_7013, %273, %int32_7014, %int64_7015 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4468 = torch.aten._unsafe_view %4466, %4467 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4468, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_7016 = torch.constant.int -2
    %4469 = torch.aten.unsqueeze %4325, %int-2_7016 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %4469, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_7017 = torch.constant.int 4
    %int4_7018 = torch.constant.int 4
    %int8_7019 = torch.constant.int 8
    %int64_7020 = torch.constant.int 64
    %4470 = torch.prim.ListConstruct %int4_7017, %273, %int4_7018, %int8_7019, %int64_7020 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7021 = torch.constant.bool false
    %4471 = torch.aten.expand %4469, %4470, %false_7021 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4471, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_7022 = torch.constant.int 0
    %4472 = torch.aten.clone %4471, %int0_7022 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4472, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_7023 = torch.constant.int 4
    %int32_7024 = torch.constant.int 32
    %int64_7025 = torch.constant.int 64
    %4473 = torch.prim.ListConstruct %int4_7023, %273, %int32_7024, %int64_7025 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4474 = torch.aten._unsafe_view %4472, %4473 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4474, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_7026 = torch.constant.int 1
    %int2_7027 = torch.constant.int 2
    %4475 = torch.aten.transpose.int %4364, %int1_7026, %int2_7027 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %4475, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_7028 = torch.constant.int 1
    %int2_7029 = torch.constant.int 2
    %4476 = torch.aten.transpose.int %4468, %int1_7028, %int2_7029 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %4476, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_7030 = torch.constant.int 1
    %int2_7031 = torch.constant.int 2
    %4477 = torch.aten.transpose.int %4474, %int1_7030, %int2_7031 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %4477, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_7032 = torch.constant.float 0.000000e+00
    %false_7033 = torch.constant.bool false
    %none_7034 = torch.constant.none
    %false_7035 = torch.constant.bool false
    %4478 = torch.aten.scaled_dot_product_attention %4475, %4476, %4477, %4462, %float0.000000e00_7032, %false_7033, %none_7034, %false_7035 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %4478, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_7036 = torch.constant.int 1
    %int2_7037 = torch.constant.int 2
    %4479 = torch.aten.transpose.int %4478, %int1_7036, %int2_7037 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4479, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_7038 = torch.constant.int 4
    %int2048_7039 = torch.constant.int 2048
    %4480 = torch.prim.ListConstruct %int4_7038, %273, %int2048_7039 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4481 = torch.aten.view %4479, %4480 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4481, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_7040 = torch.constant.int 2
    %4482 = torch.aten.view.dtype %164, %int2_7040 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %4483 = torch.aten.detach %4482 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_7041 = torch.constant.int -1
    %int17_7042 = torch.constant.int 17
    %4484 = torch.prim.ListConstruct %int-1_7041, %int17_7042 : (!torch.int, !torch.int) -> !torch.list<int>
    %4485 = torch.aten.view %4483, %4484 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_7043 = torch.constant.int 2048
    %int-1_7044 = torch.constant.int -1
    %int17_7045 = torch.constant.int 17
    %4486 = torch.prim.ListConstruct %int2048_7043, %int-1_7044, %int17_7045 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4487 = torch.aten.view %4485, %4486 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_7046 = torch.constant.int 2
    %int0_7047 = torch.constant.int 0
    %int1_7048 = torch.constant.int 1
    %int1_7049 = torch.constant.int 1
    %4488 = torch.aten.slice.Tensor %4487, %int2_7046, %int0_7047, %int1_7048, %int1_7049 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_7050 = torch.constant.int 5
    %4489 = torch.aten.view.dtype %4488, %int5_7050 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %4490 = torch.aten.detach %4489 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_7051 = torch.constant.int 2
    %int1_7052 = torch.constant.int 1
    %int9223372036854775807_7053 = torch.constant.int 9223372036854775807
    %int1_7054 = torch.constant.int 1
    %4491 = torch.aten.slice.Tensor %4487, %int2_7051, %int1_7052, %int9223372036854775807_7053, %int1_7054 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_7055 = torch.constant.int 1
    %4492 = torch.aten.view.dtype %4491, %int1_7055 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %4493 = torch.aten.detach %4492 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %4494 = torch_c.to_builtin_tensor %4481 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_7056 = tensor.cast %4494 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4495 = torch_c.to_builtin_tensor %4490 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %4496 = torch_c.to_builtin_tensor %4493 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %4497 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_7056, %4495, %4496) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_7057 = tensor.cast %4497 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %4498 = torch_c.from_builtin_tensor %cast_7057 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4498, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_7058 = torch.constant.none
    %none_7059 = torch.constant.none
    %int5_7060 = torch.constant.int 5
    %cpu_7061 = torch.constant.device "cpu"
    %int0_7062 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4498, %none_7058, %none_7059, %int5_7060, %cpu_7061, %int0_7062 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_7063 = torch.constant.int 1
    %4499 = torch.aten.add.Tensor %4258, %4498, %int1_7063 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4499, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_7064 = torch.constant.none
    %none_7065 = torch.constant.none
    %int5_7066 = torch.constant.int 5
    %cpu_7067 = torch.constant.device "cpu"
    %int0_7068 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4499, %none_7064, %none_7065, %int5_7066, %cpu_7067, %int0_7068 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7069 = torch.constant.int 6
    %4500 = torch.prims.convert_element_type %4499, %int6_7069 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4500, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_7070 = torch.constant.int 2
    %4501 = torch.aten.pow.Tensor_Scalar %4500, %int2_7070 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4501, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_7071 = torch.constant.int -1
    %4502 = torch.prim.ListConstruct %int-1_7071 : (!torch.int) -> !torch.list<int>
    %true_7072 = torch.constant.bool true
    %none_7073 = torch.constant.none
    %4503 = torch.aten.mean.dim %4501, %4502, %true_7072, %none_7073 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4503, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_7074 = torch.constant.float 9.9999997473787516E-6
    %int1_7075 = torch.constant.int 1
    %4504 = torch.aten.add.Scalar %4503, %float9.999990e-06_7074, %int1_7075 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4504, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4505 = torch.aten.rsqrt %4504 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4505, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4506 = torch.aten.mul.Tensor %4500, %4505 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4506, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_7076 = torch.constant.none
    %none_7077 = torch.constant.none
    %int6_7078 = torch.constant.int 6
    %cpu_7079 = torch.constant.device "cpu"
    %int0_7080 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4506, %none_7076, %none_7077, %int6_7078, %cpu_7079, %int0_7080 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7081 = torch.constant.int 5
    %4507 = torch.prims.convert_element_type %4506, %int5_7081 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4507, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %4508 = torch.aten.mul.Tensor %165, %4507 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4508, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_7082 = torch.constant.none
    %none_7083 = torch.constant.none
    %int6_7084 = torch.constant.int 6
    %cpu_7085 = torch.constant.device "cpu"
    %int0_7086 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4508, %none_7082, %none_7083, %int6_7084, %cpu_7085, %int0_7086 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7087 = torch.constant.int 5
    %4509 = torch.prims.convert_element_type %4508, %int5_7087 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4509, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_7088 = torch.constant.int 2
    %4510 = torch.aten.view.dtype %166, %int2_7088 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %4511 = torch.aten.detach %4510 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_7089 = torch.constant.int -1
    %int17_7090 = torch.constant.int 17
    %4512 = torch.prim.ListConstruct %int-1_7089, %int17_7090 : (!torch.int, !torch.int) -> !torch.list<int>
    %4513 = torch.aten.view %4511, %4512 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_7091 = torch.constant.int 5632
    %int-1_7092 = torch.constant.int -1
    %int17_7093 = torch.constant.int 17
    %4514 = torch.prim.ListConstruct %int5632_7091, %int-1_7092, %int17_7093 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4515 = torch.aten.view %4513, %4514 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_7094 = torch.constant.int 2
    %int0_7095 = torch.constant.int 0
    %int1_7096 = torch.constant.int 1
    %int1_7097 = torch.constant.int 1
    %4516 = torch.aten.slice.Tensor %4515, %int2_7094, %int0_7095, %int1_7096, %int1_7097 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_7098 = torch.constant.int 5
    %4517 = torch.aten.view.dtype %4516, %int5_7098 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %4518 = torch.aten.detach %4517 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_7099 = torch.constant.int 2
    %int1_7100 = torch.constant.int 1
    %int9223372036854775807_7101 = torch.constant.int 9223372036854775807
    %int1_7102 = torch.constant.int 1
    %4519 = torch.aten.slice.Tensor %4515, %int2_7099, %int1_7100, %int9223372036854775807_7101, %int1_7102 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_7103 = torch.constant.int 1
    %4520 = torch.aten.view.dtype %4519, %int1_7103 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %4521 = torch.aten.detach %4520 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %4522 = torch_c.to_builtin_tensor %4509 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_7104 = tensor.cast %4522 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4523 = torch_c.to_builtin_tensor %4518 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %4524 = torch_c.to_builtin_tensor %4521 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %4525 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_7104, %4523, %4524) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_7105 = tensor.cast %4525 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %4526 = torch_c.from_builtin_tensor %cast_7105 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %4526, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %4527 = torch.aten.silu %4526 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %4527, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_7106 = torch.constant.int 2
    %4528 = torch.aten.view.dtype %167, %int2_7106 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %4529 = torch.aten.detach %4528 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_7107 = torch.constant.int -1
    %int17_7108 = torch.constant.int 17
    %4530 = torch.prim.ListConstruct %int-1_7107, %int17_7108 : (!torch.int, !torch.int) -> !torch.list<int>
    %4531 = torch.aten.view %4529, %4530 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_7109 = torch.constant.int 5632
    %int-1_7110 = torch.constant.int -1
    %int17_7111 = torch.constant.int 17
    %4532 = torch.prim.ListConstruct %int5632_7109, %int-1_7110, %int17_7111 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4533 = torch.aten.view %4531, %4532 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_7112 = torch.constant.int 2
    %int0_7113 = torch.constant.int 0
    %int1_7114 = torch.constant.int 1
    %int1_7115 = torch.constant.int 1
    %4534 = torch.aten.slice.Tensor %4533, %int2_7112, %int0_7113, %int1_7114, %int1_7115 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_7116 = torch.constant.int 5
    %4535 = torch.aten.view.dtype %4534, %int5_7116 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %4536 = torch.aten.detach %4535 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_7117 = torch.constant.int 2
    %int1_7118 = torch.constant.int 1
    %int9223372036854775807_7119 = torch.constant.int 9223372036854775807
    %int1_7120 = torch.constant.int 1
    %4537 = torch.aten.slice.Tensor %4533, %int2_7117, %int1_7118, %int9223372036854775807_7119, %int1_7120 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_7121 = torch.constant.int 1
    %4538 = torch.aten.view.dtype %4537, %int1_7121 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %4539 = torch.aten.detach %4538 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %4540 = torch_c.to_builtin_tensor %4509 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_7122 = tensor.cast %4540 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4541 = torch_c.to_builtin_tensor %4536 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %4542 = torch_c.to_builtin_tensor %4539 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %4543 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_7122, %4541, %4542) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_7123 = tensor.cast %4543 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %4544 = torch_c.from_builtin_tensor %cast_7123 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %4544, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %4545 = torch.aten.mul.Tensor %4527, %4544 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %4545, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_7124 = torch.constant.int 2
    %4546 = torch.aten.view.dtype %168, %int2_7124 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %4547 = torch.aten.detach %4546 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_7125 = torch.constant.int -1
    %int17_7126 = torch.constant.int 17
    %4548 = torch.prim.ListConstruct %int-1_7125, %int17_7126 : (!torch.int, !torch.int) -> !torch.list<int>
    %4549 = torch.aten.view %4547, %4548 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_7127 = torch.constant.int 2048
    %int-1_7128 = torch.constant.int -1
    %int17_7129 = torch.constant.int 17
    %4550 = torch.prim.ListConstruct %int2048_7127, %int-1_7128, %int17_7129 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4551 = torch.aten.view %4549, %4550 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_7130 = torch.constant.int 2
    %int0_7131 = torch.constant.int 0
    %int1_7132 = torch.constant.int 1
    %int1_7133 = torch.constant.int 1
    %4552 = torch.aten.slice.Tensor %4551, %int2_7130, %int0_7131, %int1_7132, %int1_7133 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_7134 = torch.constant.int 5
    %4553 = torch.aten.view.dtype %4552, %int5_7134 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %4554 = torch.aten.detach %4553 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_7135 = torch.constant.int 2
    %int1_7136 = torch.constant.int 1
    %int9223372036854775807_7137 = torch.constant.int 9223372036854775807
    %int1_7138 = torch.constant.int 1
    %4555 = torch.aten.slice.Tensor %4551, %int2_7135, %int1_7136, %int9223372036854775807_7137, %int1_7138 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_7139 = torch.constant.int 1
    %4556 = torch.aten.view.dtype %4555, %int1_7139 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %4557 = torch.aten.detach %4556 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %4558 = torch_c.to_builtin_tensor %4545 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_7140 = tensor.cast %4558 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %4559 = torch_c.to_builtin_tensor %4554 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %4560 = torch_c.to_builtin_tensor %4557 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %4561 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_7140, %4559, %4560) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_7141 = tensor.cast %4561 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %4562 = torch_c.from_builtin_tensor %cast_7141 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4562, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_7142 = torch.constant.int 1
    %4563 = torch.aten.add.Tensor %4499, %4562, %int1_7142 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4563, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_7143 = torch.constant.none
    %none_7144 = torch.constant.none
    %int5_7145 = torch.constant.int 5
    %cpu_7146 = torch.constant.device "cpu"
    %int0_7147 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4563, %none_7143, %none_7144, %int5_7145, %cpu_7146, %int0_7147 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7148 = torch.constant.int 6
    %4564 = torch.prims.convert_element_type %4563, %int6_7148 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4564, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_7149 = torch.constant.int 2
    %4565 = torch.aten.pow.Tensor_Scalar %4564, %int2_7149 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4565, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_7150 = torch.constant.int -1
    %4566 = torch.prim.ListConstruct %int-1_7150 : (!torch.int) -> !torch.list<int>
    %true_7151 = torch.constant.bool true
    %none_7152 = torch.constant.none
    %4567 = torch.aten.mean.dim %4565, %4566, %true_7151, %none_7152 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4567, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_7153 = torch.constant.float 9.9999997473787516E-6
    %int1_7154 = torch.constant.int 1
    %4568 = torch.aten.add.Scalar %4567, %float9.999990e-06_7153, %int1_7154 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4568, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4569 = torch.aten.rsqrt %4568 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4569, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4570 = torch.aten.mul.Tensor %4564, %4569 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4570, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_7155 = torch.constant.none
    %none_7156 = torch.constant.none
    %int6_7157 = torch.constant.int 6
    %cpu_7158 = torch.constant.device "cpu"
    %int0_7159 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4570, %none_7155, %none_7156, %int6_7157, %cpu_7158, %int0_7159 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7160 = torch.constant.int 5
    %4571 = torch.prims.convert_element_type %4570, %int5_7160 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4571, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %4572 = torch.aten.mul.Tensor %172, %4571 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4572, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_7161 = torch.constant.none
    %none_7162 = torch.constant.none
    %int6_7163 = torch.constant.int 6
    %cpu_7164 = torch.constant.device "cpu"
    %int0_7165 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4572, %none_7161, %none_7162, %int6_7163, %cpu_7164, %int0_7165 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7166 = torch.constant.int 5
    %4573 = torch.prims.convert_element_type %4572, %int5_7166 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4573, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_7167 = torch.constant.int 2
    %4574 = torch.aten.view.dtype %173, %int2_7167 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %4575 = torch.aten.detach %4574 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_7168 = torch.constant.int -1
    %int17_7169 = torch.constant.int 17
    %4576 = torch.prim.ListConstruct %int-1_7168, %int17_7169 : (!torch.int, !torch.int) -> !torch.list<int>
    %4577 = torch.aten.view %4575, %4576 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_7170 = torch.constant.int 2048
    %int-1_7171 = torch.constant.int -1
    %int17_7172 = torch.constant.int 17
    %4578 = torch.prim.ListConstruct %int2048_7170, %int-1_7171, %int17_7172 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4579 = torch.aten.view %4577, %4578 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_7173 = torch.constant.int 2
    %int0_7174 = torch.constant.int 0
    %int1_7175 = torch.constant.int 1
    %int1_7176 = torch.constant.int 1
    %4580 = torch.aten.slice.Tensor %4579, %int2_7173, %int0_7174, %int1_7175, %int1_7176 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_7177 = torch.constant.int 5
    %4581 = torch.aten.view.dtype %4580, %int5_7177 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %4582 = torch.aten.detach %4581 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_7178 = torch.constant.int 2
    %int1_7179 = torch.constant.int 1
    %int9223372036854775807_7180 = torch.constant.int 9223372036854775807
    %int1_7181 = torch.constant.int 1
    %4583 = torch.aten.slice.Tensor %4579, %int2_7178, %int1_7179, %int9223372036854775807_7180, %int1_7181 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_7182 = torch.constant.int 1
    %4584 = torch.aten.view.dtype %4583, %int1_7182 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %4585 = torch.aten.detach %4584 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %4586 = torch_c.to_builtin_tensor %4573 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_7183 = tensor.cast %4586 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4587 = torch_c.to_builtin_tensor %4582 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %4588 = torch_c.to_builtin_tensor %4585 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %4589 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_7183, %4587, %4588) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_7184 = tensor.cast %4589 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %4590 = torch_c.from_builtin_tensor %cast_7184 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4590, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_7185 = torch.constant.int 2
    %4591 = torch.aten.view.dtype %174, %int2_7185 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %4592 = torch.aten.detach %4591 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_7186 = torch.constant.int -1
    %int17_7187 = torch.constant.int 17
    %4593 = torch.prim.ListConstruct %int-1_7186, %int17_7187 : (!torch.int, !torch.int) -> !torch.list<int>
    %4594 = torch.aten.view %4592, %4593 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_7188 = torch.constant.int 256
    %int-1_7189 = torch.constant.int -1
    %int17_7190 = torch.constant.int 17
    %4595 = torch.prim.ListConstruct %int256_7188, %int-1_7189, %int17_7190 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4596 = torch.aten.view %4594, %4595 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_7191 = torch.constant.int 2
    %int0_7192 = torch.constant.int 0
    %int1_7193 = torch.constant.int 1
    %int1_7194 = torch.constant.int 1
    %4597 = torch.aten.slice.Tensor %4596, %int2_7191, %int0_7192, %int1_7193, %int1_7194 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_7195 = torch.constant.int 5
    %4598 = torch.aten.view.dtype %4597, %int5_7195 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %4599 = torch.aten.detach %4598 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_7196 = torch.constant.int 2
    %int1_7197 = torch.constant.int 1
    %int9223372036854775807_7198 = torch.constant.int 9223372036854775807
    %int1_7199 = torch.constant.int 1
    %4600 = torch.aten.slice.Tensor %4596, %int2_7196, %int1_7197, %int9223372036854775807_7198, %int1_7199 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_7200 = torch.constant.int 1
    %4601 = torch.aten.view.dtype %4600, %int1_7200 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %4602 = torch.aten.detach %4601 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %4603 = torch_c.to_builtin_tensor %4573 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_7201 = tensor.cast %4603 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4604 = torch_c.to_builtin_tensor %4599 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %4605 = torch_c.to_builtin_tensor %4602 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %4606 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_7201, %4604, %4605) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_7202 = tensor.cast %4606 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %4607 = torch_c.from_builtin_tensor %cast_7202 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %4607, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_7203 = torch.constant.int 2
    %4608 = torch.aten.view.dtype %175, %int2_7203 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %4609 = torch.aten.detach %4608 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_7204 = torch.constant.int -1
    %int17_7205 = torch.constant.int 17
    %4610 = torch.prim.ListConstruct %int-1_7204, %int17_7205 : (!torch.int, !torch.int) -> !torch.list<int>
    %4611 = torch.aten.view %4609, %4610 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_7206 = torch.constant.int 256
    %int-1_7207 = torch.constant.int -1
    %int17_7208 = torch.constant.int 17
    %4612 = torch.prim.ListConstruct %int256_7206, %int-1_7207, %int17_7208 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4613 = torch.aten.view %4611, %4612 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_7209 = torch.constant.int 2
    %int0_7210 = torch.constant.int 0
    %int1_7211 = torch.constant.int 1
    %int1_7212 = torch.constant.int 1
    %4614 = torch.aten.slice.Tensor %4613, %int2_7209, %int0_7210, %int1_7211, %int1_7212 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_7213 = torch.constant.int 5
    %4615 = torch.aten.view.dtype %4614, %int5_7213 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %4616 = torch.aten.detach %4615 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_7214 = torch.constant.int 2
    %int1_7215 = torch.constant.int 1
    %int9223372036854775807_7216 = torch.constant.int 9223372036854775807
    %int1_7217 = torch.constant.int 1
    %4617 = torch.aten.slice.Tensor %4613, %int2_7214, %int1_7215, %int9223372036854775807_7216, %int1_7217 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_7218 = torch.constant.int 1
    %4618 = torch.aten.view.dtype %4617, %int1_7218 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %4619 = torch.aten.detach %4618 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %4620 = torch_c.to_builtin_tensor %4573 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_7219 = tensor.cast %4620 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4621 = torch_c.to_builtin_tensor %4616 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %4622 = torch_c.to_builtin_tensor %4619 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %4623 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_7219, %4621, %4622) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_7220 = tensor.cast %4623 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %4624 = torch_c.from_builtin_tensor %cast_7220 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %4624, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_7221 = torch.constant.int 4
    %int32_7222 = torch.constant.int 32
    %int64_7223 = torch.constant.int 64
    %4625 = torch.prim.ListConstruct %int4_7221, %273, %int32_7222, %int64_7223 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4626 = torch.aten.view %4590, %4625 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4626, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_7224 = torch.constant.int 4
    %int4_7225 = torch.constant.int 4
    %int64_7226 = torch.constant.int 64
    %4627 = torch.prim.ListConstruct %int4_7224, %273, %int4_7225, %int64_7226 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4628 = torch.aten.view %4607, %4627 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %4628, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_7227 = torch.constant.int 4
    %int4_7228 = torch.constant.int 4
    %int64_7229 = torch.constant.int 64
    %4629 = torch.prim.ListConstruct %int4_7227, %273, %int4_7228, %int64_7229 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4630 = torch.aten.view %4624, %4629 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %4630, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_7230 = torch.constant.int 0
    %none_7231 = torch.constant.none
    %none_7232 = torch.constant.none
    %cpu_7233 = torch.constant.device "cpu"
    %false_7234 = torch.constant.bool false
    %4631 = torch.aten.arange.start %int0_7230, %273, %none_7231, %none_7232, %cpu_7233, %false_7234 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4631, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_7235 = torch.constant.int 0
    %4632 = torch.aten.unsqueeze %4631, %int0_7235 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %4632, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_7236 = torch.constant.int 0
    %int64_7237 = torch.constant.int 64
    %int2_7238 = torch.constant.int 2
    %none_7239 = torch.constant.none
    %none_7240 = torch.constant.none
    %cpu_7241 = torch.constant.device "cpu"
    %false_7242 = torch.constant.bool false
    %4633 = torch.aten.arange.start_step %int0_7236, %int64_7237, %int2_7238, %none_7239, %none_7240, %cpu_7241, %false_7242 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_7243 = torch.constant.none
    %none_7244 = torch.constant.none
    %int4_7245 = torch.constant.int 4
    %cpu_7246 = torch.constant.device "cpu"
    %int0_7247 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4633, %none_7243, %none_7244, %int4_7245, %cpu_7246, %int0_7247 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7248 = torch.constant.int 6
    %4634 = torch.prims.convert_element_type %4633, %int6_7248 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_7249 = torch.constant.int 64
    %4635 = torch.aten.div.Scalar %4634, %int64_7249 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_7250 = torch.constant.float 1.000000e+04
    %4636 = torch.aten.pow.Scalar %float1.000000e04_7250, %4635 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %4637 = torch.aten.reciprocal %4636 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_7251 = torch.constant.float 1.000000e+00
    %4638 = torch.aten.mul.Scalar %4637, %float1.000000e00_7251 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_7252 = torch.constant.none
    %4639 = torch.aten.clone %169, %none_7252 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_7253 = torch.constant.int 0
    %4640 = torch.aten.unsqueeze %4638, %int0_7253 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_7254 = torch.constant.int 2
    %4641 = torch.aten.unsqueeze %4640, %int2_7254 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_7255 = torch.constant.none
    %none_7256 = torch.constant.none
    %int6_7257 = torch.constant.int 6
    %cpu_7258 = torch.constant.device "cpu"
    %int0_7259 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4641, %none_7255, %none_7256, %int6_7257, %cpu_7258, %int0_7259 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_7260 = torch.constant.int 1
    %int-1_7261 = torch.constant.int -1
    %int1_7262 = torch.constant.int 1
    %4642 = torch.prim.ListConstruct %int1_7260, %int-1_7261, %int1_7262 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7263 = torch.constant.bool false
    %4643 = torch.aten.expand %4641, %4642, %false_7263 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_7264 = torch.constant.int 1
    %4644 = torch.aten.unsqueeze %4632, %int1_7264 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %4644, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_7265 = torch.constant.none
    %none_7266 = torch.constant.none
    %int4_7267 = torch.constant.int 4
    %cpu_7268 = torch.constant.device "cpu"
    %int0_7269 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4644, %none_7265, %none_7266, %int4_7267, %cpu_7268, %int0_7269 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7270 = torch.constant.int 6
    %4645 = torch.prims.convert_element_type %4644, %int6_7270 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %4645, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %4646 = torch.aten.matmul %4643, %4645 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %4646, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_7271 = torch.constant.int 1
    %int2_7272 = torch.constant.int 2
    %4647 = torch.aten.transpose.int %4646, %int1_7271, %int2_7272 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4647, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4648 = torch.aten.cos %4647 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4648, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4649 = torch.aten.mul.Tensor %4648, %4639 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4649, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_7273 = torch.constant.none
    %none_7274 = torch.constant.none
    %int6_7275 = torch.constant.int 6
    %cpu_7276 = torch.constant.device "cpu"
    %int0_7277 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4649, %none_7273, %none_7274, %int6_7275, %cpu_7276, %int0_7277 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7278 = torch.constant.int 5
    %4650 = torch.prims.convert_element_type %4649, %int5_7278 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %4650, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %4651 = torch.aten.sin %4647 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4651, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4652 = torch.aten.mul.Tensor %4651, %4639 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4652, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_7279 = torch.constant.none
    %none_7280 = torch.constant.none
    %int6_7281 = torch.constant.int 6
    %cpu_7282 = torch.constant.device "cpu"
    %int0_7283 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4652, %none_7279, %none_7280, %int6_7281, %cpu_7282, %int0_7283 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7284 = torch.constant.int 5
    %4653 = torch.prims.convert_element_type %4652, %int5_7284 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %4653, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_7285 = torch.constant.int 2
    %4654 = torch.aten.unsqueeze %4650, %int2_7285 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %4654, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_7286 = torch.constant.int 2
    %4655 = torch.aten.unsqueeze %4653, %int2_7286 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %4655, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_7287 = torch.constant.none
    %none_7288 = torch.constant.none
    %int5_7289 = torch.constant.int 5
    %cpu_7290 = torch.constant.device "cpu"
    %int0_7291 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4654, %none_7287, %none_7288, %int5_7289, %cpu_7290, %int0_7291 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_7292 = torch.constant.none
    %none_7293 = torch.constant.none
    %int5_7294 = torch.constant.int 5
    %cpu_7295 = torch.constant.device "cpu"
    %int0_7296 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4655, %none_7292, %none_7293, %int5_7294, %cpu_7295, %int0_7296 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_7297 = torch.constant.none
    %none_7298 = torch.constant.none
    %int5_7299 = torch.constant.int 5
    %cpu_7300 = torch.constant.device "cpu"
    %int0_7301 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4626, %none_7297, %none_7298, %int5_7299, %cpu_7300, %int0_7301 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_7302 = torch.constant.int 3
    %int0_7303 = torch.constant.int 0
    %int64_7304 = torch.constant.int 64
    %int2_7305 = torch.constant.int 2
    %4656 = torch.aten.slice.Tensor %4626, %int3_7302, %int0_7303, %int64_7304, %int2_7305 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4656, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_7306 = torch.constant.int 3
    %int1_7307 = torch.constant.int 1
    %int64_7308 = torch.constant.int 64
    %int2_7309 = torch.constant.int 2
    %4657 = torch.aten.slice.Tensor %4626, %int3_7306, %int1_7307, %int64_7308, %int2_7309 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4657, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4658 = torch.aten.mul.Tensor %4656, %4654 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4658, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4659 = torch.aten.mul.Tensor %4657, %4655 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4659, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_7310 = torch.constant.int 1
    %4660 = torch.aten.sub.Tensor %4658, %4659, %int1_7310 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4660, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4661 = torch.aten.mul.Tensor %4657, %4654 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4661, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4662 = torch.aten.mul.Tensor %4656, %4655 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4662, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_7311 = torch.constant.int 1
    %4663 = torch.aten.add.Tensor %4661, %4662, %int1_7311 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4663, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4664 = torch_c.to_builtin_tensor %4660 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_7312 = tensor.cast %4664 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %4665 = torch_c.to_builtin_tensor %4663 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_7313 = tensor.cast %4665 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %4666 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_7312, %cast_7313) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_7314 = tensor.cast %4666 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %4667 = torch_c.from_builtin_tensor %cast_7314 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %4667, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_7315 = torch.constant.int 4
    %int32_7316 = torch.constant.int 32
    %int64_7317 = torch.constant.int 64
    %4668 = torch.prim.ListConstruct %int4_7315, %273, %int32_7316, %int64_7317 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4669 = torch.aten.view %4667, %4668 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4669, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_7318 = torch.constant.none
    %none_7319 = torch.constant.none
    %int5_7320 = torch.constant.int 5
    %cpu_7321 = torch.constant.device "cpu"
    %int0_7322 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4669, %none_7318, %none_7319, %int5_7320, %cpu_7321, %int0_7322 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_7323 = torch.constant.int 0
    %none_7324 = torch.constant.none
    %none_7325 = torch.constant.none
    %cpu_7326 = torch.constant.device "cpu"
    %false_7327 = torch.constant.bool false
    %4670 = torch.aten.arange.start %int0_7323, %273, %none_7324, %none_7325, %cpu_7326, %false_7327 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4670, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_7328 = torch.constant.int 0
    %4671 = torch.aten.unsqueeze %4670, %int0_7328 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %4671, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_7329 = torch.constant.int 0
    %int64_7330 = torch.constant.int 64
    %int2_7331 = torch.constant.int 2
    %none_7332 = torch.constant.none
    %none_7333 = torch.constant.none
    %cpu_7334 = torch.constant.device "cpu"
    %false_7335 = torch.constant.bool false
    %4672 = torch.aten.arange.start_step %int0_7329, %int64_7330, %int2_7331, %none_7332, %none_7333, %cpu_7334, %false_7335 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_7336 = torch.constant.none
    %none_7337 = torch.constant.none
    %int4_7338 = torch.constant.int 4
    %cpu_7339 = torch.constant.device "cpu"
    %int0_7340 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4672, %none_7336, %none_7337, %int4_7338, %cpu_7339, %int0_7340 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7341 = torch.constant.int 6
    %4673 = torch.prims.convert_element_type %4672, %int6_7341 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_7342 = torch.constant.int 64
    %4674 = torch.aten.div.Scalar %4673, %int64_7342 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_7343 = torch.constant.float 1.000000e+04
    %4675 = torch.aten.pow.Scalar %float1.000000e04_7343, %4674 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %4676 = torch.aten.reciprocal %4675 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_7344 = torch.constant.float 1.000000e+00
    %4677 = torch.aten.mul.Scalar %4676, %float1.000000e00_7344 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_7345 = torch.constant.none
    %4678 = torch.aten.clone %170, %none_7345 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_7346 = torch.constant.int 0
    %4679 = torch.aten.unsqueeze %4677, %int0_7346 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_7347 = torch.constant.int 2
    %4680 = torch.aten.unsqueeze %4679, %int2_7347 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_7348 = torch.constant.none
    %none_7349 = torch.constant.none
    %int6_7350 = torch.constant.int 6
    %cpu_7351 = torch.constant.device "cpu"
    %int0_7352 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4680, %none_7348, %none_7349, %int6_7350, %cpu_7351, %int0_7352 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_7353 = torch.constant.int 1
    %int-1_7354 = torch.constant.int -1
    %int1_7355 = torch.constant.int 1
    %4681 = torch.prim.ListConstruct %int1_7353, %int-1_7354, %int1_7355 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7356 = torch.constant.bool false
    %4682 = torch.aten.expand %4680, %4681, %false_7356 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_7357 = torch.constant.int 1
    %4683 = torch.aten.unsqueeze %4671, %int1_7357 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %4683, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_7358 = torch.constant.none
    %none_7359 = torch.constant.none
    %int4_7360 = torch.constant.int 4
    %cpu_7361 = torch.constant.device "cpu"
    %int0_7362 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4683, %none_7358, %none_7359, %int4_7360, %cpu_7361, %int0_7362 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7363 = torch.constant.int 6
    %4684 = torch.prims.convert_element_type %4683, %int6_7363 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %4684, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %4685 = torch.aten.matmul %4682, %4684 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %4685, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_7364 = torch.constant.int 1
    %int2_7365 = torch.constant.int 2
    %4686 = torch.aten.transpose.int %4685, %int1_7364, %int2_7365 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4686, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4687 = torch.aten.cos %4686 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4687, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4688 = torch.aten.mul.Tensor %4687, %4678 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4688, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_7366 = torch.constant.none
    %none_7367 = torch.constant.none
    %int6_7368 = torch.constant.int 6
    %cpu_7369 = torch.constant.device "cpu"
    %int0_7370 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4688, %none_7366, %none_7367, %int6_7368, %cpu_7369, %int0_7370 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7371 = torch.constant.int 5
    %4689 = torch.prims.convert_element_type %4688, %int5_7371 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %4689, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %4690 = torch.aten.sin %4686 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4690, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4691 = torch.aten.mul.Tensor %4690, %4678 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4691, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_7372 = torch.constant.none
    %none_7373 = torch.constant.none
    %int6_7374 = torch.constant.int 6
    %cpu_7375 = torch.constant.device "cpu"
    %int0_7376 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4691, %none_7372, %none_7373, %int6_7374, %cpu_7375, %int0_7376 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7377 = torch.constant.int 5
    %4692 = torch.prims.convert_element_type %4691, %int5_7377 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %4692, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_7378 = torch.constant.int 2
    %4693 = torch.aten.unsqueeze %4689, %int2_7378 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %4693, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_7379 = torch.constant.int 2
    %4694 = torch.aten.unsqueeze %4692, %int2_7379 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %4694, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_7380 = torch.constant.none
    %none_7381 = torch.constant.none
    %int5_7382 = torch.constant.int 5
    %cpu_7383 = torch.constant.device "cpu"
    %int0_7384 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4693, %none_7380, %none_7381, %int5_7382, %cpu_7383, %int0_7384 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_7385 = torch.constant.none
    %none_7386 = torch.constant.none
    %int5_7387 = torch.constant.int 5
    %cpu_7388 = torch.constant.device "cpu"
    %int0_7389 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4694, %none_7385, %none_7386, %int5_7387, %cpu_7388, %int0_7389 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_7390 = torch.constant.none
    %none_7391 = torch.constant.none
    %int5_7392 = torch.constant.int 5
    %cpu_7393 = torch.constant.device "cpu"
    %int0_7394 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4628, %none_7390, %none_7391, %int5_7392, %cpu_7393, %int0_7394 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_7395 = torch.constant.int 3
    %int0_7396 = torch.constant.int 0
    %int64_7397 = torch.constant.int 64
    %int2_7398 = torch.constant.int 2
    %4695 = torch.aten.slice.Tensor %4628, %int3_7395, %int0_7396, %int64_7397, %int2_7398 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4695, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_7399 = torch.constant.int 3
    %int1_7400 = torch.constant.int 1
    %int64_7401 = torch.constant.int 64
    %int2_7402 = torch.constant.int 2
    %4696 = torch.aten.slice.Tensor %4628, %int3_7399, %int1_7400, %int64_7401, %int2_7402 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4696, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %4697 = torch.aten.mul.Tensor %4695, %4693 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4697, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %4698 = torch.aten.mul.Tensor %4696, %4694 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4698, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_7403 = torch.constant.int 1
    %4699 = torch.aten.sub.Tensor %4697, %4698, %int1_7403 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4699, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %4700 = torch.aten.mul.Tensor %4696, %4693 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4700, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %4701 = torch.aten.mul.Tensor %4695, %4694 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4701, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_7404 = torch.constant.int 1
    %4702 = torch.aten.add.Tensor %4700, %4701, %int1_7404 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %4702, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %4703 = torch_c.to_builtin_tensor %4699 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_7405 = tensor.cast %4703 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %4704 = torch_c.to_builtin_tensor %4702 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_7406 = tensor.cast %4704 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %4705 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_7405, %cast_7406) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_7407 = tensor.cast %4705 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %4706 = torch_c.from_builtin_tensor %cast_7407 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %4706, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_7408 = torch.constant.int 4
    %int4_7409 = torch.constant.int 4
    %int64_7410 = torch.constant.int 64
    %4707 = torch.prim.ListConstruct %int4_7408, %273, %int4_7409, %int64_7410 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4708 = torch.aten.view %4706, %4707 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %4708, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_7411 = torch.constant.none
    %none_7412 = torch.constant.none
    %int5_7413 = torch.constant.int 5
    %cpu_7414 = torch.constant.device "cpu"
    %int0_7415 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4708, %none_7411, %none_7412, %int5_7413, %cpu_7414, %int0_7415 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_7416 = torch.constant.int 22
    %4709 = torch.aten.mul.Scalar %arg2, %int22_7416 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4709, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int14 = torch.constant.int 14
    %int1_7417 = torch.constant.int 1
    %4710 = torch.aten.add.Scalar %4709, %int14, %int1_7417 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4710, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_7418 = torch.constant.int 2
    %4711 = torch.aten.mul.Scalar %4710, %int2_7418 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4711, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_7419 = torch.constant.int 0
    %int1_7420 = torch.constant.int 1
    %4712 = torch.aten.add.Scalar %4711, %int0_7419, %int1_7420 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4712, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %4713 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %4714 = torch.aten.view %4712, %4713 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4714, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_7421 = torch.constant.int 4
    %int32_7422 = torch.constant.int 32
    %int4_7423 = torch.constant.int 4
    %int64_7424 = torch.constant.int 64
    %4715 = torch.prim.ListConstruct %int4_7421, %271, %int32_7422, %int4_7423, %int64_7424 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4716 = torch.aten.view %4708, %4715 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %4716, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_7425 = torch.constant.int 32
    %int4_7426 = torch.constant.int 4
    %int64_7427 = torch.constant.int 64
    %4717 = torch.prim.ListConstruct %446, %int32_7425, %int4_7426, %int64_7427 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4718 = torch.aten.view %4716, %4717 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %4718, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_7428 = torch.constant.int 1
    %int2_7429 = torch.constant.int 2
    %4719 = torch.aten.transpose.int %4718, %int1_7428, %int2_7429 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %4719, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_7430 = torch.constant.none
    %none_7431 = torch.constant.none
    %int5_7432 = torch.constant.int 5
    %cpu_7433 = torch.constant.device "cpu"
    %int0_7434 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4719, %none_7430, %none_7431, %int5_7432, %cpu_7433, %int0_7434 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_7435 = torch.constant.int 22
    %int2_7436 = torch.constant.int 2
    %int4_7437 = torch.constant.int 4
    %int32_7438 = torch.constant.int 32
    %int64_7439 = torch.constant.int 64
    %4720 = torch.prim.ListConstruct %272, %int22_7435, %int2_7436, %int4_7437, %int32_7438, %int64_7439 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4721 = torch.aten.view %4445, %4720 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4721, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_7440 = torch.constant.int 4
    %int32_7441 = torch.constant.int 32
    %int64_7442 = torch.constant.int 64
    %4722 = torch.prim.ListConstruct %439, %int4_7440, %int32_7441, %int64_7442 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4723 = torch.aten.view %4721, %4722 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %4723, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %4724 = torch.prim.ListConstruct %4714 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_7443 = torch.constant.bool false
    %4725 = torch.aten.index_put %4723, %4724, %4719, %false_7443 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %4725, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_7444 = torch.constant.int 22
    %int2_7445 = torch.constant.int 2
    %int4_7446 = torch.constant.int 4
    %int32_7447 = torch.constant.int 32
    %int64_7448 = torch.constant.int 64
    %4726 = torch.prim.ListConstruct %272, %int22_7444, %int2_7445, %int4_7446, %int32_7447, %int64_7448 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4727 = torch.aten.view %4725, %4726 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4727, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_7449 = torch.constant.int 360448
    %4728 = torch.prim.ListConstruct %272, %int360448_7449 : (!torch.int, !torch.int) -> !torch.list<int>
    %4729 = torch.aten.view %4727, %4728 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %4729, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_7450 = torch.constant.int 22
    %int2_7451 = torch.constant.int 2
    %int4_7452 = torch.constant.int 4
    %int32_7453 = torch.constant.int 32
    %int64_7454 = torch.constant.int 64
    %4730 = torch.prim.ListConstruct %272, %int22_7450, %int2_7451, %int4_7452, %int32_7453, %int64_7454 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4731 = torch.aten.view %4729, %4730 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4731, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_7455 = torch.constant.int 4
    %int32_7456 = torch.constant.int 32
    %int64_7457 = torch.constant.int 64
    %4732 = torch.prim.ListConstruct %439, %int4_7455, %int32_7456, %int64_7457 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4733 = torch.aten.view %4731, %4732 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %4733, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_7458 = torch.constant.int 22
    %4734 = torch.aten.mul.Scalar %arg2, %int22_7458 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4734, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int14_7459 = torch.constant.int 14
    %int1_7460 = torch.constant.int 1
    %4735 = torch.aten.add.Scalar %4734, %int14_7459, %int1_7460 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4735, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_7461 = torch.constant.int 2
    %4736 = torch.aten.mul.Scalar %4735, %int2_7461 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4736, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_7462 = torch.constant.int 1
    %int1_7463 = torch.constant.int 1
    %4737 = torch.aten.add.Scalar %4736, %int1_7462, %int1_7463 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4737, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %4738 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %4739 = torch.aten.view %4737, %4738 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4739, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_7464 = torch.constant.int 4
    %int32_7465 = torch.constant.int 32
    %int4_7466 = torch.constant.int 4
    %int64_7467 = torch.constant.int 64
    %4740 = torch.prim.ListConstruct %int4_7464, %271, %int32_7465, %int4_7466, %int64_7467 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4741 = torch.aten.view %4630, %4740 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %4741, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_7468 = torch.constant.int 32
    %int4_7469 = torch.constant.int 4
    %int64_7470 = torch.constant.int 64
    %4742 = torch.prim.ListConstruct %446, %int32_7468, %int4_7469, %int64_7470 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4743 = torch.aten.view %4741, %4742 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %4743, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_7471 = torch.constant.int 1
    %int2_7472 = torch.constant.int 2
    %4744 = torch.aten.transpose.int %4743, %int1_7471, %int2_7472 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %4744, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_7473 = torch.constant.none
    %none_7474 = torch.constant.none
    %int5_7475 = torch.constant.int 5
    %cpu_7476 = torch.constant.device "cpu"
    %int0_7477 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4744, %none_7473, %none_7474, %int5_7475, %cpu_7476, %int0_7477 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %4745 = torch.prim.ListConstruct %4739 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_7478 = torch.constant.bool false
    %4746 = torch.aten.index_put %4733, %4745, %4744, %false_7478 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %4746, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_7479 = torch.constant.int 22
    %int2_7480 = torch.constant.int 2
    %int4_7481 = torch.constant.int 4
    %int32_7482 = torch.constant.int 32
    %int64_7483 = torch.constant.int 64
    %4747 = torch.prim.ListConstruct %272, %int22_7479, %int2_7480, %int4_7481, %int32_7482, %int64_7483 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4748 = torch.aten.view %4746, %4747 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4748, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_7484 = torch.constant.int 360448
    %4749 = torch.prim.ListConstruct %272, %int360448_7484 : (!torch.int, !torch.int) -> !torch.list<int>
    %4750 = torch.aten.view %4748, %4749 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %4750, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_7485 = torch.constant.int 0
    %int1_7486 = torch.constant.int 1
    %none_7487 = torch.constant.none
    %none_7488 = torch.constant.none
    %cpu_7489 = torch.constant.device "cpu"
    %false_7490 = torch.constant.bool false
    %4751 = torch.aten.arange.start_step %int0_7485, %273, %int1_7486, %none_7487, %none_7488, %cpu_7489, %false_7490 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4751, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_7491 = torch.constant.int -1
    %4752 = torch.aten.unsqueeze %arg1, %int-1_7491 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %4753 = torch.aten.ge.Tensor %4751, %4752 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %4753, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_7492 = torch.constant.none
    %none_7493 = torch.constant.none
    %cpu_7494 = torch.constant.device "cpu"
    %false_7495 = torch.constant.bool false
    %4754 = torch.aten.arange %273, %none_7492, %none_7493, %cpu_7494, %false_7495 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4754, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_7496 = torch.constant.int 0
    %4755 = torch.aten.unsqueeze %4754, %int0_7496 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %4755, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_7497 = torch.constant.int 1
    %4756 = torch.aten.unsqueeze %4755, %int1_7497 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %4756, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_7498 = torch.constant.int 2
    %4757 = torch.aten.unsqueeze %4756, %int2_7498 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %4757, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_7499 = torch.constant.none
    %none_7500 = torch.constant.none
    %cpu_7501 = torch.constant.device "cpu"
    %false_7502 = torch.constant.bool false
    %4758 = torch.aten.arange %273, %none_7499, %none_7500, %cpu_7501, %false_7502 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4758, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_7503 = torch.constant.int 0
    %4759 = torch.aten.unsqueeze %4758, %int0_7503 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %4759, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_7504 = torch.constant.int 1
    %4760 = torch.aten.unsqueeze %4759, %int1_7504 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %4760, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_7505 = torch.constant.int 3
    %4761 = torch.aten.unsqueeze %4760, %int3_7505 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %4761, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %4762 = torch.aten.gt.Tensor %4757, %4761 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %4762, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_7506 = torch.constant.int 1
    %4763 = torch.aten.unsqueeze %4753, %int1_7506 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %4763, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_7507 = torch.constant.int 2
    %4764 = torch.aten.unsqueeze %4763, %int2_7507 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %4764, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %4765 = torch.aten.logical_or %4762, %4764 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %4765, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_7508 = torch.constant.none
    %4766 = torch.aten.clone %171, %none_7508 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_7509 = torch.constant.int 0
    %4767 = torch.aten.where.ScalarOther %4765, %4766, %int0_7509 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %4767, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_7510 = torch.constant.none
    %none_7511 = torch.constant.none
    %int5_7512 = torch.constant.int 5
    %cpu_7513 = torch.constant.device "cpu"
    %int0_7514 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4767, %none_7510, %none_7511, %int5_7512, %cpu_7513, %int0_7514 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_7515 = torch.constant.int -2
    %4768 = torch.aten.unsqueeze %4708, %int-2_7515 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %4768, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_7516 = torch.constant.int 4
    %int4_7517 = torch.constant.int 4
    %int8_7518 = torch.constant.int 8
    %int64_7519 = torch.constant.int 64
    %4769 = torch.prim.ListConstruct %int4_7516, %273, %int4_7517, %int8_7518, %int64_7519 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7520 = torch.constant.bool false
    %4770 = torch.aten.expand %4768, %4769, %false_7520 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4770, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_7521 = torch.constant.int 0
    %4771 = torch.aten.clone %4770, %int0_7521 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4771, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_7522 = torch.constant.int 4
    %int32_7523 = torch.constant.int 32
    %int64_7524 = torch.constant.int 64
    %4772 = torch.prim.ListConstruct %int4_7522, %273, %int32_7523, %int64_7524 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4773 = torch.aten._unsafe_view %4771, %4772 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4773, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_7525 = torch.constant.int -2
    %4774 = torch.aten.unsqueeze %4630, %int-2_7525 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %4774, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_7526 = torch.constant.int 4
    %int4_7527 = torch.constant.int 4
    %int8_7528 = torch.constant.int 8
    %int64_7529 = torch.constant.int 64
    %4775 = torch.prim.ListConstruct %int4_7526, %273, %int4_7527, %int8_7528, %int64_7529 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7530 = torch.constant.bool false
    %4776 = torch.aten.expand %4774, %4775, %false_7530 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4776, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_7531 = torch.constant.int 0
    %4777 = torch.aten.clone %4776, %int0_7531 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4777, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_7532 = torch.constant.int 4
    %int32_7533 = torch.constant.int 32
    %int64_7534 = torch.constant.int 64
    %4778 = torch.prim.ListConstruct %int4_7532, %273, %int32_7533, %int64_7534 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4779 = torch.aten._unsafe_view %4777, %4778 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4779, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_7535 = torch.constant.int 1
    %int2_7536 = torch.constant.int 2
    %4780 = torch.aten.transpose.int %4669, %int1_7535, %int2_7536 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %4780, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_7537 = torch.constant.int 1
    %int2_7538 = torch.constant.int 2
    %4781 = torch.aten.transpose.int %4773, %int1_7537, %int2_7538 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %4781, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_7539 = torch.constant.int 1
    %int2_7540 = torch.constant.int 2
    %4782 = torch.aten.transpose.int %4779, %int1_7539, %int2_7540 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %4782, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_7541 = torch.constant.float 0.000000e+00
    %false_7542 = torch.constant.bool false
    %none_7543 = torch.constant.none
    %false_7544 = torch.constant.bool false
    %4783 = torch.aten.scaled_dot_product_attention %4780, %4781, %4782, %4767, %float0.000000e00_7541, %false_7542, %none_7543, %false_7544 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %4783, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_7545 = torch.constant.int 1
    %int2_7546 = torch.constant.int 2
    %4784 = torch.aten.transpose.int %4783, %int1_7545, %int2_7546 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4784, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_7547 = torch.constant.int 4
    %int2048_7548 = torch.constant.int 2048
    %4785 = torch.prim.ListConstruct %int4_7547, %273, %int2048_7548 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4786 = torch.aten.view %4784, %4785 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4786, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_7549 = torch.constant.int 2
    %4787 = torch.aten.view.dtype %176, %int2_7549 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %4788 = torch.aten.detach %4787 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_7550 = torch.constant.int -1
    %int17_7551 = torch.constant.int 17
    %4789 = torch.prim.ListConstruct %int-1_7550, %int17_7551 : (!torch.int, !torch.int) -> !torch.list<int>
    %4790 = torch.aten.view %4788, %4789 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_7552 = torch.constant.int 2048
    %int-1_7553 = torch.constant.int -1
    %int17_7554 = torch.constant.int 17
    %4791 = torch.prim.ListConstruct %int2048_7552, %int-1_7553, %int17_7554 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4792 = torch.aten.view %4790, %4791 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_7555 = torch.constant.int 2
    %int0_7556 = torch.constant.int 0
    %int1_7557 = torch.constant.int 1
    %int1_7558 = torch.constant.int 1
    %4793 = torch.aten.slice.Tensor %4792, %int2_7555, %int0_7556, %int1_7557, %int1_7558 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_7559 = torch.constant.int 5
    %4794 = torch.aten.view.dtype %4793, %int5_7559 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %4795 = torch.aten.detach %4794 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_7560 = torch.constant.int 2
    %int1_7561 = torch.constant.int 1
    %int9223372036854775807_7562 = torch.constant.int 9223372036854775807
    %int1_7563 = torch.constant.int 1
    %4796 = torch.aten.slice.Tensor %4792, %int2_7560, %int1_7561, %int9223372036854775807_7562, %int1_7563 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_7564 = torch.constant.int 1
    %4797 = torch.aten.view.dtype %4796, %int1_7564 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %4798 = torch.aten.detach %4797 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %4799 = torch_c.to_builtin_tensor %4786 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_7565 = tensor.cast %4799 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4800 = torch_c.to_builtin_tensor %4795 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %4801 = torch_c.to_builtin_tensor %4798 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %4802 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_7565, %4800, %4801) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_7566 = tensor.cast %4802 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %4803 = torch_c.from_builtin_tensor %cast_7566 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4803, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_7567 = torch.constant.none
    %none_7568 = torch.constant.none
    %int5_7569 = torch.constant.int 5
    %cpu_7570 = torch.constant.device "cpu"
    %int0_7571 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4803, %none_7567, %none_7568, %int5_7569, %cpu_7570, %int0_7571 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_7572 = torch.constant.int 1
    %4804 = torch.aten.add.Tensor %4563, %4803, %int1_7572 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4804, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_7573 = torch.constant.none
    %none_7574 = torch.constant.none
    %int5_7575 = torch.constant.int 5
    %cpu_7576 = torch.constant.device "cpu"
    %int0_7577 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4804, %none_7573, %none_7574, %int5_7575, %cpu_7576, %int0_7577 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7578 = torch.constant.int 6
    %4805 = torch.prims.convert_element_type %4804, %int6_7578 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4805, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_7579 = torch.constant.int 2
    %4806 = torch.aten.pow.Tensor_Scalar %4805, %int2_7579 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4806, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_7580 = torch.constant.int -1
    %4807 = torch.prim.ListConstruct %int-1_7580 : (!torch.int) -> !torch.list<int>
    %true_7581 = torch.constant.bool true
    %none_7582 = torch.constant.none
    %4808 = torch.aten.mean.dim %4806, %4807, %true_7581, %none_7582 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4808, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_7583 = torch.constant.float 9.9999997473787516E-6
    %int1_7584 = torch.constant.int 1
    %4809 = torch.aten.add.Scalar %4808, %float9.999990e-06_7583, %int1_7584 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4809, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4810 = torch.aten.rsqrt %4809 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4810, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4811 = torch.aten.mul.Tensor %4805, %4810 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4811, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_7585 = torch.constant.none
    %none_7586 = torch.constant.none
    %int6_7587 = torch.constant.int 6
    %cpu_7588 = torch.constant.device "cpu"
    %int0_7589 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4811, %none_7585, %none_7586, %int6_7587, %cpu_7588, %int0_7589 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7590 = torch.constant.int 5
    %4812 = torch.prims.convert_element_type %4811, %int5_7590 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4812, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %4813 = torch.aten.mul.Tensor %177, %4812 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4813, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_7591 = torch.constant.none
    %none_7592 = torch.constant.none
    %int6_7593 = torch.constant.int 6
    %cpu_7594 = torch.constant.device "cpu"
    %int0_7595 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4813, %none_7591, %none_7592, %int6_7593, %cpu_7594, %int0_7595 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7596 = torch.constant.int 5
    %4814 = torch.prims.convert_element_type %4813, %int5_7596 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4814, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_7597 = torch.constant.int 2
    %4815 = torch.aten.view.dtype %178, %int2_7597 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %4816 = torch.aten.detach %4815 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_7598 = torch.constant.int -1
    %int17_7599 = torch.constant.int 17
    %4817 = torch.prim.ListConstruct %int-1_7598, %int17_7599 : (!torch.int, !torch.int) -> !torch.list<int>
    %4818 = torch.aten.view %4816, %4817 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_7600 = torch.constant.int 5632
    %int-1_7601 = torch.constant.int -1
    %int17_7602 = torch.constant.int 17
    %4819 = torch.prim.ListConstruct %int5632_7600, %int-1_7601, %int17_7602 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4820 = torch.aten.view %4818, %4819 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_7603 = torch.constant.int 2
    %int0_7604 = torch.constant.int 0
    %int1_7605 = torch.constant.int 1
    %int1_7606 = torch.constant.int 1
    %4821 = torch.aten.slice.Tensor %4820, %int2_7603, %int0_7604, %int1_7605, %int1_7606 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_7607 = torch.constant.int 5
    %4822 = torch.aten.view.dtype %4821, %int5_7607 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %4823 = torch.aten.detach %4822 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_7608 = torch.constant.int 2
    %int1_7609 = torch.constant.int 1
    %int9223372036854775807_7610 = torch.constant.int 9223372036854775807
    %int1_7611 = torch.constant.int 1
    %4824 = torch.aten.slice.Tensor %4820, %int2_7608, %int1_7609, %int9223372036854775807_7610, %int1_7611 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_7612 = torch.constant.int 1
    %4825 = torch.aten.view.dtype %4824, %int1_7612 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %4826 = torch.aten.detach %4825 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %4827 = torch_c.to_builtin_tensor %4814 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_7613 = tensor.cast %4827 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4828 = torch_c.to_builtin_tensor %4823 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %4829 = torch_c.to_builtin_tensor %4826 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %4830 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_7613, %4828, %4829) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_7614 = tensor.cast %4830 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %4831 = torch_c.from_builtin_tensor %cast_7614 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %4831, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %4832 = torch.aten.silu %4831 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %4832, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_7615 = torch.constant.int 2
    %4833 = torch.aten.view.dtype %179, %int2_7615 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %4834 = torch.aten.detach %4833 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_7616 = torch.constant.int -1
    %int17_7617 = torch.constant.int 17
    %4835 = torch.prim.ListConstruct %int-1_7616, %int17_7617 : (!torch.int, !torch.int) -> !torch.list<int>
    %4836 = torch.aten.view %4834, %4835 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_7618 = torch.constant.int 5632
    %int-1_7619 = torch.constant.int -1
    %int17_7620 = torch.constant.int 17
    %4837 = torch.prim.ListConstruct %int5632_7618, %int-1_7619, %int17_7620 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4838 = torch.aten.view %4836, %4837 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_7621 = torch.constant.int 2
    %int0_7622 = torch.constant.int 0
    %int1_7623 = torch.constant.int 1
    %int1_7624 = torch.constant.int 1
    %4839 = torch.aten.slice.Tensor %4838, %int2_7621, %int0_7622, %int1_7623, %int1_7624 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_7625 = torch.constant.int 5
    %4840 = torch.aten.view.dtype %4839, %int5_7625 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %4841 = torch.aten.detach %4840 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_7626 = torch.constant.int 2
    %int1_7627 = torch.constant.int 1
    %int9223372036854775807_7628 = torch.constant.int 9223372036854775807
    %int1_7629 = torch.constant.int 1
    %4842 = torch.aten.slice.Tensor %4838, %int2_7626, %int1_7627, %int9223372036854775807_7628, %int1_7629 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_7630 = torch.constant.int 1
    %4843 = torch.aten.view.dtype %4842, %int1_7630 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %4844 = torch.aten.detach %4843 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %4845 = torch_c.to_builtin_tensor %4814 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_7631 = tensor.cast %4845 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4846 = torch_c.to_builtin_tensor %4841 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %4847 = torch_c.to_builtin_tensor %4844 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %4848 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_7631, %4846, %4847) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_7632 = tensor.cast %4848 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %4849 = torch_c.from_builtin_tensor %cast_7632 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %4849, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %4850 = torch.aten.mul.Tensor %4832, %4849 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %4850, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_7633 = torch.constant.int 2
    %4851 = torch.aten.view.dtype %180, %int2_7633 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %4852 = torch.aten.detach %4851 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_7634 = torch.constant.int -1
    %int17_7635 = torch.constant.int 17
    %4853 = torch.prim.ListConstruct %int-1_7634, %int17_7635 : (!torch.int, !torch.int) -> !torch.list<int>
    %4854 = torch.aten.view %4852, %4853 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_7636 = torch.constant.int 2048
    %int-1_7637 = torch.constant.int -1
    %int17_7638 = torch.constant.int 17
    %4855 = torch.prim.ListConstruct %int2048_7636, %int-1_7637, %int17_7638 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4856 = torch.aten.view %4854, %4855 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_7639 = torch.constant.int 2
    %int0_7640 = torch.constant.int 0
    %int1_7641 = torch.constant.int 1
    %int1_7642 = torch.constant.int 1
    %4857 = torch.aten.slice.Tensor %4856, %int2_7639, %int0_7640, %int1_7641, %int1_7642 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_7643 = torch.constant.int 5
    %4858 = torch.aten.view.dtype %4857, %int5_7643 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %4859 = torch.aten.detach %4858 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_7644 = torch.constant.int 2
    %int1_7645 = torch.constant.int 1
    %int9223372036854775807_7646 = torch.constant.int 9223372036854775807
    %int1_7647 = torch.constant.int 1
    %4860 = torch.aten.slice.Tensor %4856, %int2_7644, %int1_7645, %int9223372036854775807_7646, %int1_7647 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_7648 = torch.constant.int 1
    %4861 = torch.aten.view.dtype %4860, %int1_7648 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %4862 = torch.aten.detach %4861 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %4863 = torch_c.to_builtin_tensor %4850 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_7649 = tensor.cast %4863 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %4864 = torch_c.to_builtin_tensor %4859 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %4865 = torch_c.to_builtin_tensor %4862 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %4866 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_7649, %4864, %4865) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_7650 = tensor.cast %4866 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %4867 = torch_c.from_builtin_tensor %cast_7650 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4867, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_7651 = torch.constant.int 1
    %4868 = torch.aten.add.Tensor %4804, %4867, %int1_7651 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4868, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_7652 = torch.constant.none
    %none_7653 = torch.constant.none
    %int5_7654 = torch.constant.int 5
    %cpu_7655 = torch.constant.device "cpu"
    %int0_7656 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4868, %none_7652, %none_7653, %int5_7654, %cpu_7655, %int0_7656 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7657 = torch.constant.int 6
    %4869 = torch.prims.convert_element_type %4868, %int6_7657 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4869, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_7658 = torch.constant.int 2
    %4870 = torch.aten.pow.Tensor_Scalar %4869, %int2_7658 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4870, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_7659 = torch.constant.int -1
    %4871 = torch.prim.ListConstruct %int-1_7659 : (!torch.int) -> !torch.list<int>
    %true_7660 = torch.constant.bool true
    %none_7661 = torch.constant.none
    %4872 = torch.aten.mean.dim %4870, %4871, %true_7660, %none_7661 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4872, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_7662 = torch.constant.float 9.9999997473787516E-6
    %int1_7663 = torch.constant.int 1
    %4873 = torch.aten.add.Scalar %4872, %float9.999990e-06_7662, %int1_7663 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4873, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4874 = torch.aten.rsqrt %4873 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4874, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4875 = torch.aten.mul.Tensor %4869, %4874 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4875, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_7664 = torch.constant.none
    %none_7665 = torch.constant.none
    %int6_7666 = torch.constant.int 6
    %cpu_7667 = torch.constant.device "cpu"
    %int0_7668 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4875, %none_7664, %none_7665, %int6_7666, %cpu_7667, %int0_7668 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7669 = torch.constant.int 5
    %4876 = torch.prims.convert_element_type %4875, %int5_7669 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4876, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %4877 = torch.aten.mul.Tensor %184, %4876 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %4877, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_7670 = torch.constant.none
    %none_7671 = torch.constant.none
    %int6_7672 = torch.constant.int 6
    %cpu_7673 = torch.constant.device "cpu"
    %int0_7674 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4877, %none_7670, %none_7671, %int6_7672, %cpu_7673, %int0_7674 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7675 = torch.constant.int 5
    %4878 = torch.prims.convert_element_type %4877, %int5_7675 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4878, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_7676 = torch.constant.int 2
    %4879 = torch.aten.view.dtype %185, %int2_7676 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %4880 = torch.aten.detach %4879 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_7677 = torch.constant.int -1
    %int17_7678 = torch.constant.int 17
    %4881 = torch.prim.ListConstruct %int-1_7677, %int17_7678 : (!torch.int, !torch.int) -> !torch.list<int>
    %4882 = torch.aten.view %4880, %4881 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_7679 = torch.constant.int 2048
    %int-1_7680 = torch.constant.int -1
    %int17_7681 = torch.constant.int 17
    %4883 = torch.prim.ListConstruct %int2048_7679, %int-1_7680, %int17_7681 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4884 = torch.aten.view %4882, %4883 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_7682 = torch.constant.int 2
    %int0_7683 = torch.constant.int 0
    %int1_7684 = torch.constant.int 1
    %int1_7685 = torch.constant.int 1
    %4885 = torch.aten.slice.Tensor %4884, %int2_7682, %int0_7683, %int1_7684, %int1_7685 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_7686 = torch.constant.int 5
    %4886 = torch.aten.view.dtype %4885, %int5_7686 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %4887 = torch.aten.detach %4886 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_7687 = torch.constant.int 2
    %int1_7688 = torch.constant.int 1
    %int9223372036854775807_7689 = torch.constant.int 9223372036854775807
    %int1_7690 = torch.constant.int 1
    %4888 = torch.aten.slice.Tensor %4884, %int2_7687, %int1_7688, %int9223372036854775807_7689, %int1_7690 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_7691 = torch.constant.int 1
    %4889 = torch.aten.view.dtype %4888, %int1_7691 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %4890 = torch.aten.detach %4889 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %4891 = torch_c.to_builtin_tensor %4878 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_7692 = tensor.cast %4891 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4892 = torch_c.to_builtin_tensor %4887 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %4893 = torch_c.to_builtin_tensor %4890 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %4894 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_7692, %4892, %4893) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_7693 = tensor.cast %4894 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %4895 = torch_c.from_builtin_tensor %cast_7693 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %4895, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_7694 = torch.constant.int 2
    %4896 = torch.aten.view.dtype %186, %int2_7694 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %4897 = torch.aten.detach %4896 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_7695 = torch.constant.int -1
    %int17_7696 = torch.constant.int 17
    %4898 = torch.prim.ListConstruct %int-1_7695, %int17_7696 : (!torch.int, !torch.int) -> !torch.list<int>
    %4899 = torch.aten.view %4897, %4898 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_7697 = torch.constant.int 256
    %int-1_7698 = torch.constant.int -1
    %int17_7699 = torch.constant.int 17
    %4900 = torch.prim.ListConstruct %int256_7697, %int-1_7698, %int17_7699 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4901 = torch.aten.view %4899, %4900 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_7700 = torch.constant.int 2
    %int0_7701 = torch.constant.int 0
    %int1_7702 = torch.constant.int 1
    %int1_7703 = torch.constant.int 1
    %4902 = torch.aten.slice.Tensor %4901, %int2_7700, %int0_7701, %int1_7702, %int1_7703 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_7704 = torch.constant.int 5
    %4903 = torch.aten.view.dtype %4902, %int5_7704 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %4904 = torch.aten.detach %4903 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_7705 = torch.constant.int 2
    %int1_7706 = torch.constant.int 1
    %int9223372036854775807_7707 = torch.constant.int 9223372036854775807
    %int1_7708 = torch.constant.int 1
    %4905 = torch.aten.slice.Tensor %4901, %int2_7705, %int1_7706, %int9223372036854775807_7707, %int1_7708 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_7709 = torch.constant.int 1
    %4906 = torch.aten.view.dtype %4905, %int1_7709 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %4907 = torch.aten.detach %4906 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %4908 = torch_c.to_builtin_tensor %4878 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_7710 = tensor.cast %4908 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4909 = torch_c.to_builtin_tensor %4904 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %4910 = torch_c.to_builtin_tensor %4907 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %4911 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_7710, %4909, %4910) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_7711 = tensor.cast %4911 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %4912 = torch_c.from_builtin_tensor %cast_7711 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %4912, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_7712 = torch.constant.int 2
    %4913 = torch.aten.view.dtype %187, %int2_7712 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %4914 = torch.aten.detach %4913 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_7713 = torch.constant.int -1
    %int17_7714 = torch.constant.int 17
    %4915 = torch.prim.ListConstruct %int-1_7713, %int17_7714 : (!torch.int, !torch.int) -> !torch.list<int>
    %4916 = torch.aten.view %4914, %4915 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_7715 = torch.constant.int 256
    %int-1_7716 = torch.constant.int -1
    %int17_7717 = torch.constant.int 17
    %4917 = torch.prim.ListConstruct %int256_7715, %int-1_7716, %int17_7717 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4918 = torch.aten.view %4916, %4917 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_7718 = torch.constant.int 2
    %int0_7719 = torch.constant.int 0
    %int1_7720 = torch.constant.int 1
    %int1_7721 = torch.constant.int 1
    %4919 = torch.aten.slice.Tensor %4918, %int2_7718, %int0_7719, %int1_7720, %int1_7721 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_7722 = torch.constant.int 5
    %4920 = torch.aten.view.dtype %4919, %int5_7722 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %4921 = torch.aten.detach %4920 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_7723 = torch.constant.int 2
    %int1_7724 = torch.constant.int 1
    %int9223372036854775807_7725 = torch.constant.int 9223372036854775807
    %int1_7726 = torch.constant.int 1
    %4922 = torch.aten.slice.Tensor %4918, %int2_7723, %int1_7724, %int9223372036854775807_7725, %int1_7726 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_7727 = torch.constant.int 1
    %4923 = torch.aten.view.dtype %4922, %int1_7727 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %4924 = torch.aten.detach %4923 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %4925 = torch_c.to_builtin_tensor %4878 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_7728 = tensor.cast %4925 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %4926 = torch_c.to_builtin_tensor %4921 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %4927 = torch_c.to_builtin_tensor %4924 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %4928 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_7728, %4926, %4927) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_7729 = tensor.cast %4928 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %4929 = torch_c.from_builtin_tensor %cast_7729 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %4929, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_7730 = torch.constant.int 4
    %int32_7731 = torch.constant.int 32
    %int64_7732 = torch.constant.int 64
    %4930 = torch.prim.ListConstruct %int4_7730, %273, %int32_7731, %int64_7732 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4931 = torch.aten.view %4895, %4930 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4931, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_7733 = torch.constant.int 4
    %int4_7734 = torch.constant.int 4
    %int64_7735 = torch.constant.int 64
    %4932 = torch.prim.ListConstruct %int4_7733, %273, %int4_7734, %int64_7735 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4933 = torch.aten.view %4912, %4932 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %4933, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_7736 = torch.constant.int 4
    %int4_7737 = torch.constant.int 4
    %int64_7738 = torch.constant.int 64
    %4934 = torch.prim.ListConstruct %int4_7736, %273, %int4_7737, %int64_7738 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4935 = torch.aten.view %4929, %4934 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %4935, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_7739 = torch.constant.int 0
    %none_7740 = torch.constant.none
    %none_7741 = torch.constant.none
    %cpu_7742 = torch.constant.device "cpu"
    %false_7743 = torch.constant.bool false
    %4936 = torch.aten.arange.start %int0_7739, %273, %none_7740, %none_7741, %cpu_7742, %false_7743 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4936, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_7744 = torch.constant.int 0
    %4937 = torch.aten.unsqueeze %4936, %int0_7744 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %4937, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_7745 = torch.constant.int 0
    %int64_7746 = torch.constant.int 64
    %int2_7747 = torch.constant.int 2
    %none_7748 = torch.constant.none
    %none_7749 = torch.constant.none
    %cpu_7750 = torch.constant.device "cpu"
    %false_7751 = torch.constant.bool false
    %4938 = torch.aten.arange.start_step %int0_7745, %int64_7746, %int2_7747, %none_7748, %none_7749, %cpu_7750, %false_7751 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_7752 = torch.constant.none
    %none_7753 = torch.constant.none
    %int4_7754 = torch.constant.int 4
    %cpu_7755 = torch.constant.device "cpu"
    %int0_7756 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4938, %none_7752, %none_7753, %int4_7754, %cpu_7755, %int0_7756 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7757 = torch.constant.int 6
    %4939 = torch.prims.convert_element_type %4938, %int6_7757 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_7758 = torch.constant.int 64
    %4940 = torch.aten.div.Scalar %4939, %int64_7758 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_7759 = torch.constant.float 1.000000e+04
    %4941 = torch.aten.pow.Scalar %float1.000000e04_7759, %4940 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %4942 = torch.aten.reciprocal %4941 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_7760 = torch.constant.float 1.000000e+00
    %4943 = torch.aten.mul.Scalar %4942, %float1.000000e00_7760 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_7761 = torch.constant.none
    %4944 = torch.aten.clone %181, %none_7761 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_7762 = torch.constant.int 0
    %4945 = torch.aten.unsqueeze %4943, %int0_7762 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_7763 = torch.constant.int 2
    %4946 = torch.aten.unsqueeze %4945, %int2_7763 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_7764 = torch.constant.none
    %none_7765 = torch.constant.none
    %int6_7766 = torch.constant.int 6
    %cpu_7767 = torch.constant.device "cpu"
    %int0_7768 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4946, %none_7764, %none_7765, %int6_7766, %cpu_7767, %int0_7768 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_7769 = torch.constant.int 1
    %int-1_7770 = torch.constant.int -1
    %int1_7771 = torch.constant.int 1
    %4947 = torch.prim.ListConstruct %int1_7769, %int-1_7770, %int1_7771 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7772 = torch.constant.bool false
    %4948 = torch.aten.expand %4946, %4947, %false_7772 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_7773 = torch.constant.int 1
    %4949 = torch.aten.unsqueeze %4937, %int1_7773 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %4949, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_7774 = torch.constant.none
    %none_7775 = torch.constant.none
    %int4_7776 = torch.constant.int 4
    %cpu_7777 = torch.constant.device "cpu"
    %int0_7778 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4949, %none_7774, %none_7775, %int4_7776, %cpu_7777, %int0_7778 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7779 = torch.constant.int 6
    %4950 = torch.prims.convert_element_type %4949, %int6_7779 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %4950, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %4951 = torch.aten.matmul %4948, %4950 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %4951, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_7780 = torch.constant.int 1
    %int2_7781 = torch.constant.int 2
    %4952 = torch.aten.transpose.int %4951, %int1_7780, %int2_7781 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4952, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4953 = torch.aten.cos %4952 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4953, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4954 = torch.aten.mul.Tensor %4953, %4944 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4954, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_7782 = torch.constant.none
    %none_7783 = torch.constant.none
    %int6_7784 = torch.constant.int 6
    %cpu_7785 = torch.constant.device "cpu"
    %int0_7786 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4954, %none_7782, %none_7783, %int6_7784, %cpu_7785, %int0_7786 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7787 = torch.constant.int 5
    %4955 = torch.prims.convert_element_type %4954, %int5_7787 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %4955, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %4956 = torch.aten.sin %4952 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4956, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4957 = torch.aten.mul.Tensor %4956, %4944 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4957, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_7788 = torch.constant.none
    %none_7789 = torch.constant.none
    %int6_7790 = torch.constant.int 6
    %cpu_7791 = torch.constant.device "cpu"
    %int0_7792 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4957, %none_7788, %none_7789, %int6_7790, %cpu_7791, %int0_7792 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7793 = torch.constant.int 5
    %4958 = torch.prims.convert_element_type %4957, %int5_7793 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %4958, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_7794 = torch.constant.int 2
    %4959 = torch.aten.unsqueeze %4955, %int2_7794 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %4959, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_7795 = torch.constant.int 2
    %4960 = torch.aten.unsqueeze %4958, %int2_7795 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %4960, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_7796 = torch.constant.none
    %none_7797 = torch.constant.none
    %int5_7798 = torch.constant.int 5
    %cpu_7799 = torch.constant.device "cpu"
    %int0_7800 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4959, %none_7796, %none_7797, %int5_7798, %cpu_7799, %int0_7800 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_7801 = torch.constant.none
    %none_7802 = torch.constant.none
    %int5_7803 = torch.constant.int 5
    %cpu_7804 = torch.constant.device "cpu"
    %int0_7805 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4960, %none_7801, %none_7802, %int5_7803, %cpu_7804, %int0_7805 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_7806 = torch.constant.none
    %none_7807 = torch.constant.none
    %int5_7808 = torch.constant.int 5
    %cpu_7809 = torch.constant.device "cpu"
    %int0_7810 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4931, %none_7806, %none_7807, %int5_7808, %cpu_7809, %int0_7810 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_7811 = torch.constant.int 3
    %int0_7812 = torch.constant.int 0
    %int64_7813 = torch.constant.int 64
    %int2_7814 = torch.constant.int 2
    %4961 = torch.aten.slice.Tensor %4931, %int3_7811, %int0_7812, %int64_7813, %int2_7814 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4961, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_7815 = torch.constant.int 3
    %int1_7816 = torch.constant.int 1
    %int64_7817 = torch.constant.int 64
    %int2_7818 = torch.constant.int 2
    %4962 = torch.aten.slice.Tensor %4931, %int3_7815, %int1_7816, %int64_7817, %int2_7818 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4962, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4963 = torch.aten.mul.Tensor %4961, %4959 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4963, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4964 = torch.aten.mul.Tensor %4962, %4960 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4964, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_7819 = torch.constant.int 1
    %4965 = torch.aten.sub.Tensor %4963, %4964, %int1_7819 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4965, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4966 = torch.aten.mul.Tensor %4962, %4959 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4966, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4967 = torch.aten.mul.Tensor %4961, %4960 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4967, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_7820 = torch.constant.int 1
    %4968 = torch.aten.add.Tensor %4966, %4967, %int1_7820 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %4968, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %4969 = torch_c.to_builtin_tensor %4965 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_7821 = tensor.cast %4969 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %4970 = torch_c.to_builtin_tensor %4968 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_7822 = tensor.cast %4970 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %4971 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_7821, %cast_7822) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_7823 = tensor.cast %4971 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %4972 = torch_c.from_builtin_tensor %cast_7823 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %4972, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_7824 = torch.constant.int 4
    %int32_7825 = torch.constant.int 32
    %int64_7826 = torch.constant.int 64
    %4973 = torch.prim.ListConstruct %int4_7824, %273, %int32_7825, %int64_7826 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4974 = torch.aten.view %4972, %4973 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4974, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_7827 = torch.constant.none
    %none_7828 = torch.constant.none
    %int5_7829 = torch.constant.int 5
    %cpu_7830 = torch.constant.device "cpu"
    %int0_7831 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4974, %none_7827, %none_7828, %int5_7829, %cpu_7830, %int0_7831 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_7832 = torch.constant.int 0
    %none_7833 = torch.constant.none
    %none_7834 = torch.constant.none
    %cpu_7835 = torch.constant.device "cpu"
    %false_7836 = torch.constant.bool false
    %4975 = torch.aten.arange.start %int0_7832, %273, %none_7833, %none_7834, %cpu_7835, %false_7836 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4975, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_7837 = torch.constant.int 0
    %4976 = torch.aten.unsqueeze %4975, %int0_7837 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %4976, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_7838 = torch.constant.int 0
    %int64_7839 = torch.constant.int 64
    %int2_7840 = torch.constant.int 2
    %none_7841 = torch.constant.none
    %none_7842 = torch.constant.none
    %cpu_7843 = torch.constant.device "cpu"
    %false_7844 = torch.constant.bool false
    %4977 = torch.aten.arange.start_step %int0_7838, %int64_7839, %int2_7840, %none_7841, %none_7842, %cpu_7843, %false_7844 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_7845 = torch.constant.none
    %none_7846 = torch.constant.none
    %int4_7847 = torch.constant.int 4
    %cpu_7848 = torch.constant.device "cpu"
    %int0_7849 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4977, %none_7845, %none_7846, %int4_7847, %cpu_7848, %int0_7849 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7850 = torch.constant.int 6
    %4978 = torch.prims.convert_element_type %4977, %int6_7850 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_7851 = torch.constant.int 64
    %4979 = torch.aten.div.Scalar %4978, %int64_7851 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_7852 = torch.constant.float 1.000000e+04
    %4980 = torch.aten.pow.Scalar %float1.000000e04_7852, %4979 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %4981 = torch.aten.reciprocal %4980 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_7853 = torch.constant.float 1.000000e+00
    %4982 = torch.aten.mul.Scalar %4981, %float1.000000e00_7853 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_7854 = torch.constant.none
    %4983 = torch.aten.clone %182, %none_7854 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_7855 = torch.constant.int 0
    %4984 = torch.aten.unsqueeze %4982, %int0_7855 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_7856 = torch.constant.int 2
    %4985 = torch.aten.unsqueeze %4984, %int2_7856 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_7857 = torch.constant.none
    %none_7858 = torch.constant.none
    %int6_7859 = torch.constant.int 6
    %cpu_7860 = torch.constant.device "cpu"
    %int0_7861 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4985, %none_7857, %none_7858, %int6_7859, %cpu_7860, %int0_7861 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_7862 = torch.constant.int 1
    %int-1_7863 = torch.constant.int -1
    %int1_7864 = torch.constant.int 1
    %4986 = torch.prim.ListConstruct %int1_7862, %int-1_7863, %int1_7864 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7865 = torch.constant.bool false
    %4987 = torch.aten.expand %4985, %4986, %false_7865 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_7866 = torch.constant.int 1
    %4988 = torch.aten.unsqueeze %4976, %int1_7866 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %4988, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_7867 = torch.constant.none
    %none_7868 = torch.constant.none
    %int4_7869 = torch.constant.int 4
    %cpu_7870 = torch.constant.device "cpu"
    %int0_7871 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4988, %none_7867, %none_7868, %int4_7869, %cpu_7870, %int0_7871 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7872 = torch.constant.int 6
    %4989 = torch.prims.convert_element_type %4988, %int6_7872 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %4989, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %4990 = torch.aten.matmul %4987, %4989 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %4990, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_7873 = torch.constant.int 1
    %int2_7874 = torch.constant.int 2
    %4991 = torch.aten.transpose.int %4990, %int1_7873, %int2_7874 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4991, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4992 = torch.aten.cos %4991 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4992, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4993 = torch.aten.mul.Tensor %4992, %4983 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4993, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_7875 = torch.constant.none
    %none_7876 = torch.constant.none
    %int6_7877 = torch.constant.int 6
    %cpu_7878 = torch.constant.device "cpu"
    %int0_7879 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4993, %none_7875, %none_7876, %int6_7877, %cpu_7878, %int0_7879 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7880 = torch.constant.int 5
    %4994 = torch.prims.convert_element_type %4993, %int5_7880 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %4994, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %4995 = torch.aten.sin %4991 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4995, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %4996 = torch.aten.mul.Tensor %4995, %4983 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %4996, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_7881 = torch.constant.none
    %none_7882 = torch.constant.none
    %int6_7883 = torch.constant.int 6
    %cpu_7884 = torch.constant.device "cpu"
    %int0_7885 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4996, %none_7881, %none_7882, %int6_7883, %cpu_7884, %int0_7885 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7886 = torch.constant.int 5
    %4997 = torch.prims.convert_element_type %4996, %int5_7886 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %4997, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_7887 = torch.constant.int 2
    %4998 = torch.aten.unsqueeze %4994, %int2_7887 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %4998, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_7888 = torch.constant.int 2
    %4999 = torch.aten.unsqueeze %4997, %int2_7888 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %4999, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_7889 = torch.constant.none
    %none_7890 = torch.constant.none
    %int5_7891 = torch.constant.int 5
    %cpu_7892 = torch.constant.device "cpu"
    %int0_7893 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4998, %none_7889, %none_7890, %int5_7891, %cpu_7892, %int0_7893 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_7894 = torch.constant.none
    %none_7895 = torch.constant.none
    %int5_7896 = torch.constant.int 5
    %cpu_7897 = torch.constant.device "cpu"
    %int0_7898 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4999, %none_7894, %none_7895, %int5_7896, %cpu_7897, %int0_7898 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_7899 = torch.constant.none
    %none_7900 = torch.constant.none
    %int5_7901 = torch.constant.int 5
    %cpu_7902 = torch.constant.device "cpu"
    %int0_7903 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4933, %none_7899, %none_7900, %int5_7901, %cpu_7902, %int0_7903 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_7904 = torch.constant.int 3
    %int0_7905 = torch.constant.int 0
    %int64_7906 = torch.constant.int 64
    %int2_7907 = torch.constant.int 2
    %5000 = torch.aten.slice.Tensor %4933, %int3_7904, %int0_7905, %int64_7906, %int2_7907 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5000, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_7908 = torch.constant.int 3
    %int1_7909 = torch.constant.int 1
    %int64_7910 = torch.constant.int 64
    %int2_7911 = torch.constant.int 2
    %5001 = torch.aten.slice.Tensor %4933, %int3_7908, %int1_7909, %int64_7910, %int2_7911 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5001, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5002 = torch.aten.mul.Tensor %5000, %4998 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5002, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5003 = torch.aten.mul.Tensor %5001, %4999 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5003, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_7912 = torch.constant.int 1
    %5004 = torch.aten.sub.Tensor %5002, %5003, %int1_7912 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5004, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5005 = torch.aten.mul.Tensor %5001, %4998 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5005, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5006 = torch.aten.mul.Tensor %5000, %4999 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5006, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_7913 = torch.constant.int 1
    %5007 = torch.aten.add.Tensor %5005, %5006, %int1_7913 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5007, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5008 = torch_c.to_builtin_tensor %5004 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_7914 = tensor.cast %5008 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %5009 = torch_c.to_builtin_tensor %5007 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_7915 = tensor.cast %5009 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %5010 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_7914, %cast_7915) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_7916 = tensor.cast %5010 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %5011 = torch_c.from_builtin_tensor %cast_7916 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %5011, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_7917 = torch.constant.int 4
    %int4_7918 = torch.constant.int 4
    %int64_7919 = torch.constant.int 64
    %5012 = torch.prim.ListConstruct %int4_7917, %273, %int4_7918, %int64_7919 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5013 = torch.aten.view %5011, %5012 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %5013, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_7920 = torch.constant.none
    %none_7921 = torch.constant.none
    %int5_7922 = torch.constant.int 5
    %cpu_7923 = torch.constant.device "cpu"
    %int0_7924 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5013, %none_7920, %none_7921, %int5_7922, %cpu_7923, %int0_7924 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_7925 = torch.constant.int 22
    %5014 = torch.aten.mul.Scalar %arg2, %int22_7925 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5014, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int15 = torch.constant.int 15
    %int1_7926 = torch.constant.int 1
    %5015 = torch.aten.add.Scalar %5014, %int15, %int1_7926 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5015, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_7927 = torch.constant.int 2
    %5016 = torch.aten.mul.Scalar %5015, %int2_7927 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5016, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_7928 = torch.constant.int 0
    %int1_7929 = torch.constant.int 1
    %5017 = torch.aten.add.Scalar %5016, %int0_7928, %int1_7929 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5017, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %5018 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %5019 = torch.aten.view %5017, %5018 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5019, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_7930 = torch.constant.int 4
    %int32_7931 = torch.constant.int 32
    %int4_7932 = torch.constant.int 4
    %int64_7933 = torch.constant.int 64
    %5020 = torch.prim.ListConstruct %int4_7930, %271, %int32_7931, %int4_7932, %int64_7933 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5021 = torch.aten.view %5013, %5020 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5021, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_7934 = torch.constant.int 32
    %int4_7935 = torch.constant.int 4
    %int64_7936 = torch.constant.int 64
    %5022 = torch.prim.ListConstruct %446, %int32_7934, %int4_7935, %int64_7936 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5023 = torch.aten.view %5021, %5022 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %5023, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_7937 = torch.constant.int 1
    %int2_7938 = torch.constant.int 2
    %5024 = torch.aten.transpose.int %5023, %int1_7937, %int2_7938 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5024, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_7939 = torch.constant.none
    %none_7940 = torch.constant.none
    %int5_7941 = torch.constant.int 5
    %cpu_7942 = torch.constant.device "cpu"
    %int0_7943 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5024, %none_7939, %none_7940, %int5_7941, %cpu_7942, %int0_7943 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_7944 = torch.constant.int 22
    %int2_7945 = torch.constant.int 2
    %int4_7946 = torch.constant.int 4
    %int32_7947 = torch.constant.int 32
    %int64_7948 = torch.constant.int 64
    %5025 = torch.prim.ListConstruct %272, %int22_7944, %int2_7945, %int4_7946, %int32_7947, %int64_7948 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5026 = torch.aten.view %4750, %5025 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5026, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_7949 = torch.constant.int 4
    %int32_7950 = torch.constant.int 32
    %int64_7951 = torch.constant.int 64
    %5027 = torch.prim.ListConstruct %439, %int4_7949, %int32_7950, %int64_7951 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5028 = torch.aten.view %5026, %5027 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5028, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %5029 = torch.prim.ListConstruct %5019 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_7952 = torch.constant.bool false
    %5030 = torch.aten.index_put %5028, %5029, %5024, %false_7952 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5030, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_7953 = torch.constant.int 22
    %int2_7954 = torch.constant.int 2
    %int4_7955 = torch.constant.int 4
    %int32_7956 = torch.constant.int 32
    %int64_7957 = torch.constant.int 64
    %5031 = torch.prim.ListConstruct %272, %int22_7953, %int2_7954, %int4_7955, %int32_7956, %int64_7957 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5032 = torch.aten.view %5030, %5031 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5032, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_7958 = torch.constant.int 360448
    %5033 = torch.prim.ListConstruct %272, %int360448_7958 : (!torch.int, !torch.int) -> !torch.list<int>
    %5034 = torch.aten.view %5032, %5033 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %5034, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_7959 = torch.constant.int 22
    %int2_7960 = torch.constant.int 2
    %int4_7961 = torch.constant.int 4
    %int32_7962 = torch.constant.int 32
    %int64_7963 = torch.constant.int 64
    %5035 = torch.prim.ListConstruct %272, %int22_7959, %int2_7960, %int4_7961, %int32_7962, %int64_7963 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5036 = torch.aten.view %5034, %5035 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5036, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_7964 = torch.constant.int 4
    %int32_7965 = torch.constant.int 32
    %int64_7966 = torch.constant.int 64
    %5037 = torch.prim.ListConstruct %439, %int4_7964, %int32_7965, %int64_7966 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5038 = torch.aten.view %5036, %5037 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5038, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_7967 = torch.constant.int 22
    %5039 = torch.aten.mul.Scalar %arg2, %int22_7967 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5039, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int15_7968 = torch.constant.int 15
    %int1_7969 = torch.constant.int 1
    %5040 = torch.aten.add.Scalar %5039, %int15_7968, %int1_7969 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5040, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_7970 = torch.constant.int 2
    %5041 = torch.aten.mul.Scalar %5040, %int2_7970 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5041, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_7971 = torch.constant.int 1
    %int1_7972 = torch.constant.int 1
    %5042 = torch.aten.add.Scalar %5041, %int1_7971, %int1_7972 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5042, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %5043 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %5044 = torch.aten.view %5042, %5043 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5044, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_7973 = torch.constant.int 4
    %int32_7974 = torch.constant.int 32
    %int4_7975 = torch.constant.int 4
    %int64_7976 = torch.constant.int 64
    %5045 = torch.prim.ListConstruct %int4_7973, %271, %int32_7974, %int4_7975, %int64_7976 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5046 = torch.aten.view %4935, %5045 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5046, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_7977 = torch.constant.int 32
    %int4_7978 = torch.constant.int 4
    %int64_7979 = torch.constant.int 64
    %5047 = torch.prim.ListConstruct %446, %int32_7977, %int4_7978, %int64_7979 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5048 = torch.aten.view %5046, %5047 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %5048, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_7980 = torch.constant.int 1
    %int2_7981 = torch.constant.int 2
    %5049 = torch.aten.transpose.int %5048, %int1_7980, %int2_7981 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5049, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_7982 = torch.constant.none
    %none_7983 = torch.constant.none
    %int5_7984 = torch.constant.int 5
    %cpu_7985 = torch.constant.device "cpu"
    %int0_7986 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5049, %none_7982, %none_7983, %int5_7984, %cpu_7985, %int0_7986 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %5050 = torch.prim.ListConstruct %5044 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_7987 = torch.constant.bool false
    %5051 = torch.aten.index_put %5038, %5050, %5049, %false_7987 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5051, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_7988 = torch.constant.int 22
    %int2_7989 = torch.constant.int 2
    %int4_7990 = torch.constant.int 4
    %int32_7991 = torch.constant.int 32
    %int64_7992 = torch.constant.int 64
    %5052 = torch.prim.ListConstruct %272, %int22_7988, %int2_7989, %int4_7990, %int32_7991, %int64_7992 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5053 = torch.aten.view %5051, %5052 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5053, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_7993 = torch.constant.int 360448
    %5054 = torch.prim.ListConstruct %272, %int360448_7993 : (!torch.int, !torch.int) -> !torch.list<int>
    %5055 = torch.aten.view %5053, %5054 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %5055, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_7994 = torch.constant.int 0
    %int1_7995 = torch.constant.int 1
    %none_7996 = torch.constant.none
    %none_7997 = torch.constant.none
    %cpu_7998 = torch.constant.device "cpu"
    %false_7999 = torch.constant.bool false
    %5056 = torch.aten.arange.start_step %int0_7994, %273, %int1_7995, %none_7996, %none_7997, %cpu_7998, %false_7999 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5056, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_8000 = torch.constant.int -1
    %5057 = torch.aten.unsqueeze %arg1, %int-1_8000 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %5058 = torch.aten.ge.Tensor %5056, %5057 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %5058, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_8001 = torch.constant.none
    %none_8002 = torch.constant.none
    %cpu_8003 = torch.constant.device "cpu"
    %false_8004 = torch.constant.bool false
    %5059 = torch.aten.arange %273, %none_8001, %none_8002, %cpu_8003, %false_8004 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5059, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_8005 = torch.constant.int 0
    %5060 = torch.aten.unsqueeze %5059, %int0_8005 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %5060, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_8006 = torch.constant.int 1
    %5061 = torch.aten.unsqueeze %5060, %int1_8006 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %5061, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_8007 = torch.constant.int 2
    %5062 = torch.aten.unsqueeze %5061, %int2_8007 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %5062, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_8008 = torch.constant.none
    %none_8009 = torch.constant.none
    %cpu_8010 = torch.constant.device "cpu"
    %false_8011 = torch.constant.bool false
    %5063 = torch.aten.arange %273, %none_8008, %none_8009, %cpu_8010, %false_8011 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5063, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_8012 = torch.constant.int 0
    %5064 = torch.aten.unsqueeze %5063, %int0_8012 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %5064, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_8013 = torch.constant.int 1
    %5065 = torch.aten.unsqueeze %5064, %int1_8013 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %5065, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_8014 = torch.constant.int 3
    %5066 = torch.aten.unsqueeze %5065, %int3_8014 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %5066, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %5067 = torch.aten.gt.Tensor %5062, %5066 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %5067, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_8015 = torch.constant.int 1
    %5068 = torch.aten.unsqueeze %5058, %int1_8015 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %5068, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_8016 = torch.constant.int 2
    %5069 = torch.aten.unsqueeze %5068, %int2_8016 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %5069, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %5070 = torch.aten.logical_or %5067, %5069 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %5070, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_8017 = torch.constant.none
    %5071 = torch.aten.clone %183, %none_8017 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_8018 = torch.constant.int 0
    %5072 = torch.aten.where.ScalarOther %5070, %5071, %int0_8018 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %5072, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_8019 = torch.constant.none
    %none_8020 = torch.constant.none
    %int5_8021 = torch.constant.int 5
    %cpu_8022 = torch.constant.device "cpu"
    %int0_8023 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5072, %none_8019, %none_8020, %int5_8021, %cpu_8022, %int0_8023 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_8024 = torch.constant.int -2
    %5073 = torch.aten.unsqueeze %5013, %int-2_8024 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %5073, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_8025 = torch.constant.int 4
    %int4_8026 = torch.constant.int 4
    %int8_8027 = torch.constant.int 8
    %int64_8028 = torch.constant.int 64
    %5074 = torch.prim.ListConstruct %int4_8025, %273, %int4_8026, %int8_8027, %int64_8028 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8029 = torch.constant.bool false
    %5075 = torch.aten.expand %5073, %5074, %false_8029 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5075, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_8030 = torch.constant.int 0
    %5076 = torch.aten.clone %5075, %int0_8030 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5076, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_8031 = torch.constant.int 4
    %int32_8032 = torch.constant.int 32
    %int64_8033 = torch.constant.int 64
    %5077 = torch.prim.ListConstruct %int4_8031, %273, %int32_8032, %int64_8033 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5078 = torch.aten._unsafe_view %5076, %5077 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5078, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_8034 = torch.constant.int -2
    %5079 = torch.aten.unsqueeze %4935, %int-2_8034 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %5079, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_8035 = torch.constant.int 4
    %int4_8036 = torch.constant.int 4
    %int8_8037 = torch.constant.int 8
    %int64_8038 = torch.constant.int 64
    %5080 = torch.prim.ListConstruct %int4_8035, %273, %int4_8036, %int8_8037, %int64_8038 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8039 = torch.constant.bool false
    %5081 = torch.aten.expand %5079, %5080, %false_8039 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5081, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_8040 = torch.constant.int 0
    %5082 = torch.aten.clone %5081, %int0_8040 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5082, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_8041 = torch.constant.int 4
    %int32_8042 = torch.constant.int 32
    %int64_8043 = torch.constant.int 64
    %5083 = torch.prim.ListConstruct %int4_8041, %273, %int32_8042, %int64_8043 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5084 = torch.aten._unsafe_view %5082, %5083 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5084, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_8044 = torch.constant.int 1
    %int2_8045 = torch.constant.int 2
    %5085 = torch.aten.transpose.int %4974, %int1_8044, %int2_8045 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %5085, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_8046 = torch.constant.int 1
    %int2_8047 = torch.constant.int 2
    %5086 = torch.aten.transpose.int %5078, %int1_8046, %int2_8047 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %5086, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_8048 = torch.constant.int 1
    %int2_8049 = torch.constant.int 2
    %5087 = torch.aten.transpose.int %5084, %int1_8048, %int2_8049 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %5087, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_8050 = torch.constant.float 0.000000e+00
    %false_8051 = torch.constant.bool false
    %none_8052 = torch.constant.none
    %false_8053 = torch.constant.bool false
    %5088 = torch.aten.scaled_dot_product_attention %5085, %5086, %5087, %5072, %float0.000000e00_8050, %false_8051, %none_8052, %false_8053 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %5088, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_8054 = torch.constant.int 1
    %int2_8055 = torch.constant.int 2
    %5089 = torch.aten.transpose.int %5088, %int1_8054, %int2_8055 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5089, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_8056 = torch.constant.int 4
    %int2048_8057 = torch.constant.int 2048
    %5090 = torch.prim.ListConstruct %int4_8056, %273, %int2048_8057 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5091 = torch.aten.view %5089, %5090 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5091, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_8058 = torch.constant.int 2
    %5092 = torch.aten.view.dtype %188, %int2_8058 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %5093 = torch.aten.detach %5092 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_8059 = torch.constant.int -1
    %int17_8060 = torch.constant.int 17
    %5094 = torch.prim.ListConstruct %int-1_8059, %int17_8060 : (!torch.int, !torch.int) -> !torch.list<int>
    %5095 = torch.aten.view %5093, %5094 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_8061 = torch.constant.int 2048
    %int-1_8062 = torch.constant.int -1
    %int17_8063 = torch.constant.int 17
    %5096 = torch.prim.ListConstruct %int2048_8061, %int-1_8062, %int17_8063 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5097 = torch.aten.view %5095, %5096 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_8064 = torch.constant.int 2
    %int0_8065 = torch.constant.int 0
    %int1_8066 = torch.constant.int 1
    %int1_8067 = torch.constant.int 1
    %5098 = torch.aten.slice.Tensor %5097, %int2_8064, %int0_8065, %int1_8066, %int1_8067 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_8068 = torch.constant.int 5
    %5099 = torch.aten.view.dtype %5098, %int5_8068 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %5100 = torch.aten.detach %5099 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_8069 = torch.constant.int 2
    %int1_8070 = torch.constant.int 1
    %int9223372036854775807_8071 = torch.constant.int 9223372036854775807
    %int1_8072 = torch.constant.int 1
    %5101 = torch.aten.slice.Tensor %5097, %int2_8069, %int1_8070, %int9223372036854775807_8071, %int1_8072 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_8073 = torch.constant.int 1
    %5102 = torch.aten.view.dtype %5101, %int1_8073 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %5103 = torch.aten.detach %5102 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %5104 = torch_c.to_builtin_tensor %5091 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_8074 = tensor.cast %5104 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %5105 = torch_c.to_builtin_tensor %5100 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %5106 = torch_c.to_builtin_tensor %5103 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %5107 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_8074, %5105, %5106) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_8075 = tensor.cast %5107 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %5108 = torch_c.from_builtin_tensor %cast_8075 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5108, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_8076 = torch.constant.none
    %none_8077 = torch.constant.none
    %int5_8078 = torch.constant.int 5
    %cpu_8079 = torch.constant.device "cpu"
    %int0_8080 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5108, %none_8076, %none_8077, %int5_8078, %cpu_8079, %int0_8080 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_8081 = torch.constant.int 1
    %5109 = torch.aten.add.Tensor %4868, %5108, %int1_8081 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5109, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_8082 = torch.constant.none
    %none_8083 = torch.constant.none
    %int5_8084 = torch.constant.int 5
    %cpu_8085 = torch.constant.device "cpu"
    %int0_8086 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5109, %none_8082, %none_8083, %int5_8084, %cpu_8085, %int0_8086 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8087 = torch.constant.int 6
    %5110 = torch.prims.convert_element_type %5109, %int6_8087 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5110, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_8088 = torch.constant.int 2
    %5111 = torch.aten.pow.Tensor_Scalar %5110, %int2_8088 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5111, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_8089 = torch.constant.int -1
    %5112 = torch.prim.ListConstruct %int-1_8089 : (!torch.int) -> !torch.list<int>
    %true_8090 = torch.constant.bool true
    %none_8091 = torch.constant.none
    %5113 = torch.aten.mean.dim %5111, %5112, %true_8090, %none_8091 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5113, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_8092 = torch.constant.float 9.9999997473787516E-6
    %int1_8093 = torch.constant.int 1
    %5114 = torch.aten.add.Scalar %5113, %float9.999990e-06_8092, %int1_8093 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5114, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5115 = torch.aten.rsqrt %5114 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5115, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5116 = torch.aten.mul.Tensor %5110, %5115 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5116, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_8094 = torch.constant.none
    %none_8095 = torch.constant.none
    %int6_8096 = torch.constant.int 6
    %cpu_8097 = torch.constant.device "cpu"
    %int0_8098 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5116, %none_8094, %none_8095, %int6_8096, %cpu_8097, %int0_8098 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8099 = torch.constant.int 5
    %5117 = torch.prims.convert_element_type %5116, %int5_8099 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5117, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %5118 = torch.aten.mul.Tensor %189, %5117 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5118, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_8100 = torch.constant.none
    %none_8101 = torch.constant.none
    %int6_8102 = torch.constant.int 6
    %cpu_8103 = torch.constant.device "cpu"
    %int0_8104 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5118, %none_8100, %none_8101, %int6_8102, %cpu_8103, %int0_8104 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8105 = torch.constant.int 5
    %5119 = torch.prims.convert_element_type %5118, %int5_8105 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5119, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_8106 = torch.constant.int 2
    %5120 = torch.aten.view.dtype %190, %int2_8106 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %5121 = torch.aten.detach %5120 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_8107 = torch.constant.int -1
    %int17_8108 = torch.constant.int 17
    %5122 = torch.prim.ListConstruct %int-1_8107, %int17_8108 : (!torch.int, !torch.int) -> !torch.list<int>
    %5123 = torch.aten.view %5121, %5122 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_8109 = torch.constant.int 5632
    %int-1_8110 = torch.constant.int -1
    %int17_8111 = torch.constant.int 17
    %5124 = torch.prim.ListConstruct %int5632_8109, %int-1_8110, %int17_8111 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5125 = torch.aten.view %5123, %5124 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_8112 = torch.constant.int 2
    %int0_8113 = torch.constant.int 0
    %int1_8114 = torch.constant.int 1
    %int1_8115 = torch.constant.int 1
    %5126 = torch.aten.slice.Tensor %5125, %int2_8112, %int0_8113, %int1_8114, %int1_8115 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_8116 = torch.constant.int 5
    %5127 = torch.aten.view.dtype %5126, %int5_8116 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %5128 = torch.aten.detach %5127 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_8117 = torch.constant.int 2
    %int1_8118 = torch.constant.int 1
    %int9223372036854775807_8119 = torch.constant.int 9223372036854775807
    %int1_8120 = torch.constant.int 1
    %5129 = torch.aten.slice.Tensor %5125, %int2_8117, %int1_8118, %int9223372036854775807_8119, %int1_8120 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_8121 = torch.constant.int 1
    %5130 = torch.aten.view.dtype %5129, %int1_8121 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %5131 = torch.aten.detach %5130 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %5132 = torch_c.to_builtin_tensor %5119 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_8122 = tensor.cast %5132 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %5133 = torch_c.to_builtin_tensor %5128 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %5134 = torch_c.to_builtin_tensor %5131 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %5135 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_8122, %5133, %5134) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_8123 = tensor.cast %5135 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %5136 = torch_c.from_builtin_tensor %cast_8123 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %5136, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %5137 = torch.aten.silu %5136 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %5137, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_8124 = torch.constant.int 2
    %5138 = torch.aten.view.dtype %191, %int2_8124 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %5139 = torch.aten.detach %5138 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_8125 = torch.constant.int -1
    %int17_8126 = torch.constant.int 17
    %5140 = torch.prim.ListConstruct %int-1_8125, %int17_8126 : (!torch.int, !torch.int) -> !torch.list<int>
    %5141 = torch.aten.view %5139, %5140 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_8127 = torch.constant.int 5632
    %int-1_8128 = torch.constant.int -1
    %int17_8129 = torch.constant.int 17
    %5142 = torch.prim.ListConstruct %int5632_8127, %int-1_8128, %int17_8129 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5143 = torch.aten.view %5141, %5142 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_8130 = torch.constant.int 2
    %int0_8131 = torch.constant.int 0
    %int1_8132 = torch.constant.int 1
    %int1_8133 = torch.constant.int 1
    %5144 = torch.aten.slice.Tensor %5143, %int2_8130, %int0_8131, %int1_8132, %int1_8133 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_8134 = torch.constant.int 5
    %5145 = torch.aten.view.dtype %5144, %int5_8134 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %5146 = torch.aten.detach %5145 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_8135 = torch.constant.int 2
    %int1_8136 = torch.constant.int 1
    %int9223372036854775807_8137 = torch.constant.int 9223372036854775807
    %int1_8138 = torch.constant.int 1
    %5147 = torch.aten.slice.Tensor %5143, %int2_8135, %int1_8136, %int9223372036854775807_8137, %int1_8138 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_8139 = torch.constant.int 1
    %5148 = torch.aten.view.dtype %5147, %int1_8139 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %5149 = torch.aten.detach %5148 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %5150 = torch_c.to_builtin_tensor %5119 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_8140 = tensor.cast %5150 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %5151 = torch_c.to_builtin_tensor %5146 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %5152 = torch_c.to_builtin_tensor %5149 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %5153 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_8140, %5151, %5152) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_8141 = tensor.cast %5153 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %5154 = torch_c.from_builtin_tensor %cast_8141 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %5154, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %5155 = torch.aten.mul.Tensor %5137, %5154 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %5155, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_8142 = torch.constant.int 2
    %5156 = torch.aten.view.dtype %192, %int2_8142 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %5157 = torch.aten.detach %5156 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_8143 = torch.constant.int -1
    %int17_8144 = torch.constant.int 17
    %5158 = torch.prim.ListConstruct %int-1_8143, %int17_8144 : (!torch.int, !torch.int) -> !torch.list<int>
    %5159 = torch.aten.view %5157, %5158 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_8145 = torch.constant.int 2048
    %int-1_8146 = torch.constant.int -1
    %int17_8147 = torch.constant.int 17
    %5160 = torch.prim.ListConstruct %int2048_8145, %int-1_8146, %int17_8147 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5161 = torch.aten.view %5159, %5160 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_8148 = torch.constant.int 2
    %int0_8149 = torch.constant.int 0
    %int1_8150 = torch.constant.int 1
    %int1_8151 = torch.constant.int 1
    %5162 = torch.aten.slice.Tensor %5161, %int2_8148, %int0_8149, %int1_8150, %int1_8151 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_8152 = torch.constant.int 5
    %5163 = torch.aten.view.dtype %5162, %int5_8152 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %5164 = torch.aten.detach %5163 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_8153 = torch.constant.int 2
    %int1_8154 = torch.constant.int 1
    %int9223372036854775807_8155 = torch.constant.int 9223372036854775807
    %int1_8156 = torch.constant.int 1
    %5165 = torch.aten.slice.Tensor %5161, %int2_8153, %int1_8154, %int9223372036854775807_8155, %int1_8156 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_8157 = torch.constant.int 1
    %5166 = torch.aten.view.dtype %5165, %int1_8157 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %5167 = torch.aten.detach %5166 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %5168 = torch_c.to_builtin_tensor %5155 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_8158 = tensor.cast %5168 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %5169 = torch_c.to_builtin_tensor %5164 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %5170 = torch_c.to_builtin_tensor %5167 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %5171 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_8158, %5169, %5170) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_8159 = tensor.cast %5171 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %5172 = torch_c.from_builtin_tensor %cast_8159 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5172, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_8160 = torch.constant.int 1
    %5173 = torch.aten.add.Tensor %5109, %5172, %int1_8160 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5173, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_8161 = torch.constant.none
    %none_8162 = torch.constant.none
    %int5_8163 = torch.constant.int 5
    %cpu_8164 = torch.constant.device "cpu"
    %int0_8165 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5173, %none_8161, %none_8162, %int5_8163, %cpu_8164, %int0_8165 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8166 = torch.constant.int 6
    %5174 = torch.prims.convert_element_type %5173, %int6_8166 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5174, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_8167 = torch.constant.int 2
    %5175 = torch.aten.pow.Tensor_Scalar %5174, %int2_8167 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5175, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_8168 = torch.constant.int -1
    %5176 = torch.prim.ListConstruct %int-1_8168 : (!torch.int) -> !torch.list<int>
    %true_8169 = torch.constant.bool true
    %none_8170 = torch.constant.none
    %5177 = torch.aten.mean.dim %5175, %5176, %true_8169, %none_8170 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5177, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_8171 = torch.constant.float 9.9999997473787516E-6
    %int1_8172 = torch.constant.int 1
    %5178 = torch.aten.add.Scalar %5177, %float9.999990e-06_8171, %int1_8172 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5178, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5179 = torch.aten.rsqrt %5178 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5179, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5180 = torch.aten.mul.Tensor %5174, %5179 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5180, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_8173 = torch.constant.none
    %none_8174 = torch.constant.none
    %int6_8175 = torch.constant.int 6
    %cpu_8176 = torch.constant.device "cpu"
    %int0_8177 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5180, %none_8173, %none_8174, %int6_8175, %cpu_8176, %int0_8177 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8178 = torch.constant.int 5
    %5181 = torch.prims.convert_element_type %5180, %int5_8178 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5181, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %5182 = torch.aten.mul.Tensor %196, %5181 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5182, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_8179 = torch.constant.none
    %none_8180 = torch.constant.none
    %int6_8181 = torch.constant.int 6
    %cpu_8182 = torch.constant.device "cpu"
    %int0_8183 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5182, %none_8179, %none_8180, %int6_8181, %cpu_8182, %int0_8183 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8184 = torch.constant.int 5
    %5183 = torch.prims.convert_element_type %5182, %int5_8184 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5183, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_8185 = torch.constant.int 2
    %5184 = torch.aten.view.dtype %197, %int2_8185 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %5185 = torch.aten.detach %5184 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_8186 = torch.constant.int -1
    %int17_8187 = torch.constant.int 17
    %5186 = torch.prim.ListConstruct %int-1_8186, %int17_8187 : (!torch.int, !torch.int) -> !torch.list<int>
    %5187 = torch.aten.view %5185, %5186 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_8188 = torch.constant.int 2048
    %int-1_8189 = torch.constant.int -1
    %int17_8190 = torch.constant.int 17
    %5188 = torch.prim.ListConstruct %int2048_8188, %int-1_8189, %int17_8190 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5189 = torch.aten.view %5187, %5188 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_8191 = torch.constant.int 2
    %int0_8192 = torch.constant.int 0
    %int1_8193 = torch.constant.int 1
    %int1_8194 = torch.constant.int 1
    %5190 = torch.aten.slice.Tensor %5189, %int2_8191, %int0_8192, %int1_8193, %int1_8194 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_8195 = torch.constant.int 5
    %5191 = torch.aten.view.dtype %5190, %int5_8195 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %5192 = torch.aten.detach %5191 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_8196 = torch.constant.int 2
    %int1_8197 = torch.constant.int 1
    %int9223372036854775807_8198 = torch.constant.int 9223372036854775807
    %int1_8199 = torch.constant.int 1
    %5193 = torch.aten.slice.Tensor %5189, %int2_8196, %int1_8197, %int9223372036854775807_8198, %int1_8199 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_8200 = torch.constant.int 1
    %5194 = torch.aten.view.dtype %5193, %int1_8200 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %5195 = torch.aten.detach %5194 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %5196 = torch_c.to_builtin_tensor %5183 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_8201 = tensor.cast %5196 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %5197 = torch_c.to_builtin_tensor %5192 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %5198 = torch_c.to_builtin_tensor %5195 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %5199 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_8201, %5197, %5198) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_8202 = tensor.cast %5199 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %5200 = torch_c.from_builtin_tensor %cast_8202 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5200, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_8203 = torch.constant.int 2
    %5201 = torch.aten.view.dtype %198, %int2_8203 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %5202 = torch.aten.detach %5201 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_8204 = torch.constant.int -1
    %int17_8205 = torch.constant.int 17
    %5203 = torch.prim.ListConstruct %int-1_8204, %int17_8205 : (!torch.int, !torch.int) -> !torch.list<int>
    %5204 = torch.aten.view %5202, %5203 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_8206 = torch.constant.int 256
    %int-1_8207 = torch.constant.int -1
    %int17_8208 = torch.constant.int 17
    %5205 = torch.prim.ListConstruct %int256_8206, %int-1_8207, %int17_8208 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5206 = torch.aten.view %5204, %5205 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_8209 = torch.constant.int 2
    %int0_8210 = torch.constant.int 0
    %int1_8211 = torch.constant.int 1
    %int1_8212 = torch.constant.int 1
    %5207 = torch.aten.slice.Tensor %5206, %int2_8209, %int0_8210, %int1_8211, %int1_8212 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_8213 = torch.constant.int 5
    %5208 = torch.aten.view.dtype %5207, %int5_8213 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %5209 = torch.aten.detach %5208 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_8214 = torch.constant.int 2
    %int1_8215 = torch.constant.int 1
    %int9223372036854775807_8216 = torch.constant.int 9223372036854775807
    %int1_8217 = torch.constant.int 1
    %5210 = torch.aten.slice.Tensor %5206, %int2_8214, %int1_8215, %int9223372036854775807_8216, %int1_8217 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_8218 = torch.constant.int 1
    %5211 = torch.aten.view.dtype %5210, %int1_8218 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %5212 = torch.aten.detach %5211 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %5213 = torch_c.to_builtin_tensor %5183 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_8219 = tensor.cast %5213 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %5214 = torch_c.to_builtin_tensor %5209 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %5215 = torch_c.to_builtin_tensor %5212 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %5216 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_8219, %5214, %5215) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_8220 = tensor.cast %5216 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %5217 = torch_c.from_builtin_tensor %cast_8220 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %5217, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_8221 = torch.constant.int 2
    %5218 = torch.aten.view.dtype %199, %int2_8221 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %5219 = torch.aten.detach %5218 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_8222 = torch.constant.int -1
    %int17_8223 = torch.constant.int 17
    %5220 = torch.prim.ListConstruct %int-1_8222, %int17_8223 : (!torch.int, !torch.int) -> !torch.list<int>
    %5221 = torch.aten.view %5219, %5220 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_8224 = torch.constant.int 256
    %int-1_8225 = torch.constant.int -1
    %int17_8226 = torch.constant.int 17
    %5222 = torch.prim.ListConstruct %int256_8224, %int-1_8225, %int17_8226 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5223 = torch.aten.view %5221, %5222 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_8227 = torch.constant.int 2
    %int0_8228 = torch.constant.int 0
    %int1_8229 = torch.constant.int 1
    %int1_8230 = torch.constant.int 1
    %5224 = torch.aten.slice.Tensor %5223, %int2_8227, %int0_8228, %int1_8229, %int1_8230 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_8231 = torch.constant.int 5
    %5225 = torch.aten.view.dtype %5224, %int5_8231 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %5226 = torch.aten.detach %5225 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_8232 = torch.constant.int 2
    %int1_8233 = torch.constant.int 1
    %int9223372036854775807_8234 = torch.constant.int 9223372036854775807
    %int1_8235 = torch.constant.int 1
    %5227 = torch.aten.slice.Tensor %5223, %int2_8232, %int1_8233, %int9223372036854775807_8234, %int1_8235 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_8236 = torch.constant.int 1
    %5228 = torch.aten.view.dtype %5227, %int1_8236 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %5229 = torch.aten.detach %5228 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %5230 = torch_c.to_builtin_tensor %5183 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_8237 = tensor.cast %5230 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %5231 = torch_c.to_builtin_tensor %5226 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %5232 = torch_c.to_builtin_tensor %5229 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %5233 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_8237, %5231, %5232) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_8238 = tensor.cast %5233 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %5234 = torch_c.from_builtin_tensor %cast_8238 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %5234, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_8239 = torch.constant.int 4
    %int32_8240 = torch.constant.int 32
    %int64_8241 = torch.constant.int 64
    %5235 = torch.prim.ListConstruct %int4_8239, %273, %int32_8240, %int64_8241 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5236 = torch.aten.view %5200, %5235 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5236, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_8242 = torch.constant.int 4
    %int4_8243 = torch.constant.int 4
    %int64_8244 = torch.constant.int 64
    %5237 = torch.prim.ListConstruct %int4_8242, %273, %int4_8243, %int64_8244 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5238 = torch.aten.view %5217, %5237 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %5238, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_8245 = torch.constant.int 4
    %int4_8246 = torch.constant.int 4
    %int64_8247 = torch.constant.int 64
    %5239 = torch.prim.ListConstruct %int4_8245, %273, %int4_8246, %int64_8247 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5240 = torch.aten.view %5234, %5239 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %5240, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_8248 = torch.constant.int 0
    %none_8249 = torch.constant.none
    %none_8250 = torch.constant.none
    %cpu_8251 = torch.constant.device "cpu"
    %false_8252 = torch.constant.bool false
    %5241 = torch.aten.arange.start %int0_8248, %273, %none_8249, %none_8250, %cpu_8251, %false_8252 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5241, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_8253 = torch.constant.int 0
    %5242 = torch.aten.unsqueeze %5241, %int0_8253 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %5242, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_8254 = torch.constant.int 0
    %int64_8255 = torch.constant.int 64
    %int2_8256 = torch.constant.int 2
    %none_8257 = torch.constant.none
    %none_8258 = torch.constant.none
    %cpu_8259 = torch.constant.device "cpu"
    %false_8260 = torch.constant.bool false
    %5243 = torch.aten.arange.start_step %int0_8254, %int64_8255, %int2_8256, %none_8257, %none_8258, %cpu_8259, %false_8260 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_8261 = torch.constant.none
    %none_8262 = torch.constant.none
    %int4_8263 = torch.constant.int 4
    %cpu_8264 = torch.constant.device "cpu"
    %int0_8265 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5243, %none_8261, %none_8262, %int4_8263, %cpu_8264, %int0_8265 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8266 = torch.constant.int 6
    %5244 = torch.prims.convert_element_type %5243, %int6_8266 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_8267 = torch.constant.int 64
    %5245 = torch.aten.div.Scalar %5244, %int64_8267 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_8268 = torch.constant.float 1.000000e+04
    %5246 = torch.aten.pow.Scalar %float1.000000e04_8268, %5245 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %5247 = torch.aten.reciprocal %5246 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_8269 = torch.constant.float 1.000000e+00
    %5248 = torch.aten.mul.Scalar %5247, %float1.000000e00_8269 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_8270 = torch.constant.none
    %5249 = torch.aten.clone %193, %none_8270 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_8271 = torch.constant.int 0
    %5250 = torch.aten.unsqueeze %5248, %int0_8271 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_8272 = torch.constant.int 2
    %5251 = torch.aten.unsqueeze %5250, %int2_8272 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_8273 = torch.constant.none
    %none_8274 = torch.constant.none
    %int6_8275 = torch.constant.int 6
    %cpu_8276 = torch.constant.device "cpu"
    %int0_8277 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5251, %none_8273, %none_8274, %int6_8275, %cpu_8276, %int0_8277 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_8278 = torch.constant.int 1
    %int-1_8279 = torch.constant.int -1
    %int1_8280 = torch.constant.int 1
    %5252 = torch.prim.ListConstruct %int1_8278, %int-1_8279, %int1_8280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8281 = torch.constant.bool false
    %5253 = torch.aten.expand %5251, %5252, %false_8281 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_8282 = torch.constant.int 1
    %5254 = torch.aten.unsqueeze %5242, %int1_8282 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %5254, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_8283 = torch.constant.none
    %none_8284 = torch.constant.none
    %int4_8285 = torch.constant.int 4
    %cpu_8286 = torch.constant.device "cpu"
    %int0_8287 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5254, %none_8283, %none_8284, %int4_8285, %cpu_8286, %int0_8287 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8288 = torch.constant.int 6
    %5255 = torch.prims.convert_element_type %5254, %int6_8288 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %5255, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %5256 = torch.aten.matmul %5253, %5255 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %5256, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_8289 = torch.constant.int 1
    %int2_8290 = torch.constant.int 2
    %5257 = torch.aten.transpose.int %5256, %int1_8289, %int2_8290 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5257, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %5258 = torch.aten.cos %5257 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5258, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %5259 = torch.aten.mul.Tensor %5258, %5249 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5259, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_8291 = torch.constant.none
    %none_8292 = torch.constant.none
    %int6_8293 = torch.constant.int 6
    %cpu_8294 = torch.constant.device "cpu"
    %int0_8295 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5259, %none_8291, %none_8292, %int6_8293, %cpu_8294, %int0_8295 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8296 = torch.constant.int 5
    %5260 = torch.prims.convert_element_type %5259, %int5_8296 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %5260, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %5261 = torch.aten.sin %5257 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5261, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %5262 = torch.aten.mul.Tensor %5261, %5249 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5262, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_8297 = torch.constant.none
    %none_8298 = torch.constant.none
    %int6_8299 = torch.constant.int 6
    %cpu_8300 = torch.constant.device "cpu"
    %int0_8301 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5262, %none_8297, %none_8298, %int6_8299, %cpu_8300, %int0_8301 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8302 = torch.constant.int 5
    %5263 = torch.prims.convert_element_type %5262, %int5_8302 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %5263, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_8303 = torch.constant.int 2
    %5264 = torch.aten.unsqueeze %5260, %int2_8303 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %5264, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_8304 = torch.constant.int 2
    %5265 = torch.aten.unsqueeze %5263, %int2_8304 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %5265, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_8305 = torch.constant.none
    %none_8306 = torch.constant.none
    %int5_8307 = torch.constant.int 5
    %cpu_8308 = torch.constant.device "cpu"
    %int0_8309 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5264, %none_8305, %none_8306, %int5_8307, %cpu_8308, %int0_8309 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_8310 = torch.constant.none
    %none_8311 = torch.constant.none
    %int5_8312 = torch.constant.int 5
    %cpu_8313 = torch.constant.device "cpu"
    %int0_8314 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5265, %none_8310, %none_8311, %int5_8312, %cpu_8313, %int0_8314 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_8315 = torch.constant.none
    %none_8316 = torch.constant.none
    %int5_8317 = torch.constant.int 5
    %cpu_8318 = torch.constant.device "cpu"
    %int0_8319 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5236, %none_8315, %none_8316, %int5_8317, %cpu_8318, %int0_8319 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_8320 = torch.constant.int 3
    %int0_8321 = torch.constant.int 0
    %int64_8322 = torch.constant.int 64
    %int2_8323 = torch.constant.int 2
    %5266 = torch.aten.slice.Tensor %5236, %int3_8320, %int0_8321, %int64_8322, %int2_8323 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5266, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_8324 = torch.constant.int 3
    %int1_8325 = torch.constant.int 1
    %int64_8326 = torch.constant.int 64
    %int2_8327 = torch.constant.int 2
    %5267 = torch.aten.slice.Tensor %5236, %int3_8324, %int1_8325, %int64_8326, %int2_8327 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5267, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %5268 = torch.aten.mul.Tensor %5266, %5264 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5268, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %5269 = torch.aten.mul.Tensor %5267, %5265 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5269, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_8328 = torch.constant.int 1
    %5270 = torch.aten.sub.Tensor %5268, %5269, %int1_8328 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5270, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %5271 = torch.aten.mul.Tensor %5267, %5264 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5271, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %5272 = torch.aten.mul.Tensor %5266, %5265 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5272, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_8329 = torch.constant.int 1
    %5273 = torch.aten.add.Tensor %5271, %5272, %int1_8329 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5273, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %5274 = torch_c.to_builtin_tensor %5270 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_8330 = tensor.cast %5274 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %5275 = torch_c.to_builtin_tensor %5273 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_8331 = tensor.cast %5275 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %5276 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_8330, %cast_8331) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_8332 = tensor.cast %5276 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %5277 = torch_c.from_builtin_tensor %cast_8332 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %5277, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_8333 = torch.constant.int 4
    %int32_8334 = torch.constant.int 32
    %int64_8335 = torch.constant.int 64
    %5278 = torch.prim.ListConstruct %int4_8333, %273, %int32_8334, %int64_8335 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5279 = torch.aten.view %5277, %5278 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5279, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_8336 = torch.constant.none
    %none_8337 = torch.constant.none
    %int5_8338 = torch.constant.int 5
    %cpu_8339 = torch.constant.device "cpu"
    %int0_8340 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5279, %none_8336, %none_8337, %int5_8338, %cpu_8339, %int0_8340 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_8341 = torch.constant.int 0
    %none_8342 = torch.constant.none
    %none_8343 = torch.constant.none
    %cpu_8344 = torch.constant.device "cpu"
    %false_8345 = torch.constant.bool false
    %5280 = torch.aten.arange.start %int0_8341, %273, %none_8342, %none_8343, %cpu_8344, %false_8345 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5280, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_8346 = torch.constant.int 0
    %5281 = torch.aten.unsqueeze %5280, %int0_8346 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %5281, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_8347 = torch.constant.int 0
    %int64_8348 = torch.constant.int 64
    %int2_8349 = torch.constant.int 2
    %none_8350 = torch.constant.none
    %none_8351 = torch.constant.none
    %cpu_8352 = torch.constant.device "cpu"
    %false_8353 = torch.constant.bool false
    %5282 = torch.aten.arange.start_step %int0_8347, %int64_8348, %int2_8349, %none_8350, %none_8351, %cpu_8352, %false_8353 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_8354 = torch.constant.none
    %none_8355 = torch.constant.none
    %int4_8356 = torch.constant.int 4
    %cpu_8357 = torch.constant.device "cpu"
    %int0_8358 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5282, %none_8354, %none_8355, %int4_8356, %cpu_8357, %int0_8358 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8359 = torch.constant.int 6
    %5283 = torch.prims.convert_element_type %5282, %int6_8359 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_8360 = torch.constant.int 64
    %5284 = torch.aten.div.Scalar %5283, %int64_8360 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_8361 = torch.constant.float 1.000000e+04
    %5285 = torch.aten.pow.Scalar %float1.000000e04_8361, %5284 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %5286 = torch.aten.reciprocal %5285 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_8362 = torch.constant.float 1.000000e+00
    %5287 = torch.aten.mul.Scalar %5286, %float1.000000e00_8362 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_8363 = torch.constant.none
    %5288 = torch.aten.clone %194, %none_8363 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_8364 = torch.constant.int 0
    %5289 = torch.aten.unsqueeze %5287, %int0_8364 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_8365 = torch.constant.int 2
    %5290 = torch.aten.unsqueeze %5289, %int2_8365 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_8366 = torch.constant.none
    %none_8367 = torch.constant.none
    %int6_8368 = torch.constant.int 6
    %cpu_8369 = torch.constant.device "cpu"
    %int0_8370 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5290, %none_8366, %none_8367, %int6_8368, %cpu_8369, %int0_8370 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_8371 = torch.constant.int 1
    %int-1_8372 = torch.constant.int -1
    %int1_8373 = torch.constant.int 1
    %5291 = torch.prim.ListConstruct %int1_8371, %int-1_8372, %int1_8373 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8374 = torch.constant.bool false
    %5292 = torch.aten.expand %5290, %5291, %false_8374 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_8375 = torch.constant.int 1
    %5293 = torch.aten.unsqueeze %5281, %int1_8375 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %5293, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_8376 = torch.constant.none
    %none_8377 = torch.constant.none
    %int4_8378 = torch.constant.int 4
    %cpu_8379 = torch.constant.device "cpu"
    %int0_8380 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5293, %none_8376, %none_8377, %int4_8378, %cpu_8379, %int0_8380 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8381 = torch.constant.int 6
    %5294 = torch.prims.convert_element_type %5293, %int6_8381 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %5294, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %5295 = torch.aten.matmul %5292, %5294 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %5295, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_8382 = torch.constant.int 1
    %int2_8383 = torch.constant.int 2
    %5296 = torch.aten.transpose.int %5295, %int1_8382, %int2_8383 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5296, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %5297 = torch.aten.cos %5296 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5297, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %5298 = torch.aten.mul.Tensor %5297, %5288 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5298, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_8384 = torch.constant.none
    %none_8385 = torch.constant.none
    %int6_8386 = torch.constant.int 6
    %cpu_8387 = torch.constant.device "cpu"
    %int0_8388 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5298, %none_8384, %none_8385, %int6_8386, %cpu_8387, %int0_8388 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8389 = torch.constant.int 5
    %5299 = torch.prims.convert_element_type %5298, %int5_8389 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %5299, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %5300 = torch.aten.sin %5296 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5300, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %5301 = torch.aten.mul.Tensor %5300, %5288 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5301, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_8390 = torch.constant.none
    %none_8391 = torch.constant.none
    %int6_8392 = torch.constant.int 6
    %cpu_8393 = torch.constant.device "cpu"
    %int0_8394 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5301, %none_8390, %none_8391, %int6_8392, %cpu_8393, %int0_8394 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8395 = torch.constant.int 5
    %5302 = torch.prims.convert_element_type %5301, %int5_8395 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %5302, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_8396 = torch.constant.int 2
    %5303 = torch.aten.unsqueeze %5299, %int2_8396 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %5303, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_8397 = torch.constant.int 2
    %5304 = torch.aten.unsqueeze %5302, %int2_8397 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %5304, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_8398 = torch.constant.none
    %none_8399 = torch.constant.none
    %int5_8400 = torch.constant.int 5
    %cpu_8401 = torch.constant.device "cpu"
    %int0_8402 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5303, %none_8398, %none_8399, %int5_8400, %cpu_8401, %int0_8402 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_8403 = torch.constant.none
    %none_8404 = torch.constant.none
    %int5_8405 = torch.constant.int 5
    %cpu_8406 = torch.constant.device "cpu"
    %int0_8407 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5304, %none_8403, %none_8404, %int5_8405, %cpu_8406, %int0_8407 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_8408 = torch.constant.none
    %none_8409 = torch.constant.none
    %int5_8410 = torch.constant.int 5
    %cpu_8411 = torch.constant.device "cpu"
    %int0_8412 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5238, %none_8408, %none_8409, %int5_8410, %cpu_8411, %int0_8412 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_8413 = torch.constant.int 3
    %int0_8414 = torch.constant.int 0
    %int64_8415 = torch.constant.int 64
    %int2_8416 = torch.constant.int 2
    %5305 = torch.aten.slice.Tensor %5238, %int3_8413, %int0_8414, %int64_8415, %int2_8416 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5305, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_8417 = torch.constant.int 3
    %int1_8418 = torch.constant.int 1
    %int64_8419 = torch.constant.int 64
    %int2_8420 = torch.constant.int 2
    %5306 = torch.aten.slice.Tensor %5238, %int3_8417, %int1_8418, %int64_8419, %int2_8420 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5306, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5307 = torch.aten.mul.Tensor %5305, %5303 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5307, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5308 = torch.aten.mul.Tensor %5306, %5304 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5308, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_8421 = torch.constant.int 1
    %5309 = torch.aten.sub.Tensor %5307, %5308, %int1_8421 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5309, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5310 = torch.aten.mul.Tensor %5306, %5303 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5310, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5311 = torch.aten.mul.Tensor %5305, %5304 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5311, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_8422 = torch.constant.int 1
    %5312 = torch.aten.add.Tensor %5310, %5311, %int1_8422 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5312, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5313 = torch_c.to_builtin_tensor %5309 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_8423 = tensor.cast %5313 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %5314 = torch_c.to_builtin_tensor %5312 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_8424 = tensor.cast %5314 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %5315 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_8423, %cast_8424) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_8425 = tensor.cast %5315 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %5316 = torch_c.from_builtin_tensor %cast_8425 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %5316, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_8426 = torch.constant.int 4
    %int4_8427 = torch.constant.int 4
    %int64_8428 = torch.constant.int 64
    %5317 = torch.prim.ListConstruct %int4_8426, %273, %int4_8427, %int64_8428 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5318 = torch.aten.view %5316, %5317 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %5318, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_8429 = torch.constant.none
    %none_8430 = torch.constant.none
    %int5_8431 = torch.constant.int 5
    %cpu_8432 = torch.constant.device "cpu"
    %int0_8433 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5318, %none_8429, %none_8430, %int5_8431, %cpu_8432, %int0_8433 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_8434 = torch.constant.int 22
    %5319 = torch.aten.mul.Scalar %arg2, %int22_8434 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5319, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int16 = torch.constant.int 16
    %int1_8435 = torch.constant.int 1
    %5320 = torch.aten.add.Scalar %5319, %int16, %int1_8435 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5320, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_8436 = torch.constant.int 2
    %5321 = torch.aten.mul.Scalar %5320, %int2_8436 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5321, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_8437 = torch.constant.int 0
    %int1_8438 = torch.constant.int 1
    %5322 = torch.aten.add.Scalar %5321, %int0_8437, %int1_8438 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5322, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %5323 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %5324 = torch.aten.view %5322, %5323 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5324, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_8439 = torch.constant.int 4
    %int32_8440 = torch.constant.int 32
    %int4_8441 = torch.constant.int 4
    %int64_8442 = torch.constant.int 64
    %5325 = torch.prim.ListConstruct %int4_8439, %271, %int32_8440, %int4_8441, %int64_8442 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5326 = torch.aten.view %5318, %5325 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5326, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_8443 = torch.constant.int 32
    %int4_8444 = torch.constant.int 4
    %int64_8445 = torch.constant.int 64
    %5327 = torch.prim.ListConstruct %446, %int32_8443, %int4_8444, %int64_8445 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5328 = torch.aten.view %5326, %5327 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %5328, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_8446 = torch.constant.int 1
    %int2_8447 = torch.constant.int 2
    %5329 = torch.aten.transpose.int %5328, %int1_8446, %int2_8447 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5329, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_8448 = torch.constant.none
    %none_8449 = torch.constant.none
    %int5_8450 = torch.constant.int 5
    %cpu_8451 = torch.constant.device "cpu"
    %int0_8452 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5329, %none_8448, %none_8449, %int5_8450, %cpu_8451, %int0_8452 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_8453 = torch.constant.int 22
    %int2_8454 = torch.constant.int 2
    %int4_8455 = torch.constant.int 4
    %int32_8456 = torch.constant.int 32
    %int64_8457 = torch.constant.int 64
    %5330 = torch.prim.ListConstruct %272, %int22_8453, %int2_8454, %int4_8455, %int32_8456, %int64_8457 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5331 = torch.aten.view %5055, %5330 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5331, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_8458 = torch.constant.int 4
    %int32_8459 = torch.constant.int 32
    %int64_8460 = torch.constant.int 64
    %5332 = torch.prim.ListConstruct %439, %int4_8458, %int32_8459, %int64_8460 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5333 = torch.aten.view %5331, %5332 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5333, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %5334 = torch.prim.ListConstruct %5324 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_8461 = torch.constant.bool false
    %5335 = torch.aten.index_put %5333, %5334, %5329, %false_8461 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5335, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_8462 = torch.constant.int 22
    %int2_8463 = torch.constant.int 2
    %int4_8464 = torch.constant.int 4
    %int32_8465 = torch.constant.int 32
    %int64_8466 = torch.constant.int 64
    %5336 = torch.prim.ListConstruct %272, %int22_8462, %int2_8463, %int4_8464, %int32_8465, %int64_8466 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5337 = torch.aten.view %5335, %5336 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5337, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_8467 = torch.constant.int 360448
    %5338 = torch.prim.ListConstruct %272, %int360448_8467 : (!torch.int, !torch.int) -> !torch.list<int>
    %5339 = torch.aten.view %5337, %5338 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %5339, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_8468 = torch.constant.int 22
    %int2_8469 = torch.constant.int 2
    %int4_8470 = torch.constant.int 4
    %int32_8471 = torch.constant.int 32
    %int64_8472 = torch.constant.int 64
    %5340 = torch.prim.ListConstruct %272, %int22_8468, %int2_8469, %int4_8470, %int32_8471, %int64_8472 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5341 = torch.aten.view %5339, %5340 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5341, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_8473 = torch.constant.int 4
    %int32_8474 = torch.constant.int 32
    %int64_8475 = torch.constant.int 64
    %5342 = torch.prim.ListConstruct %439, %int4_8473, %int32_8474, %int64_8475 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5343 = torch.aten.view %5341, %5342 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5343, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_8476 = torch.constant.int 22
    %5344 = torch.aten.mul.Scalar %arg2, %int22_8476 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5344, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int16_8477 = torch.constant.int 16
    %int1_8478 = torch.constant.int 1
    %5345 = torch.aten.add.Scalar %5344, %int16_8477, %int1_8478 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5345, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_8479 = torch.constant.int 2
    %5346 = torch.aten.mul.Scalar %5345, %int2_8479 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5346, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_8480 = torch.constant.int 1
    %int1_8481 = torch.constant.int 1
    %5347 = torch.aten.add.Scalar %5346, %int1_8480, %int1_8481 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5347, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %5348 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %5349 = torch.aten.view %5347, %5348 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5349, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_8482 = torch.constant.int 4
    %int32_8483 = torch.constant.int 32
    %int4_8484 = torch.constant.int 4
    %int64_8485 = torch.constant.int 64
    %5350 = torch.prim.ListConstruct %int4_8482, %271, %int32_8483, %int4_8484, %int64_8485 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5351 = torch.aten.view %5240, %5350 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5351, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_8486 = torch.constant.int 32
    %int4_8487 = torch.constant.int 4
    %int64_8488 = torch.constant.int 64
    %5352 = torch.prim.ListConstruct %446, %int32_8486, %int4_8487, %int64_8488 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5353 = torch.aten.view %5351, %5352 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %5353, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_8489 = torch.constant.int 1
    %int2_8490 = torch.constant.int 2
    %5354 = torch.aten.transpose.int %5353, %int1_8489, %int2_8490 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5354, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_8491 = torch.constant.none
    %none_8492 = torch.constant.none
    %int5_8493 = torch.constant.int 5
    %cpu_8494 = torch.constant.device "cpu"
    %int0_8495 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5354, %none_8491, %none_8492, %int5_8493, %cpu_8494, %int0_8495 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %5355 = torch.prim.ListConstruct %5349 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_8496 = torch.constant.bool false
    %5356 = torch.aten.index_put %5343, %5355, %5354, %false_8496 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5356, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_8497 = torch.constant.int 22
    %int2_8498 = torch.constant.int 2
    %int4_8499 = torch.constant.int 4
    %int32_8500 = torch.constant.int 32
    %int64_8501 = torch.constant.int 64
    %5357 = torch.prim.ListConstruct %272, %int22_8497, %int2_8498, %int4_8499, %int32_8500, %int64_8501 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5358 = torch.aten.view %5356, %5357 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5358, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_8502 = torch.constant.int 360448
    %5359 = torch.prim.ListConstruct %272, %int360448_8502 : (!torch.int, !torch.int) -> !torch.list<int>
    %5360 = torch.aten.view %5358, %5359 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %5360, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_8503 = torch.constant.int 0
    %int1_8504 = torch.constant.int 1
    %none_8505 = torch.constant.none
    %none_8506 = torch.constant.none
    %cpu_8507 = torch.constant.device "cpu"
    %false_8508 = torch.constant.bool false
    %5361 = torch.aten.arange.start_step %int0_8503, %273, %int1_8504, %none_8505, %none_8506, %cpu_8507, %false_8508 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5361, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_8509 = torch.constant.int -1
    %5362 = torch.aten.unsqueeze %arg1, %int-1_8509 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %5363 = torch.aten.ge.Tensor %5361, %5362 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %5363, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_8510 = torch.constant.none
    %none_8511 = torch.constant.none
    %cpu_8512 = torch.constant.device "cpu"
    %false_8513 = torch.constant.bool false
    %5364 = torch.aten.arange %273, %none_8510, %none_8511, %cpu_8512, %false_8513 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5364, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_8514 = torch.constant.int 0
    %5365 = torch.aten.unsqueeze %5364, %int0_8514 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %5365, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_8515 = torch.constant.int 1
    %5366 = torch.aten.unsqueeze %5365, %int1_8515 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %5366, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_8516 = torch.constant.int 2
    %5367 = torch.aten.unsqueeze %5366, %int2_8516 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %5367, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_8517 = torch.constant.none
    %none_8518 = torch.constant.none
    %cpu_8519 = torch.constant.device "cpu"
    %false_8520 = torch.constant.bool false
    %5368 = torch.aten.arange %273, %none_8517, %none_8518, %cpu_8519, %false_8520 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5368, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_8521 = torch.constant.int 0
    %5369 = torch.aten.unsqueeze %5368, %int0_8521 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %5369, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_8522 = torch.constant.int 1
    %5370 = torch.aten.unsqueeze %5369, %int1_8522 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %5370, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_8523 = torch.constant.int 3
    %5371 = torch.aten.unsqueeze %5370, %int3_8523 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %5371, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %5372 = torch.aten.gt.Tensor %5367, %5371 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %5372, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_8524 = torch.constant.int 1
    %5373 = torch.aten.unsqueeze %5363, %int1_8524 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %5373, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_8525 = torch.constant.int 2
    %5374 = torch.aten.unsqueeze %5373, %int2_8525 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %5374, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %5375 = torch.aten.logical_or %5372, %5374 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %5375, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_8526 = torch.constant.none
    %5376 = torch.aten.clone %195, %none_8526 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_8527 = torch.constant.int 0
    %5377 = torch.aten.where.ScalarOther %5375, %5376, %int0_8527 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %5377, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_8528 = torch.constant.none
    %none_8529 = torch.constant.none
    %int5_8530 = torch.constant.int 5
    %cpu_8531 = torch.constant.device "cpu"
    %int0_8532 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5377, %none_8528, %none_8529, %int5_8530, %cpu_8531, %int0_8532 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_8533 = torch.constant.int -2
    %5378 = torch.aten.unsqueeze %5318, %int-2_8533 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %5378, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_8534 = torch.constant.int 4
    %int4_8535 = torch.constant.int 4
    %int8_8536 = torch.constant.int 8
    %int64_8537 = torch.constant.int 64
    %5379 = torch.prim.ListConstruct %int4_8534, %273, %int4_8535, %int8_8536, %int64_8537 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8538 = torch.constant.bool false
    %5380 = torch.aten.expand %5378, %5379, %false_8538 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5380, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_8539 = torch.constant.int 0
    %5381 = torch.aten.clone %5380, %int0_8539 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5381, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_8540 = torch.constant.int 4
    %int32_8541 = torch.constant.int 32
    %int64_8542 = torch.constant.int 64
    %5382 = torch.prim.ListConstruct %int4_8540, %273, %int32_8541, %int64_8542 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5383 = torch.aten._unsafe_view %5381, %5382 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5383, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_8543 = torch.constant.int -2
    %5384 = torch.aten.unsqueeze %5240, %int-2_8543 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %5384, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_8544 = torch.constant.int 4
    %int4_8545 = torch.constant.int 4
    %int8_8546 = torch.constant.int 8
    %int64_8547 = torch.constant.int 64
    %5385 = torch.prim.ListConstruct %int4_8544, %273, %int4_8545, %int8_8546, %int64_8547 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8548 = torch.constant.bool false
    %5386 = torch.aten.expand %5384, %5385, %false_8548 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5386, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_8549 = torch.constant.int 0
    %5387 = torch.aten.clone %5386, %int0_8549 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5387, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_8550 = torch.constant.int 4
    %int32_8551 = torch.constant.int 32
    %int64_8552 = torch.constant.int 64
    %5388 = torch.prim.ListConstruct %int4_8550, %273, %int32_8551, %int64_8552 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5389 = torch.aten._unsafe_view %5387, %5388 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5389, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_8553 = torch.constant.int 1
    %int2_8554 = torch.constant.int 2
    %5390 = torch.aten.transpose.int %5279, %int1_8553, %int2_8554 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %5390, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_8555 = torch.constant.int 1
    %int2_8556 = torch.constant.int 2
    %5391 = torch.aten.transpose.int %5383, %int1_8555, %int2_8556 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %5391, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_8557 = torch.constant.int 1
    %int2_8558 = torch.constant.int 2
    %5392 = torch.aten.transpose.int %5389, %int1_8557, %int2_8558 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %5392, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_8559 = torch.constant.float 0.000000e+00
    %false_8560 = torch.constant.bool false
    %none_8561 = torch.constant.none
    %false_8562 = torch.constant.bool false
    %5393 = torch.aten.scaled_dot_product_attention %5390, %5391, %5392, %5377, %float0.000000e00_8559, %false_8560, %none_8561, %false_8562 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %5393, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_8563 = torch.constant.int 1
    %int2_8564 = torch.constant.int 2
    %5394 = torch.aten.transpose.int %5393, %int1_8563, %int2_8564 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5394, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_8565 = torch.constant.int 4
    %int2048_8566 = torch.constant.int 2048
    %5395 = torch.prim.ListConstruct %int4_8565, %273, %int2048_8566 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5396 = torch.aten.view %5394, %5395 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5396, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_8567 = torch.constant.int 2
    %5397 = torch.aten.view.dtype %200, %int2_8567 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %5398 = torch.aten.detach %5397 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_8568 = torch.constant.int -1
    %int17_8569 = torch.constant.int 17
    %5399 = torch.prim.ListConstruct %int-1_8568, %int17_8569 : (!torch.int, !torch.int) -> !torch.list<int>
    %5400 = torch.aten.view %5398, %5399 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_8570 = torch.constant.int 2048
    %int-1_8571 = torch.constant.int -1
    %int17_8572 = torch.constant.int 17
    %5401 = torch.prim.ListConstruct %int2048_8570, %int-1_8571, %int17_8572 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5402 = torch.aten.view %5400, %5401 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_8573 = torch.constant.int 2
    %int0_8574 = torch.constant.int 0
    %int1_8575 = torch.constant.int 1
    %int1_8576 = torch.constant.int 1
    %5403 = torch.aten.slice.Tensor %5402, %int2_8573, %int0_8574, %int1_8575, %int1_8576 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_8577 = torch.constant.int 5
    %5404 = torch.aten.view.dtype %5403, %int5_8577 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %5405 = torch.aten.detach %5404 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_8578 = torch.constant.int 2
    %int1_8579 = torch.constant.int 1
    %int9223372036854775807_8580 = torch.constant.int 9223372036854775807
    %int1_8581 = torch.constant.int 1
    %5406 = torch.aten.slice.Tensor %5402, %int2_8578, %int1_8579, %int9223372036854775807_8580, %int1_8581 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_8582 = torch.constant.int 1
    %5407 = torch.aten.view.dtype %5406, %int1_8582 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %5408 = torch.aten.detach %5407 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %5409 = torch_c.to_builtin_tensor %5396 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_8583 = tensor.cast %5409 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %5410 = torch_c.to_builtin_tensor %5405 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %5411 = torch_c.to_builtin_tensor %5408 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %5412 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_8583, %5410, %5411) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_8584 = tensor.cast %5412 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %5413 = torch_c.from_builtin_tensor %cast_8584 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5413, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_8585 = torch.constant.none
    %none_8586 = torch.constant.none
    %int5_8587 = torch.constant.int 5
    %cpu_8588 = torch.constant.device "cpu"
    %int0_8589 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5413, %none_8585, %none_8586, %int5_8587, %cpu_8588, %int0_8589 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_8590 = torch.constant.int 1
    %5414 = torch.aten.add.Tensor %5173, %5413, %int1_8590 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5414, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_8591 = torch.constant.none
    %none_8592 = torch.constant.none
    %int5_8593 = torch.constant.int 5
    %cpu_8594 = torch.constant.device "cpu"
    %int0_8595 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5414, %none_8591, %none_8592, %int5_8593, %cpu_8594, %int0_8595 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8596 = torch.constant.int 6
    %5415 = torch.prims.convert_element_type %5414, %int6_8596 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5415, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_8597 = torch.constant.int 2
    %5416 = torch.aten.pow.Tensor_Scalar %5415, %int2_8597 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5416, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_8598 = torch.constant.int -1
    %5417 = torch.prim.ListConstruct %int-1_8598 : (!torch.int) -> !torch.list<int>
    %true_8599 = torch.constant.bool true
    %none_8600 = torch.constant.none
    %5418 = torch.aten.mean.dim %5416, %5417, %true_8599, %none_8600 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5418, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_8601 = torch.constant.float 9.9999997473787516E-6
    %int1_8602 = torch.constant.int 1
    %5419 = torch.aten.add.Scalar %5418, %float9.999990e-06_8601, %int1_8602 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5419, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5420 = torch.aten.rsqrt %5419 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5420, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5421 = torch.aten.mul.Tensor %5415, %5420 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5421, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_8603 = torch.constant.none
    %none_8604 = torch.constant.none
    %int6_8605 = torch.constant.int 6
    %cpu_8606 = torch.constant.device "cpu"
    %int0_8607 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5421, %none_8603, %none_8604, %int6_8605, %cpu_8606, %int0_8607 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8608 = torch.constant.int 5
    %5422 = torch.prims.convert_element_type %5421, %int5_8608 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5422, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %5423 = torch.aten.mul.Tensor %201, %5422 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5423, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_8609 = torch.constant.none
    %none_8610 = torch.constant.none
    %int6_8611 = torch.constant.int 6
    %cpu_8612 = torch.constant.device "cpu"
    %int0_8613 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5423, %none_8609, %none_8610, %int6_8611, %cpu_8612, %int0_8613 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8614 = torch.constant.int 5
    %5424 = torch.prims.convert_element_type %5423, %int5_8614 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5424, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_8615 = torch.constant.int 2
    %5425 = torch.aten.view.dtype %202, %int2_8615 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %5426 = torch.aten.detach %5425 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_8616 = torch.constant.int -1
    %int17_8617 = torch.constant.int 17
    %5427 = torch.prim.ListConstruct %int-1_8616, %int17_8617 : (!torch.int, !torch.int) -> !torch.list<int>
    %5428 = torch.aten.view %5426, %5427 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_8618 = torch.constant.int 5632
    %int-1_8619 = torch.constant.int -1
    %int17_8620 = torch.constant.int 17
    %5429 = torch.prim.ListConstruct %int5632_8618, %int-1_8619, %int17_8620 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5430 = torch.aten.view %5428, %5429 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_8621 = torch.constant.int 2
    %int0_8622 = torch.constant.int 0
    %int1_8623 = torch.constant.int 1
    %int1_8624 = torch.constant.int 1
    %5431 = torch.aten.slice.Tensor %5430, %int2_8621, %int0_8622, %int1_8623, %int1_8624 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_8625 = torch.constant.int 5
    %5432 = torch.aten.view.dtype %5431, %int5_8625 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %5433 = torch.aten.detach %5432 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_8626 = torch.constant.int 2
    %int1_8627 = torch.constant.int 1
    %int9223372036854775807_8628 = torch.constant.int 9223372036854775807
    %int1_8629 = torch.constant.int 1
    %5434 = torch.aten.slice.Tensor %5430, %int2_8626, %int1_8627, %int9223372036854775807_8628, %int1_8629 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_8630 = torch.constant.int 1
    %5435 = torch.aten.view.dtype %5434, %int1_8630 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %5436 = torch.aten.detach %5435 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %5437 = torch_c.to_builtin_tensor %5424 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_8631 = tensor.cast %5437 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %5438 = torch_c.to_builtin_tensor %5433 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %5439 = torch_c.to_builtin_tensor %5436 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %5440 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_8631, %5438, %5439) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_8632 = tensor.cast %5440 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %5441 = torch_c.from_builtin_tensor %cast_8632 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %5441, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %5442 = torch.aten.silu %5441 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %5442, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_8633 = torch.constant.int 2
    %5443 = torch.aten.view.dtype %203, %int2_8633 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %5444 = torch.aten.detach %5443 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_8634 = torch.constant.int -1
    %int17_8635 = torch.constant.int 17
    %5445 = torch.prim.ListConstruct %int-1_8634, %int17_8635 : (!torch.int, !torch.int) -> !torch.list<int>
    %5446 = torch.aten.view %5444, %5445 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_8636 = torch.constant.int 5632
    %int-1_8637 = torch.constant.int -1
    %int17_8638 = torch.constant.int 17
    %5447 = torch.prim.ListConstruct %int5632_8636, %int-1_8637, %int17_8638 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5448 = torch.aten.view %5446, %5447 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_8639 = torch.constant.int 2
    %int0_8640 = torch.constant.int 0
    %int1_8641 = torch.constant.int 1
    %int1_8642 = torch.constant.int 1
    %5449 = torch.aten.slice.Tensor %5448, %int2_8639, %int0_8640, %int1_8641, %int1_8642 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_8643 = torch.constant.int 5
    %5450 = torch.aten.view.dtype %5449, %int5_8643 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %5451 = torch.aten.detach %5450 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_8644 = torch.constant.int 2
    %int1_8645 = torch.constant.int 1
    %int9223372036854775807_8646 = torch.constant.int 9223372036854775807
    %int1_8647 = torch.constant.int 1
    %5452 = torch.aten.slice.Tensor %5448, %int2_8644, %int1_8645, %int9223372036854775807_8646, %int1_8647 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_8648 = torch.constant.int 1
    %5453 = torch.aten.view.dtype %5452, %int1_8648 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %5454 = torch.aten.detach %5453 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %5455 = torch_c.to_builtin_tensor %5424 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_8649 = tensor.cast %5455 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %5456 = torch_c.to_builtin_tensor %5451 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %5457 = torch_c.to_builtin_tensor %5454 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %5458 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_8649, %5456, %5457) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_8650 = tensor.cast %5458 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %5459 = torch_c.from_builtin_tensor %cast_8650 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %5459, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %5460 = torch.aten.mul.Tensor %5442, %5459 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %5460, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_8651 = torch.constant.int 2
    %5461 = torch.aten.view.dtype %204, %int2_8651 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %5462 = torch.aten.detach %5461 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_8652 = torch.constant.int -1
    %int17_8653 = torch.constant.int 17
    %5463 = torch.prim.ListConstruct %int-1_8652, %int17_8653 : (!torch.int, !torch.int) -> !torch.list<int>
    %5464 = torch.aten.view %5462, %5463 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_8654 = torch.constant.int 2048
    %int-1_8655 = torch.constant.int -1
    %int17_8656 = torch.constant.int 17
    %5465 = torch.prim.ListConstruct %int2048_8654, %int-1_8655, %int17_8656 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5466 = torch.aten.view %5464, %5465 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_8657 = torch.constant.int 2
    %int0_8658 = torch.constant.int 0
    %int1_8659 = torch.constant.int 1
    %int1_8660 = torch.constant.int 1
    %5467 = torch.aten.slice.Tensor %5466, %int2_8657, %int0_8658, %int1_8659, %int1_8660 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_8661 = torch.constant.int 5
    %5468 = torch.aten.view.dtype %5467, %int5_8661 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %5469 = torch.aten.detach %5468 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_8662 = torch.constant.int 2
    %int1_8663 = torch.constant.int 1
    %int9223372036854775807_8664 = torch.constant.int 9223372036854775807
    %int1_8665 = torch.constant.int 1
    %5470 = torch.aten.slice.Tensor %5466, %int2_8662, %int1_8663, %int9223372036854775807_8664, %int1_8665 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_8666 = torch.constant.int 1
    %5471 = torch.aten.view.dtype %5470, %int1_8666 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %5472 = torch.aten.detach %5471 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %5473 = torch_c.to_builtin_tensor %5460 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_8667 = tensor.cast %5473 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %5474 = torch_c.to_builtin_tensor %5469 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %5475 = torch_c.to_builtin_tensor %5472 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %5476 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_8667, %5474, %5475) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_8668 = tensor.cast %5476 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %5477 = torch_c.from_builtin_tensor %cast_8668 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5477, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_8669 = torch.constant.int 1
    %5478 = torch.aten.add.Tensor %5414, %5477, %int1_8669 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5478, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_8670 = torch.constant.none
    %none_8671 = torch.constant.none
    %int5_8672 = torch.constant.int 5
    %cpu_8673 = torch.constant.device "cpu"
    %int0_8674 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5478, %none_8670, %none_8671, %int5_8672, %cpu_8673, %int0_8674 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8675 = torch.constant.int 6
    %5479 = torch.prims.convert_element_type %5478, %int6_8675 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5479, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_8676 = torch.constant.int 2
    %5480 = torch.aten.pow.Tensor_Scalar %5479, %int2_8676 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5480, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_8677 = torch.constant.int -1
    %5481 = torch.prim.ListConstruct %int-1_8677 : (!torch.int) -> !torch.list<int>
    %true_8678 = torch.constant.bool true
    %none_8679 = torch.constant.none
    %5482 = torch.aten.mean.dim %5480, %5481, %true_8678, %none_8679 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5482, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_8680 = torch.constant.float 9.9999997473787516E-6
    %int1_8681 = torch.constant.int 1
    %5483 = torch.aten.add.Scalar %5482, %float9.999990e-06_8680, %int1_8681 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5483, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5484 = torch.aten.rsqrt %5483 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5484, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5485 = torch.aten.mul.Tensor %5479, %5484 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5485, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_8682 = torch.constant.none
    %none_8683 = torch.constant.none
    %int6_8684 = torch.constant.int 6
    %cpu_8685 = torch.constant.device "cpu"
    %int0_8686 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5485, %none_8682, %none_8683, %int6_8684, %cpu_8685, %int0_8686 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8687 = torch.constant.int 5
    %5486 = torch.prims.convert_element_type %5485, %int5_8687 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5486, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %5487 = torch.aten.mul.Tensor %208, %5486 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5487, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_8688 = torch.constant.none
    %none_8689 = torch.constant.none
    %int6_8690 = torch.constant.int 6
    %cpu_8691 = torch.constant.device "cpu"
    %int0_8692 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5487, %none_8688, %none_8689, %int6_8690, %cpu_8691, %int0_8692 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8693 = torch.constant.int 5
    %5488 = torch.prims.convert_element_type %5487, %int5_8693 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5488, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_8694 = torch.constant.int 2
    %5489 = torch.aten.view.dtype %209, %int2_8694 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %5490 = torch.aten.detach %5489 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_8695 = torch.constant.int -1
    %int17_8696 = torch.constant.int 17
    %5491 = torch.prim.ListConstruct %int-1_8695, %int17_8696 : (!torch.int, !torch.int) -> !torch.list<int>
    %5492 = torch.aten.view %5490, %5491 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_8697 = torch.constant.int 2048
    %int-1_8698 = torch.constant.int -1
    %int17_8699 = torch.constant.int 17
    %5493 = torch.prim.ListConstruct %int2048_8697, %int-1_8698, %int17_8699 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5494 = torch.aten.view %5492, %5493 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_8700 = torch.constant.int 2
    %int0_8701 = torch.constant.int 0
    %int1_8702 = torch.constant.int 1
    %int1_8703 = torch.constant.int 1
    %5495 = torch.aten.slice.Tensor %5494, %int2_8700, %int0_8701, %int1_8702, %int1_8703 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_8704 = torch.constant.int 5
    %5496 = torch.aten.view.dtype %5495, %int5_8704 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %5497 = torch.aten.detach %5496 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_8705 = torch.constant.int 2
    %int1_8706 = torch.constant.int 1
    %int9223372036854775807_8707 = torch.constant.int 9223372036854775807
    %int1_8708 = torch.constant.int 1
    %5498 = torch.aten.slice.Tensor %5494, %int2_8705, %int1_8706, %int9223372036854775807_8707, %int1_8708 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_8709 = torch.constant.int 1
    %5499 = torch.aten.view.dtype %5498, %int1_8709 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %5500 = torch.aten.detach %5499 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %5501 = torch_c.to_builtin_tensor %5488 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_8710 = tensor.cast %5501 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %5502 = torch_c.to_builtin_tensor %5497 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %5503 = torch_c.to_builtin_tensor %5500 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %5504 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_8710, %5502, %5503) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_8711 = tensor.cast %5504 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %5505 = torch_c.from_builtin_tensor %cast_8711 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5505, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_8712 = torch.constant.int 2
    %5506 = torch.aten.view.dtype %210, %int2_8712 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %5507 = torch.aten.detach %5506 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_8713 = torch.constant.int -1
    %int17_8714 = torch.constant.int 17
    %5508 = torch.prim.ListConstruct %int-1_8713, %int17_8714 : (!torch.int, !torch.int) -> !torch.list<int>
    %5509 = torch.aten.view %5507, %5508 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_8715 = torch.constant.int 256
    %int-1_8716 = torch.constant.int -1
    %int17_8717 = torch.constant.int 17
    %5510 = torch.prim.ListConstruct %int256_8715, %int-1_8716, %int17_8717 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5511 = torch.aten.view %5509, %5510 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_8718 = torch.constant.int 2
    %int0_8719 = torch.constant.int 0
    %int1_8720 = torch.constant.int 1
    %int1_8721 = torch.constant.int 1
    %5512 = torch.aten.slice.Tensor %5511, %int2_8718, %int0_8719, %int1_8720, %int1_8721 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_8722 = torch.constant.int 5
    %5513 = torch.aten.view.dtype %5512, %int5_8722 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %5514 = torch.aten.detach %5513 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_8723 = torch.constant.int 2
    %int1_8724 = torch.constant.int 1
    %int9223372036854775807_8725 = torch.constant.int 9223372036854775807
    %int1_8726 = torch.constant.int 1
    %5515 = torch.aten.slice.Tensor %5511, %int2_8723, %int1_8724, %int9223372036854775807_8725, %int1_8726 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_8727 = torch.constant.int 1
    %5516 = torch.aten.view.dtype %5515, %int1_8727 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %5517 = torch.aten.detach %5516 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %5518 = torch_c.to_builtin_tensor %5488 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_8728 = tensor.cast %5518 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %5519 = torch_c.to_builtin_tensor %5514 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %5520 = torch_c.to_builtin_tensor %5517 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %5521 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_8728, %5519, %5520) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_8729 = tensor.cast %5521 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %5522 = torch_c.from_builtin_tensor %cast_8729 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %5522, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_8730 = torch.constant.int 2
    %5523 = torch.aten.view.dtype %211, %int2_8730 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %5524 = torch.aten.detach %5523 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_8731 = torch.constant.int -1
    %int17_8732 = torch.constant.int 17
    %5525 = torch.prim.ListConstruct %int-1_8731, %int17_8732 : (!torch.int, !torch.int) -> !torch.list<int>
    %5526 = torch.aten.view %5524, %5525 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_8733 = torch.constant.int 256
    %int-1_8734 = torch.constant.int -1
    %int17_8735 = torch.constant.int 17
    %5527 = torch.prim.ListConstruct %int256_8733, %int-1_8734, %int17_8735 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5528 = torch.aten.view %5526, %5527 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_8736 = torch.constant.int 2
    %int0_8737 = torch.constant.int 0
    %int1_8738 = torch.constant.int 1
    %int1_8739 = torch.constant.int 1
    %5529 = torch.aten.slice.Tensor %5528, %int2_8736, %int0_8737, %int1_8738, %int1_8739 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_8740 = torch.constant.int 5
    %5530 = torch.aten.view.dtype %5529, %int5_8740 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %5531 = torch.aten.detach %5530 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_8741 = torch.constant.int 2
    %int1_8742 = torch.constant.int 1
    %int9223372036854775807_8743 = torch.constant.int 9223372036854775807
    %int1_8744 = torch.constant.int 1
    %5532 = torch.aten.slice.Tensor %5528, %int2_8741, %int1_8742, %int9223372036854775807_8743, %int1_8744 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_8745 = torch.constant.int 1
    %5533 = torch.aten.view.dtype %5532, %int1_8745 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %5534 = torch.aten.detach %5533 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %5535 = torch_c.to_builtin_tensor %5488 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_8746 = tensor.cast %5535 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %5536 = torch_c.to_builtin_tensor %5531 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %5537 = torch_c.to_builtin_tensor %5534 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %5538 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_8746, %5536, %5537) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_8747 = tensor.cast %5538 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %5539 = torch_c.from_builtin_tensor %cast_8747 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %5539, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_8748 = torch.constant.int 4
    %int32_8749 = torch.constant.int 32
    %int64_8750 = torch.constant.int 64
    %5540 = torch.prim.ListConstruct %int4_8748, %273, %int32_8749, %int64_8750 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5541 = torch.aten.view %5505, %5540 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5541, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_8751 = torch.constant.int 4
    %int4_8752 = torch.constant.int 4
    %int64_8753 = torch.constant.int 64
    %5542 = torch.prim.ListConstruct %int4_8751, %273, %int4_8752, %int64_8753 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5543 = torch.aten.view %5522, %5542 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %5543, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_8754 = torch.constant.int 4
    %int4_8755 = torch.constant.int 4
    %int64_8756 = torch.constant.int 64
    %5544 = torch.prim.ListConstruct %int4_8754, %273, %int4_8755, %int64_8756 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5545 = torch.aten.view %5539, %5544 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %5545, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_8757 = torch.constant.int 0
    %none_8758 = torch.constant.none
    %none_8759 = torch.constant.none
    %cpu_8760 = torch.constant.device "cpu"
    %false_8761 = torch.constant.bool false
    %5546 = torch.aten.arange.start %int0_8757, %273, %none_8758, %none_8759, %cpu_8760, %false_8761 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5546, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_8762 = torch.constant.int 0
    %5547 = torch.aten.unsqueeze %5546, %int0_8762 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %5547, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_8763 = torch.constant.int 0
    %int64_8764 = torch.constant.int 64
    %int2_8765 = torch.constant.int 2
    %none_8766 = torch.constant.none
    %none_8767 = torch.constant.none
    %cpu_8768 = torch.constant.device "cpu"
    %false_8769 = torch.constant.bool false
    %5548 = torch.aten.arange.start_step %int0_8763, %int64_8764, %int2_8765, %none_8766, %none_8767, %cpu_8768, %false_8769 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_8770 = torch.constant.none
    %none_8771 = torch.constant.none
    %int4_8772 = torch.constant.int 4
    %cpu_8773 = torch.constant.device "cpu"
    %int0_8774 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5548, %none_8770, %none_8771, %int4_8772, %cpu_8773, %int0_8774 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8775 = torch.constant.int 6
    %5549 = torch.prims.convert_element_type %5548, %int6_8775 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_8776 = torch.constant.int 64
    %5550 = torch.aten.div.Scalar %5549, %int64_8776 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_8777 = torch.constant.float 1.000000e+04
    %5551 = torch.aten.pow.Scalar %float1.000000e04_8777, %5550 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %5552 = torch.aten.reciprocal %5551 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_8778 = torch.constant.float 1.000000e+00
    %5553 = torch.aten.mul.Scalar %5552, %float1.000000e00_8778 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_8779 = torch.constant.none
    %5554 = torch.aten.clone %205, %none_8779 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_8780 = torch.constant.int 0
    %5555 = torch.aten.unsqueeze %5553, %int0_8780 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_8781 = torch.constant.int 2
    %5556 = torch.aten.unsqueeze %5555, %int2_8781 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_8782 = torch.constant.none
    %none_8783 = torch.constant.none
    %int6_8784 = torch.constant.int 6
    %cpu_8785 = torch.constant.device "cpu"
    %int0_8786 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5556, %none_8782, %none_8783, %int6_8784, %cpu_8785, %int0_8786 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_8787 = torch.constant.int 1
    %int-1_8788 = torch.constant.int -1
    %int1_8789 = torch.constant.int 1
    %5557 = torch.prim.ListConstruct %int1_8787, %int-1_8788, %int1_8789 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8790 = torch.constant.bool false
    %5558 = torch.aten.expand %5556, %5557, %false_8790 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_8791 = torch.constant.int 1
    %5559 = torch.aten.unsqueeze %5547, %int1_8791 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %5559, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_8792 = torch.constant.none
    %none_8793 = torch.constant.none
    %int4_8794 = torch.constant.int 4
    %cpu_8795 = torch.constant.device "cpu"
    %int0_8796 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5559, %none_8792, %none_8793, %int4_8794, %cpu_8795, %int0_8796 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8797 = torch.constant.int 6
    %5560 = torch.prims.convert_element_type %5559, %int6_8797 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %5560, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %5561 = torch.aten.matmul %5558, %5560 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %5561, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_8798 = torch.constant.int 1
    %int2_8799 = torch.constant.int 2
    %5562 = torch.aten.transpose.int %5561, %int1_8798, %int2_8799 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5562, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %5563 = torch.aten.cos %5562 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5563, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %5564 = torch.aten.mul.Tensor %5563, %5554 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5564, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_8800 = torch.constant.none
    %none_8801 = torch.constant.none
    %int6_8802 = torch.constant.int 6
    %cpu_8803 = torch.constant.device "cpu"
    %int0_8804 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5564, %none_8800, %none_8801, %int6_8802, %cpu_8803, %int0_8804 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8805 = torch.constant.int 5
    %5565 = torch.prims.convert_element_type %5564, %int5_8805 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %5565, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %5566 = torch.aten.sin %5562 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5566, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %5567 = torch.aten.mul.Tensor %5566, %5554 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5567, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_8806 = torch.constant.none
    %none_8807 = torch.constant.none
    %int6_8808 = torch.constant.int 6
    %cpu_8809 = torch.constant.device "cpu"
    %int0_8810 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5567, %none_8806, %none_8807, %int6_8808, %cpu_8809, %int0_8810 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8811 = torch.constant.int 5
    %5568 = torch.prims.convert_element_type %5567, %int5_8811 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %5568, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_8812 = torch.constant.int 2
    %5569 = torch.aten.unsqueeze %5565, %int2_8812 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %5569, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_8813 = torch.constant.int 2
    %5570 = torch.aten.unsqueeze %5568, %int2_8813 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %5570, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_8814 = torch.constant.none
    %none_8815 = torch.constant.none
    %int5_8816 = torch.constant.int 5
    %cpu_8817 = torch.constant.device "cpu"
    %int0_8818 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5569, %none_8814, %none_8815, %int5_8816, %cpu_8817, %int0_8818 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_8819 = torch.constant.none
    %none_8820 = torch.constant.none
    %int5_8821 = torch.constant.int 5
    %cpu_8822 = torch.constant.device "cpu"
    %int0_8823 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5570, %none_8819, %none_8820, %int5_8821, %cpu_8822, %int0_8823 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_8824 = torch.constant.none
    %none_8825 = torch.constant.none
    %int5_8826 = torch.constant.int 5
    %cpu_8827 = torch.constant.device "cpu"
    %int0_8828 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5541, %none_8824, %none_8825, %int5_8826, %cpu_8827, %int0_8828 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_8829 = torch.constant.int 3
    %int0_8830 = torch.constant.int 0
    %int64_8831 = torch.constant.int 64
    %int2_8832 = torch.constant.int 2
    %5571 = torch.aten.slice.Tensor %5541, %int3_8829, %int0_8830, %int64_8831, %int2_8832 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5571, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_8833 = torch.constant.int 3
    %int1_8834 = torch.constant.int 1
    %int64_8835 = torch.constant.int 64
    %int2_8836 = torch.constant.int 2
    %5572 = torch.aten.slice.Tensor %5541, %int3_8833, %int1_8834, %int64_8835, %int2_8836 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5572, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %5573 = torch.aten.mul.Tensor %5571, %5569 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5573, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %5574 = torch.aten.mul.Tensor %5572, %5570 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5574, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_8837 = torch.constant.int 1
    %5575 = torch.aten.sub.Tensor %5573, %5574, %int1_8837 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5575, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %5576 = torch.aten.mul.Tensor %5572, %5569 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5576, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %5577 = torch.aten.mul.Tensor %5571, %5570 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5577, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_8838 = torch.constant.int 1
    %5578 = torch.aten.add.Tensor %5576, %5577, %int1_8838 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5578, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %5579 = torch_c.to_builtin_tensor %5575 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_8839 = tensor.cast %5579 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %5580 = torch_c.to_builtin_tensor %5578 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_8840 = tensor.cast %5580 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %5581 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_8839, %cast_8840) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_8841 = tensor.cast %5581 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %5582 = torch_c.from_builtin_tensor %cast_8841 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %5582, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_8842 = torch.constant.int 4
    %int32_8843 = torch.constant.int 32
    %int64_8844 = torch.constant.int 64
    %5583 = torch.prim.ListConstruct %int4_8842, %273, %int32_8843, %int64_8844 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5584 = torch.aten.view %5582, %5583 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5584, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_8845 = torch.constant.none
    %none_8846 = torch.constant.none
    %int5_8847 = torch.constant.int 5
    %cpu_8848 = torch.constant.device "cpu"
    %int0_8849 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5584, %none_8845, %none_8846, %int5_8847, %cpu_8848, %int0_8849 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_8850 = torch.constant.int 0
    %none_8851 = torch.constant.none
    %none_8852 = torch.constant.none
    %cpu_8853 = torch.constant.device "cpu"
    %false_8854 = torch.constant.bool false
    %5585 = torch.aten.arange.start %int0_8850, %273, %none_8851, %none_8852, %cpu_8853, %false_8854 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5585, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_8855 = torch.constant.int 0
    %5586 = torch.aten.unsqueeze %5585, %int0_8855 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %5586, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_8856 = torch.constant.int 0
    %int64_8857 = torch.constant.int 64
    %int2_8858 = torch.constant.int 2
    %none_8859 = torch.constant.none
    %none_8860 = torch.constant.none
    %cpu_8861 = torch.constant.device "cpu"
    %false_8862 = torch.constant.bool false
    %5587 = torch.aten.arange.start_step %int0_8856, %int64_8857, %int2_8858, %none_8859, %none_8860, %cpu_8861, %false_8862 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_8863 = torch.constant.none
    %none_8864 = torch.constant.none
    %int4_8865 = torch.constant.int 4
    %cpu_8866 = torch.constant.device "cpu"
    %int0_8867 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5587, %none_8863, %none_8864, %int4_8865, %cpu_8866, %int0_8867 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8868 = torch.constant.int 6
    %5588 = torch.prims.convert_element_type %5587, %int6_8868 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_8869 = torch.constant.int 64
    %5589 = torch.aten.div.Scalar %5588, %int64_8869 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_8870 = torch.constant.float 1.000000e+04
    %5590 = torch.aten.pow.Scalar %float1.000000e04_8870, %5589 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %5591 = torch.aten.reciprocal %5590 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_8871 = torch.constant.float 1.000000e+00
    %5592 = torch.aten.mul.Scalar %5591, %float1.000000e00_8871 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_8872 = torch.constant.none
    %5593 = torch.aten.clone %206, %none_8872 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_8873 = torch.constant.int 0
    %5594 = torch.aten.unsqueeze %5592, %int0_8873 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_8874 = torch.constant.int 2
    %5595 = torch.aten.unsqueeze %5594, %int2_8874 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_8875 = torch.constant.none
    %none_8876 = torch.constant.none
    %int6_8877 = torch.constant.int 6
    %cpu_8878 = torch.constant.device "cpu"
    %int0_8879 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5595, %none_8875, %none_8876, %int6_8877, %cpu_8878, %int0_8879 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_8880 = torch.constant.int 1
    %int-1_8881 = torch.constant.int -1
    %int1_8882 = torch.constant.int 1
    %5596 = torch.prim.ListConstruct %int1_8880, %int-1_8881, %int1_8882 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8883 = torch.constant.bool false
    %5597 = torch.aten.expand %5595, %5596, %false_8883 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_8884 = torch.constant.int 1
    %5598 = torch.aten.unsqueeze %5586, %int1_8884 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %5598, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_8885 = torch.constant.none
    %none_8886 = torch.constant.none
    %int4_8887 = torch.constant.int 4
    %cpu_8888 = torch.constant.device "cpu"
    %int0_8889 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5598, %none_8885, %none_8886, %int4_8887, %cpu_8888, %int0_8889 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8890 = torch.constant.int 6
    %5599 = torch.prims.convert_element_type %5598, %int6_8890 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %5599, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %5600 = torch.aten.matmul %5597, %5599 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %5600, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_8891 = torch.constant.int 1
    %int2_8892 = torch.constant.int 2
    %5601 = torch.aten.transpose.int %5600, %int1_8891, %int2_8892 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5601, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %5602 = torch.aten.cos %5601 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5602, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %5603 = torch.aten.mul.Tensor %5602, %5593 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5603, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_8893 = torch.constant.none
    %none_8894 = torch.constant.none
    %int6_8895 = torch.constant.int 6
    %cpu_8896 = torch.constant.device "cpu"
    %int0_8897 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5603, %none_8893, %none_8894, %int6_8895, %cpu_8896, %int0_8897 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8898 = torch.constant.int 5
    %5604 = torch.prims.convert_element_type %5603, %int5_8898 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %5604, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %5605 = torch.aten.sin %5601 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5605, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %5606 = torch.aten.mul.Tensor %5605, %5593 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5606, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_8899 = torch.constant.none
    %none_8900 = torch.constant.none
    %int6_8901 = torch.constant.int 6
    %cpu_8902 = torch.constant.device "cpu"
    %int0_8903 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5606, %none_8899, %none_8900, %int6_8901, %cpu_8902, %int0_8903 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8904 = torch.constant.int 5
    %5607 = torch.prims.convert_element_type %5606, %int5_8904 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %5607, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_8905 = torch.constant.int 2
    %5608 = torch.aten.unsqueeze %5604, %int2_8905 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %5608, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_8906 = torch.constant.int 2
    %5609 = torch.aten.unsqueeze %5607, %int2_8906 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %5609, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_8907 = torch.constant.none
    %none_8908 = torch.constant.none
    %int5_8909 = torch.constant.int 5
    %cpu_8910 = torch.constant.device "cpu"
    %int0_8911 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5608, %none_8907, %none_8908, %int5_8909, %cpu_8910, %int0_8911 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_8912 = torch.constant.none
    %none_8913 = torch.constant.none
    %int5_8914 = torch.constant.int 5
    %cpu_8915 = torch.constant.device "cpu"
    %int0_8916 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5609, %none_8912, %none_8913, %int5_8914, %cpu_8915, %int0_8916 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_8917 = torch.constant.none
    %none_8918 = torch.constant.none
    %int5_8919 = torch.constant.int 5
    %cpu_8920 = torch.constant.device "cpu"
    %int0_8921 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5543, %none_8917, %none_8918, %int5_8919, %cpu_8920, %int0_8921 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_8922 = torch.constant.int 3
    %int0_8923 = torch.constant.int 0
    %int64_8924 = torch.constant.int 64
    %int2_8925 = torch.constant.int 2
    %5610 = torch.aten.slice.Tensor %5543, %int3_8922, %int0_8923, %int64_8924, %int2_8925 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5610, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_8926 = torch.constant.int 3
    %int1_8927 = torch.constant.int 1
    %int64_8928 = torch.constant.int 64
    %int2_8929 = torch.constant.int 2
    %5611 = torch.aten.slice.Tensor %5543, %int3_8926, %int1_8927, %int64_8928, %int2_8929 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5611, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5612 = torch.aten.mul.Tensor %5610, %5608 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5612, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5613 = torch.aten.mul.Tensor %5611, %5609 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5613, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_8930 = torch.constant.int 1
    %5614 = torch.aten.sub.Tensor %5612, %5613, %int1_8930 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5614, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5615 = torch.aten.mul.Tensor %5611, %5608 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5615, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5616 = torch.aten.mul.Tensor %5610, %5609 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5616, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_8931 = torch.constant.int 1
    %5617 = torch.aten.add.Tensor %5615, %5616, %int1_8931 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5617, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5618 = torch_c.to_builtin_tensor %5614 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_8932 = tensor.cast %5618 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %5619 = torch_c.to_builtin_tensor %5617 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_8933 = tensor.cast %5619 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %5620 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_8932, %cast_8933) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_8934 = tensor.cast %5620 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %5621 = torch_c.from_builtin_tensor %cast_8934 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %5621, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_8935 = torch.constant.int 4
    %int4_8936 = torch.constant.int 4
    %int64_8937 = torch.constant.int 64
    %5622 = torch.prim.ListConstruct %int4_8935, %273, %int4_8936, %int64_8937 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5623 = torch.aten.view %5621, %5622 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %5623, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_8938 = torch.constant.none
    %none_8939 = torch.constant.none
    %int5_8940 = torch.constant.int 5
    %cpu_8941 = torch.constant.device "cpu"
    %int0_8942 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5623, %none_8938, %none_8939, %int5_8940, %cpu_8941, %int0_8942 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_8943 = torch.constant.int 22
    %5624 = torch.aten.mul.Scalar %arg2, %int22_8943 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5624, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int17_8944 = torch.constant.int 17
    %int1_8945 = torch.constant.int 1
    %5625 = torch.aten.add.Scalar %5624, %int17_8944, %int1_8945 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5625, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_8946 = torch.constant.int 2
    %5626 = torch.aten.mul.Scalar %5625, %int2_8946 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5626, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_8947 = torch.constant.int 0
    %int1_8948 = torch.constant.int 1
    %5627 = torch.aten.add.Scalar %5626, %int0_8947, %int1_8948 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5627, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %5628 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %5629 = torch.aten.view %5627, %5628 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5629, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_8949 = torch.constant.int 4
    %int32_8950 = torch.constant.int 32
    %int4_8951 = torch.constant.int 4
    %int64_8952 = torch.constant.int 64
    %5630 = torch.prim.ListConstruct %int4_8949, %271, %int32_8950, %int4_8951, %int64_8952 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5631 = torch.aten.view %5623, %5630 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5631, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_8953 = torch.constant.int 32
    %int4_8954 = torch.constant.int 4
    %int64_8955 = torch.constant.int 64
    %5632 = torch.prim.ListConstruct %446, %int32_8953, %int4_8954, %int64_8955 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5633 = torch.aten.view %5631, %5632 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %5633, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_8956 = torch.constant.int 1
    %int2_8957 = torch.constant.int 2
    %5634 = torch.aten.transpose.int %5633, %int1_8956, %int2_8957 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5634, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_8958 = torch.constant.none
    %none_8959 = torch.constant.none
    %int5_8960 = torch.constant.int 5
    %cpu_8961 = torch.constant.device "cpu"
    %int0_8962 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5634, %none_8958, %none_8959, %int5_8960, %cpu_8961, %int0_8962 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_8963 = torch.constant.int 22
    %int2_8964 = torch.constant.int 2
    %int4_8965 = torch.constant.int 4
    %int32_8966 = torch.constant.int 32
    %int64_8967 = torch.constant.int 64
    %5635 = torch.prim.ListConstruct %272, %int22_8963, %int2_8964, %int4_8965, %int32_8966, %int64_8967 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5636 = torch.aten.view %5360, %5635 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5636, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_8968 = torch.constant.int 4
    %int32_8969 = torch.constant.int 32
    %int64_8970 = torch.constant.int 64
    %5637 = torch.prim.ListConstruct %439, %int4_8968, %int32_8969, %int64_8970 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5638 = torch.aten.view %5636, %5637 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5638, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %5639 = torch.prim.ListConstruct %5629 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_8971 = torch.constant.bool false
    %5640 = torch.aten.index_put %5638, %5639, %5634, %false_8971 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5640, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_8972 = torch.constant.int 22
    %int2_8973 = torch.constant.int 2
    %int4_8974 = torch.constant.int 4
    %int32_8975 = torch.constant.int 32
    %int64_8976 = torch.constant.int 64
    %5641 = torch.prim.ListConstruct %272, %int22_8972, %int2_8973, %int4_8974, %int32_8975, %int64_8976 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5642 = torch.aten.view %5640, %5641 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5642, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_8977 = torch.constant.int 360448
    %5643 = torch.prim.ListConstruct %272, %int360448_8977 : (!torch.int, !torch.int) -> !torch.list<int>
    %5644 = torch.aten.view %5642, %5643 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %5644, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_8978 = torch.constant.int 22
    %int2_8979 = torch.constant.int 2
    %int4_8980 = torch.constant.int 4
    %int32_8981 = torch.constant.int 32
    %int64_8982 = torch.constant.int 64
    %5645 = torch.prim.ListConstruct %272, %int22_8978, %int2_8979, %int4_8980, %int32_8981, %int64_8982 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5646 = torch.aten.view %5644, %5645 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5646, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_8983 = torch.constant.int 4
    %int32_8984 = torch.constant.int 32
    %int64_8985 = torch.constant.int 64
    %5647 = torch.prim.ListConstruct %439, %int4_8983, %int32_8984, %int64_8985 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5648 = torch.aten.view %5646, %5647 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5648, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_8986 = torch.constant.int 22
    %5649 = torch.aten.mul.Scalar %arg2, %int22_8986 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5649, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int17_8987 = torch.constant.int 17
    %int1_8988 = torch.constant.int 1
    %5650 = torch.aten.add.Scalar %5649, %int17_8987, %int1_8988 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5650, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_8989 = torch.constant.int 2
    %5651 = torch.aten.mul.Scalar %5650, %int2_8989 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5651, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_8990 = torch.constant.int 1
    %int1_8991 = torch.constant.int 1
    %5652 = torch.aten.add.Scalar %5651, %int1_8990, %int1_8991 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5652, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %5653 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %5654 = torch.aten.view %5652, %5653 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5654, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_8992 = torch.constant.int 4
    %int32_8993 = torch.constant.int 32
    %int4_8994 = torch.constant.int 4
    %int64_8995 = torch.constant.int 64
    %5655 = torch.prim.ListConstruct %int4_8992, %271, %int32_8993, %int4_8994, %int64_8995 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5656 = torch.aten.view %5545, %5655 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5656, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_8996 = torch.constant.int 32
    %int4_8997 = torch.constant.int 4
    %int64_8998 = torch.constant.int 64
    %5657 = torch.prim.ListConstruct %446, %int32_8996, %int4_8997, %int64_8998 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5658 = torch.aten.view %5656, %5657 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %5658, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_8999 = torch.constant.int 1
    %int2_9000 = torch.constant.int 2
    %5659 = torch.aten.transpose.int %5658, %int1_8999, %int2_9000 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5659, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_9001 = torch.constant.none
    %none_9002 = torch.constant.none
    %int5_9003 = torch.constant.int 5
    %cpu_9004 = torch.constant.device "cpu"
    %int0_9005 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5659, %none_9001, %none_9002, %int5_9003, %cpu_9004, %int0_9005 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %5660 = torch.prim.ListConstruct %5654 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_9006 = torch.constant.bool false
    %5661 = torch.aten.index_put %5648, %5660, %5659, %false_9006 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5661, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_9007 = torch.constant.int 22
    %int2_9008 = torch.constant.int 2
    %int4_9009 = torch.constant.int 4
    %int32_9010 = torch.constant.int 32
    %int64_9011 = torch.constant.int 64
    %5662 = torch.prim.ListConstruct %272, %int22_9007, %int2_9008, %int4_9009, %int32_9010, %int64_9011 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5663 = torch.aten.view %5661, %5662 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5663, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_9012 = torch.constant.int 360448
    %5664 = torch.prim.ListConstruct %272, %int360448_9012 : (!torch.int, !torch.int) -> !torch.list<int>
    %5665 = torch.aten.view %5663, %5664 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %5665, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_9013 = torch.constant.int 0
    %int1_9014 = torch.constant.int 1
    %none_9015 = torch.constant.none
    %none_9016 = torch.constant.none
    %cpu_9017 = torch.constant.device "cpu"
    %false_9018 = torch.constant.bool false
    %5666 = torch.aten.arange.start_step %int0_9013, %273, %int1_9014, %none_9015, %none_9016, %cpu_9017, %false_9018 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5666, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_9019 = torch.constant.int -1
    %5667 = torch.aten.unsqueeze %arg1, %int-1_9019 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %5668 = torch.aten.ge.Tensor %5666, %5667 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %5668, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_9020 = torch.constant.none
    %none_9021 = torch.constant.none
    %cpu_9022 = torch.constant.device "cpu"
    %false_9023 = torch.constant.bool false
    %5669 = torch.aten.arange %273, %none_9020, %none_9021, %cpu_9022, %false_9023 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5669, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_9024 = torch.constant.int 0
    %5670 = torch.aten.unsqueeze %5669, %int0_9024 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %5670, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_9025 = torch.constant.int 1
    %5671 = torch.aten.unsqueeze %5670, %int1_9025 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %5671, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_9026 = torch.constant.int 2
    %5672 = torch.aten.unsqueeze %5671, %int2_9026 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %5672, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_9027 = torch.constant.none
    %none_9028 = torch.constant.none
    %cpu_9029 = torch.constant.device "cpu"
    %false_9030 = torch.constant.bool false
    %5673 = torch.aten.arange %273, %none_9027, %none_9028, %cpu_9029, %false_9030 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5673, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_9031 = torch.constant.int 0
    %5674 = torch.aten.unsqueeze %5673, %int0_9031 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %5674, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_9032 = torch.constant.int 1
    %5675 = torch.aten.unsqueeze %5674, %int1_9032 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %5675, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_9033 = torch.constant.int 3
    %5676 = torch.aten.unsqueeze %5675, %int3_9033 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %5676, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %5677 = torch.aten.gt.Tensor %5672, %5676 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %5677, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_9034 = torch.constant.int 1
    %5678 = torch.aten.unsqueeze %5668, %int1_9034 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %5678, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_9035 = torch.constant.int 2
    %5679 = torch.aten.unsqueeze %5678, %int2_9035 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %5679, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %5680 = torch.aten.logical_or %5677, %5679 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %5680, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_9036 = torch.constant.none
    %5681 = torch.aten.clone %207, %none_9036 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_9037 = torch.constant.int 0
    %5682 = torch.aten.where.ScalarOther %5680, %5681, %int0_9037 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %5682, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_9038 = torch.constant.none
    %none_9039 = torch.constant.none
    %int5_9040 = torch.constant.int 5
    %cpu_9041 = torch.constant.device "cpu"
    %int0_9042 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5682, %none_9038, %none_9039, %int5_9040, %cpu_9041, %int0_9042 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_9043 = torch.constant.int -2
    %5683 = torch.aten.unsqueeze %5623, %int-2_9043 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %5683, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_9044 = torch.constant.int 4
    %int4_9045 = torch.constant.int 4
    %int8_9046 = torch.constant.int 8
    %int64_9047 = torch.constant.int 64
    %5684 = torch.prim.ListConstruct %int4_9044, %273, %int4_9045, %int8_9046, %int64_9047 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_9048 = torch.constant.bool false
    %5685 = torch.aten.expand %5683, %5684, %false_9048 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5685, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_9049 = torch.constant.int 0
    %5686 = torch.aten.clone %5685, %int0_9049 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5686, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_9050 = torch.constant.int 4
    %int32_9051 = torch.constant.int 32
    %int64_9052 = torch.constant.int 64
    %5687 = torch.prim.ListConstruct %int4_9050, %273, %int32_9051, %int64_9052 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5688 = torch.aten._unsafe_view %5686, %5687 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5688, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_9053 = torch.constant.int -2
    %5689 = torch.aten.unsqueeze %5545, %int-2_9053 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %5689, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_9054 = torch.constant.int 4
    %int4_9055 = torch.constant.int 4
    %int8_9056 = torch.constant.int 8
    %int64_9057 = torch.constant.int 64
    %5690 = torch.prim.ListConstruct %int4_9054, %273, %int4_9055, %int8_9056, %int64_9057 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_9058 = torch.constant.bool false
    %5691 = torch.aten.expand %5689, %5690, %false_9058 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5691, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_9059 = torch.constant.int 0
    %5692 = torch.aten.clone %5691, %int0_9059 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5692, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_9060 = torch.constant.int 4
    %int32_9061 = torch.constant.int 32
    %int64_9062 = torch.constant.int 64
    %5693 = torch.prim.ListConstruct %int4_9060, %273, %int32_9061, %int64_9062 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5694 = torch.aten._unsafe_view %5692, %5693 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5694, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_9063 = torch.constant.int 1
    %int2_9064 = torch.constant.int 2
    %5695 = torch.aten.transpose.int %5584, %int1_9063, %int2_9064 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %5695, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_9065 = torch.constant.int 1
    %int2_9066 = torch.constant.int 2
    %5696 = torch.aten.transpose.int %5688, %int1_9065, %int2_9066 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %5696, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_9067 = torch.constant.int 1
    %int2_9068 = torch.constant.int 2
    %5697 = torch.aten.transpose.int %5694, %int1_9067, %int2_9068 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %5697, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_9069 = torch.constant.float 0.000000e+00
    %false_9070 = torch.constant.bool false
    %none_9071 = torch.constant.none
    %false_9072 = torch.constant.bool false
    %5698 = torch.aten.scaled_dot_product_attention %5695, %5696, %5697, %5682, %float0.000000e00_9069, %false_9070, %none_9071, %false_9072 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %5698, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_9073 = torch.constant.int 1
    %int2_9074 = torch.constant.int 2
    %5699 = torch.aten.transpose.int %5698, %int1_9073, %int2_9074 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5699, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_9075 = torch.constant.int 4
    %int2048_9076 = torch.constant.int 2048
    %5700 = torch.prim.ListConstruct %int4_9075, %273, %int2048_9076 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5701 = torch.aten.view %5699, %5700 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5701, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_9077 = torch.constant.int 2
    %5702 = torch.aten.view.dtype %212, %int2_9077 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %5703 = torch.aten.detach %5702 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_9078 = torch.constant.int -1
    %int17_9079 = torch.constant.int 17
    %5704 = torch.prim.ListConstruct %int-1_9078, %int17_9079 : (!torch.int, !torch.int) -> !torch.list<int>
    %5705 = torch.aten.view %5703, %5704 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_9080 = torch.constant.int 2048
    %int-1_9081 = torch.constant.int -1
    %int17_9082 = torch.constant.int 17
    %5706 = torch.prim.ListConstruct %int2048_9080, %int-1_9081, %int17_9082 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5707 = torch.aten.view %5705, %5706 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_9083 = torch.constant.int 2
    %int0_9084 = torch.constant.int 0
    %int1_9085 = torch.constant.int 1
    %int1_9086 = torch.constant.int 1
    %5708 = torch.aten.slice.Tensor %5707, %int2_9083, %int0_9084, %int1_9085, %int1_9086 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_9087 = torch.constant.int 5
    %5709 = torch.aten.view.dtype %5708, %int5_9087 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %5710 = torch.aten.detach %5709 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_9088 = torch.constant.int 2
    %int1_9089 = torch.constant.int 1
    %int9223372036854775807_9090 = torch.constant.int 9223372036854775807
    %int1_9091 = torch.constant.int 1
    %5711 = torch.aten.slice.Tensor %5707, %int2_9088, %int1_9089, %int9223372036854775807_9090, %int1_9091 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_9092 = torch.constant.int 1
    %5712 = torch.aten.view.dtype %5711, %int1_9092 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %5713 = torch.aten.detach %5712 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %5714 = torch_c.to_builtin_tensor %5701 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_9093 = tensor.cast %5714 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %5715 = torch_c.to_builtin_tensor %5710 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %5716 = torch_c.to_builtin_tensor %5713 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %5717 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_9093, %5715, %5716) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_9094 = tensor.cast %5717 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %5718 = torch_c.from_builtin_tensor %cast_9094 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5718, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_9095 = torch.constant.none
    %none_9096 = torch.constant.none
    %int5_9097 = torch.constant.int 5
    %cpu_9098 = torch.constant.device "cpu"
    %int0_9099 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5718, %none_9095, %none_9096, %int5_9097, %cpu_9098, %int0_9099 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_9100 = torch.constant.int 1
    %5719 = torch.aten.add.Tensor %5478, %5718, %int1_9100 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5719, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_9101 = torch.constant.none
    %none_9102 = torch.constant.none
    %int5_9103 = torch.constant.int 5
    %cpu_9104 = torch.constant.device "cpu"
    %int0_9105 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5719, %none_9101, %none_9102, %int5_9103, %cpu_9104, %int0_9105 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9106 = torch.constant.int 6
    %5720 = torch.prims.convert_element_type %5719, %int6_9106 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5720, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_9107 = torch.constant.int 2
    %5721 = torch.aten.pow.Tensor_Scalar %5720, %int2_9107 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5721, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_9108 = torch.constant.int -1
    %5722 = torch.prim.ListConstruct %int-1_9108 : (!torch.int) -> !torch.list<int>
    %true_9109 = torch.constant.bool true
    %none_9110 = torch.constant.none
    %5723 = torch.aten.mean.dim %5721, %5722, %true_9109, %none_9110 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5723, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_9111 = torch.constant.float 9.9999997473787516E-6
    %int1_9112 = torch.constant.int 1
    %5724 = torch.aten.add.Scalar %5723, %float9.999990e-06_9111, %int1_9112 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5724, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5725 = torch.aten.rsqrt %5724 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5725, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5726 = torch.aten.mul.Tensor %5720, %5725 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5726, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_9113 = torch.constant.none
    %none_9114 = torch.constant.none
    %int6_9115 = torch.constant.int 6
    %cpu_9116 = torch.constant.device "cpu"
    %int0_9117 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5726, %none_9113, %none_9114, %int6_9115, %cpu_9116, %int0_9117 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9118 = torch.constant.int 5
    %5727 = torch.prims.convert_element_type %5726, %int5_9118 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5727, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %5728 = torch.aten.mul.Tensor %213, %5727 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5728, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_9119 = torch.constant.none
    %none_9120 = torch.constant.none
    %int6_9121 = torch.constant.int 6
    %cpu_9122 = torch.constant.device "cpu"
    %int0_9123 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5728, %none_9119, %none_9120, %int6_9121, %cpu_9122, %int0_9123 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9124 = torch.constant.int 5
    %5729 = torch.prims.convert_element_type %5728, %int5_9124 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5729, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_9125 = torch.constant.int 2
    %5730 = torch.aten.view.dtype %214, %int2_9125 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %5731 = torch.aten.detach %5730 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_9126 = torch.constant.int -1
    %int17_9127 = torch.constant.int 17
    %5732 = torch.prim.ListConstruct %int-1_9126, %int17_9127 : (!torch.int, !torch.int) -> !torch.list<int>
    %5733 = torch.aten.view %5731, %5732 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_9128 = torch.constant.int 5632
    %int-1_9129 = torch.constant.int -1
    %int17_9130 = torch.constant.int 17
    %5734 = torch.prim.ListConstruct %int5632_9128, %int-1_9129, %int17_9130 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5735 = torch.aten.view %5733, %5734 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_9131 = torch.constant.int 2
    %int0_9132 = torch.constant.int 0
    %int1_9133 = torch.constant.int 1
    %int1_9134 = torch.constant.int 1
    %5736 = torch.aten.slice.Tensor %5735, %int2_9131, %int0_9132, %int1_9133, %int1_9134 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_9135 = torch.constant.int 5
    %5737 = torch.aten.view.dtype %5736, %int5_9135 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %5738 = torch.aten.detach %5737 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_9136 = torch.constant.int 2
    %int1_9137 = torch.constant.int 1
    %int9223372036854775807_9138 = torch.constant.int 9223372036854775807
    %int1_9139 = torch.constant.int 1
    %5739 = torch.aten.slice.Tensor %5735, %int2_9136, %int1_9137, %int9223372036854775807_9138, %int1_9139 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_9140 = torch.constant.int 1
    %5740 = torch.aten.view.dtype %5739, %int1_9140 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %5741 = torch.aten.detach %5740 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %5742 = torch_c.to_builtin_tensor %5729 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_9141 = tensor.cast %5742 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %5743 = torch_c.to_builtin_tensor %5738 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %5744 = torch_c.to_builtin_tensor %5741 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %5745 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_9141, %5743, %5744) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_9142 = tensor.cast %5745 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %5746 = torch_c.from_builtin_tensor %cast_9142 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %5746, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %5747 = torch.aten.silu %5746 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %5747, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_9143 = torch.constant.int 2
    %5748 = torch.aten.view.dtype %215, %int2_9143 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %5749 = torch.aten.detach %5748 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_9144 = torch.constant.int -1
    %int17_9145 = torch.constant.int 17
    %5750 = torch.prim.ListConstruct %int-1_9144, %int17_9145 : (!torch.int, !torch.int) -> !torch.list<int>
    %5751 = torch.aten.view %5749, %5750 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_9146 = torch.constant.int 5632
    %int-1_9147 = torch.constant.int -1
    %int17_9148 = torch.constant.int 17
    %5752 = torch.prim.ListConstruct %int5632_9146, %int-1_9147, %int17_9148 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5753 = torch.aten.view %5751, %5752 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_9149 = torch.constant.int 2
    %int0_9150 = torch.constant.int 0
    %int1_9151 = torch.constant.int 1
    %int1_9152 = torch.constant.int 1
    %5754 = torch.aten.slice.Tensor %5753, %int2_9149, %int0_9150, %int1_9151, %int1_9152 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_9153 = torch.constant.int 5
    %5755 = torch.aten.view.dtype %5754, %int5_9153 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %5756 = torch.aten.detach %5755 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_9154 = torch.constant.int 2
    %int1_9155 = torch.constant.int 1
    %int9223372036854775807_9156 = torch.constant.int 9223372036854775807
    %int1_9157 = torch.constant.int 1
    %5757 = torch.aten.slice.Tensor %5753, %int2_9154, %int1_9155, %int9223372036854775807_9156, %int1_9157 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_9158 = torch.constant.int 1
    %5758 = torch.aten.view.dtype %5757, %int1_9158 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %5759 = torch.aten.detach %5758 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %5760 = torch_c.to_builtin_tensor %5729 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_9159 = tensor.cast %5760 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %5761 = torch_c.to_builtin_tensor %5756 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %5762 = torch_c.to_builtin_tensor %5759 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %5763 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_9159, %5761, %5762) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_9160 = tensor.cast %5763 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %5764 = torch_c.from_builtin_tensor %cast_9160 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %5764, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %5765 = torch.aten.mul.Tensor %5747, %5764 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %5765, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_9161 = torch.constant.int 2
    %5766 = torch.aten.view.dtype %216, %int2_9161 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %5767 = torch.aten.detach %5766 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_9162 = torch.constant.int -1
    %int17_9163 = torch.constant.int 17
    %5768 = torch.prim.ListConstruct %int-1_9162, %int17_9163 : (!torch.int, !torch.int) -> !torch.list<int>
    %5769 = torch.aten.view %5767, %5768 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_9164 = torch.constant.int 2048
    %int-1_9165 = torch.constant.int -1
    %int17_9166 = torch.constant.int 17
    %5770 = torch.prim.ListConstruct %int2048_9164, %int-1_9165, %int17_9166 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5771 = torch.aten.view %5769, %5770 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_9167 = torch.constant.int 2
    %int0_9168 = torch.constant.int 0
    %int1_9169 = torch.constant.int 1
    %int1_9170 = torch.constant.int 1
    %5772 = torch.aten.slice.Tensor %5771, %int2_9167, %int0_9168, %int1_9169, %int1_9170 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_9171 = torch.constant.int 5
    %5773 = torch.aten.view.dtype %5772, %int5_9171 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %5774 = torch.aten.detach %5773 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_9172 = torch.constant.int 2
    %int1_9173 = torch.constant.int 1
    %int9223372036854775807_9174 = torch.constant.int 9223372036854775807
    %int1_9175 = torch.constant.int 1
    %5775 = torch.aten.slice.Tensor %5771, %int2_9172, %int1_9173, %int9223372036854775807_9174, %int1_9175 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_9176 = torch.constant.int 1
    %5776 = torch.aten.view.dtype %5775, %int1_9176 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %5777 = torch.aten.detach %5776 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %5778 = torch_c.to_builtin_tensor %5765 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_9177 = tensor.cast %5778 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %5779 = torch_c.to_builtin_tensor %5774 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %5780 = torch_c.to_builtin_tensor %5777 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %5781 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_9177, %5779, %5780) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_9178 = tensor.cast %5781 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %5782 = torch_c.from_builtin_tensor %cast_9178 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5782, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_9179 = torch.constant.int 1
    %5783 = torch.aten.add.Tensor %5719, %5782, %int1_9179 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5783, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_9180 = torch.constant.none
    %none_9181 = torch.constant.none
    %int5_9182 = torch.constant.int 5
    %cpu_9183 = torch.constant.device "cpu"
    %int0_9184 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5783, %none_9180, %none_9181, %int5_9182, %cpu_9183, %int0_9184 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9185 = torch.constant.int 6
    %5784 = torch.prims.convert_element_type %5783, %int6_9185 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5784, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_9186 = torch.constant.int 2
    %5785 = torch.aten.pow.Tensor_Scalar %5784, %int2_9186 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5785, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_9187 = torch.constant.int -1
    %5786 = torch.prim.ListConstruct %int-1_9187 : (!torch.int) -> !torch.list<int>
    %true_9188 = torch.constant.bool true
    %none_9189 = torch.constant.none
    %5787 = torch.aten.mean.dim %5785, %5786, %true_9188, %none_9189 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5787, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_9190 = torch.constant.float 9.9999997473787516E-6
    %int1_9191 = torch.constant.int 1
    %5788 = torch.aten.add.Scalar %5787, %float9.999990e-06_9190, %int1_9191 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5788, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5789 = torch.aten.rsqrt %5788 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5789, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5790 = torch.aten.mul.Tensor %5784, %5789 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5790, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_9192 = torch.constant.none
    %none_9193 = torch.constant.none
    %int6_9194 = torch.constant.int 6
    %cpu_9195 = torch.constant.device "cpu"
    %int0_9196 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5790, %none_9192, %none_9193, %int6_9194, %cpu_9195, %int0_9196 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9197 = torch.constant.int 5
    %5791 = torch.prims.convert_element_type %5790, %int5_9197 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5791, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %5792 = torch.aten.mul.Tensor %220, %5791 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %5792, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_9198 = torch.constant.none
    %none_9199 = torch.constant.none
    %int6_9200 = torch.constant.int 6
    %cpu_9201 = torch.constant.device "cpu"
    %int0_9202 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5792, %none_9198, %none_9199, %int6_9200, %cpu_9201, %int0_9202 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9203 = torch.constant.int 5
    %5793 = torch.prims.convert_element_type %5792, %int5_9203 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5793, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_9204 = torch.constant.int 2
    %5794 = torch.aten.view.dtype %221, %int2_9204 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %5795 = torch.aten.detach %5794 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_9205 = torch.constant.int -1
    %int17_9206 = torch.constant.int 17
    %5796 = torch.prim.ListConstruct %int-1_9205, %int17_9206 : (!torch.int, !torch.int) -> !torch.list<int>
    %5797 = torch.aten.view %5795, %5796 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_9207 = torch.constant.int 2048
    %int-1_9208 = torch.constant.int -1
    %int17_9209 = torch.constant.int 17
    %5798 = torch.prim.ListConstruct %int2048_9207, %int-1_9208, %int17_9209 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5799 = torch.aten.view %5797, %5798 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_9210 = torch.constant.int 2
    %int0_9211 = torch.constant.int 0
    %int1_9212 = torch.constant.int 1
    %int1_9213 = torch.constant.int 1
    %5800 = torch.aten.slice.Tensor %5799, %int2_9210, %int0_9211, %int1_9212, %int1_9213 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_9214 = torch.constant.int 5
    %5801 = torch.aten.view.dtype %5800, %int5_9214 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %5802 = torch.aten.detach %5801 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_9215 = torch.constant.int 2
    %int1_9216 = torch.constant.int 1
    %int9223372036854775807_9217 = torch.constant.int 9223372036854775807
    %int1_9218 = torch.constant.int 1
    %5803 = torch.aten.slice.Tensor %5799, %int2_9215, %int1_9216, %int9223372036854775807_9217, %int1_9218 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_9219 = torch.constant.int 1
    %5804 = torch.aten.view.dtype %5803, %int1_9219 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %5805 = torch.aten.detach %5804 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %5806 = torch_c.to_builtin_tensor %5793 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_9220 = tensor.cast %5806 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %5807 = torch_c.to_builtin_tensor %5802 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %5808 = torch_c.to_builtin_tensor %5805 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %5809 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_9220, %5807, %5808) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_9221 = tensor.cast %5809 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %5810 = torch_c.from_builtin_tensor %cast_9221 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %5810, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_9222 = torch.constant.int 2
    %5811 = torch.aten.view.dtype %222, %int2_9222 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %5812 = torch.aten.detach %5811 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_9223 = torch.constant.int -1
    %int17_9224 = torch.constant.int 17
    %5813 = torch.prim.ListConstruct %int-1_9223, %int17_9224 : (!torch.int, !torch.int) -> !torch.list<int>
    %5814 = torch.aten.view %5812, %5813 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_9225 = torch.constant.int 256
    %int-1_9226 = torch.constant.int -1
    %int17_9227 = torch.constant.int 17
    %5815 = torch.prim.ListConstruct %int256_9225, %int-1_9226, %int17_9227 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5816 = torch.aten.view %5814, %5815 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_9228 = torch.constant.int 2
    %int0_9229 = torch.constant.int 0
    %int1_9230 = torch.constant.int 1
    %int1_9231 = torch.constant.int 1
    %5817 = torch.aten.slice.Tensor %5816, %int2_9228, %int0_9229, %int1_9230, %int1_9231 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_9232 = torch.constant.int 5
    %5818 = torch.aten.view.dtype %5817, %int5_9232 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %5819 = torch.aten.detach %5818 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_9233 = torch.constant.int 2
    %int1_9234 = torch.constant.int 1
    %int9223372036854775807_9235 = torch.constant.int 9223372036854775807
    %int1_9236 = torch.constant.int 1
    %5820 = torch.aten.slice.Tensor %5816, %int2_9233, %int1_9234, %int9223372036854775807_9235, %int1_9236 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_9237 = torch.constant.int 1
    %5821 = torch.aten.view.dtype %5820, %int1_9237 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %5822 = torch.aten.detach %5821 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %5823 = torch_c.to_builtin_tensor %5793 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_9238 = tensor.cast %5823 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %5824 = torch_c.to_builtin_tensor %5819 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %5825 = torch_c.to_builtin_tensor %5822 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %5826 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_9238, %5824, %5825) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_9239 = tensor.cast %5826 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %5827 = torch_c.from_builtin_tensor %cast_9239 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %5827, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_9240 = torch.constant.int 2
    %5828 = torch.aten.view.dtype %223, %int2_9240 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %5829 = torch.aten.detach %5828 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_9241 = torch.constant.int -1
    %int17_9242 = torch.constant.int 17
    %5830 = torch.prim.ListConstruct %int-1_9241, %int17_9242 : (!torch.int, !torch.int) -> !torch.list<int>
    %5831 = torch.aten.view %5829, %5830 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_9243 = torch.constant.int 256
    %int-1_9244 = torch.constant.int -1
    %int17_9245 = torch.constant.int 17
    %5832 = torch.prim.ListConstruct %int256_9243, %int-1_9244, %int17_9245 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5833 = torch.aten.view %5831, %5832 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_9246 = torch.constant.int 2
    %int0_9247 = torch.constant.int 0
    %int1_9248 = torch.constant.int 1
    %int1_9249 = torch.constant.int 1
    %5834 = torch.aten.slice.Tensor %5833, %int2_9246, %int0_9247, %int1_9248, %int1_9249 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_9250 = torch.constant.int 5
    %5835 = torch.aten.view.dtype %5834, %int5_9250 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %5836 = torch.aten.detach %5835 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_9251 = torch.constant.int 2
    %int1_9252 = torch.constant.int 1
    %int9223372036854775807_9253 = torch.constant.int 9223372036854775807
    %int1_9254 = torch.constant.int 1
    %5837 = torch.aten.slice.Tensor %5833, %int2_9251, %int1_9252, %int9223372036854775807_9253, %int1_9254 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_9255 = torch.constant.int 1
    %5838 = torch.aten.view.dtype %5837, %int1_9255 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %5839 = torch.aten.detach %5838 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %5840 = torch_c.to_builtin_tensor %5793 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_9256 = tensor.cast %5840 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %5841 = torch_c.to_builtin_tensor %5836 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %5842 = torch_c.to_builtin_tensor %5839 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %5843 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_9256, %5841, %5842) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_9257 = tensor.cast %5843 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %5844 = torch_c.from_builtin_tensor %cast_9257 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %5844, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_9258 = torch.constant.int 4
    %int32_9259 = torch.constant.int 32
    %int64_9260 = torch.constant.int 64
    %5845 = torch.prim.ListConstruct %int4_9258, %273, %int32_9259, %int64_9260 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5846 = torch.aten.view %5810, %5845 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5846, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_9261 = torch.constant.int 4
    %int4_9262 = torch.constant.int 4
    %int64_9263 = torch.constant.int 64
    %5847 = torch.prim.ListConstruct %int4_9261, %273, %int4_9262, %int64_9263 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5848 = torch.aten.view %5827, %5847 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %5848, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_9264 = torch.constant.int 4
    %int4_9265 = torch.constant.int 4
    %int64_9266 = torch.constant.int 64
    %5849 = torch.prim.ListConstruct %int4_9264, %273, %int4_9265, %int64_9266 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5850 = torch.aten.view %5844, %5849 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %5850, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_9267 = torch.constant.int 0
    %none_9268 = torch.constant.none
    %none_9269 = torch.constant.none
    %cpu_9270 = torch.constant.device "cpu"
    %false_9271 = torch.constant.bool false
    %5851 = torch.aten.arange.start %int0_9267, %273, %none_9268, %none_9269, %cpu_9270, %false_9271 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5851, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_9272 = torch.constant.int 0
    %5852 = torch.aten.unsqueeze %5851, %int0_9272 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %5852, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_9273 = torch.constant.int 0
    %int64_9274 = torch.constant.int 64
    %int2_9275 = torch.constant.int 2
    %none_9276 = torch.constant.none
    %none_9277 = torch.constant.none
    %cpu_9278 = torch.constant.device "cpu"
    %false_9279 = torch.constant.bool false
    %5853 = torch.aten.arange.start_step %int0_9273, %int64_9274, %int2_9275, %none_9276, %none_9277, %cpu_9278, %false_9279 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_9280 = torch.constant.none
    %none_9281 = torch.constant.none
    %int4_9282 = torch.constant.int 4
    %cpu_9283 = torch.constant.device "cpu"
    %int0_9284 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5853, %none_9280, %none_9281, %int4_9282, %cpu_9283, %int0_9284 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9285 = torch.constant.int 6
    %5854 = torch.prims.convert_element_type %5853, %int6_9285 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_9286 = torch.constant.int 64
    %5855 = torch.aten.div.Scalar %5854, %int64_9286 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_9287 = torch.constant.float 1.000000e+04
    %5856 = torch.aten.pow.Scalar %float1.000000e04_9287, %5855 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %5857 = torch.aten.reciprocal %5856 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_9288 = torch.constant.float 1.000000e+00
    %5858 = torch.aten.mul.Scalar %5857, %float1.000000e00_9288 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_9289 = torch.constant.none
    %5859 = torch.aten.clone %217, %none_9289 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_9290 = torch.constant.int 0
    %5860 = torch.aten.unsqueeze %5858, %int0_9290 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_9291 = torch.constant.int 2
    %5861 = torch.aten.unsqueeze %5860, %int2_9291 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_9292 = torch.constant.none
    %none_9293 = torch.constant.none
    %int6_9294 = torch.constant.int 6
    %cpu_9295 = torch.constant.device "cpu"
    %int0_9296 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5861, %none_9292, %none_9293, %int6_9294, %cpu_9295, %int0_9296 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_9297 = torch.constant.int 1
    %int-1_9298 = torch.constant.int -1
    %int1_9299 = torch.constant.int 1
    %5862 = torch.prim.ListConstruct %int1_9297, %int-1_9298, %int1_9299 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_9300 = torch.constant.bool false
    %5863 = torch.aten.expand %5861, %5862, %false_9300 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_9301 = torch.constant.int 1
    %5864 = torch.aten.unsqueeze %5852, %int1_9301 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %5864, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_9302 = torch.constant.none
    %none_9303 = torch.constant.none
    %int4_9304 = torch.constant.int 4
    %cpu_9305 = torch.constant.device "cpu"
    %int0_9306 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5864, %none_9302, %none_9303, %int4_9304, %cpu_9305, %int0_9306 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9307 = torch.constant.int 6
    %5865 = torch.prims.convert_element_type %5864, %int6_9307 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %5865, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %5866 = torch.aten.matmul %5863, %5865 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %5866, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_9308 = torch.constant.int 1
    %int2_9309 = torch.constant.int 2
    %5867 = torch.aten.transpose.int %5866, %int1_9308, %int2_9309 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5867, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %5868 = torch.aten.cos %5867 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5868, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %5869 = torch.aten.mul.Tensor %5868, %5859 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5869, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_9310 = torch.constant.none
    %none_9311 = torch.constant.none
    %int6_9312 = torch.constant.int 6
    %cpu_9313 = torch.constant.device "cpu"
    %int0_9314 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5869, %none_9310, %none_9311, %int6_9312, %cpu_9313, %int0_9314 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9315 = torch.constant.int 5
    %5870 = torch.prims.convert_element_type %5869, %int5_9315 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %5870, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %5871 = torch.aten.sin %5867 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5871, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %5872 = torch.aten.mul.Tensor %5871, %5859 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5872, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_9316 = torch.constant.none
    %none_9317 = torch.constant.none
    %int6_9318 = torch.constant.int 6
    %cpu_9319 = torch.constant.device "cpu"
    %int0_9320 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5872, %none_9316, %none_9317, %int6_9318, %cpu_9319, %int0_9320 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9321 = torch.constant.int 5
    %5873 = torch.prims.convert_element_type %5872, %int5_9321 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %5873, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_9322 = torch.constant.int 2
    %5874 = torch.aten.unsqueeze %5870, %int2_9322 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %5874, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_9323 = torch.constant.int 2
    %5875 = torch.aten.unsqueeze %5873, %int2_9323 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %5875, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_9324 = torch.constant.none
    %none_9325 = torch.constant.none
    %int5_9326 = torch.constant.int 5
    %cpu_9327 = torch.constant.device "cpu"
    %int0_9328 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5874, %none_9324, %none_9325, %int5_9326, %cpu_9327, %int0_9328 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_9329 = torch.constant.none
    %none_9330 = torch.constant.none
    %int5_9331 = torch.constant.int 5
    %cpu_9332 = torch.constant.device "cpu"
    %int0_9333 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5875, %none_9329, %none_9330, %int5_9331, %cpu_9332, %int0_9333 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_9334 = torch.constant.none
    %none_9335 = torch.constant.none
    %int5_9336 = torch.constant.int 5
    %cpu_9337 = torch.constant.device "cpu"
    %int0_9338 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5846, %none_9334, %none_9335, %int5_9336, %cpu_9337, %int0_9338 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_9339 = torch.constant.int 3
    %int0_9340 = torch.constant.int 0
    %int64_9341 = torch.constant.int 64
    %int2_9342 = torch.constant.int 2
    %5876 = torch.aten.slice.Tensor %5846, %int3_9339, %int0_9340, %int64_9341, %int2_9342 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5876, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_9343 = torch.constant.int 3
    %int1_9344 = torch.constant.int 1
    %int64_9345 = torch.constant.int 64
    %int2_9346 = torch.constant.int 2
    %5877 = torch.aten.slice.Tensor %5846, %int3_9343, %int1_9344, %int64_9345, %int2_9346 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5877, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %5878 = torch.aten.mul.Tensor %5876, %5874 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5878, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %5879 = torch.aten.mul.Tensor %5877, %5875 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5879, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_9347 = torch.constant.int 1
    %5880 = torch.aten.sub.Tensor %5878, %5879, %int1_9347 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5880, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %5881 = torch.aten.mul.Tensor %5877, %5874 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5881, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %5882 = torch.aten.mul.Tensor %5876, %5875 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5882, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_9348 = torch.constant.int 1
    %5883 = torch.aten.add.Tensor %5881, %5882, %int1_9348 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %5883, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %5884 = torch_c.to_builtin_tensor %5880 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_9349 = tensor.cast %5884 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %5885 = torch_c.to_builtin_tensor %5883 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_9350 = tensor.cast %5885 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %5886 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_9349, %cast_9350) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_9351 = tensor.cast %5886 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %5887 = torch_c.from_builtin_tensor %cast_9351 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %5887, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_9352 = torch.constant.int 4
    %int32_9353 = torch.constant.int 32
    %int64_9354 = torch.constant.int 64
    %5888 = torch.prim.ListConstruct %int4_9352, %273, %int32_9353, %int64_9354 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5889 = torch.aten.view %5887, %5888 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5889, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_9355 = torch.constant.none
    %none_9356 = torch.constant.none
    %int5_9357 = torch.constant.int 5
    %cpu_9358 = torch.constant.device "cpu"
    %int0_9359 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5889, %none_9355, %none_9356, %int5_9357, %cpu_9358, %int0_9359 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_9360 = torch.constant.int 0
    %none_9361 = torch.constant.none
    %none_9362 = torch.constant.none
    %cpu_9363 = torch.constant.device "cpu"
    %false_9364 = torch.constant.bool false
    %5890 = torch.aten.arange.start %int0_9360, %273, %none_9361, %none_9362, %cpu_9363, %false_9364 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5890, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_9365 = torch.constant.int 0
    %5891 = torch.aten.unsqueeze %5890, %int0_9365 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %5891, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_9366 = torch.constant.int 0
    %int64_9367 = torch.constant.int 64
    %int2_9368 = torch.constant.int 2
    %none_9369 = torch.constant.none
    %none_9370 = torch.constant.none
    %cpu_9371 = torch.constant.device "cpu"
    %false_9372 = torch.constant.bool false
    %5892 = torch.aten.arange.start_step %int0_9366, %int64_9367, %int2_9368, %none_9369, %none_9370, %cpu_9371, %false_9372 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_9373 = torch.constant.none
    %none_9374 = torch.constant.none
    %int4_9375 = torch.constant.int 4
    %cpu_9376 = torch.constant.device "cpu"
    %int0_9377 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5892, %none_9373, %none_9374, %int4_9375, %cpu_9376, %int0_9377 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9378 = torch.constant.int 6
    %5893 = torch.prims.convert_element_type %5892, %int6_9378 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_9379 = torch.constant.int 64
    %5894 = torch.aten.div.Scalar %5893, %int64_9379 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_9380 = torch.constant.float 1.000000e+04
    %5895 = torch.aten.pow.Scalar %float1.000000e04_9380, %5894 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %5896 = torch.aten.reciprocal %5895 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_9381 = torch.constant.float 1.000000e+00
    %5897 = torch.aten.mul.Scalar %5896, %float1.000000e00_9381 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_9382 = torch.constant.none
    %5898 = torch.aten.clone %218, %none_9382 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_9383 = torch.constant.int 0
    %5899 = torch.aten.unsqueeze %5897, %int0_9383 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_9384 = torch.constant.int 2
    %5900 = torch.aten.unsqueeze %5899, %int2_9384 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_9385 = torch.constant.none
    %none_9386 = torch.constant.none
    %int6_9387 = torch.constant.int 6
    %cpu_9388 = torch.constant.device "cpu"
    %int0_9389 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5900, %none_9385, %none_9386, %int6_9387, %cpu_9388, %int0_9389 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_9390 = torch.constant.int 1
    %int-1_9391 = torch.constant.int -1
    %int1_9392 = torch.constant.int 1
    %5901 = torch.prim.ListConstruct %int1_9390, %int-1_9391, %int1_9392 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_9393 = torch.constant.bool false
    %5902 = torch.aten.expand %5900, %5901, %false_9393 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_9394 = torch.constant.int 1
    %5903 = torch.aten.unsqueeze %5891, %int1_9394 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %5903, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_9395 = torch.constant.none
    %none_9396 = torch.constant.none
    %int4_9397 = torch.constant.int 4
    %cpu_9398 = torch.constant.device "cpu"
    %int0_9399 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5903, %none_9395, %none_9396, %int4_9397, %cpu_9398, %int0_9399 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9400 = torch.constant.int 6
    %5904 = torch.prims.convert_element_type %5903, %int6_9400 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %5904, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %5905 = torch.aten.matmul %5902, %5904 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %5905, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_9401 = torch.constant.int 1
    %int2_9402 = torch.constant.int 2
    %5906 = torch.aten.transpose.int %5905, %int1_9401, %int2_9402 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5906, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %5907 = torch.aten.cos %5906 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5907, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %5908 = torch.aten.mul.Tensor %5907, %5898 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5908, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_9403 = torch.constant.none
    %none_9404 = torch.constant.none
    %int6_9405 = torch.constant.int 6
    %cpu_9406 = torch.constant.device "cpu"
    %int0_9407 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5908, %none_9403, %none_9404, %int6_9405, %cpu_9406, %int0_9407 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9408 = torch.constant.int 5
    %5909 = torch.prims.convert_element_type %5908, %int5_9408 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %5909, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %5910 = torch.aten.sin %5906 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5910, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %5911 = torch.aten.mul.Tensor %5910, %5898 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %5911, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_9409 = torch.constant.none
    %none_9410 = torch.constant.none
    %int6_9411 = torch.constant.int 6
    %cpu_9412 = torch.constant.device "cpu"
    %int0_9413 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5911, %none_9409, %none_9410, %int6_9411, %cpu_9412, %int0_9413 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9414 = torch.constant.int 5
    %5912 = torch.prims.convert_element_type %5911, %int5_9414 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %5912, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_9415 = torch.constant.int 2
    %5913 = torch.aten.unsqueeze %5909, %int2_9415 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %5913, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_9416 = torch.constant.int 2
    %5914 = torch.aten.unsqueeze %5912, %int2_9416 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %5914, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_9417 = torch.constant.none
    %none_9418 = torch.constant.none
    %int5_9419 = torch.constant.int 5
    %cpu_9420 = torch.constant.device "cpu"
    %int0_9421 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5913, %none_9417, %none_9418, %int5_9419, %cpu_9420, %int0_9421 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_9422 = torch.constant.none
    %none_9423 = torch.constant.none
    %int5_9424 = torch.constant.int 5
    %cpu_9425 = torch.constant.device "cpu"
    %int0_9426 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5914, %none_9422, %none_9423, %int5_9424, %cpu_9425, %int0_9426 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_9427 = torch.constant.none
    %none_9428 = torch.constant.none
    %int5_9429 = torch.constant.int 5
    %cpu_9430 = torch.constant.device "cpu"
    %int0_9431 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5848, %none_9427, %none_9428, %int5_9429, %cpu_9430, %int0_9431 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_9432 = torch.constant.int 3
    %int0_9433 = torch.constant.int 0
    %int64_9434 = torch.constant.int 64
    %int2_9435 = torch.constant.int 2
    %5915 = torch.aten.slice.Tensor %5848, %int3_9432, %int0_9433, %int64_9434, %int2_9435 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5915, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_9436 = torch.constant.int 3
    %int1_9437 = torch.constant.int 1
    %int64_9438 = torch.constant.int 64
    %int2_9439 = torch.constant.int 2
    %5916 = torch.aten.slice.Tensor %5848, %int3_9436, %int1_9437, %int64_9438, %int2_9439 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5916, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5917 = torch.aten.mul.Tensor %5915, %5913 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5917, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5918 = torch.aten.mul.Tensor %5916, %5914 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5918, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_9440 = torch.constant.int 1
    %5919 = torch.aten.sub.Tensor %5917, %5918, %int1_9440 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5919, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5920 = torch.aten.mul.Tensor %5916, %5913 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5920, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5921 = torch.aten.mul.Tensor %5915, %5914 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5921, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_9441 = torch.constant.int 1
    %5922 = torch.aten.add.Tensor %5920, %5921, %int1_9441 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %5922, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %5923 = torch_c.to_builtin_tensor %5919 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_9442 = tensor.cast %5923 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %5924 = torch_c.to_builtin_tensor %5922 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_9443 = tensor.cast %5924 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %5925 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_9442, %cast_9443) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_9444 = tensor.cast %5925 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %5926 = torch_c.from_builtin_tensor %cast_9444 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %5926, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_9445 = torch.constant.int 4
    %int4_9446 = torch.constant.int 4
    %int64_9447 = torch.constant.int 64
    %5927 = torch.prim.ListConstruct %int4_9445, %273, %int4_9446, %int64_9447 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5928 = torch.aten.view %5926, %5927 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %5928, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_9448 = torch.constant.none
    %none_9449 = torch.constant.none
    %int5_9450 = torch.constant.int 5
    %cpu_9451 = torch.constant.device "cpu"
    %int0_9452 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5928, %none_9448, %none_9449, %int5_9450, %cpu_9451, %int0_9452 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_9453 = torch.constant.int 22
    %5929 = torch.aten.mul.Scalar %arg2, %int22_9453 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5929, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int18 = torch.constant.int 18
    %int1_9454 = torch.constant.int 1
    %5930 = torch.aten.add.Scalar %5929, %int18, %int1_9454 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5930, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_9455 = torch.constant.int 2
    %5931 = torch.aten.mul.Scalar %5930, %int2_9455 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5931, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_9456 = torch.constant.int 0
    %int1_9457 = torch.constant.int 1
    %5932 = torch.aten.add.Scalar %5931, %int0_9456, %int1_9457 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5932, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %5933 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %5934 = torch.aten.view %5932, %5933 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5934, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_9458 = torch.constant.int 4
    %int32_9459 = torch.constant.int 32
    %int4_9460 = torch.constant.int 4
    %int64_9461 = torch.constant.int 64
    %5935 = torch.prim.ListConstruct %int4_9458, %271, %int32_9459, %int4_9460, %int64_9461 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5936 = torch.aten.view %5928, %5935 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5936, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_9462 = torch.constant.int 32
    %int4_9463 = torch.constant.int 4
    %int64_9464 = torch.constant.int 64
    %5937 = torch.prim.ListConstruct %446, %int32_9462, %int4_9463, %int64_9464 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5938 = torch.aten.view %5936, %5937 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %5938, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_9465 = torch.constant.int 1
    %int2_9466 = torch.constant.int 2
    %5939 = torch.aten.transpose.int %5938, %int1_9465, %int2_9466 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5939, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_9467 = torch.constant.none
    %none_9468 = torch.constant.none
    %int5_9469 = torch.constant.int 5
    %cpu_9470 = torch.constant.device "cpu"
    %int0_9471 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5939, %none_9467, %none_9468, %int5_9469, %cpu_9470, %int0_9471 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_9472 = torch.constant.int 22
    %int2_9473 = torch.constant.int 2
    %int4_9474 = torch.constant.int 4
    %int32_9475 = torch.constant.int 32
    %int64_9476 = torch.constant.int 64
    %5940 = torch.prim.ListConstruct %272, %int22_9472, %int2_9473, %int4_9474, %int32_9475, %int64_9476 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5941 = torch.aten.view %5665, %5940 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5941, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_9477 = torch.constant.int 4
    %int32_9478 = torch.constant.int 32
    %int64_9479 = torch.constant.int 64
    %5942 = torch.prim.ListConstruct %439, %int4_9477, %int32_9478, %int64_9479 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5943 = torch.aten.view %5941, %5942 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5943, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %5944 = torch.prim.ListConstruct %5934 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_9480 = torch.constant.bool false
    %5945 = torch.aten.index_put %5943, %5944, %5939, %false_9480 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5945, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_9481 = torch.constant.int 22
    %int2_9482 = torch.constant.int 2
    %int4_9483 = torch.constant.int 4
    %int32_9484 = torch.constant.int 32
    %int64_9485 = torch.constant.int 64
    %5946 = torch.prim.ListConstruct %272, %int22_9481, %int2_9482, %int4_9483, %int32_9484, %int64_9485 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5947 = torch.aten.view %5945, %5946 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5947, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_9486 = torch.constant.int 360448
    %5948 = torch.prim.ListConstruct %272, %int360448_9486 : (!torch.int, !torch.int) -> !torch.list<int>
    %5949 = torch.aten.view %5947, %5948 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %5949, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_9487 = torch.constant.int 22
    %int2_9488 = torch.constant.int 2
    %int4_9489 = torch.constant.int 4
    %int32_9490 = torch.constant.int 32
    %int64_9491 = torch.constant.int 64
    %5950 = torch.prim.ListConstruct %272, %int22_9487, %int2_9488, %int4_9489, %int32_9490, %int64_9491 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5951 = torch.aten.view %5949, %5950 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5951, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_9492 = torch.constant.int 4
    %int32_9493 = torch.constant.int 32
    %int64_9494 = torch.constant.int 64
    %5952 = torch.prim.ListConstruct %439, %int4_9492, %int32_9493, %int64_9494 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5953 = torch.aten.view %5951, %5952 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5953, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_9495 = torch.constant.int 22
    %5954 = torch.aten.mul.Scalar %arg2, %int22_9495 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5954, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int18_9496 = torch.constant.int 18
    %int1_9497 = torch.constant.int 1
    %5955 = torch.aten.add.Scalar %5954, %int18_9496, %int1_9497 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5955, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_9498 = torch.constant.int 2
    %5956 = torch.aten.mul.Scalar %5955, %int2_9498 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5956, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_9499 = torch.constant.int 1
    %int1_9500 = torch.constant.int 1
    %5957 = torch.aten.add.Scalar %5956, %int1_9499, %int1_9500 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5957, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %5958 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %5959 = torch.aten.view %5957, %5958 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5959, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_9501 = torch.constant.int 4
    %int32_9502 = torch.constant.int 32
    %int4_9503 = torch.constant.int 4
    %int64_9504 = torch.constant.int 64
    %5960 = torch.prim.ListConstruct %int4_9501, %271, %int32_9502, %int4_9503, %int64_9504 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5961 = torch.aten.view %5850, %5960 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5961, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_9505 = torch.constant.int 32
    %int4_9506 = torch.constant.int 4
    %int64_9507 = torch.constant.int 64
    %5962 = torch.prim.ListConstruct %446, %int32_9505, %int4_9506, %int64_9507 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5963 = torch.aten.view %5961, %5962 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %5963, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_9508 = torch.constant.int 1
    %int2_9509 = torch.constant.int 2
    %5964 = torch.aten.transpose.int %5963, %int1_9508, %int2_9509 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5964, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_9510 = torch.constant.none
    %none_9511 = torch.constant.none
    %int5_9512 = torch.constant.int 5
    %cpu_9513 = torch.constant.device "cpu"
    %int0_9514 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5964, %none_9510, %none_9511, %int5_9512, %cpu_9513, %int0_9514 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %5965 = torch.prim.ListConstruct %5959 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_9515 = torch.constant.bool false
    %5966 = torch.aten.index_put %5953, %5965, %5964, %false_9515 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %5966, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_9516 = torch.constant.int 22
    %int2_9517 = torch.constant.int 2
    %int4_9518 = torch.constant.int 4
    %int32_9519 = torch.constant.int 32
    %int64_9520 = torch.constant.int 64
    %5967 = torch.prim.ListConstruct %272, %int22_9516, %int2_9517, %int4_9518, %int32_9519, %int64_9520 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5968 = torch.aten.view %5966, %5967 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5968, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_9521 = torch.constant.int 360448
    %5969 = torch.prim.ListConstruct %272, %int360448_9521 : (!torch.int, !torch.int) -> !torch.list<int>
    %5970 = torch.aten.view %5968, %5969 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %5970, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_9522 = torch.constant.int 0
    %int1_9523 = torch.constant.int 1
    %none_9524 = torch.constant.none
    %none_9525 = torch.constant.none
    %cpu_9526 = torch.constant.device "cpu"
    %false_9527 = torch.constant.bool false
    %5971 = torch.aten.arange.start_step %int0_9522, %273, %int1_9523, %none_9524, %none_9525, %cpu_9526, %false_9527 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5971, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_9528 = torch.constant.int -1
    %5972 = torch.aten.unsqueeze %arg1, %int-1_9528 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %5973 = torch.aten.ge.Tensor %5971, %5972 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %5973, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_9529 = torch.constant.none
    %none_9530 = torch.constant.none
    %cpu_9531 = torch.constant.device "cpu"
    %false_9532 = torch.constant.bool false
    %5974 = torch.aten.arange %273, %none_9529, %none_9530, %cpu_9531, %false_9532 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5974, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_9533 = torch.constant.int 0
    %5975 = torch.aten.unsqueeze %5974, %int0_9533 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %5975, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_9534 = torch.constant.int 1
    %5976 = torch.aten.unsqueeze %5975, %int1_9534 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %5976, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_9535 = torch.constant.int 2
    %5977 = torch.aten.unsqueeze %5976, %int2_9535 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %5977, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_9536 = torch.constant.none
    %none_9537 = torch.constant.none
    %cpu_9538 = torch.constant.device "cpu"
    %false_9539 = torch.constant.bool false
    %5978 = torch.aten.arange %273, %none_9536, %none_9537, %cpu_9538, %false_9539 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5978, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_9540 = torch.constant.int 0
    %5979 = torch.aten.unsqueeze %5978, %int0_9540 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %5979, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_9541 = torch.constant.int 1
    %5980 = torch.aten.unsqueeze %5979, %int1_9541 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %5980, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_9542 = torch.constant.int 3
    %5981 = torch.aten.unsqueeze %5980, %int3_9542 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %5981, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %5982 = torch.aten.gt.Tensor %5977, %5981 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %5982, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_9543 = torch.constant.int 1
    %5983 = torch.aten.unsqueeze %5973, %int1_9543 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %5983, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_9544 = torch.constant.int 2
    %5984 = torch.aten.unsqueeze %5983, %int2_9544 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %5984, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %5985 = torch.aten.logical_or %5982, %5984 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %5985, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_9545 = torch.constant.none
    %5986 = torch.aten.clone %219, %none_9545 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_9546 = torch.constant.int 0
    %5987 = torch.aten.where.ScalarOther %5985, %5986, %int0_9546 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %5987, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_9547 = torch.constant.none
    %none_9548 = torch.constant.none
    %int5_9549 = torch.constant.int 5
    %cpu_9550 = torch.constant.device "cpu"
    %int0_9551 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5987, %none_9547, %none_9548, %int5_9549, %cpu_9550, %int0_9551 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_9552 = torch.constant.int -2
    %5988 = torch.aten.unsqueeze %5928, %int-2_9552 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %5988, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_9553 = torch.constant.int 4
    %int4_9554 = torch.constant.int 4
    %int8_9555 = torch.constant.int 8
    %int64_9556 = torch.constant.int 64
    %5989 = torch.prim.ListConstruct %int4_9553, %273, %int4_9554, %int8_9555, %int64_9556 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_9557 = torch.constant.bool false
    %5990 = torch.aten.expand %5988, %5989, %false_9557 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5990, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_9558 = torch.constant.int 0
    %5991 = torch.aten.clone %5990, %int0_9558 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5991, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_9559 = torch.constant.int 4
    %int32_9560 = torch.constant.int 32
    %int64_9561 = torch.constant.int 64
    %5992 = torch.prim.ListConstruct %int4_9559, %273, %int32_9560, %int64_9561 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5993 = torch.aten._unsafe_view %5991, %5992 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5993, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_9562 = torch.constant.int -2
    %5994 = torch.aten.unsqueeze %5850, %int-2_9562 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %5994, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_9563 = torch.constant.int 4
    %int4_9564 = torch.constant.int 4
    %int8_9565 = torch.constant.int 8
    %int64_9566 = torch.constant.int 64
    %5995 = torch.prim.ListConstruct %int4_9563, %273, %int4_9564, %int8_9565, %int64_9566 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_9567 = torch.constant.bool false
    %5996 = torch.aten.expand %5994, %5995, %false_9567 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5996, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_9568 = torch.constant.int 0
    %5997 = torch.aten.clone %5996, %int0_9568 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5997, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_9569 = torch.constant.int 4
    %int32_9570 = torch.constant.int 32
    %int64_9571 = torch.constant.int 64
    %5998 = torch.prim.ListConstruct %int4_9569, %273, %int32_9570, %int64_9571 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5999 = torch.aten._unsafe_view %5997, %5998 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5999, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_9572 = torch.constant.int 1
    %int2_9573 = torch.constant.int 2
    %6000 = torch.aten.transpose.int %5889, %int1_9572, %int2_9573 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6000, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_9574 = torch.constant.int 1
    %int2_9575 = torch.constant.int 2
    %6001 = torch.aten.transpose.int %5993, %int1_9574, %int2_9575 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6001, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_9576 = torch.constant.int 1
    %int2_9577 = torch.constant.int 2
    %6002 = torch.aten.transpose.int %5999, %int1_9576, %int2_9577 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6002, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_9578 = torch.constant.float 0.000000e+00
    %false_9579 = torch.constant.bool false
    %none_9580 = torch.constant.none
    %false_9581 = torch.constant.bool false
    %6003 = torch.aten.scaled_dot_product_attention %6000, %6001, %6002, %5987, %float0.000000e00_9578, %false_9579, %none_9580, %false_9581 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6003, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_9582 = torch.constant.int 1
    %int2_9583 = torch.constant.int 2
    %6004 = torch.aten.transpose.int %6003, %int1_9582, %int2_9583 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6004, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_9584 = torch.constant.int 4
    %int2048_9585 = torch.constant.int 2048
    %6005 = torch.prim.ListConstruct %int4_9584, %273, %int2048_9585 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6006 = torch.aten.view %6004, %6005 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6006, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_9586 = torch.constant.int 2
    %6007 = torch.aten.view.dtype %224, %int2_9586 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %6008 = torch.aten.detach %6007 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_9587 = torch.constant.int -1
    %int17_9588 = torch.constant.int 17
    %6009 = torch.prim.ListConstruct %int-1_9587, %int17_9588 : (!torch.int, !torch.int) -> !torch.list<int>
    %6010 = torch.aten.view %6008, %6009 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_9589 = torch.constant.int 2048
    %int-1_9590 = torch.constant.int -1
    %int17_9591 = torch.constant.int 17
    %6011 = torch.prim.ListConstruct %int2048_9589, %int-1_9590, %int17_9591 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6012 = torch.aten.view %6010, %6011 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_9592 = torch.constant.int 2
    %int0_9593 = torch.constant.int 0
    %int1_9594 = torch.constant.int 1
    %int1_9595 = torch.constant.int 1
    %6013 = torch.aten.slice.Tensor %6012, %int2_9592, %int0_9593, %int1_9594, %int1_9595 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_9596 = torch.constant.int 5
    %6014 = torch.aten.view.dtype %6013, %int5_9596 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %6015 = torch.aten.detach %6014 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_9597 = torch.constant.int 2
    %int1_9598 = torch.constant.int 1
    %int9223372036854775807_9599 = torch.constant.int 9223372036854775807
    %int1_9600 = torch.constant.int 1
    %6016 = torch.aten.slice.Tensor %6012, %int2_9597, %int1_9598, %int9223372036854775807_9599, %int1_9600 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_9601 = torch.constant.int 1
    %6017 = torch.aten.view.dtype %6016, %int1_9601 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %6018 = torch.aten.detach %6017 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %6019 = torch_c.to_builtin_tensor %6006 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_9602 = tensor.cast %6019 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6020 = torch_c.to_builtin_tensor %6015 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %6021 = torch_c.to_builtin_tensor %6018 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %6022 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_9602, %6020, %6021) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_9603 = tensor.cast %6022 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %6023 = torch_c.from_builtin_tensor %cast_9603 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6023, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_9604 = torch.constant.none
    %none_9605 = torch.constant.none
    %int5_9606 = torch.constant.int 5
    %cpu_9607 = torch.constant.device "cpu"
    %int0_9608 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6023, %none_9604, %none_9605, %int5_9606, %cpu_9607, %int0_9608 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_9609 = torch.constant.int 1
    %6024 = torch.aten.add.Tensor %5783, %6023, %int1_9609 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6024, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_9610 = torch.constant.none
    %none_9611 = torch.constant.none
    %int5_9612 = torch.constant.int 5
    %cpu_9613 = torch.constant.device "cpu"
    %int0_9614 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6024, %none_9610, %none_9611, %int5_9612, %cpu_9613, %int0_9614 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9615 = torch.constant.int 6
    %6025 = torch.prims.convert_element_type %6024, %int6_9615 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6025, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_9616 = torch.constant.int 2
    %6026 = torch.aten.pow.Tensor_Scalar %6025, %int2_9616 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6026, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_9617 = torch.constant.int -1
    %6027 = torch.prim.ListConstruct %int-1_9617 : (!torch.int) -> !torch.list<int>
    %true_9618 = torch.constant.bool true
    %none_9619 = torch.constant.none
    %6028 = torch.aten.mean.dim %6026, %6027, %true_9618, %none_9619 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6028, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_9620 = torch.constant.float 9.9999997473787516E-6
    %int1_9621 = torch.constant.int 1
    %6029 = torch.aten.add.Scalar %6028, %float9.999990e-06_9620, %int1_9621 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6029, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6030 = torch.aten.rsqrt %6029 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6030, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6031 = torch.aten.mul.Tensor %6025, %6030 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6031, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_9622 = torch.constant.none
    %none_9623 = torch.constant.none
    %int6_9624 = torch.constant.int 6
    %cpu_9625 = torch.constant.device "cpu"
    %int0_9626 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6031, %none_9622, %none_9623, %int6_9624, %cpu_9625, %int0_9626 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9627 = torch.constant.int 5
    %6032 = torch.prims.convert_element_type %6031, %int5_9627 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6032, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %6033 = torch.aten.mul.Tensor %225, %6032 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6033, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_9628 = torch.constant.none
    %none_9629 = torch.constant.none
    %int6_9630 = torch.constant.int 6
    %cpu_9631 = torch.constant.device "cpu"
    %int0_9632 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6033, %none_9628, %none_9629, %int6_9630, %cpu_9631, %int0_9632 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9633 = torch.constant.int 5
    %6034 = torch.prims.convert_element_type %6033, %int5_9633 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6034, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_9634 = torch.constant.int 2
    %6035 = torch.aten.view.dtype %226, %int2_9634 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %6036 = torch.aten.detach %6035 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_9635 = torch.constant.int -1
    %int17_9636 = torch.constant.int 17
    %6037 = torch.prim.ListConstruct %int-1_9635, %int17_9636 : (!torch.int, !torch.int) -> !torch.list<int>
    %6038 = torch.aten.view %6036, %6037 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_9637 = torch.constant.int 5632
    %int-1_9638 = torch.constant.int -1
    %int17_9639 = torch.constant.int 17
    %6039 = torch.prim.ListConstruct %int5632_9637, %int-1_9638, %int17_9639 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6040 = torch.aten.view %6038, %6039 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_9640 = torch.constant.int 2
    %int0_9641 = torch.constant.int 0
    %int1_9642 = torch.constant.int 1
    %int1_9643 = torch.constant.int 1
    %6041 = torch.aten.slice.Tensor %6040, %int2_9640, %int0_9641, %int1_9642, %int1_9643 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_9644 = torch.constant.int 5
    %6042 = torch.aten.view.dtype %6041, %int5_9644 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %6043 = torch.aten.detach %6042 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_9645 = torch.constant.int 2
    %int1_9646 = torch.constant.int 1
    %int9223372036854775807_9647 = torch.constant.int 9223372036854775807
    %int1_9648 = torch.constant.int 1
    %6044 = torch.aten.slice.Tensor %6040, %int2_9645, %int1_9646, %int9223372036854775807_9647, %int1_9648 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_9649 = torch.constant.int 1
    %6045 = torch.aten.view.dtype %6044, %int1_9649 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %6046 = torch.aten.detach %6045 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %6047 = torch_c.to_builtin_tensor %6034 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_9650 = tensor.cast %6047 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6048 = torch_c.to_builtin_tensor %6043 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %6049 = torch_c.to_builtin_tensor %6046 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %6050 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_9650, %6048, %6049) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_9651 = tensor.cast %6050 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %6051 = torch_c.from_builtin_tensor %cast_9651 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %6051, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %6052 = torch.aten.silu %6051 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %6052, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_9652 = torch.constant.int 2
    %6053 = torch.aten.view.dtype %227, %int2_9652 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %6054 = torch.aten.detach %6053 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_9653 = torch.constant.int -1
    %int17_9654 = torch.constant.int 17
    %6055 = torch.prim.ListConstruct %int-1_9653, %int17_9654 : (!torch.int, !torch.int) -> !torch.list<int>
    %6056 = torch.aten.view %6054, %6055 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_9655 = torch.constant.int 5632
    %int-1_9656 = torch.constant.int -1
    %int17_9657 = torch.constant.int 17
    %6057 = torch.prim.ListConstruct %int5632_9655, %int-1_9656, %int17_9657 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6058 = torch.aten.view %6056, %6057 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_9658 = torch.constant.int 2
    %int0_9659 = torch.constant.int 0
    %int1_9660 = torch.constant.int 1
    %int1_9661 = torch.constant.int 1
    %6059 = torch.aten.slice.Tensor %6058, %int2_9658, %int0_9659, %int1_9660, %int1_9661 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_9662 = torch.constant.int 5
    %6060 = torch.aten.view.dtype %6059, %int5_9662 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %6061 = torch.aten.detach %6060 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_9663 = torch.constant.int 2
    %int1_9664 = torch.constant.int 1
    %int9223372036854775807_9665 = torch.constant.int 9223372036854775807
    %int1_9666 = torch.constant.int 1
    %6062 = torch.aten.slice.Tensor %6058, %int2_9663, %int1_9664, %int9223372036854775807_9665, %int1_9666 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_9667 = torch.constant.int 1
    %6063 = torch.aten.view.dtype %6062, %int1_9667 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %6064 = torch.aten.detach %6063 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %6065 = torch_c.to_builtin_tensor %6034 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_9668 = tensor.cast %6065 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6066 = torch_c.to_builtin_tensor %6061 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %6067 = torch_c.to_builtin_tensor %6064 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %6068 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_9668, %6066, %6067) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_9669 = tensor.cast %6068 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %6069 = torch_c.from_builtin_tensor %cast_9669 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %6069, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %6070 = torch.aten.mul.Tensor %6052, %6069 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %6070, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_9670 = torch.constant.int 2
    %6071 = torch.aten.view.dtype %228, %int2_9670 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %6072 = torch.aten.detach %6071 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_9671 = torch.constant.int -1
    %int17_9672 = torch.constant.int 17
    %6073 = torch.prim.ListConstruct %int-1_9671, %int17_9672 : (!torch.int, !torch.int) -> !torch.list<int>
    %6074 = torch.aten.view %6072, %6073 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_9673 = torch.constant.int 2048
    %int-1_9674 = torch.constant.int -1
    %int17_9675 = torch.constant.int 17
    %6075 = torch.prim.ListConstruct %int2048_9673, %int-1_9674, %int17_9675 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6076 = torch.aten.view %6074, %6075 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_9676 = torch.constant.int 2
    %int0_9677 = torch.constant.int 0
    %int1_9678 = torch.constant.int 1
    %int1_9679 = torch.constant.int 1
    %6077 = torch.aten.slice.Tensor %6076, %int2_9676, %int0_9677, %int1_9678, %int1_9679 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_9680 = torch.constant.int 5
    %6078 = torch.aten.view.dtype %6077, %int5_9680 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %6079 = torch.aten.detach %6078 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_9681 = torch.constant.int 2
    %int1_9682 = torch.constant.int 1
    %int9223372036854775807_9683 = torch.constant.int 9223372036854775807
    %int1_9684 = torch.constant.int 1
    %6080 = torch.aten.slice.Tensor %6076, %int2_9681, %int1_9682, %int9223372036854775807_9683, %int1_9684 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_9685 = torch.constant.int 1
    %6081 = torch.aten.view.dtype %6080, %int1_9685 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %6082 = torch.aten.detach %6081 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %6083 = torch_c.to_builtin_tensor %6070 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_9686 = tensor.cast %6083 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %6084 = torch_c.to_builtin_tensor %6079 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %6085 = torch_c.to_builtin_tensor %6082 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %6086 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_9686, %6084, %6085) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_9687 = tensor.cast %6086 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %6087 = torch_c.from_builtin_tensor %cast_9687 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6087, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_9688 = torch.constant.int 1
    %6088 = torch.aten.add.Tensor %6024, %6087, %int1_9688 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6088, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_9689 = torch.constant.none
    %none_9690 = torch.constant.none
    %int5_9691 = torch.constant.int 5
    %cpu_9692 = torch.constant.device "cpu"
    %int0_9693 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6088, %none_9689, %none_9690, %int5_9691, %cpu_9692, %int0_9693 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9694 = torch.constant.int 6
    %6089 = torch.prims.convert_element_type %6088, %int6_9694 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6089, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_9695 = torch.constant.int 2
    %6090 = torch.aten.pow.Tensor_Scalar %6089, %int2_9695 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6090, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_9696 = torch.constant.int -1
    %6091 = torch.prim.ListConstruct %int-1_9696 : (!torch.int) -> !torch.list<int>
    %true_9697 = torch.constant.bool true
    %none_9698 = torch.constant.none
    %6092 = torch.aten.mean.dim %6090, %6091, %true_9697, %none_9698 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6092, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_9699 = torch.constant.float 9.9999997473787516E-6
    %int1_9700 = torch.constant.int 1
    %6093 = torch.aten.add.Scalar %6092, %float9.999990e-06_9699, %int1_9700 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6093, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6094 = torch.aten.rsqrt %6093 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6094, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6095 = torch.aten.mul.Tensor %6089, %6094 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6095, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_9701 = torch.constant.none
    %none_9702 = torch.constant.none
    %int6_9703 = torch.constant.int 6
    %cpu_9704 = torch.constant.device "cpu"
    %int0_9705 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6095, %none_9701, %none_9702, %int6_9703, %cpu_9704, %int0_9705 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9706 = torch.constant.int 5
    %6096 = torch.prims.convert_element_type %6095, %int5_9706 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6096, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %6097 = torch.aten.mul.Tensor %232, %6096 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6097, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_9707 = torch.constant.none
    %none_9708 = torch.constant.none
    %int6_9709 = torch.constant.int 6
    %cpu_9710 = torch.constant.device "cpu"
    %int0_9711 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6097, %none_9707, %none_9708, %int6_9709, %cpu_9710, %int0_9711 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9712 = torch.constant.int 5
    %6098 = torch.prims.convert_element_type %6097, %int5_9712 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6098, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_9713 = torch.constant.int 2
    %6099 = torch.aten.view.dtype %233, %int2_9713 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %6100 = torch.aten.detach %6099 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_9714 = torch.constant.int -1
    %int17_9715 = torch.constant.int 17
    %6101 = torch.prim.ListConstruct %int-1_9714, %int17_9715 : (!torch.int, !torch.int) -> !torch.list<int>
    %6102 = torch.aten.view %6100, %6101 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_9716 = torch.constant.int 2048
    %int-1_9717 = torch.constant.int -1
    %int17_9718 = torch.constant.int 17
    %6103 = torch.prim.ListConstruct %int2048_9716, %int-1_9717, %int17_9718 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6104 = torch.aten.view %6102, %6103 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_9719 = torch.constant.int 2
    %int0_9720 = torch.constant.int 0
    %int1_9721 = torch.constant.int 1
    %int1_9722 = torch.constant.int 1
    %6105 = torch.aten.slice.Tensor %6104, %int2_9719, %int0_9720, %int1_9721, %int1_9722 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_9723 = torch.constant.int 5
    %6106 = torch.aten.view.dtype %6105, %int5_9723 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %6107 = torch.aten.detach %6106 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_9724 = torch.constant.int 2
    %int1_9725 = torch.constant.int 1
    %int9223372036854775807_9726 = torch.constant.int 9223372036854775807
    %int1_9727 = torch.constant.int 1
    %6108 = torch.aten.slice.Tensor %6104, %int2_9724, %int1_9725, %int9223372036854775807_9726, %int1_9727 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_9728 = torch.constant.int 1
    %6109 = torch.aten.view.dtype %6108, %int1_9728 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %6110 = torch.aten.detach %6109 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %6111 = torch_c.to_builtin_tensor %6098 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_9729 = tensor.cast %6111 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6112 = torch_c.to_builtin_tensor %6107 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %6113 = torch_c.to_builtin_tensor %6110 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %6114 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_9729, %6112, %6113) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_9730 = tensor.cast %6114 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %6115 = torch_c.from_builtin_tensor %cast_9730 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6115, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_9731 = torch.constant.int 2
    %6116 = torch.aten.view.dtype %234, %int2_9731 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %6117 = torch.aten.detach %6116 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_9732 = torch.constant.int -1
    %int17_9733 = torch.constant.int 17
    %6118 = torch.prim.ListConstruct %int-1_9732, %int17_9733 : (!torch.int, !torch.int) -> !torch.list<int>
    %6119 = torch.aten.view %6117, %6118 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_9734 = torch.constant.int 256
    %int-1_9735 = torch.constant.int -1
    %int17_9736 = torch.constant.int 17
    %6120 = torch.prim.ListConstruct %int256_9734, %int-1_9735, %int17_9736 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6121 = torch.aten.view %6119, %6120 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_9737 = torch.constant.int 2
    %int0_9738 = torch.constant.int 0
    %int1_9739 = torch.constant.int 1
    %int1_9740 = torch.constant.int 1
    %6122 = torch.aten.slice.Tensor %6121, %int2_9737, %int0_9738, %int1_9739, %int1_9740 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_9741 = torch.constant.int 5
    %6123 = torch.aten.view.dtype %6122, %int5_9741 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %6124 = torch.aten.detach %6123 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_9742 = torch.constant.int 2
    %int1_9743 = torch.constant.int 1
    %int9223372036854775807_9744 = torch.constant.int 9223372036854775807
    %int1_9745 = torch.constant.int 1
    %6125 = torch.aten.slice.Tensor %6121, %int2_9742, %int1_9743, %int9223372036854775807_9744, %int1_9745 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_9746 = torch.constant.int 1
    %6126 = torch.aten.view.dtype %6125, %int1_9746 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %6127 = torch.aten.detach %6126 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %6128 = torch_c.to_builtin_tensor %6098 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_9747 = tensor.cast %6128 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6129 = torch_c.to_builtin_tensor %6124 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %6130 = torch_c.to_builtin_tensor %6127 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %6131 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_9747, %6129, %6130) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_9748 = tensor.cast %6131 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %6132 = torch_c.from_builtin_tensor %cast_9748 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %6132, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_9749 = torch.constant.int 2
    %6133 = torch.aten.view.dtype %235, %int2_9749 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %6134 = torch.aten.detach %6133 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_9750 = torch.constant.int -1
    %int17_9751 = torch.constant.int 17
    %6135 = torch.prim.ListConstruct %int-1_9750, %int17_9751 : (!torch.int, !torch.int) -> !torch.list<int>
    %6136 = torch.aten.view %6134, %6135 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_9752 = torch.constant.int 256
    %int-1_9753 = torch.constant.int -1
    %int17_9754 = torch.constant.int 17
    %6137 = torch.prim.ListConstruct %int256_9752, %int-1_9753, %int17_9754 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6138 = torch.aten.view %6136, %6137 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_9755 = torch.constant.int 2
    %int0_9756 = torch.constant.int 0
    %int1_9757 = torch.constant.int 1
    %int1_9758 = torch.constant.int 1
    %6139 = torch.aten.slice.Tensor %6138, %int2_9755, %int0_9756, %int1_9757, %int1_9758 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_9759 = torch.constant.int 5
    %6140 = torch.aten.view.dtype %6139, %int5_9759 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %6141 = torch.aten.detach %6140 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_9760 = torch.constant.int 2
    %int1_9761 = torch.constant.int 1
    %int9223372036854775807_9762 = torch.constant.int 9223372036854775807
    %int1_9763 = torch.constant.int 1
    %6142 = torch.aten.slice.Tensor %6138, %int2_9760, %int1_9761, %int9223372036854775807_9762, %int1_9763 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_9764 = torch.constant.int 1
    %6143 = torch.aten.view.dtype %6142, %int1_9764 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %6144 = torch.aten.detach %6143 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %6145 = torch_c.to_builtin_tensor %6098 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_9765 = tensor.cast %6145 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6146 = torch_c.to_builtin_tensor %6141 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %6147 = torch_c.to_builtin_tensor %6144 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %6148 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_9765, %6146, %6147) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_9766 = tensor.cast %6148 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %6149 = torch_c.from_builtin_tensor %cast_9766 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %6149, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_9767 = torch.constant.int 4
    %int32_9768 = torch.constant.int 32
    %int64_9769 = torch.constant.int 64
    %6150 = torch.prim.ListConstruct %int4_9767, %273, %int32_9768, %int64_9769 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6151 = torch.aten.view %6115, %6150 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6151, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_9770 = torch.constant.int 4
    %int4_9771 = torch.constant.int 4
    %int64_9772 = torch.constant.int 64
    %6152 = torch.prim.ListConstruct %int4_9770, %273, %int4_9771, %int64_9772 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6153 = torch.aten.view %6132, %6152 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %6153, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_9773 = torch.constant.int 4
    %int4_9774 = torch.constant.int 4
    %int64_9775 = torch.constant.int 64
    %6154 = torch.prim.ListConstruct %int4_9773, %273, %int4_9774, %int64_9775 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6155 = torch.aten.view %6149, %6154 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %6155, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_9776 = torch.constant.int 0
    %none_9777 = torch.constant.none
    %none_9778 = torch.constant.none
    %cpu_9779 = torch.constant.device "cpu"
    %false_9780 = torch.constant.bool false
    %6156 = torch.aten.arange.start %int0_9776, %273, %none_9777, %none_9778, %cpu_9779, %false_9780 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6156, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_9781 = torch.constant.int 0
    %6157 = torch.aten.unsqueeze %6156, %int0_9781 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %6157, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_9782 = torch.constant.int 0
    %int64_9783 = torch.constant.int 64
    %int2_9784 = torch.constant.int 2
    %none_9785 = torch.constant.none
    %none_9786 = torch.constant.none
    %cpu_9787 = torch.constant.device "cpu"
    %false_9788 = torch.constant.bool false
    %6158 = torch.aten.arange.start_step %int0_9782, %int64_9783, %int2_9784, %none_9785, %none_9786, %cpu_9787, %false_9788 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_9789 = torch.constant.none
    %none_9790 = torch.constant.none
    %int4_9791 = torch.constant.int 4
    %cpu_9792 = torch.constant.device "cpu"
    %int0_9793 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6158, %none_9789, %none_9790, %int4_9791, %cpu_9792, %int0_9793 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9794 = torch.constant.int 6
    %6159 = torch.prims.convert_element_type %6158, %int6_9794 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_9795 = torch.constant.int 64
    %6160 = torch.aten.div.Scalar %6159, %int64_9795 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_9796 = torch.constant.float 1.000000e+04
    %6161 = torch.aten.pow.Scalar %float1.000000e04_9796, %6160 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %6162 = torch.aten.reciprocal %6161 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_9797 = torch.constant.float 1.000000e+00
    %6163 = torch.aten.mul.Scalar %6162, %float1.000000e00_9797 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_9798 = torch.constant.none
    %6164 = torch.aten.clone %229, %none_9798 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_9799 = torch.constant.int 0
    %6165 = torch.aten.unsqueeze %6163, %int0_9799 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_9800 = torch.constant.int 2
    %6166 = torch.aten.unsqueeze %6165, %int2_9800 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_9801 = torch.constant.none
    %none_9802 = torch.constant.none
    %int6_9803 = torch.constant.int 6
    %cpu_9804 = torch.constant.device "cpu"
    %int0_9805 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6166, %none_9801, %none_9802, %int6_9803, %cpu_9804, %int0_9805 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_9806 = torch.constant.int 1
    %int-1_9807 = torch.constant.int -1
    %int1_9808 = torch.constant.int 1
    %6167 = torch.prim.ListConstruct %int1_9806, %int-1_9807, %int1_9808 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_9809 = torch.constant.bool false
    %6168 = torch.aten.expand %6166, %6167, %false_9809 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_9810 = torch.constant.int 1
    %6169 = torch.aten.unsqueeze %6157, %int1_9810 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %6169, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_9811 = torch.constant.none
    %none_9812 = torch.constant.none
    %int4_9813 = torch.constant.int 4
    %cpu_9814 = torch.constant.device "cpu"
    %int0_9815 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6169, %none_9811, %none_9812, %int4_9813, %cpu_9814, %int0_9815 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9816 = torch.constant.int 6
    %6170 = torch.prims.convert_element_type %6169, %int6_9816 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %6170, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %6171 = torch.aten.matmul %6168, %6170 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %6171, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_9817 = torch.constant.int 1
    %int2_9818 = torch.constant.int 2
    %6172 = torch.aten.transpose.int %6171, %int1_9817, %int2_9818 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6172, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %6173 = torch.aten.cos %6172 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6173, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %6174 = torch.aten.mul.Tensor %6173, %6164 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6174, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_9819 = torch.constant.none
    %none_9820 = torch.constant.none
    %int6_9821 = torch.constant.int 6
    %cpu_9822 = torch.constant.device "cpu"
    %int0_9823 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6174, %none_9819, %none_9820, %int6_9821, %cpu_9822, %int0_9823 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9824 = torch.constant.int 5
    %6175 = torch.prims.convert_element_type %6174, %int5_9824 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %6175, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %6176 = torch.aten.sin %6172 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6176, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %6177 = torch.aten.mul.Tensor %6176, %6164 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6177, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_9825 = torch.constant.none
    %none_9826 = torch.constant.none
    %int6_9827 = torch.constant.int 6
    %cpu_9828 = torch.constant.device "cpu"
    %int0_9829 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6177, %none_9825, %none_9826, %int6_9827, %cpu_9828, %int0_9829 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9830 = torch.constant.int 5
    %6178 = torch.prims.convert_element_type %6177, %int5_9830 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %6178, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_9831 = torch.constant.int 2
    %6179 = torch.aten.unsqueeze %6175, %int2_9831 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %6179, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_9832 = torch.constant.int 2
    %6180 = torch.aten.unsqueeze %6178, %int2_9832 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %6180, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_9833 = torch.constant.none
    %none_9834 = torch.constant.none
    %int5_9835 = torch.constant.int 5
    %cpu_9836 = torch.constant.device "cpu"
    %int0_9837 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6179, %none_9833, %none_9834, %int5_9835, %cpu_9836, %int0_9837 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_9838 = torch.constant.none
    %none_9839 = torch.constant.none
    %int5_9840 = torch.constant.int 5
    %cpu_9841 = torch.constant.device "cpu"
    %int0_9842 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6180, %none_9838, %none_9839, %int5_9840, %cpu_9841, %int0_9842 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_9843 = torch.constant.none
    %none_9844 = torch.constant.none
    %int5_9845 = torch.constant.int 5
    %cpu_9846 = torch.constant.device "cpu"
    %int0_9847 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6151, %none_9843, %none_9844, %int5_9845, %cpu_9846, %int0_9847 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_9848 = torch.constant.int 3
    %int0_9849 = torch.constant.int 0
    %int64_9850 = torch.constant.int 64
    %int2_9851 = torch.constant.int 2
    %6181 = torch.aten.slice.Tensor %6151, %int3_9848, %int0_9849, %int64_9850, %int2_9851 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6181, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_9852 = torch.constant.int 3
    %int1_9853 = torch.constant.int 1
    %int64_9854 = torch.constant.int 64
    %int2_9855 = torch.constant.int 2
    %6182 = torch.aten.slice.Tensor %6151, %int3_9852, %int1_9853, %int64_9854, %int2_9855 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6182, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %6183 = torch.aten.mul.Tensor %6181, %6179 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6183, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %6184 = torch.aten.mul.Tensor %6182, %6180 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6184, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_9856 = torch.constant.int 1
    %6185 = torch.aten.sub.Tensor %6183, %6184, %int1_9856 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6185, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %6186 = torch.aten.mul.Tensor %6182, %6179 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6186, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %6187 = torch.aten.mul.Tensor %6181, %6180 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6187, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_9857 = torch.constant.int 1
    %6188 = torch.aten.add.Tensor %6186, %6187, %int1_9857 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6188, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %6189 = torch_c.to_builtin_tensor %6185 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_9858 = tensor.cast %6189 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %6190 = torch_c.to_builtin_tensor %6188 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_9859 = tensor.cast %6190 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %6191 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_9858, %cast_9859) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_9860 = tensor.cast %6191 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %6192 = torch_c.from_builtin_tensor %cast_9860 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %6192, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_9861 = torch.constant.int 4
    %int32_9862 = torch.constant.int 32
    %int64_9863 = torch.constant.int 64
    %6193 = torch.prim.ListConstruct %int4_9861, %273, %int32_9862, %int64_9863 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6194 = torch.aten.view %6192, %6193 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6194, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_9864 = torch.constant.none
    %none_9865 = torch.constant.none
    %int5_9866 = torch.constant.int 5
    %cpu_9867 = torch.constant.device "cpu"
    %int0_9868 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6194, %none_9864, %none_9865, %int5_9866, %cpu_9867, %int0_9868 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_9869 = torch.constant.int 0
    %none_9870 = torch.constant.none
    %none_9871 = torch.constant.none
    %cpu_9872 = torch.constant.device "cpu"
    %false_9873 = torch.constant.bool false
    %6195 = torch.aten.arange.start %int0_9869, %273, %none_9870, %none_9871, %cpu_9872, %false_9873 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6195, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_9874 = torch.constant.int 0
    %6196 = torch.aten.unsqueeze %6195, %int0_9874 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %6196, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_9875 = torch.constant.int 0
    %int64_9876 = torch.constant.int 64
    %int2_9877 = torch.constant.int 2
    %none_9878 = torch.constant.none
    %none_9879 = torch.constant.none
    %cpu_9880 = torch.constant.device "cpu"
    %false_9881 = torch.constant.bool false
    %6197 = torch.aten.arange.start_step %int0_9875, %int64_9876, %int2_9877, %none_9878, %none_9879, %cpu_9880, %false_9881 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_9882 = torch.constant.none
    %none_9883 = torch.constant.none
    %int4_9884 = torch.constant.int 4
    %cpu_9885 = torch.constant.device "cpu"
    %int0_9886 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6197, %none_9882, %none_9883, %int4_9884, %cpu_9885, %int0_9886 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9887 = torch.constant.int 6
    %6198 = torch.prims.convert_element_type %6197, %int6_9887 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_9888 = torch.constant.int 64
    %6199 = torch.aten.div.Scalar %6198, %int64_9888 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_9889 = torch.constant.float 1.000000e+04
    %6200 = torch.aten.pow.Scalar %float1.000000e04_9889, %6199 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %6201 = torch.aten.reciprocal %6200 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_9890 = torch.constant.float 1.000000e+00
    %6202 = torch.aten.mul.Scalar %6201, %float1.000000e00_9890 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_9891 = torch.constant.none
    %6203 = torch.aten.clone %230, %none_9891 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_9892 = torch.constant.int 0
    %6204 = torch.aten.unsqueeze %6202, %int0_9892 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_9893 = torch.constant.int 2
    %6205 = torch.aten.unsqueeze %6204, %int2_9893 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_9894 = torch.constant.none
    %none_9895 = torch.constant.none
    %int6_9896 = torch.constant.int 6
    %cpu_9897 = torch.constant.device "cpu"
    %int0_9898 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6205, %none_9894, %none_9895, %int6_9896, %cpu_9897, %int0_9898 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_9899 = torch.constant.int 1
    %int-1_9900 = torch.constant.int -1
    %int1_9901 = torch.constant.int 1
    %6206 = torch.prim.ListConstruct %int1_9899, %int-1_9900, %int1_9901 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_9902 = torch.constant.bool false
    %6207 = torch.aten.expand %6205, %6206, %false_9902 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_9903 = torch.constant.int 1
    %6208 = torch.aten.unsqueeze %6196, %int1_9903 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %6208, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_9904 = torch.constant.none
    %none_9905 = torch.constant.none
    %int4_9906 = torch.constant.int 4
    %cpu_9907 = torch.constant.device "cpu"
    %int0_9908 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6208, %none_9904, %none_9905, %int4_9906, %cpu_9907, %int0_9908 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9909 = torch.constant.int 6
    %6209 = torch.prims.convert_element_type %6208, %int6_9909 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %6209, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %6210 = torch.aten.matmul %6207, %6209 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %6210, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_9910 = torch.constant.int 1
    %int2_9911 = torch.constant.int 2
    %6211 = torch.aten.transpose.int %6210, %int1_9910, %int2_9911 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6211, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %6212 = torch.aten.cos %6211 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6212, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %6213 = torch.aten.mul.Tensor %6212, %6203 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6213, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_9912 = torch.constant.none
    %none_9913 = torch.constant.none
    %int6_9914 = torch.constant.int 6
    %cpu_9915 = torch.constant.device "cpu"
    %int0_9916 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6213, %none_9912, %none_9913, %int6_9914, %cpu_9915, %int0_9916 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9917 = torch.constant.int 5
    %6214 = torch.prims.convert_element_type %6213, %int5_9917 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %6214, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %6215 = torch.aten.sin %6211 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6215, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %6216 = torch.aten.mul.Tensor %6215, %6203 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6216, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_9918 = torch.constant.none
    %none_9919 = torch.constant.none
    %int6_9920 = torch.constant.int 6
    %cpu_9921 = torch.constant.device "cpu"
    %int0_9922 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6216, %none_9918, %none_9919, %int6_9920, %cpu_9921, %int0_9922 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9923 = torch.constant.int 5
    %6217 = torch.prims.convert_element_type %6216, %int5_9923 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %6217, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_9924 = torch.constant.int 2
    %6218 = torch.aten.unsqueeze %6214, %int2_9924 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %6218, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_9925 = torch.constant.int 2
    %6219 = torch.aten.unsqueeze %6217, %int2_9925 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %6219, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_9926 = torch.constant.none
    %none_9927 = torch.constant.none
    %int5_9928 = torch.constant.int 5
    %cpu_9929 = torch.constant.device "cpu"
    %int0_9930 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6218, %none_9926, %none_9927, %int5_9928, %cpu_9929, %int0_9930 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_9931 = torch.constant.none
    %none_9932 = torch.constant.none
    %int5_9933 = torch.constant.int 5
    %cpu_9934 = torch.constant.device "cpu"
    %int0_9935 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6219, %none_9931, %none_9932, %int5_9933, %cpu_9934, %int0_9935 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_9936 = torch.constant.none
    %none_9937 = torch.constant.none
    %int5_9938 = torch.constant.int 5
    %cpu_9939 = torch.constant.device "cpu"
    %int0_9940 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6153, %none_9936, %none_9937, %int5_9938, %cpu_9939, %int0_9940 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_9941 = torch.constant.int 3
    %int0_9942 = torch.constant.int 0
    %int64_9943 = torch.constant.int 64
    %int2_9944 = torch.constant.int 2
    %6220 = torch.aten.slice.Tensor %6153, %int3_9941, %int0_9942, %int64_9943, %int2_9944 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6220, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_9945 = torch.constant.int 3
    %int1_9946 = torch.constant.int 1
    %int64_9947 = torch.constant.int 64
    %int2_9948 = torch.constant.int 2
    %6221 = torch.aten.slice.Tensor %6153, %int3_9945, %int1_9946, %int64_9947, %int2_9948 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6221, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %6222 = torch.aten.mul.Tensor %6220, %6218 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6222, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %6223 = torch.aten.mul.Tensor %6221, %6219 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6223, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_9949 = torch.constant.int 1
    %6224 = torch.aten.sub.Tensor %6222, %6223, %int1_9949 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6224, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %6225 = torch.aten.mul.Tensor %6221, %6218 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6225, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %6226 = torch.aten.mul.Tensor %6220, %6219 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6226, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_9950 = torch.constant.int 1
    %6227 = torch.aten.add.Tensor %6225, %6226, %int1_9950 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6227, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %6228 = torch_c.to_builtin_tensor %6224 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_9951 = tensor.cast %6228 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %6229 = torch_c.to_builtin_tensor %6227 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_9952 = tensor.cast %6229 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %6230 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_9951, %cast_9952) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_9953 = tensor.cast %6230 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %6231 = torch_c.from_builtin_tensor %cast_9953 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %6231, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_9954 = torch.constant.int 4
    %int4_9955 = torch.constant.int 4
    %int64_9956 = torch.constant.int 64
    %6232 = torch.prim.ListConstruct %int4_9954, %273, %int4_9955, %int64_9956 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6233 = torch.aten.view %6231, %6232 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %6233, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_9957 = torch.constant.none
    %none_9958 = torch.constant.none
    %int5_9959 = torch.constant.int 5
    %cpu_9960 = torch.constant.device "cpu"
    %int0_9961 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6233, %none_9957, %none_9958, %int5_9959, %cpu_9960, %int0_9961 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_9962 = torch.constant.int 22
    %6234 = torch.aten.mul.Scalar %arg2, %int22_9962 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6234, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int19 = torch.constant.int 19
    %int1_9963 = torch.constant.int 1
    %6235 = torch.aten.add.Scalar %6234, %int19, %int1_9963 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6235, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_9964 = torch.constant.int 2
    %6236 = torch.aten.mul.Scalar %6235, %int2_9964 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6236, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_9965 = torch.constant.int 0
    %int1_9966 = torch.constant.int 1
    %6237 = torch.aten.add.Scalar %6236, %int0_9965, %int1_9966 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6237, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %6238 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %6239 = torch.aten.view %6237, %6238 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6239, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_9967 = torch.constant.int 4
    %int32_9968 = torch.constant.int 32
    %int4_9969 = torch.constant.int 4
    %int64_9970 = torch.constant.int 64
    %6240 = torch.prim.ListConstruct %int4_9967, %271, %int32_9968, %int4_9969, %int64_9970 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6241 = torch.aten.view %6233, %6240 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %6241, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_9971 = torch.constant.int 32
    %int4_9972 = torch.constant.int 4
    %int64_9973 = torch.constant.int 64
    %6242 = torch.prim.ListConstruct %446, %int32_9971, %int4_9972, %int64_9973 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6243 = torch.aten.view %6241, %6242 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %6243, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_9974 = torch.constant.int 1
    %int2_9975 = torch.constant.int 2
    %6244 = torch.aten.transpose.int %6243, %int1_9974, %int2_9975 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %6244, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_9976 = torch.constant.none
    %none_9977 = torch.constant.none
    %int5_9978 = torch.constant.int 5
    %cpu_9979 = torch.constant.device "cpu"
    %int0_9980 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6244, %none_9976, %none_9977, %int5_9978, %cpu_9979, %int0_9980 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_9981 = torch.constant.int 22
    %int2_9982 = torch.constant.int 2
    %int4_9983 = torch.constant.int 4
    %int32_9984 = torch.constant.int 32
    %int64_9985 = torch.constant.int 64
    %6245 = torch.prim.ListConstruct %272, %int22_9981, %int2_9982, %int4_9983, %int32_9984, %int64_9985 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6246 = torch.aten.view %5970, %6245 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6246, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_9986 = torch.constant.int 4
    %int32_9987 = torch.constant.int 32
    %int64_9988 = torch.constant.int 64
    %6247 = torch.prim.ListConstruct %439, %int4_9986, %int32_9987, %int64_9988 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6248 = torch.aten.view %6246, %6247 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %6248, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %6249 = torch.prim.ListConstruct %6239 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_9989 = torch.constant.bool false
    %6250 = torch.aten.index_put %6248, %6249, %6244, %false_9989 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %6250, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_9990 = torch.constant.int 22
    %int2_9991 = torch.constant.int 2
    %int4_9992 = torch.constant.int 4
    %int32_9993 = torch.constant.int 32
    %int64_9994 = torch.constant.int 64
    %6251 = torch.prim.ListConstruct %272, %int22_9990, %int2_9991, %int4_9992, %int32_9993, %int64_9994 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6252 = torch.aten.view %6250, %6251 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6252, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_9995 = torch.constant.int 360448
    %6253 = torch.prim.ListConstruct %272, %int360448_9995 : (!torch.int, !torch.int) -> !torch.list<int>
    %6254 = torch.aten.view %6252, %6253 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %6254, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_9996 = torch.constant.int 22
    %int2_9997 = torch.constant.int 2
    %int4_9998 = torch.constant.int 4
    %int32_9999 = torch.constant.int 32
    %int64_10000 = torch.constant.int 64
    %6255 = torch.prim.ListConstruct %272, %int22_9996, %int2_9997, %int4_9998, %int32_9999, %int64_10000 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6256 = torch.aten.view %6254, %6255 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6256, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_10001 = torch.constant.int 4
    %int32_10002 = torch.constant.int 32
    %int64_10003 = torch.constant.int 64
    %6257 = torch.prim.ListConstruct %439, %int4_10001, %int32_10002, %int64_10003 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6258 = torch.aten.view %6256, %6257 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %6258, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_10004 = torch.constant.int 22
    %6259 = torch.aten.mul.Scalar %arg2, %int22_10004 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6259, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int19_10005 = torch.constant.int 19
    %int1_10006 = torch.constant.int 1
    %6260 = torch.aten.add.Scalar %6259, %int19_10005, %int1_10006 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6260, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_10007 = torch.constant.int 2
    %6261 = torch.aten.mul.Scalar %6260, %int2_10007 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6261, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_10008 = torch.constant.int 1
    %int1_10009 = torch.constant.int 1
    %6262 = torch.aten.add.Scalar %6261, %int1_10008, %int1_10009 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6262, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %6263 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %6264 = torch.aten.view %6262, %6263 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6264, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_10010 = torch.constant.int 4
    %int32_10011 = torch.constant.int 32
    %int4_10012 = torch.constant.int 4
    %int64_10013 = torch.constant.int 64
    %6265 = torch.prim.ListConstruct %int4_10010, %271, %int32_10011, %int4_10012, %int64_10013 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6266 = torch.aten.view %6155, %6265 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %6266, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_10014 = torch.constant.int 32
    %int4_10015 = torch.constant.int 4
    %int64_10016 = torch.constant.int 64
    %6267 = torch.prim.ListConstruct %446, %int32_10014, %int4_10015, %int64_10016 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6268 = torch.aten.view %6266, %6267 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %6268, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_10017 = torch.constant.int 1
    %int2_10018 = torch.constant.int 2
    %6269 = torch.aten.transpose.int %6268, %int1_10017, %int2_10018 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %6269, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_10019 = torch.constant.none
    %none_10020 = torch.constant.none
    %int5_10021 = torch.constant.int 5
    %cpu_10022 = torch.constant.device "cpu"
    %int0_10023 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6269, %none_10019, %none_10020, %int5_10021, %cpu_10022, %int0_10023 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %6270 = torch.prim.ListConstruct %6264 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_10024 = torch.constant.bool false
    %6271 = torch.aten.index_put %6258, %6270, %6269, %false_10024 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %6271, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_10025 = torch.constant.int 22
    %int2_10026 = torch.constant.int 2
    %int4_10027 = torch.constant.int 4
    %int32_10028 = torch.constant.int 32
    %int64_10029 = torch.constant.int 64
    %6272 = torch.prim.ListConstruct %272, %int22_10025, %int2_10026, %int4_10027, %int32_10028, %int64_10029 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6273 = torch.aten.view %6271, %6272 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6273, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_10030 = torch.constant.int 360448
    %6274 = torch.prim.ListConstruct %272, %int360448_10030 : (!torch.int, !torch.int) -> !torch.list<int>
    %6275 = torch.aten.view %6273, %6274 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %6275, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_10031 = torch.constant.int 0
    %int1_10032 = torch.constant.int 1
    %none_10033 = torch.constant.none
    %none_10034 = torch.constant.none
    %cpu_10035 = torch.constant.device "cpu"
    %false_10036 = torch.constant.bool false
    %6276 = torch.aten.arange.start_step %int0_10031, %273, %int1_10032, %none_10033, %none_10034, %cpu_10035, %false_10036 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6276, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_10037 = torch.constant.int -1
    %6277 = torch.aten.unsqueeze %arg1, %int-1_10037 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %6278 = torch.aten.ge.Tensor %6276, %6277 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %6278, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_10038 = torch.constant.none
    %none_10039 = torch.constant.none
    %cpu_10040 = torch.constant.device "cpu"
    %false_10041 = torch.constant.bool false
    %6279 = torch.aten.arange %273, %none_10038, %none_10039, %cpu_10040, %false_10041 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6279, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_10042 = torch.constant.int 0
    %6280 = torch.aten.unsqueeze %6279, %int0_10042 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %6280, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_10043 = torch.constant.int 1
    %6281 = torch.aten.unsqueeze %6280, %int1_10043 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %6281, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_10044 = torch.constant.int 2
    %6282 = torch.aten.unsqueeze %6281, %int2_10044 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %6282, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_10045 = torch.constant.none
    %none_10046 = torch.constant.none
    %cpu_10047 = torch.constant.device "cpu"
    %false_10048 = torch.constant.bool false
    %6283 = torch.aten.arange %273, %none_10045, %none_10046, %cpu_10047, %false_10048 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6283, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_10049 = torch.constant.int 0
    %6284 = torch.aten.unsqueeze %6283, %int0_10049 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %6284, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_10050 = torch.constant.int 1
    %6285 = torch.aten.unsqueeze %6284, %int1_10050 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %6285, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_10051 = torch.constant.int 3
    %6286 = torch.aten.unsqueeze %6285, %int3_10051 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %6286, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %6287 = torch.aten.gt.Tensor %6282, %6286 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %6287, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_10052 = torch.constant.int 1
    %6288 = torch.aten.unsqueeze %6278, %int1_10052 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %6288, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_10053 = torch.constant.int 2
    %6289 = torch.aten.unsqueeze %6288, %int2_10053 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %6289, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %6290 = torch.aten.logical_or %6287, %6289 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %6290, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_10054 = torch.constant.none
    %6291 = torch.aten.clone %231, %none_10054 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_10055 = torch.constant.int 0
    %6292 = torch.aten.where.ScalarOther %6290, %6291, %int0_10055 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %6292, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_10056 = torch.constant.none
    %none_10057 = torch.constant.none
    %int5_10058 = torch.constant.int 5
    %cpu_10059 = torch.constant.device "cpu"
    %int0_10060 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6292, %none_10056, %none_10057, %int5_10058, %cpu_10059, %int0_10060 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_10061 = torch.constant.int -2
    %6293 = torch.aten.unsqueeze %6233, %int-2_10061 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %6293, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_10062 = torch.constant.int 4
    %int4_10063 = torch.constant.int 4
    %int8_10064 = torch.constant.int 8
    %int64_10065 = torch.constant.int 64
    %6294 = torch.prim.ListConstruct %int4_10062, %273, %int4_10063, %int8_10064, %int64_10065 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_10066 = torch.constant.bool false
    %6295 = torch.aten.expand %6293, %6294, %false_10066 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6295, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_10067 = torch.constant.int 0
    %6296 = torch.aten.clone %6295, %int0_10067 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6296, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_10068 = torch.constant.int 4
    %int32_10069 = torch.constant.int 32
    %int64_10070 = torch.constant.int 64
    %6297 = torch.prim.ListConstruct %int4_10068, %273, %int32_10069, %int64_10070 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6298 = torch.aten._unsafe_view %6296, %6297 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6298, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_10071 = torch.constant.int -2
    %6299 = torch.aten.unsqueeze %6155, %int-2_10071 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %6299, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_10072 = torch.constant.int 4
    %int4_10073 = torch.constant.int 4
    %int8_10074 = torch.constant.int 8
    %int64_10075 = torch.constant.int 64
    %6300 = torch.prim.ListConstruct %int4_10072, %273, %int4_10073, %int8_10074, %int64_10075 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_10076 = torch.constant.bool false
    %6301 = torch.aten.expand %6299, %6300, %false_10076 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6301, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_10077 = torch.constant.int 0
    %6302 = torch.aten.clone %6301, %int0_10077 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6302, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_10078 = torch.constant.int 4
    %int32_10079 = torch.constant.int 32
    %int64_10080 = torch.constant.int 64
    %6303 = torch.prim.ListConstruct %int4_10078, %273, %int32_10079, %int64_10080 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6304 = torch.aten._unsafe_view %6302, %6303 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6304, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_10081 = torch.constant.int 1
    %int2_10082 = torch.constant.int 2
    %6305 = torch.aten.transpose.int %6194, %int1_10081, %int2_10082 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6305, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_10083 = torch.constant.int 1
    %int2_10084 = torch.constant.int 2
    %6306 = torch.aten.transpose.int %6298, %int1_10083, %int2_10084 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6306, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_10085 = torch.constant.int 1
    %int2_10086 = torch.constant.int 2
    %6307 = torch.aten.transpose.int %6304, %int1_10085, %int2_10086 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6307, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_10087 = torch.constant.float 0.000000e+00
    %false_10088 = torch.constant.bool false
    %none_10089 = torch.constant.none
    %false_10090 = torch.constant.bool false
    %6308 = torch.aten.scaled_dot_product_attention %6305, %6306, %6307, %6292, %float0.000000e00_10087, %false_10088, %none_10089, %false_10090 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6308, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_10091 = torch.constant.int 1
    %int2_10092 = torch.constant.int 2
    %6309 = torch.aten.transpose.int %6308, %int1_10091, %int2_10092 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6309, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_10093 = torch.constant.int 4
    %int2048_10094 = torch.constant.int 2048
    %6310 = torch.prim.ListConstruct %int4_10093, %273, %int2048_10094 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6311 = torch.aten.view %6309, %6310 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6311, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_10095 = torch.constant.int 2
    %6312 = torch.aten.view.dtype %236, %int2_10095 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %6313 = torch.aten.detach %6312 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_10096 = torch.constant.int -1
    %int17_10097 = torch.constant.int 17
    %6314 = torch.prim.ListConstruct %int-1_10096, %int17_10097 : (!torch.int, !torch.int) -> !torch.list<int>
    %6315 = torch.aten.view %6313, %6314 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_10098 = torch.constant.int 2048
    %int-1_10099 = torch.constant.int -1
    %int17_10100 = torch.constant.int 17
    %6316 = torch.prim.ListConstruct %int2048_10098, %int-1_10099, %int17_10100 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6317 = torch.aten.view %6315, %6316 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_10101 = torch.constant.int 2
    %int0_10102 = torch.constant.int 0
    %int1_10103 = torch.constant.int 1
    %int1_10104 = torch.constant.int 1
    %6318 = torch.aten.slice.Tensor %6317, %int2_10101, %int0_10102, %int1_10103, %int1_10104 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_10105 = torch.constant.int 5
    %6319 = torch.aten.view.dtype %6318, %int5_10105 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %6320 = torch.aten.detach %6319 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_10106 = torch.constant.int 2
    %int1_10107 = torch.constant.int 1
    %int9223372036854775807_10108 = torch.constant.int 9223372036854775807
    %int1_10109 = torch.constant.int 1
    %6321 = torch.aten.slice.Tensor %6317, %int2_10106, %int1_10107, %int9223372036854775807_10108, %int1_10109 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_10110 = torch.constant.int 1
    %6322 = torch.aten.view.dtype %6321, %int1_10110 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %6323 = torch.aten.detach %6322 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %6324 = torch_c.to_builtin_tensor %6311 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_10111 = tensor.cast %6324 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6325 = torch_c.to_builtin_tensor %6320 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %6326 = torch_c.to_builtin_tensor %6323 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %6327 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_10111, %6325, %6326) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_10112 = tensor.cast %6327 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %6328 = torch_c.from_builtin_tensor %cast_10112 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6328, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_10113 = torch.constant.none
    %none_10114 = torch.constant.none
    %int5_10115 = torch.constant.int 5
    %cpu_10116 = torch.constant.device "cpu"
    %int0_10117 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6328, %none_10113, %none_10114, %int5_10115, %cpu_10116, %int0_10117 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_10118 = torch.constant.int 1
    %6329 = torch.aten.add.Tensor %6088, %6328, %int1_10118 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6329, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_10119 = torch.constant.none
    %none_10120 = torch.constant.none
    %int5_10121 = torch.constant.int 5
    %cpu_10122 = torch.constant.device "cpu"
    %int0_10123 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6329, %none_10119, %none_10120, %int5_10121, %cpu_10122, %int0_10123 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10124 = torch.constant.int 6
    %6330 = torch.prims.convert_element_type %6329, %int6_10124 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6330, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_10125 = torch.constant.int 2
    %6331 = torch.aten.pow.Tensor_Scalar %6330, %int2_10125 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6331, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_10126 = torch.constant.int -1
    %6332 = torch.prim.ListConstruct %int-1_10126 : (!torch.int) -> !torch.list<int>
    %true_10127 = torch.constant.bool true
    %none_10128 = torch.constant.none
    %6333 = torch.aten.mean.dim %6331, %6332, %true_10127, %none_10128 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6333, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_10129 = torch.constant.float 9.9999997473787516E-6
    %int1_10130 = torch.constant.int 1
    %6334 = torch.aten.add.Scalar %6333, %float9.999990e-06_10129, %int1_10130 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6334, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6335 = torch.aten.rsqrt %6334 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6335, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6336 = torch.aten.mul.Tensor %6330, %6335 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6336, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_10131 = torch.constant.none
    %none_10132 = torch.constant.none
    %int6_10133 = torch.constant.int 6
    %cpu_10134 = torch.constant.device "cpu"
    %int0_10135 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6336, %none_10131, %none_10132, %int6_10133, %cpu_10134, %int0_10135 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10136 = torch.constant.int 5
    %6337 = torch.prims.convert_element_type %6336, %int5_10136 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6337, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %6338 = torch.aten.mul.Tensor %237, %6337 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6338, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_10137 = torch.constant.none
    %none_10138 = torch.constant.none
    %int6_10139 = torch.constant.int 6
    %cpu_10140 = torch.constant.device "cpu"
    %int0_10141 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6338, %none_10137, %none_10138, %int6_10139, %cpu_10140, %int0_10141 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10142 = torch.constant.int 5
    %6339 = torch.prims.convert_element_type %6338, %int5_10142 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6339, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_10143 = torch.constant.int 2
    %6340 = torch.aten.view.dtype %238, %int2_10143 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %6341 = torch.aten.detach %6340 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_10144 = torch.constant.int -1
    %int17_10145 = torch.constant.int 17
    %6342 = torch.prim.ListConstruct %int-1_10144, %int17_10145 : (!torch.int, !torch.int) -> !torch.list<int>
    %6343 = torch.aten.view %6341, %6342 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_10146 = torch.constant.int 5632
    %int-1_10147 = torch.constant.int -1
    %int17_10148 = torch.constant.int 17
    %6344 = torch.prim.ListConstruct %int5632_10146, %int-1_10147, %int17_10148 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6345 = torch.aten.view %6343, %6344 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_10149 = torch.constant.int 2
    %int0_10150 = torch.constant.int 0
    %int1_10151 = torch.constant.int 1
    %int1_10152 = torch.constant.int 1
    %6346 = torch.aten.slice.Tensor %6345, %int2_10149, %int0_10150, %int1_10151, %int1_10152 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_10153 = torch.constant.int 5
    %6347 = torch.aten.view.dtype %6346, %int5_10153 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %6348 = torch.aten.detach %6347 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_10154 = torch.constant.int 2
    %int1_10155 = torch.constant.int 1
    %int9223372036854775807_10156 = torch.constant.int 9223372036854775807
    %int1_10157 = torch.constant.int 1
    %6349 = torch.aten.slice.Tensor %6345, %int2_10154, %int1_10155, %int9223372036854775807_10156, %int1_10157 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_10158 = torch.constant.int 1
    %6350 = torch.aten.view.dtype %6349, %int1_10158 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %6351 = torch.aten.detach %6350 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %6352 = torch_c.to_builtin_tensor %6339 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_10159 = tensor.cast %6352 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6353 = torch_c.to_builtin_tensor %6348 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %6354 = torch_c.to_builtin_tensor %6351 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %6355 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_10159, %6353, %6354) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_10160 = tensor.cast %6355 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %6356 = torch_c.from_builtin_tensor %cast_10160 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %6356, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %6357 = torch.aten.silu %6356 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %6357, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_10161 = torch.constant.int 2
    %6358 = torch.aten.view.dtype %239, %int2_10161 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %6359 = torch.aten.detach %6358 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_10162 = torch.constant.int -1
    %int17_10163 = torch.constant.int 17
    %6360 = torch.prim.ListConstruct %int-1_10162, %int17_10163 : (!torch.int, !torch.int) -> !torch.list<int>
    %6361 = torch.aten.view %6359, %6360 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_10164 = torch.constant.int 5632
    %int-1_10165 = torch.constant.int -1
    %int17_10166 = torch.constant.int 17
    %6362 = torch.prim.ListConstruct %int5632_10164, %int-1_10165, %int17_10166 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6363 = torch.aten.view %6361, %6362 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_10167 = torch.constant.int 2
    %int0_10168 = torch.constant.int 0
    %int1_10169 = torch.constant.int 1
    %int1_10170 = torch.constant.int 1
    %6364 = torch.aten.slice.Tensor %6363, %int2_10167, %int0_10168, %int1_10169, %int1_10170 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_10171 = torch.constant.int 5
    %6365 = torch.aten.view.dtype %6364, %int5_10171 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %6366 = torch.aten.detach %6365 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_10172 = torch.constant.int 2
    %int1_10173 = torch.constant.int 1
    %int9223372036854775807_10174 = torch.constant.int 9223372036854775807
    %int1_10175 = torch.constant.int 1
    %6367 = torch.aten.slice.Tensor %6363, %int2_10172, %int1_10173, %int9223372036854775807_10174, %int1_10175 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_10176 = torch.constant.int 1
    %6368 = torch.aten.view.dtype %6367, %int1_10176 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %6369 = torch.aten.detach %6368 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %6370 = torch_c.to_builtin_tensor %6339 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_10177 = tensor.cast %6370 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6371 = torch_c.to_builtin_tensor %6366 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %6372 = torch_c.to_builtin_tensor %6369 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %6373 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_10177, %6371, %6372) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_10178 = tensor.cast %6373 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %6374 = torch_c.from_builtin_tensor %cast_10178 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %6374, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %6375 = torch.aten.mul.Tensor %6357, %6374 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %6375, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_10179 = torch.constant.int 2
    %6376 = torch.aten.view.dtype %240, %int2_10179 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %6377 = torch.aten.detach %6376 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_10180 = torch.constant.int -1
    %int17_10181 = torch.constant.int 17
    %6378 = torch.prim.ListConstruct %int-1_10180, %int17_10181 : (!torch.int, !torch.int) -> !torch.list<int>
    %6379 = torch.aten.view %6377, %6378 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_10182 = torch.constant.int 2048
    %int-1_10183 = torch.constant.int -1
    %int17_10184 = torch.constant.int 17
    %6380 = torch.prim.ListConstruct %int2048_10182, %int-1_10183, %int17_10184 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6381 = torch.aten.view %6379, %6380 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_10185 = torch.constant.int 2
    %int0_10186 = torch.constant.int 0
    %int1_10187 = torch.constant.int 1
    %int1_10188 = torch.constant.int 1
    %6382 = torch.aten.slice.Tensor %6381, %int2_10185, %int0_10186, %int1_10187, %int1_10188 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_10189 = torch.constant.int 5
    %6383 = torch.aten.view.dtype %6382, %int5_10189 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %6384 = torch.aten.detach %6383 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_10190 = torch.constant.int 2
    %int1_10191 = torch.constant.int 1
    %int9223372036854775807_10192 = torch.constant.int 9223372036854775807
    %int1_10193 = torch.constant.int 1
    %6385 = torch.aten.slice.Tensor %6381, %int2_10190, %int1_10191, %int9223372036854775807_10192, %int1_10193 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_10194 = torch.constant.int 1
    %6386 = torch.aten.view.dtype %6385, %int1_10194 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %6387 = torch.aten.detach %6386 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %6388 = torch_c.to_builtin_tensor %6375 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_10195 = tensor.cast %6388 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %6389 = torch_c.to_builtin_tensor %6384 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %6390 = torch_c.to_builtin_tensor %6387 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %6391 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_10195, %6389, %6390) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_10196 = tensor.cast %6391 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %6392 = torch_c.from_builtin_tensor %cast_10196 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6392, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_10197 = torch.constant.int 1
    %6393 = torch.aten.add.Tensor %6329, %6392, %int1_10197 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6393, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_10198 = torch.constant.none
    %none_10199 = torch.constant.none
    %int5_10200 = torch.constant.int 5
    %cpu_10201 = torch.constant.device "cpu"
    %int0_10202 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6393, %none_10198, %none_10199, %int5_10200, %cpu_10201, %int0_10202 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10203 = torch.constant.int 6
    %6394 = torch.prims.convert_element_type %6393, %int6_10203 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6394, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_10204 = torch.constant.int 2
    %6395 = torch.aten.pow.Tensor_Scalar %6394, %int2_10204 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6395, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_10205 = torch.constant.int -1
    %6396 = torch.prim.ListConstruct %int-1_10205 : (!torch.int) -> !torch.list<int>
    %true_10206 = torch.constant.bool true
    %none_10207 = torch.constant.none
    %6397 = torch.aten.mean.dim %6395, %6396, %true_10206, %none_10207 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6397, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_10208 = torch.constant.float 9.9999997473787516E-6
    %int1_10209 = torch.constant.int 1
    %6398 = torch.aten.add.Scalar %6397, %float9.999990e-06_10208, %int1_10209 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6398, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6399 = torch.aten.rsqrt %6398 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6399, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6400 = torch.aten.mul.Tensor %6394, %6399 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6400, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_10210 = torch.constant.none
    %none_10211 = torch.constant.none
    %int6_10212 = torch.constant.int 6
    %cpu_10213 = torch.constant.device "cpu"
    %int0_10214 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6400, %none_10210, %none_10211, %int6_10212, %cpu_10213, %int0_10214 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10215 = torch.constant.int 5
    %6401 = torch.prims.convert_element_type %6400, %int5_10215 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6401, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %6402 = torch.aten.mul.Tensor %244, %6401 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6402, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_10216 = torch.constant.none
    %none_10217 = torch.constant.none
    %int6_10218 = torch.constant.int 6
    %cpu_10219 = torch.constant.device "cpu"
    %int0_10220 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6402, %none_10216, %none_10217, %int6_10218, %cpu_10219, %int0_10220 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10221 = torch.constant.int 5
    %6403 = torch.prims.convert_element_type %6402, %int5_10221 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6403, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_10222 = torch.constant.int 2
    %6404 = torch.aten.view.dtype %245, %int2_10222 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %6405 = torch.aten.detach %6404 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_10223 = torch.constant.int -1
    %int17_10224 = torch.constant.int 17
    %6406 = torch.prim.ListConstruct %int-1_10223, %int17_10224 : (!torch.int, !torch.int) -> !torch.list<int>
    %6407 = torch.aten.view %6405, %6406 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_10225 = torch.constant.int 2048
    %int-1_10226 = torch.constant.int -1
    %int17_10227 = torch.constant.int 17
    %6408 = torch.prim.ListConstruct %int2048_10225, %int-1_10226, %int17_10227 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6409 = torch.aten.view %6407, %6408 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_10228 = torch.constant.int 2
    %int0_10229 = torch.constant.int 0
    %int1_10230 = torch.constant.int 1
    %int1_10231 = torch.constant.int 1
    %6410 = torch.aten.slice.Tensor %6409, %int2_10228, %int0_10229, %int1_10230, %int1_10231 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_10232 = torch.constant.int 5
    %6411 = torch.aten.view.dtype %6410, %int5_10232 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %6412 = torch.aten.detach %6411 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_10233 = torch.constant.int 2
    %int1_10234 = torch.constant.int 1
    %int9223372036854775807_10235 = torch.constant.int 9223372036854775807
    %int1_10236 = torch.constant.int 1
    %6413 = torch.aten.slice.Tensor %6409, %int2_10233, %int1_10234, %int9223372036854775807_10235, %int1_10236 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_10237 = torch.constant.int 1
    %6414 = torch.aten.view.dtype %6413, %int1_10237 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %6415 = torch.aten.detach %6414 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %6416 = torch_c.to_builtin_tensor %6403 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_10238 = tensor.cast %6416 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6417 = torch_c.to_builtin_tensor %6412 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %6418 = torch_c.to_builtin_tensor %6415 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %6419 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_10238, %6417, %6418) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_10239 = tensor.cast %6419 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %6420 = torch_c.from_builtin_tensor %cast_10239 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6420, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_10240 = torch.constant.int 2
    %6421 = torch.aten.view.dtype %246, %int2_10240 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %6422 = torch.aten.detach %6421 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_10241 = torch.constant.int -1
    %int17_10242 = torch.constant.int 17
    %6423 = torch.prim.ListConstruct %int-1_10241, %int17_10242 : (!torch.int, !torch.int) -> !torch.list<int>
    %6424 = torch.aten.view %6422, %6423 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_10243 = torch.constant.int 256
    %int-1_10244 = torch.constant.int -1
    %int17_10245 = torch.constant.int 17
    %6425 = torch.prim.ListConstruct %int256_10243, %int-1_10244, %int17_10245 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6426 = torch.aten.view %6424, %6425 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_10246 = torch.constant.int 2
    %int0_10247 = torch.constant.int 0
    %int1_10248 = torch.constant.int 1
    %int1_10249 = torch.constant.int 1
    %6427 = torch.aten.slice.Tensor %6426, %int2_10246, %int0_10247, %int1_10248, %int1_10249 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_10250 = torch.constant.int 5
    %6428 = torch.aten.view.dtype %6427, %int5_10250 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %6429 = torch.aten.detach %6428 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_10251 = torch.constant.int 2
    %int1_10252 = torch.constant.int 1
    %int9223372036854775807_10253 = torch.constant.int 9223372036854775807
    %int1_10254 = torch.constant.int 1
    %6430 = torch.aten.slice.Tensor %6426, %int2_10251, %int1_10252, %int9223372036854775807_10253, %int1_10254 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_10255 = torch.constant.int 1
    %6431 = torch.aten.view.dtype %6430, %int1_10255 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %6432 = torch.aten.detach %6431 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %6433 = torch_c.to_builtin_tensor %6403 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_10256 = tensor.cast %6433 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6434 = torch_c.to_builtin_tensor %6429 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %6435 = torch_c.to_builtin_tensor %6432 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %6436 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_10256, %6434, %6435) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_10257 = tensor.cast %6436 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %6437 = torch_c.from_builtin_tensor %cast_10257 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %6437, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_10258 = torch.constant.int 2
    %6438 = torch.aten.view.dtype %247, %int2_10258 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %6439 = torch.aten.detach %6438 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_10259 = torch.constant.int -1
    %int17_10260 = torch.constant.int 17
    %6440 = torch.prim.ListConstruct %int-1_10259, %int17_10260 : (!torch.int, !torch.int) -> !torch.list<int>
    %6441 = torch.aten.view %6439, %6440 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_10261 = torch.constant.int 256
    %int-1_10262 = torch.constant.int -1
    %int17_10263 = torch.constant.int 17
    %6442 = torch.prim.ListConstruct %int256_10261, %int-1_10262, %int17_10263 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6443 = torch.aten.view %6441, %6442 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_10264 = torch.constant.int 2
    %int0_10265 = torch.constant.int 0
    %int1_10266 = torch.constant.int 1
    %int1_10267 = torch.constant.int 1
    %6444 = torch.aten.slice.Tensor %6443, %int2_10264, %int0_10265, %int1_10266, %int1_10267 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_10268 = torch.constant.int 5
    %6445 = torch.aten.view.dtype %6444, %int5_10268 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %6446 = torch.aten.detach %6445 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_10269 = torch.constant.int 2
    %int1_10270 = torch.constant.int 1
    %int9223372036854775807_10271 = torch.constant.int 9223372036854775807
    %int1_10272 = torch.constant.int 1
    %6447 = torch.aten.slice.Tensor %6443, %int2_10269, %int1_10270, %int9223372036854775807_10271, %int1_10272 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_10273 = torch.constant.int 1
    %6448 = torch.aten.view.dtype %6447, %int1_10273 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %6449 = torch.aten.detach %6448 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %6450 = torch_c.to_builtin_tensor %6403 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_10274 = tensor.cast %6450 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6451 = torch_c.to_builtin_tensor %6446 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %6452 = torch_c.to_builtin_tensor %6449 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %6453 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_10274, %6451, %6452) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_10275 = tensor.cast %6453 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %6454 = torch_c.from_builtin_tensor %cast_10275 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %6454, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_10276 = torch.constant.int 4
    %int32_10277 = torch.constant.int 32
    %int64_10278 = torch.constant.int 64
    %6455 = torch.prim.ListConstruct %int4_10276, %273, %int32_10277, %int64_10278 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6456 = torch.aten.view %6420, %6455 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6456, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_10279 = torch.constant.int 4
    %int4_10280 = torch.constant.int 4
    %int64_10281 = torch.constant.int 64
    %6457 = torch.prim.ListConstruct %int4_10279, %273, %int4_10280, %int64_10281 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6458 = torch.aten.view %6437, %6457 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %6458, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_10282 = torch.constant.int 4
    %int4_10283 = torch.constant.int 4
    %int64_10284 = torch.constant.int 64
    %6459 = torch.prim.ListConstruct %int4_10282, %273, %int4_10283, %int64_10284 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6460 = torch.aten.view %6454, %6459 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %6460, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_10285 = torch.constant.int 0
    %none_10286 = torch.constant.none
    %none_10287 = torch.constant.none
    %cpu_10288 = torch.constant.device "cpu"
    %false_10289 = torch.constant.bool false
    %6461 = torch.aten.arange.start %int0_10285, %273, %none_10286, %none_10287, %cpu_10288, %false_10289 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6461, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_10290 = torch.constant.int 0
    %6462 = torch.aten.unsqueeze %6461, %int0_10290 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %6462, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_10291 = torch.constant.int 0
    %int64_10292 = torch.constant.int 64
    %int2_10293 = torch.constant.int 2
    %none_10294 = torch.constant.none
    %none_10295 = torch.constant.none
    %cpu_10296 = torch.constant.device "cpu"
    %false_10297 = torch.constant.bool false
    %6463 = torch.aten.arange.start_step %int0_10291, %int64_10292, %int2_10293, %none_10294, %none_10295, %cpu_10296, %false_10297 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_10298 = torch.constant.none
    %none_10299 = torch.constant.none
    %int4_10300 = torch.constant.int 4
    %cpu_10301 = torch.constant.device "cpu"
    %int0_10302 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6463, %none_10298, %none_10299, %int4_10300, %cpu_10301, %int0_10302 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10303 = torch.constant.int 6
    %6464 = torch.prims.convert_element_type %6463, %int6_10303 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_10304 = torch.constant.int 64
    %6465 = torch.aten.div.Scalar %6464, %int64_10304 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_10305 = torch.constant.float 1.000000e+04
    %6466 = torch.aten.pow.Scalar %float1.000000e04_10305, %6465 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %6467 = torch.aten.reciprocal %6466 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_10306 = torch.constant.float 1.000000e+00
    %6468 = torch.aten.mul.Scalar %6467, %float1.000000e00_10306 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_10307 = torch.constant.none
    %6469 = torch.aten.clone %241, %none_10307 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_10308 = torch.constant.int 0
    %6470 = torch.aten.unsqueeze %6468, %int0_10308 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_10309 = torch.constant.int 2
    %6471 = torch.aten.unsqueeze %6470, %int2_10309 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_10310 = torch.constant.none
    %none_10311 = torch.constant.none
    %int6_10312 = torch.constant.int 6
    %cpu_10313 = torch.constant.device "cpu"
    %int0_10314 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6471, %none_10310, %none_10311, %int6_10312, %cpu_10313, %int0_10314 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_10315 = torch.constant.int 1
    %int-1_10316 = torch.constant.int -1
    %int1_10317 = torch.constant.int 1
    %6472 = torch.prim.ListConstruct %int1_10315, %int-1_10316, %int1_10317 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_10318 = torch.constant.bool false
    %6473 = torch.aten.expand %6471, %6472, %false_10318 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_10319 = torch.constant.int 1
    %6474 = torch.aten.unsqueeze %6462, %int1_10319 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %6474, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_10320 = torch.constant.none
    %none_10321 = torch.constant.none
    %int4_10322 = torch.constant.int 4
    %cpu_10323 = torch.constant.device "cpu"
    %int0_10324 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6474, %none_10320, %none_10321, %int4_10322, %cpu_10323, %int0_10324 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10325 = torch.constant.int 6
    %6475 = torch.prims.convert_element_type %6474, %int6_10325 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %6475, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %6476 = torch.aten.matmul %6473, %6475 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %6476, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_10326 = torch.constant.int 1
    %int2_10327 = torch.constant.int 2
    %6477 = torch.aten.transpose.int %6476, %int1_10326, %int2_10327 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6477, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %6478 = torch.aten.cos %6477 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6478, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %6479 = torch.aten.mul.Tensor %6478, %6469 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6479, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_10328 = torch.constant.none
    %none_10329 = torch.constant.none
    %int6_10330 = torch.constant.int 6
    %cpu_10331 = torch.constant.device "cpu"
    %int0_10332 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6479, %none_10328, %none_10329, %int6_10330, %cpu_10331, %int0_10332 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10333 = torch.constant.int 5
    %6480 = torch.prims.convert_element_type %6479, %int5_10333 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %6480, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %6481 = torch.aten.sin %6477 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6481, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %6482 = torch.aten.mul.Tensor %6481, %6469 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6482, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_10334 = torch.constant.none
    %none_10335 = torch.constant.none
    %int6_10336 = torch.constant.int 6
    %cpu_10337 = torch.constant.device "cpu"
    %int0_10338 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6482, %none_10334, %none_10335, %int6_10336, %cpu_10337, %int0_10338 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10339 = torch.constant.int 5
    %6483 = torch.prims.convert_element_type %6482, %int5_10339 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %6483, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_10340 = torch.constant.int 2
    %6484 = torch.aten.unsqueeze %6480, %int2_10340 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %6484, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_10341 = torch.constant.int 2
    %6485 = torch.aten.unsqueeze %6483, %int2_10341 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %6485, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_10342 = torch.constant.none
    %none_10343 = torch.constant.none
    %int5_10344 = torch.constant.int 5
    %cpu_10345 = torch.constant.device "cpu"
    %int0_10346 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6484, %none_10342, %none_10343, %int5_10344, %cpu_10345, %int0_10346 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_10347 = torch.constant.none
    %none_10348 = torch.constant.none
    %int5_10349 = torch.constant.int 5
    %cpu_10350 = torch.constant.device "cpu"
    %int0_10351 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6485, %none_10347, %none_10348, %int5_10349, %cpu_10350, %int0_10351 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_10352 = torch.constant.none
    %none_10353 = torch.constant.none
    %int5_10354 = torch.constant.int 5
    %cpu_10355 = torch.constant.device "cpu"
    %int0_10356 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6456, %none_10352, %none_10353, %int5_10354, %cpu_10355, %int0_10356 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_10357 = torch.constant.int 3
    %int0_10358 = torch.constant.int 0
    %int64_10359 = torch.constant.int 64
    %int2_10360 = torch.constant.int 2
    %6486 = torch.aten.slice.Tensor %6456, %int3_10357, %int0_10358, %int64_10359, %int2_10360 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6486, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_10361 = torch.constant.int 3
    %int1_10362 = torch.constant.int 1
    %int64_10363 = torch.constant.int 64
    %int2_10364 = torch.constant.int 2
    %6487 = torch.aten.slice.Tensor %6456, %int3_10361, %int1_10362, %int64_10363, %int2_10364 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6487, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %6488 = torch.aten.mul.Tensor %6486, %6484 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6488, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %6489 = torch.aten.mul.Tensor %6487, %6485 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6489, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_10365 = torch.constant.int 1
    %6490 = torch.aten.sub.Tensor %6488, %6489, %int1_10365 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6490, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %6491 = torch.aten.mul.Tensor %6487, %6484 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6491, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %6492 = torch.aten.mul.Tensor %6486, %6485 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6492, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_10366 = torch.constant.int 1
    %6493 = torch.aten.add.Tensor %6491, %6492, %int1_10366 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6493, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %6494 = torch_c.to_builtin_tensor %6490 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_10367 = tensor.cast %6494 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %6495 = torch_c.to_builtin_tensor %6493 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_10368 = tensor.cast %6495 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %6496 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_10367, %cast_10368) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_10369 = tensor.cast %6496 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %6497 = torch_c.from_builtin_tensor %cast_10369 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %6497, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_10370 = torch.constant.int 4
    %int32_10371 = torch.constant.int 32
    %int64_10372 = torch.constant.int 64
    %6498 = torch.prim.ListConstruct %int4_10370, %273, %int32_10371, %int64_10372 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6499 = torch.aten.view %6497, %6498 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6499, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_10373 = torch.constant.none
    %none_10374 = torch.constant.none
    %int5_10375 = torch.constant.int 5
    %cpu_10376 = torch.constant.device "cpu"
    %int0_10377 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6499, %none_10373, %none_10374, %int5_10375, %cpu_10376, %int0_10377 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_10378 = torch.constant.int 0
    %none_10379 = torch.constant.none
    %none_10380 = torch.constant.none
    %cpu_10381 = torch.constant.device "cpu"
    %false_10382 = torch.constant.bool false
    %6500 = torch.aten.arange.start %int0_10378, %273, %none_10379, %none_10380, %cpu_10381, %false_10382 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6500, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_10383 = torch.constant.int 0
    %6501 = torch.aten.unsqueeze %6500, %int0_10383 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %6501, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_10384 = torch.constant.int 0
    %int64_10385 = torch.constant.int 64
    %int2_10386 = torch.constant.int 2
    %none_10387 = torch.constant.none
    %none_10388 = torch.constant.none
    %cpu_10389 = torch.constant.device "cpu"
    %false_10390 = torch.constant.bool false
    %6502 = torch.aten.arange.start_step %int0_10384, %int64_10385, %int2_10386, %none_10387, %none_10388, %cpu_10389, %false_10390 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_10391 = torch.constant.none
    %none_10392 = torch.constant.none
    %int4_10393 = torch.constant.int 4
    %cpu_10394 = torch.constant.device "cpu"
    %int0_10395 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6502, %none_10391, %none_10392, %int4_10393, %cpu_10394, %int0_10395 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10396 = torch.constant.int 6
    %6503 = torch.prims.convert_element_type %6502, %int6_10396 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_10397 = torch.constant.int 64
    %6504 = torch.aten.div.Scalar %6503, %int64_10397 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_10398 = torch.constant.float 1.000000e+04
    %6505 = torch.aten.pow.Scalar %float1.000000e04_10398, %6504 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %6506 = torch.aten.reciprocal %6505 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_10399 = torch.constant.float 1.000000e+00
    %6507 = torch.aten.mul.Scalar %6506, %float1.000000e00_10399 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_10400 = torch.constant.none
    %6508 = torch.aten.clone %242, %none_10400 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_10401 = torch.constant.int 0
    %6509 = torch.aten.unsqueeze %6507, %int0_10401 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_10402 = torch.constant.int 2
    %6510 = torch.aten.unsqueeze %6509, %int2_10402 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_10403 = torch.constant.none
    %none_10404 = torch.constant.none
    %int6_10405 = torch.constant.int 6
    %cpu_10406 = torch.constant.device "cpu"
    %int0_10407 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6510, %none_10403, %none_10404, %int6_10405, %cpu_10406, %int0_10407 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_10408 = torch.constant.int 1
    %int-1_10409 = torch.constant.int -1
    %int1_10410 = torch.constant.int 1
    %6511 = torch.prim.ListConstruct %int1_10408, %int-1_10409, %int1_10410 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_10411 = torch.constant.bool false
    %6512 = torch.aten.expand %6510, %6511, %false_10411 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_10412 = torch.constant.int 1
    %6513 = torch.aten.unsqueeze %6501, %int1_10412 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %6513, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_10413 = torch.constant.none
    %none_10414 = torch.constant.none
    %int4_10415 = torch.constant.int 4
    %cpu_10416 = torch.constant.device "cpu"
    %int0_10417 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6513, %none_10413, %none_10414, %int4_10415, %cpu_10416, %int0_10417 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10418 = torch.constant.int 6
    %6514 = torch.prims.convert_element_type %6513, %int6_10418 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %6514, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %6515 = torch.aten.matmul %6512, %6514 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %6515, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_10419 = torch.constant.int 1
    %int2_10420 = torch.constant.int 2
    %6516 = torch.aten.transpose.int %6515, %int1_10419, %int2_10420 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6516, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %6517 = torch.aten.cos %6516 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6517, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %6518 = torch.aten.mul.Tensor %6517, %6508 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6518, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_10421 = torch.constant.none
    %none_10422 = torch.constant.none
    %int6_10423 = torch.constant.int 6
    %cpu_10424 = torch.constant.device "cpu"
    %int0_10425 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6518, %none_10421, %none_10422, %int6_10423, %cpu_10424, %int0_10425 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10426 = torch.constant.int 5
    %6519 = torch.prims.convert_element_type %6518, %int5_10426 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %6519, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %6520 = torch.aten.sin %6516 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6520, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %6521 = torch.aten.mul.Tensor %6520, %6508 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6521, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_10427 = torch.constant.none
    %none_10428 = torch.constant.none
    %int6_10429 = torch.constant.int 6
    %cpu_10430 = torch.constant.device "cpu"
    %int0_10431 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6521, %none_10427, %none_10428, %int6_10429, %cpu_10430, %int0_10431 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10432 = torch.constant.int 5
    %6522 = torch.prims.convert_element_type %6521, %int5_10432 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %6522, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_10433 = torch.constant.int 2
    %6523 = torch.aten.unsqueeze %6519, %int2_10433 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %6523, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_10434 = torch.constant.int 2
    %6524 = torch.aten.unsqueeze %6522, %int2_10434 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %6524, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_10435 = torch.constant.none
    %none_10436 = torch.constant.none
    %int5_10437 = torch.constant.int 5
    %cpu_10438 = torch.constant.device "cpu"
    %int0_10439 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6523, %none_10435, %none_10436, %int5_10437, %cpu_10438, %int0_10439 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_10440 = torch.constant.none
    %none_10441 = torch.constant.none
    %int5_10442 = torch.constant.int 5
    %cpu_10443 = torch.constant.device "cpu"
    %int0_10444 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6524, %none_10440, %none_10441, %int5_10442, %cpu_10443, %int0_10444 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_10445 = torch.constant.none
    %none_10446 = torch.constant.none
    %int5_10447 = torch.constant.int 5
    %cpu_10448 = torch.constant.device "cpu"
    %int0_10449 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6458, %none_10445, %none_10446, %int5_10447, %cpu_10448, %int0_10449 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_10450 = torch.constant.int 3
    %int0_10451 = torch.constant.int 0
    %int64_10452 = torch.constant.int 64
    %int2_10453 = torch.constant.int 2
    %6525 = torch.aten.slice.Tensor %6458, %int3_10450, %int0_10451, %int64_10452, %int2_10453 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6525, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_10454 = torch.constant.int 3
    %int1_10455 = torch.constant.int 1
    %int64_10456 = torch.constant.int 64
    %int2_10457 = torch.constant.int 2
    %6526 = torch.aten.slice.Tensor %6458, %int3_10454, %int1_10455, %int64_10456, %int2_10457 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6526, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %6527 = torch.aten.mul.Tensor %6525, %6523 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6527, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %6528 = torch.aten.mul.Tensor %6526, %6524 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6528, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_10458 = torch.constant.int 1
    %6529 = torch.aten.sub.Tensor %6527, %6528, %int1_10458 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6529, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %6530 = torch.aten.mul.Tensor %6526, %6523 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6530, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %6531 = torch.aten.mul.Tensor %6525, %6524 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6531, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_10459 = torch.constant.int 1
    %6532 = torch.aten.add.Tensor %6530, %6531, %int1_10459 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6532, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %6533 = torch_c.to_builtin_tensor %6529 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_10460 = tensor.cast %6533 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %6534 = torch_c.to_builtin_tensor %6532 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_10461 = tensor.cast %6534 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %6535 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_10460, %cast_10461) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_10462 = tensor.cast %6535 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %6536 = torch_c.from_builtin_tensor %cast_10462 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %6536, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_10463 = torch.constant.int 4
    %int4_10464 = torch.constant.int 4
    %int64_10465 = torch.constant.int 64
    %6537 = torch.prim.ListConstruct %int4_10463, %273, %int4_10464, %int64_10465 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6538 = torch.aten.view %6536, %6537 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %6538, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_10466 = torch.constant.none
    %none_10467 = torch.constant.none
    %int5_10468 = torch.constant.int 5
    %cpu_10469 = torch.constant.device "cpu"
    %int0_10470 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6538, %none_10466, %none_10467, %int5_10468, %cpu_10469, %int0_10470 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_10471 = torch.constant.int 22
    %6539 = torch.aten.mul.Scalar %arg2, %int22_10471 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6539, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int20 = torch.constant.int 20
    %int1_10472 = torch.constant.int 1
    %6540 = torch.aten.add.Scalar %6539, %int20, %int1_10472 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6540, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_10473 = torch.constant.int 2
    %6541 = torch.aten.mul.Scalar %6540, %int2_10473 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6541, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_10474 = torch.constant.int 0
    %int1_10475 = torch.constant.int 1
    %6542 = torch.aten.add.Scalar %6541, %int0_10474, %int1_10475 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6542, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %6543 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %6544 = torch.aten.view %6542, %6543 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6544, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_10476 = torch.constant.int 4
    %int32_10477 = torch.constant.int 32
    %int4_10478 = torch.constant.int 4
    %int64_10479 = torch.constant.int 64
    %6545 = torch.prim.ListConstruct %int4_10476, %271, %int32_10477, %int4_10478, %int64_10479 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6546 = torch.aten.view %6538, %6545 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %6546, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_10480 = torch.constant.int 32
    %int4_10481 = torch.constant.int 4
    %int64_10482 = torch.constant.int 64
    %6547 = torch.prim.ListConstruct %446, %int32_10480, %int4_10481, %int64_10482 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6548 = torch.aten.view %6546, %6547 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %6548, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_10483 = torch.constant.int 1
    %int2_10484 = torch.constant.int 2
    %6549 = torch.aten.transpose.int %6548, %int1_10483, %int2_10484 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %6549, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_10485 = torch.constant.none
    %none_10486 = torch.constant.none
    %int5_10487 = torch.constant.int 5
    %cpu_10488 = torch.constant.device "cpu"
    %int0_10489 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6549, %none_10485, %none_10486, %int5_10487, %cpu_10488, %int0_10489 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_10490 = torch.constant.int 22
    %int2_10491 = torch.constant.int 2
    %int4_10492 = torch.constant.int 4
    %int32_10493 = torch.constant.int 32
    %int64_10494 = torch.constant.int 64
    %6550 = torch.prim.ListConstruct %272, %int22_10490, %int2_10491, %int4_10492, %int32_10493, %int64_10494 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6551 = torch.aten.view %6275, %6550 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6551, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_10495 = torch.constant.int 4
    %int32_10496 = torch.constant.int 32
    %int64_10497 = torch.constant.int 64
    %6552 = torch.prim.ListConstruct %439, %int4_10495, %int32_10496, %int64_10497 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6553 = torch.aten.view %6551, %6552 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %6553, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %6554 = torch.prim.ListConstruct %6544 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_10498 = torch.constant.bool false
    %6555 = torch.aten.index_put %6553, %6554, %6549, %false_10498 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %6555, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_10499 = torch.constant.int 22
    %int2_10500 = torch.constant.int 2
    %int4_10501 = torch.constant.int 4
    %int32_10502 = torch.constant.int 32
    %int64_10503 = torch.constant.int 64
    %6556 = torch.prim.ListConstruct %272, %int22_10499, %int2_10500, %int4_10501, %int32_10502, %int64_10503 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6557 = torch.aten.view %6555, %6556 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6557, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_10504 = torch.constant.int 360448
    %6558 = torch.prim.ListConstruct %272, %int360448_10504 : (!torch.int, !torch.int) -> !torch.list<int>
    %6559 = torch.aten.view %6557, %6558 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %6559, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_10505 = torch.constant.int 22
    %int2_10506 = torch.constant.int 2
    %int4_10507 = torch.constant.int 4
    %int32_10508 = torch.constant.int 32
    %int64_10509 = torch.constant.int 64
    %6560 = torch.prim.ListConstruct %272, %int22_10505, %int2_10506, %int4_10507, %int32_10508, %int64_10509 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6561 = torch.aten.view %6559, %6560 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6561, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_10510 = torch.constant.int 4
    %int32_10511 = torch.constant.int 32
    %int64_10512 = torch.constant.int 64
    %6562 = torch.prim.ListConstruct %439, %int4_10510, %int32_10511, %int64_10512 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6563 = torch.aten.view %6561, %6562 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %6563, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_10513 = torch.constant.int 22
    %6564 = torch.aten.mul.Scalar %arg2, %int22_10513 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6564, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int20_10514 = torch.constant.int 20
    %int1_10515 = torch.constant.int 1
    %6565 = torch.aten.add.Scalar %6564, %int20_10514, %int1_10515 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6565, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_10516 = torch.constant.int 2
    %6566 = torch.aten.mul.Scalar %6565, %int2_10516 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6566, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_10517 = torch.constant.int 1
    %int1_10518 = torch.constant.int 1
    %6567 = torch.aten.add.Scalar %6566, %int1_10517, %int1_10518 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6567, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %6568 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %6569 = torch.aten.view %6567, %6568 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6569, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_10519 = torch.constant.int 4
    %int32_10520 = torch.constant.int 32
    %int4_10521 = torch.constant.int 4
    %int64_10522 = torch.constant.int 64
    %6570 = torch.prim.ListConstruct %int4_10519, %271, %int32_10520, %int4_10521, %int64_10522 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6571 = torch.aten.view %6460, %6570 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %6571, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_10523 = torch.constant.int 32
    %int4_10524 = torch.constant.int 4
    %int64_10525 = torch.constant.int 64
    %6572 = torch.prim.ListConstruct %446, %int32_10523, %int4_10524, %int64_10525 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6573 = torch.aten.view %6571, %6572 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %6573, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_10526 = torch.constant.int 1
    %int2_10527 = torch.constant.int 2
    %6574 = torch.aten.transpose.int %6573, %int1_10526, %int2_10527 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %6574, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_10528 = torch.constant.none
    %none_10529 = torch.constant.none
    %int5_10530 = torch.constant.int 5
    %cpu_10531 = torch.constant.device "cpu"
    %int0_10532 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6574, %none_10528, %none_10529, %int5_10530, %cpu_10531, %int0_10532 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %6575 = torch.prim.ListConstruct %6569 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_10533 = torch.constant.bool false
    %6576 = torch.aten.index_put %6563, %6575, %6574, %false_10533 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %6576, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_10534 = torch.constant.int 22
    %int2_10535 = torch.constant.int 2
    %int4_10536 = torch.constant.int 4
    %int32_10537 = torch.constant.int 32
    %int64_10538 = torch.constant.int 64
    %6577 = torch.prim.ListConstruct %272, %int22_10534, %int2_10535, %int4_10536, %int32_10537, %int64_10538 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6578 = torch.aten.view %6576, %6577 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6578, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_10539 = torch.constant.int 360448
    %6579 = torch.prim.ListConstruct %272, %int360448_10539 : (!torch.int, !torch.int) -> !torch.list<int>
    %6580 = torch.aten.view %6578, %6579 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %6580, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_10540 = torch.constant.int 0
    %int1_10541 = torch.constant.int 1
    %none_10542 = torch.constant.none
    %none_10543 = torch.constant.none
    %cpu_10544 = torch.constant.device "cpu"
    %false_10545 = torch.constant.bool false
    %6581 = torch.aten.arange.start_step %int0_10540, %273, %int1_10541, %none_10542, %none_10543, %cpu_10544, %false_10545 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6581, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_10546 = torch.constant.int -1
    %6582 = torch.aten.unsqueeze %arg1, %int-1_10546 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %6583 = torch.aten.ge.Tensor %6581, %6582 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %6583, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_10547 = torch.constant.none
    %none_10548 = torch.constant.none
    %cpu_10549 = torch.constant.device "cpu"
    %false_10550 = torch.constant.bool false
    %6584 = torch.aten.arange %273, %none_10547, %none_10548, %cpu_10549, %false_10550 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6584, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_10551 = torch.constant.int 0
    %6585 = torch.aten.unsqueeze %6584, %int0_10551 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %6585, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_10552 = torch.constant.int 1
    %6586 = torch.aten.unsqueeze %6585, %int1_10552 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %6586, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_10553 = torch.constant.int 2
    %6587 = torch.aten.unsqueeze %6586, %int2_10553 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %6587, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_10554 = torch.constant.none
    %none_10555 = torch.constant.none
    %cpu_10556 = torch.constant.device "cpu"
    %false_10557 = torch.constant.bool false
    %6588 = torch.aten.arange %273, %none_10554, %none_10555, %cpu_10556, %false_10557 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6588, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_10558 = torch.constant.int 0
    %6589 = torch.aten.unsqueeze %6588, %int0_10558 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %6589, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_10559 = torch.constant.int 1
    %6590 = torch.aten.unsqueeze %6589, %int1_10559 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %6590, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_10560 = torch.constant.int 3
    %6591 = torch.aten.unsqueeze %6590, %int3_10560 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %6591, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %6592 = torch.aten.gt.Tensor %6587, %6591 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %6592, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_10561 = torch.constant.int 1
    %6593 = torch.aten.unsqueeze %6583, %int1_10561 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %6593, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_10562 = torch.constant.int 2
    %6594 = torch.aten.unsqueeze %6593, %int2_10562 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %6594, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %6595 = torch.aten.logical_or %6592, %6594 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %6595, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_10563 = torch.constant.none
    %6596 = torch.aten.clone %243, %none_10563 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_10564 = torch.constant.int 0
    %6597 = torch.aten.where.ScalarOther %6595, %6596, %int0_10564 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %6597, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_10565 = torch.constant.none
    %none_10566 = torch.constant.none
    %int5_10567 = torch.constant.int 5
    %cpu_10568 = torch.constant.device "cpu"
    %int0_10569 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6597, %none_10565, %none_10566, %int5_10567, %cpu_10568, %int0_10569 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_10570 = torch.constant.int -2
    %6598 = torch.aten.unsqueeze %6538, %int-2_10570 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %6598, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_10571 = torch.constant.int 4
    %int4_10572 = torch.constant.int 4
    %int8_10573 = torch.constant.int 8
    %int64_10574 = torch.constant.int 64
    %6599 = torch.prim.ListConstruct %int4_10571, %273, %int4_10572, %int8_10573, %int64_10574 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_10575 = torch.constant.bool false
    %6600 = torch.aten.expand %6598, %6599, %false_10575 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6600, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_10576 = torch.constant.int 0
    %6601 = torch.aten.clone %6600, %int0_10576 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6601, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_10577 = torch.constant.int 4
    %int32_10578 = torch.constant.int 32
    %int64_10579 = torch.constant.int 64
    %6602 = torch.prim.ListConstruct %int4_10577, %273, %int32_10578, %int64_10579 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6603 = torch.aten._unsafe_view %6601, %6602 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6603, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_10580 = torch.constant.int -2
    %6604 = torch.aten.unsqueeze %6460, %int-2_10580 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %6604, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_10581 = torch.constant.int 4
    %int4_10582 = torch.constant.int 4
    %int8_10583 = torch.constant.int 8
    %int64_10584 = torch.constant.int 64
    %6605 = torch.prim.ListConstruct %int4_10581, %273, %int4_10582, %int8_10583, %int64_10584 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_10585 = torch.constant.bool false
    %6606 = torch.aten.expand %6604, %6605, %false_10585 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6606, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_10586 = torch.constant.int 0
    %6607 = torch.aten.clone %6606, %int0_10586 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6607, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_10587 = torch.constant.int 4
    %int32_10588 = torch.constant.int 32
    %int64_10589 = torch.constant.int 64
    %6608 = torch.prim.ListConstruct %int4_10587, %273, %int32_10588, %int64_10589 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6609 = torch.aten._unsafe_view %6607, %6608 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6609, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_10590 = torch.constant.int 1
    %int2_10591 = torch.constant.int 2
    %6610 = torch.aten.transpose.int %6499, %int1_10590, %int2_10591 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6610, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_10592 = torch.constant.int 1
    %int2_10593 = torch.constant.int 2
    %6611 = torch.aten.transpose.int %6603, %int1_10592, %int2_10593 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6611, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_10594 = torch.constant.int 1
    %int2_10595 = torch.constant.int 2
    %6612 = torch.aten.transpose.int %6609, %int1_10594, %int2_10595 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6612, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_10596 = torch.constant.float 0.000000e+00
    %false_10597 = torch.constant.bool false
    %none_10598 = torch.constant.none
    %false_10599 = torch.constant.bool false
    %6613 = torch.aten.scaled_dot_product_attention %6610, %6611, %6612, %6597, %float0.000000e00_10596, %false_10597, %none_10598, %false_10599 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6613, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_10600 = torch.constant.int 1
    %int2_10601 = torch.constant.int 2
    %6614 = torch.aten.transpose.int %6613, %int1_10600, %int2_10601 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6614, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_10602 = torch.constant.int 4
    %int2048_10603 = torch.constant.int 2048
    %6615 = torch.prim.ListConstruct %int4_10602, %273, %int2048_10603 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6616 = torch.aten.view %6614, %6615 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6616, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_10604 = torch.constant.int 2
    %6617 = torch.aten.view.dtype %248, %int2_10604 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %6618 = torch.aten.detach %6617 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_10605 = torch.constant.int -1
    %int17_10606 = torch.constant.int 17
    %6619 = torch.prim.ListConstruct %int-1_10605, %int17_10606 : (!torch.int, !torch.int) -> !torch.list<int>
    %6620 = torch.aten.view %6618, %6619 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_10607 = torch.constant.int 2048
    %int-1_10608 = torch.constant.int -1
    %int17_10609 = torch.constant.int 17
    %6621 = torch.prim.ListConstruct %int2048_10607, %int-1_10608, %int17_10609 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6622 = torch.aten.view %6620, %6621 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_10610 = torch.constant.int 2
    %int0_10611 = torch.constant.int 0
    %int1_10612 = torch.constant.int 1
    %int1_10613 = torch.constant.int 1
    %6623 = torch.aten.slice.Tensor %6622, %int2_10610, %int0_10611, %int1_10612, %int1_10613 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_10614 = torch.constant.int 5
    %6624 = torch.aten.view.dtype %6623, %int5_10614 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %6625 = torch.aten.detach %6624 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_10615 = torch.constant.int 2
    %int1_10616 = torch.constant.int 1
    %int9223372036854775807_10617 = torch.constant.int 9223372036854775807
    %int1_10618 = torch.constant.int 1
    %6626 = torch.aten.slice.Tensor %6622, %int2_10615, %int1_10616, %int9223372036854775807_10617, %int1_10618 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_10619 = torch.constant.int 1
    %6627 = torch.aten.view.dtype %6626, %int1_10619 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %6628 = torch.aten.detach %6627 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %6629 = torch_c.to_builtin_tensor %6616 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_10620 = tensor.cast %6629 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6630 = torch_c.to_builtin_tensor %6625 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %6631 = torch_c.to_builtin_tensor %6628 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %6632 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_10620, %6630, %6631) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_10621 = tensor.cast %6632 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %6633 = torch_c.from_builtin_tensor %cast_10621 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6633, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_10622 = torch.constant.none
    %none_10623 = torch.constant.none
    %int5_10624 = torch.constant.int 5
    %cpu_10625 = torch.constant.device "cpu"
    %int0_10626 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6633, %none_10622, %none_10623, %int5_10624, %cpu_10625, %int0_10626 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_10627 = torch.constant.int 1
    %6634 = torch.aten.add.Tensor %6393, %6633, %int1_10627 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6634, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_10628 = torch.constant.none
    %none_10629 = torch.constant.none
    %int5_10630 = torch.constant.int 5
    %cpu_10631 = torch.constant.device "cpu"
    %int0_10632 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6634, %none_10628, %none_10629, %int5_10630, %cpu_10631, %int0_10632 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10633 = torch.constant.int 6
    %6635 = torch.prims.convert_element_type %6634, %int6_10633 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6635, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_10634 = torch.constant.int 2
    %6636 = torch.aten.pow.Tensor_Scalar %6635, %int2_10634 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6636, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_10635 = torch.constant.int -1
    %6637 = torch.prim.ListConstruct %int-1_10635 : (!torch.int) -> !torch.list<int>
    %true_10636 = torch.constant.bool true
    %none_10637 = torch.constant.none
    %6638 = torch.aten.mean.dim %6636, %6637, %true_10636, %none_10637 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6638, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_10638 = torch.constant.float 9.9999997473787516E-6
    %int1_10639 = torch.constant.int 1
    %6639 = torch.aten.add.Scalar %6638, %float9.999990e-06_10638, %int1_10639 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6639, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6640 = torch.aten.rsqrt %6639 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6640, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6641 = torch.aten.mul.Tensor %6635, %6640 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6641, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_10640 = torch.constant.none
    %none_10641 = torch.constant.none
    %int6_10642 = torch.constant.int 6
    %cpu_10643 = torch.constant.device "cpu"
    %int0_10644 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6641, %none_10640, %none_10641, %int6_10642, %cpu_10643, %int0_10644 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10645 = torch.constant.int 5
    %6642 = torch.prims.convert_element_type %6641, %int5_10645 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6642, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %6643 = torch.aten.mul.Tensor %249, %6642 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6643, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_10646 = torch.constant.none
    %none_10647 = torch.constant.none
    %int6_10648 = torch.constant.int 6
    %cpu_10649 = torch.constant.device "cpu"
    %int0_10650 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6643, %none_10646, %none_10647, %int6_10648, %cpu_10649, %int0_10650 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10651 = torch.constant.int 5
    %6644 = torch.prims.convert_element_type %6643, %int5_10651 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6644, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_10652 = torch.constant.int 2
    %6645 = torch.aten.view.dtype %250, %int2_10652 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %6646 = torch.aten.detach %6645 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_10653 = torch.constant.int -1
    %int17_10654 = torch.constant.int 17
    %6647 = torch.prim.ListConstruct %int-1_10653, %int17_10654 : (!torch.int, !torch.int) -> !torch.list<int>
    %6648 = torch.aten.view %6646, %6647 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_10655 = torch.constant.int 5632
    %int-1_10656 = torch.constant.int -1
    %int17_10657 = torch.constant.int 17
    %6649 = torch.prim.ListConstruct %int5632_10655, %int-1_10656, %int17_10657 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6650 = torch.aten.view %6648, %6649 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_10658 = torch.constant.int 2
    %int0_10659 = torch.constant.int 0
    %int1_10660 = torch.constant.int 1
    %int1_10661 = torch.constant.int 1
    %6651 = torch.aten.slice.Tensor %6650, %int2_10658, %int0_10659, %int1_10660, %int1_10661 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_10662 = torch.constant.int 5
    %6652 = torch.aten.view.dtype %6651, %int5_10662 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %6653 = torch.aten.detach %6652 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_10663 = torch.constant.int 2
    %int1_10664 = torch.constant.int 1
    %int9223372036854775807_10665 = torch.constant.int 9223372036854775807
    %int1_10666 = torch.constant.int 1
    %6654 = torch.aten.slice.Tensor %6650, %int2_10663, %int1_10664, %int9223372036854775807_10665, %int1_10666 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_10667 = torch.constant.int 1
    %6655 = torch.aten.view.dtype %6654, %int1_10667 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %6656 = torch.aten.detach %6655 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %6657 = torch_c.to_builtin_tensor %6644 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_10668 = tensor.cast %6657 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6658 = torch_c.to_builtin_tensor %6653 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %6659 = torch_c.to_builtin_tensor %6656 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %6660 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_10668, %6658, %6659) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_10669 = tensor.cast %6660 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %6661 = torch_c.from_builtin_tensor %cast_10669 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %6661, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %6662 = torch.aten.silu %6661 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %6662, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_10670 = torch.constant.int 2
    %6663 = torch.aten.view.dtype %251, %int2_10670 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %6664 = torch.aten.detach %6663 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_10671 = torch.constant.int -1
    %int17_10672 = torch.constant.int 17
    %6665 = torch.prim.ListConstruct %int-1_10671, %int17_10672 : (!torch.int, !torch.int) -> !torch.list<int>
    %6666 = torch.aten.view %6664, %6665 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_10673 = torch.constant.int 5632
    %int-1_10674 = torch.constant.int -1
    %int17_10675 = torch.constant.int 17
    %6667 = torch.prim.ListConstruct %int5632_10673, %int-1_10674, %int17_10675 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6668 = torch.aten.view %6666, %6667 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_10676 = torch.constant.int 2
    %int0_10677 = torch.constant.int 0
    %int1_10678 = torch.constant.int 1
    %int1_10679 = torch.constant.int 1
    %6669 = torch.aten.slice.Tensor %6668, %int2_10676, %int0_10677, %int1_10678, %int1_10679 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_10680 = torch.constant.int 5
    %6670 = torch.aten.view.dtype %6669, %int5_10680 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %6671 = torch.aten.detach %6670 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_10681 = torch.constant.int 2
    %int1_10682 = torch.constant.int 1
    %int9223372036854775807_10683 = torch.constant.int 9223372036854775807
    %int1_10684 = torch.constant.int 1
    %6672 = torch.aten.slice.Tensor %6668, %int2_10681, %int1_10682, %int9223372036854775807_10683, %int1_10684 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_10685 = torch.constant.int 1
    %6673 = torch.aten.view.dtype %6672, %int1_10685 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %6674 = torch.aten.detach %6673 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %6675 = torch_c.to_builtin_tensor %6644 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_10686 = tensor.cast %6675 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6676 = torch_c.to_builtin_tensor %6671 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %6677 = torch_c.to_builtin_tensor %6674 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %6678 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_10686, %6676, %6677) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_10687 = tensor.cast %6678 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %6679 = torch_c.from_builtin_tensor %cast_10687 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %6679, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %6680 = torch.aten.mul.Tensor %6662, %6679 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %6680, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_10688 = torch.constant.int 2
    %6681 = torch.aten.view.dtype %252, %int2_10688 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %6682 = torch.aten.detach %6681 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_10689 = torch.constant.int -1
    %int17_10690 = torch.constant.int 17
    %6683 = torch.prim.ListConstruct %int-1_10689, %int17_10690 : (!torch.int, !torch.int) -> !torch.list<int>
    %6684 = torch.aten.view %6682, %6683 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_10691 = torch.constant.int 2048
    %int-1_10692 = torch.constant.int -1
    %int17_10693 = torch.constant.int 17
    %6685 = torch.prim.ListConstruct %int2048_10691, %int-1_10692, %int17_10693 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6686 = torch.aten.view %6684, %6685 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_10694 = torch.constant.int 2
    %int0_10695 = torch.constant.int 0
    %int1_10696 = torch.constant.int 1
    %int1_10697 = torch.constant.int 1
    %6687 = torch.aten.slice.Tensor %6686, %int2_10694, %int0_10695, %int1_10696, %int1_10697 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_10698 = torch.constant.int 5
    %6688 = torch.aten.view.dtype %6687, %int5_10698 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %6689 = torch.aten.detach %6688 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_10699 = torch.constant.int 2
    %int1_10700 = torch.constant.int 1
    %int9223372036854775807_10701 = torch.constant.int 9223372036854775807
    %int1_10702 = torch.constant.int 1
    %6690 = torch.aten.slice.Tensor %6686, %int2_10699, %int1_10700, %int9223372036854775807_10701, %int1_10702 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_10703 = torch.constant.int 1
    %6691 = torch.aten.view.dtype %6690, %int1_10703 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %6692 = torch.aten.detach %6691 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %6693 = torch_c.to_builtin_tensor %6680 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_10704 = tensor.cast %6693 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %6694 = torch_c.to_builtin_tensor %6689 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %6695 = torch_c.to_builtin_tensor %6692 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %6696 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_10704, %6694, %6695) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_10705 = tensor.cast %6696 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %6697 = torch_c.from_builtin_tensor %cast_10705 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6697, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_10706 = torch.constant.int 1
    %6698 = torch.aten.add.Tensor %6634, %6697, %int1_10706 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6698, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_10707 = torch.constant.none
    %none_10708 = torch.constant.none
    %int5_10709 = torch.constant.int 5
    %cpu_10710 = torch.constant.device "cpu"
    %int0_10711 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6698, %none_10707, %none_10708, %int5_10709, %cpu_10710, %int0_10711 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10712 = torch.constant.int 6
    %6699 = torch.prims.convert_element_type %6698, %int6_10712 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6699, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_10713 = torch.constant.int 2
    %6700 = torch.aten.pow.Tensor_Scalar %6699, %int2_10713 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6700, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_10714 = torch.constant.int -1
    %6701 = torch.prim.ListConstruct %int-1_10714 : (!torch.int) -> !torch.list<int>
    %true_10715 = torch.constant.bool true
    %none_10716 = torch.constant.none
    %6702 = torch.aten.mean.dim %6700, %6701, %true_10715, %none_10716 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6702, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_10717 = torch.constant.float 9.9999997473787516E-6
    %int1_10718 = torch.constant.int 1
    %6703 = torch.aten.add.Scalar %6702, %float9.999990e-06_10717, %int1_10718 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6703, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6704 = torch.aten.rsqrt %6703 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6704, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6705 = torch.aten.mul.Tensor %6699, %6704 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6705, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_10719 = torch.constant.none
    %none_10720 = torch.constant.none
    %int6_10721 = torch.constant.int 6
    %cpu_10722 = torch.constant.device "cpu"
    %int0_10723 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6705, %none_10719, %none_10720, %int6_10721, %cpu_10722, %int0_10723 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10724 = torch.constant.int 5
    %6706 = torch.prims.convert_element_type %6705, %int5_10724 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6706, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %6707 = torch.aten.mul.Tensor %256, %6706 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6707, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_10725 = torch.constant.none
    %none_10726 = torch.constant.none
    %int6_10727 = torch.constant.int 6
    %cpu_10728 = torch.constant.device "cpu"
    %int0_10729 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6707, %none_10725, %none_10726, %int6_10727, %cpu_10728, %int0_10729 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10730 = torch.constant.int 5
    %6708 = torch.prims.convert_element_type %6707, %int5_10730 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6708, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_10731 = torch.constant.int 2
    %6709 = torch.aten.view.dtype %257, %int2_10731 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %6710 = torch.aten.detach %6709 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_10732 = torch.constant.int -1
    %int17_10733 = torch.constant.int 17
    %6711 = torch.prim.ListConstruct %int-1_10732, %int17_10733 : (!torch.int, !torch.int) -> !torch.list<int>
    %6712 = torch.aten.view %6710, %6711 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_10734 = torch.constant.int 2048
    %int-1_10735 = torch.constant.int -1
    %int17_10736 = torch.constant.int 17
    %6713 = torch.prim.ListConstruct %int2048_10734, %int-1_10735, %int17_10736 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6714 = torch.aten.view %6712, %6713 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_10737 = torch.constant.int 2
    %int0_10738 = torch.constant.int 0
    %int1_10739 = torch.constant.int 1
    %int1_10740 = torch.constant.int 1
    %6715 = torch.aten.slice.Tensor %6714, %int2_10737, %int0_10738, %int1_10739, %int1_10740 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_10741 = torch.constant.int 5
    %6716 = torch.aten.view.dtype %6715, %int5_10741 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %6717 = torch.aten.detach %6716 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_10742 = torch.constant.int 2
    %int1_10743 = torch.constant.int 1
    %int9223372036854775807_10744 = torch.constant.int 9223372036854775807
    %int1_10745 = torch.constant.int 1
    %6718 = torch.aten.slice.Tensor %6714, %int2_10742, %int1_10743, %int9223372036854775807_10744, %int1_10745 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_10746 = torch.constant.int 1
    %6719 = torch.aten.view.dtype %6718, %int1_10746 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %6720 = torch.aten.detach %6719 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %6721 = torch_c.to_builtin_tensor %6708 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_10747 = tensor.cast %6721 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6722 = torch_c.to_builtin_tensor %6717 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %6723 = torch_c.to_builtin_tensor %6720 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %6724 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_10747, %6722, %6723) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_10748 = tensor.cast %6724 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %6725 = torch_c.from_builtin_tensor %cast_10748 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6725, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_10749 = torch.constant.int 2
    %6726 = torch.aten.view.dtype %258, %int2_10749 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %6727 = torch.aten.detach %6726 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_10750 = torch.constant.int -1
    %int17_10751 = torch.constant.int 17
    %6728 = torch.prim.ListConstruct %int-1_10750, %int17_10751 : (!torch.int, !torch.int) -> !torch.list<int>
    %6729 = torch.aten.view %6727, %6728 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_10752 = torch.constant.int 256
    %int-1_10753 = torch.constant.int -1
    %int17_10754 = torch.constant.int 17
    %6730 = torch.prim.ListConstruct %int256_10752, %int-1_10753, %int17_10754 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6731 = torch.aten.view %6729, %6730 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_10755 = torch.constant.int 2
    %int0_10756 = torch.constant.int 0
    %int1_10757 = torch.constant.int 1
    %int1_10758 = torch.constant.int 1
    %6732 = torch.aten.slice.Tensor %6731, %int2_10755, %int0_10756, %int1_10757, %int1_10758 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_10759 = torch.constant.int 5
    %6733 = torch.aten.view.dtype %6732, %int5_10759 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %6734 = torch.aten.detach %6733 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_10760 = torch.constant.int 2
    %int1_10761 = torch.constant.int 1
    %int9223372036854775807_10762 = torch.constant.int 9223372036854775807
    %int1_10763 = torch.constant.int 1
    %6735 = torch.aten.slice.Tensor %6731, %int2_10760, %int1_10761, %int9223372036854775807_10762, %int1_10763 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_10764 = torch.constant.int 1
    %6736 = torch.aten.view.dtype %6735, %int1_10764 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %6737 = torch.aten.detach %6736 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %6738 = torch_c.to_builtin_tensor %6708 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_10765 = tensor.cast %6738 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6739 = torch_c.to_builtin_tensor %6734 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %6740 = torch_c.to_builtin_tensor %6737 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %6741 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_10765, %6739, %6740) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_10766 = tensor.cast %6741 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %6742 = torch_c.from_builtin_tensor %cast_10766 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %6742, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int2_10767 = torch.constant.int 2
    %6743 = torch.aten.view.dtype %259, %int2_10767 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %6744 = torch.aten.detach %6743 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_10768 = torch.constant.int -1
    %int17_10769 = torch.constant.int 17
    %6745 = torch.prim.ListConstruct %int-1_10768, %int17_10769 : (!torch.int, !torch.int) -> !torch.list<int>
    %6746 = torch.aten.view %6744, %6745 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_10770 = torch.constant.int 256
    %int-1_10771 = torch.constant.int -1
    %int17_10772 = torch.constant.int 17
    %6747 = torch.prim.ListConstruct %int256_10770, %int-1_10771, %int17_10772 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6748 = torch.aten.view %6746, %6747 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_10773 = torch.constant.int 2
    %int0_10774 = torch.constant.int 0
    %int1_10775 = torch.constant.int 1
    %int1_10776 = torch.constant.int 1
    %6749 = torch.aten.slice.Tensor %6748, %int2_10773, %int0_10774, %int1_10775, %int1_10776 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_10777 = torch.constant.int 5
    %6750 = torch.aten.view.dtype %6749, %int5_10777 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %6751 = torch.aten.detach %6750 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_10778 = torch.constant.int 2
    %int1_10779 = torch.constant.int 1
    %int9223372036854775807_10780 = torch.constant.int 9223372036854775807
    %int1_10781 = torch.constant.int 1
    %6752 = torch.aten.slice.Tensor %6748, %int2_10778, %int1_10779, %int9223372036854775807_10780, %int1_10781 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_10782 = torch.constant.int 1
    %6753 = torch.aten.view.dtype %6752, %int1_10782 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %6754 = torch.aten.detach %6753 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %6755 = torch_c.to_builtin_tensor %6708 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_10783 = tensor.cast %6755 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6756 = torch_c.to_builtin_tensor %6751 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %6757 = torch_c.to_builtin_tensor %6754 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %6758 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_10783, %6756, %6757) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_10784 = tensor.cast %6758 : tensor<?x?x256xf16> to tensor<4x?x256xf16>
    %6759 = torch_c.from_builtin_tensor %cast_10784 : tensor<4x?x256xf16> -> !torch.vtensor<[4,?,256],f16>
    torch.bind_symbolic_shape %6759, [%269], affine_map<()[s0] -> (4, s0 * 32, 256)> : !torch.vtensor<[4,?,256],f16>
    %int4_10785 = torch.constant.int 4
    %int32_10786 = torch.constant.int 32
    %int64_10787 = torch.constant.int 64
    %6760 = torch.prim.ListConstruct %int4_10785, %273, %int32_10786, %int64_10787 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6761 = torch.aten.view %6725, %6760 : !torch.vtensor<[4,?,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6761, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_10788 = torch.constant.int 4
    %int4_10789 = torch.constant.int 4
    %int64_10790 = torch.constant.int 64
    %6762 = torch.prim.ListConstruct %int4_10788, %273, %int4_10789, %int64_10790 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6763 = torch.aten.view %6742, %6762 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %6763, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int4_10791 = torch.constant.int 4
    %int4_10792 = torch.constant.int 4
    %int64_10793 = torch.constant.int 64
    %6764 = torch.prim.ListConstruct %int4_10791, %273, %int4_10792, %int64_10793 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6765 = torch.aten.view %6759, %6764 : !torch.vtensor<[4,?,256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %6765, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_10794 = torch.constant.int 0
    %none_10795 = torch.constant.none
    %none_10796 = torch.constant.none
    %cpu_10797 = torch.constant.device "cpu"
    %false_10798 = torch.constant.bool false
    %6766 = torch.aten.arange.start %int0_10794, %273, %none_10795, %none_10796, %cpu_10797, %false_10798 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6766, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_10799 = torch.constant.int 0
    %6767 = torch.aten.unsqueeze %6766, %int0_10799 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %6767, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_10800 = torch.constant.int 0
    %int64_10801 = torch.constant.int 64
    %int2_10802 = torch.constant.int 2
    %none_10803 = torch.constant.none
    %none_10804 = torch.constant.none
    %cpu_10805 = torch.constant.device "cpu"
    %false_10806 = torch.constant.bool false
    %6768 = torch.aten.arange.start_step %int0_10800, %int64_10801, %int2_10802, %none_10803, %none_10804, %cpu_10805, %false_10806 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_10807 = torch.constant.none
    %none_10808 = torch.constant.none
    %int4_10809 = torch.constant.int 4
    %cpu_10810 = torch.constant.device "cpu"
    %int0_10811 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6768, %none_10807, %none_10808, %int4_10809, %cpu_10810, %int0_10811 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10812 = torch.constant.int 6
    %6769 = torch.prims.convert_element_type %6768, %int6_10812 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_10813 = torch.constant.int 64
    %6770 = torch.aten.div.Scalar %6769, %int64_10813 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_10814 = torch.constant.float 1.000000e+04
    %6771 = torch.aten.pow.Scalar %float1.000000e04_10814, %6770 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %6772 = torch.aten.reciprocal %6771 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_10815 = torch.constant.float 1.000000e+00
    %6773 = torch.aten.mul.Scalar %6772, %float1.000000e00_10815 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_10816 = torch.constant.none
    %6774 = torch.aten.clone %253, %none_10816 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_10817 = torch.constant.int 0
    %6775 = torch.aten.unsqueeze %6773, %int0_10817 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_10818 = torch.constant.int 2
    %6776 = torch.aten.unsqueeze %6775, %int2_10818 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_10819 = torch.constant.none
    %none_10820 = torch.constant.none
    %int6_10821 = torch.constant.int 6
    %cpu_10822 = torch.constant.device "cpu"
    %int0_10823 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6776, %none_10819, %none_10820, %int6_10821, %cpu_10822, %int0_10823 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_10824 = torch.constant.int 1
    %int-1_10825 = torch.constant.int -1
    %int1_10826 = torch.constant.int 1
    %6777 = torch.prim.ListConstruct %int1_10824, %int-1_10825, %int1_10826 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_10827 = torch.constant.bool false
    %6778 = torch.aten.expand %6776, %6777, %false_10827 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_10828 = torch.constant.int 1
    %6779 = torch.aten.unsqueeze %6767, %int1_10828 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %6779, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_10829 = torch.constant.none
    %none_10830 = torch.constant.none
    %int4_10831 = torch.constant.int 4
    %cpu_10832 = torch.constant.device "cpu"
    %int0_10833 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6779, %none_10829, %none_10830, %int4_10831, %cpu_10832, %int0_10833 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10834 = torch.constant.int 6
    %6780 = torch.prims.convert_element_type %6779, %int6_10834 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %6780, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %6781 = torch.aten.matmul %6778, %6780 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %6781, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_10835 = torch.constant.int 1
    %int2_10836 = torch.constant.int 2
    %6782 = torch.aten.transpose.int %6781, %int1_10835, %int2_10836 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6782, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %6783 = torch.aten.cos %6782 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6783, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %6784 = torch.aten.mul.Tensor %6783, %6774 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6784, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_10837 = torch.constant.none
    %none_10838 = torch.constant.none
    %int6_10839 = torch.constant.int 6
    %cpu_10840 = torch.constant.device "cpu"
    %int0_10841 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6784, %none_10837, %none_10838, %int6_10839, %cpu_10840, %int0_10841 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10842 = torch.constant.int 5
    %6785 = torch.prims.convert_element_type %6784, %int5_10842 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %6785, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %6786 = torch.aten.sin %6782 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6786, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %6787 = torch.aten.mul.Tensor %6786, %6774 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6787, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_10843 = torch.constant.none
    %none_10844 = torch.constant.none
    %int6_10845 = torch.constant.int 6
    %cpu_10846 = torch.constant.device "cpu"
    %int0_10847 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6787, %none_10843, %none_10844, %int6_10845, %cpu_10846, %int0_10847 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10848 = torch.constant.int 5
    %6788 = torch.prims.convert_element_type %6787, %int5_10848 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %6788, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_10849 = torch.constant.int 2
    %6789 = torch.aten.unsqueeze %6785, %int2_10849 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %6789, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_10850 = torch.constant.int 2
    %6790 = torch.aten.unsqueeze %6788, %int2_10850 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %6790, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_10851 = torch.constant.none
    %none_10852 = torch.constant.none
    %int5_10853 = torch.constant.int 5
    %cpu_10854 = torch.constant.device "cpu"
    %int0_10855 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6789, %none_10851, %none_10852, %int5_10853, %cpu_10854, %int0_10855 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_10856 = torch.constant.none
    %none_10857 = torch.constant.none
    %int5_10858 = torch.constant.int 5
    %cpu_10859 = torch.constant.device "cpu"
    %int0_10860 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6790, %none_10856, %none_10857, %int5_10858, %cpu_10859, %int0_10860 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_10861 = torch.constant.none
    %none_10862 = torch.constant.none
    %int5_10863 = torch.constant.int 5
    %cpu_10864 = torch.constant.device "cpu"
    %int0_10865 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6761, %none_10861, %none_10862, %int5_10863, %cpu_10864, %int0_10865 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_10866 = torch.constant.int 3
    %int0_10867 = torch.constant.int 0
    %int64_10868 = torch.constant.int 64
    %int2_10869 = torch.constant.int 2
    %6791 = torch.aten.slice.Tensor %6761, %int3_10866, %int0_10867, %int64_10868, %int2_10869 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6791, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int3_10870 = torch.constant.int 3
    %int1_10871 = torch.constant.int 1
    %int64_10872 = torch.constant.int 64
    %int2_10873 = torch.constant.int 2
    %6792 = torch.aten.slice.Tensor %6761, %int3_10870, %int1_10871, %int64_10872, %int2_10873 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6792, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %6793 = torch.aten.mul.Tensor %6791, %6789 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6793, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %6794 = torch.aten.mul.Tensor %6792, %6790 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6794, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_10874 = torch.constant.int 1
    %6795 = torch.aten.sub.Tensor %6793, %6794, %int1_10874 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6795, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %6796 = torch.aten.mul.Tensor %6792, %6789 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6796, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %6797 = torch.aten.mul.Tensor %6791, %6790 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6797, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %int1_10875 = torch.constant.int 1
    %6798 = torch.aten.add.Tensor %6796, %6797, %int1_10875 : !torch.vtensor<[4,?,32,32],f16>, !torch.vtensor<[4,?,32,32],f16>, !torch.int -> !torch.vtensor<[4,?,32,32],f16>
    torch.bind_symbolic_shape %6798, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 32)> : !torch.vtensor<[4,?,32,32],f16>
    %6799 = torch_c.to_builtin_tensor %6795 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_10876 = tensor.cast %6799 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %6800 = torch_c.to_builtin_tensor %6798 : !torch.vtensor<[4,?,32,32],f16> -> tensor<4x?x32x32xf16>
    %cast_10877 = tensor.cast %6800 : tensor<4x?x32x32xf16> to tensor<?x?x?x?xf16>
    %6801 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_10876, %cast_10877) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_10878 = tensor.cast %6801 : tensor<?x?x?x2x?xf16> to tensor<4x?x32x2x32xf16>
    %6802 = torch_c.from_builtin_tensor %cast_10878 : tensor<4x?x32x2x32xf16> -> !torch.vtensor<[4,?,32,2,32],f16>
    torch.bind_symbolic_shape %6802, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 2, 32)> : !torch.vtensor<[4,?,32,2,32],f16>
    %int4_10879 = torch.constant.int 4
    %int32_10880 = torch.constant.int 32
    %int64_10881 = torch.constant.int 64
    %6803 = torch.prim.ListConstruct %int4_10879, %273, %int32_10880, %int64_10881 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6804 = torch.aten.view %6802, %6803 : !torch.vtensor<[4,?,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6804, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %none_10882 = torch.constant.none
    %none_10883 = torch.constant.none
    %int5_10884 = torch.constant.int 5
    %cpu_10885 = torch.constant.device "cpu"
    %int0_10886 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6804, %none_10882, %none_10883, %int5_10884, %cpu_10885, %int0_10886 : !torch.vtensor<[4,?,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_10887 = torch.constant.int 0
    %none_10888 = torch.constant.none
    %none_10889 = torch.constant.none
    %cpu_10890 = torch.constant.device "cpu"
    %false_10891 = torch.constant.bool false
    %6805 = torch.aten.arange.start %int0_10887, %273, %none_10888, %none_10889, %cpu_10890, %false_10891 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6805, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_10892 = torch.constant.int 0
    %6806 = torch.aten.unsqueeze %6805, %int0_10892 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %6806, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int0_10893 = torch.constant.int 0
    %int64_10894 = torch.constant.int 64
    %int2_10895 = torch.constant.int 2
    %none_10896 = torch.constant.none
    %none_10897 = torch.constant.none
    %cpu_10898 = torch.constant.device "cpu"
    %false_10899 = torch.constant.bool false
    %6807 = torch.aten.arange.start_step %int0_10893, %int64_10894, %int2_10895, %none_10896, %none_10897, %cpu_10898, %false_10899 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_10900 = torch.constant.none
    %none_10901 = torch.constant.none
    %int4_10902 = torch.constant.int 4
    %cpu_10903 = torch.constant.device "cpu"
    %int0_10904 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6807, %none_10900, %none_10901, %int4_10902, %cpu_10903, %int0_10904 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10905 = torch.constant.int 6
    %6808 = torch.prims.convert_element_type %6807, %int6_10905 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_10906 = torch.constant.int 64
    %6809 = torch.aten.div.Scalar %6808, %int64_10906 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_10907 = torch.constant.float 1.000000e+04
    %6810 = torch.aten.pow.Scalar %float1.000000e04_10907, %6809 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %6811 = torch.aten.reciprocal %6810 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_10908 = torch.constant.float 1.000000e+00
    %6812 = torch.aten.mul.Scalar %6811, %float1.000000e00_10908 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_10909 = torch.constant.none
    %6813 = torch.aten.clone %254, %none_10909 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_10910 = torch.constant.int 0
    %6814 = torch.aten.unsqueeze %6812, %int0_10910 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_10911 = torch.constant.int 2
    %6815 = torch.aten.unsqueeze %6814, %int2_10911 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_10912 = torch.constant.none
    %none_10913 = torch.constant.none
    %int6_10914 = torch.constant.int 6
    %cpu_10915 = torch.constant.device "cpu"
    %int0_10916 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6815, %none_10912, %none_10913, %int6_10914, %cpu_10915, %int0_10916 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_10917 = torch.constant.int 1
    %int-1_10918 = torch.constant.int -1
    %int1_10919 = torch.constant.int 1
    %6816 = torch.prim.ListConstruct %int1_10917, %int-1_10918, %int1_10919 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_10920 = torch.constant.bool false
    %6817 = torch.aten.expand %6815, %6816, %false_10920 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,32,1],f32>
    %int1_10921 = torch.constant.int 1
    %6818 = torch.aten.unsqueeze %6806, %int1_10921 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %6818, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %none_10922 = torch.constant.none
    %none_10923 = torch.constant.none
    %int4_10924 = torch.constant.int 4
    %cpu_10925 = torch.constant.device "cpu"
    %int0_10926 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6818, %none_10922, %none_10923, %int4_10924, %cpu_10925, %int0_10926 : !torch.vtensor<[1,1,?],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10927 = torch.constant.int 6
    %6819 = torch.prims.convert_element_type %6818, %int6_10927 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],f32>
    torch.bind_symbolic_shape %6819, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],f32>
    %6820 = torch.aten.matmul %6817, %6819 : !torch.vtensor<[1,32,1],f32>, !torch.vtensor<[1,1,?],f32> -> !torch.vtensor<[1,32,?],f32>
    torch.bind_symbolic_shape %6820, [%269], affine_map<()[s0] -> (1, 32, s0 * 32)> : !torch.vtensor<[1,32,?],f32>
    %int1_10928 = torch.constant.int 1
    %int2_10929 = torch.constant.int 2
    %6821 = torch.aten.transpose.int %6820, %int1_10928, %int2_10929 : !torch.vtensor<[1,32,?],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6821, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %6822 = torch.aten.cos %6821 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6822, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %6823 = torch.aten.mul.Tensor %6822, %6813 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6823, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_10930 = torch.constant.none
    %none_10931 = torch.constant.none
    %int6_10932 = torch.constant.int 6
    %cpu_10933 = torch.constant.device "cpu"
    %int0_10934 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6823, %none_10930, %none_10931, %int6_10932, %cpu_10933, %int0_10934 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10935 = torch.constant.int 5
    %6824 = torch.prims.convert_element_type %6823, %int5_10935 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %6824, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %6825 = torch.aten.sin %6821 : !torch.vtensor<[1,?,32],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6825, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %6826 = torch.aten.mul.Tensor %6825, %6813 : !torch.vtensor<[1,?,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,?,32],f32>
    torch.bind_symbolic_shape %6826, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f32>
    %none_10936 = torch.constant.none
    %none_10937 = torch.constant.none
    %int6_10938 = torch.constant.int 6
    %cpu_10939 = torch.constant.device "cpu"
    %int0_10940 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6826, %none_10936, %none_10937, %int6_10938, %cpu_10939, %int0_10940 : !torch.vtensor<[1,?,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10941 = torch.constant.int 5
    %6827 = torch.prims.convert_element_type %6826, %int5_10941 : !torch.vtensor<[1,?,32],f32>, !torch.int -> !torch.vtensor<[1,?,32],f16>
    torch.bind_symbolic_shape %6827, [%269], affine_map<()[s0] -> (1, s0 * 32, 32)> : !torch.vtensor<[1,?,32],f16>
    %int2_10942 = torch.constant.int 2
    %6828 = torch.aten.unsqueeze %6824, %int2_10942 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %6828, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %int2_10943 = torch.constant.int 2
    %6829 = torch.aten.unsqueeze %6827, %int2_10943 : !torch.vtensor<[1,?,32],f16>, !torch.int -> !torch.vtensor<[1,?,1,32],f16>
    torch.bind_symbolic_shape %6829, [%269], affine_map<()[s0] -> (1, s0 * 32, 1, 32)> : !torch.vtensor<[1,?,1,32],f16>
    %none_10944 = torch.constant.none
    %none_10945 = torch.constant.none
    %int5_10946 = torch.constant.int 5
    %cpu_10947 = torch.constant.device "cpu"
    %int0_10948 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6828, %none_10944, %none_10945, %int5_10946, %cpu_10947, %int0_10948 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_10949 = torch.constant.none
    %none_10950 = torch.constant.none
    %int5_10951 = torch.constant.int 5
    %cpu_10952 = torch.constant.device "cpu"
    %int0_10953 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6829, %none_10949, %none_10950, %int5_10951, %cpu_10952, %int0_10953 : !torch.vtensor<[1,?,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_10954 = torch.constant.none
    %none_10955 = torch.constant.none
    %int5_10956 = torch.constant.int 5
    %cpu_10957 = torch.constant.device "cpu"
    %int0_10958 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6763, %none_10954, %none_10955, %int5_10956, %cpu_10957, %int0_10958 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_10959 = torch.constant.int 3
    %int0_10960 = torch.constant.int 0
    %int64_10961 = torch.constant.int 64
    %int2_10962 = torch.constant.int 2
    %6830 = torch.aten.slice.Tensor %6763, %int3_10959, %int0_10960, %int64_10961, %int2_10962 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6830, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int3_10963 = torch.constant.int 3
    %int1_10964 = torch.constant.int 1
    %int64_10965 = torch.constant.int 64
    %int2_10966 = torch.constant.int 2
    %6831 = torch.aten.slice.Tensor %6763, %int3_10963, %int1_10964, %int64_10965, %int2_10966 : !torch.vtensor<[4,?,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6831, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %6832 = torch.aten.mul.Tensor %6830, %6828 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6832, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %6833 = torch.aten.mul.Tensor %6831, %6829 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6833, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_10967 = torch.constant.int 1
    %6834 = torch.aten.sub.Tensor %6832, %6833, %int1_10967 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6834, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %6835 = torch.aten.mul.Tensor %6831, %6828 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6835, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %6836 = torch.aten.mul.Tensor %6830, %6829 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[1,?,1,32],f16> -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6836, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %int1_10968 = torch.constant.int 1
    %6837 = torch.aten.add.Tensor %6835, %6836, %int1_10968 : !torch.vtensor<[4,?,4,32],f16>, !torch.vtensor<[4,?,4,32],f16>, !torch.int -> !torch.vtensor<[4,?,4,32],f16>
    torch.bind_symbolic_shape %6837, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 32)> : !torch.vtensor<[4,?,4,32],f16>
    %6838 = torch_c.to_builtin_tensor %6834 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_10969 = tensor.cast %6838 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %6839 = torch_c.to_builtin_tensor %6837 : !torch.vtensor<[4,?,4,32],f16> -> tensor<4x?x4x32xf16>
    %cast_10970 = tensor.cast %6839 : tensor<4x?x4x32xf16> to tensor<?x?x?x?xf16>
    %6840 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_10969, %cast_10970) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_10971 = tensor.cast %6840 : tensor<?x?x?x2x?xf16> to tensor<4x?x4x2x32xf16>
    %6841 = torch_c.from_builtin_tensor %cast_10971 : tensor<4x?x4x2x32xf16> -> !torch.vtensor<[4,?,4,2,32],f16>
    torch.bind_symbolic_shape %6841, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 2, 32)> : !torch.vtensor<[4,?,4,2,32],f16>
    %int4_10972 = torch.constant.int 4
    %int4_10973 = torch.constant.int 4
    %int64_10974 = torch.constant.int 64
    %6842 = torch.prim.ListConstruct %int4_10972, %273, %int4_10973, %int64_10974 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6843 = torch.aten.view %6841, %6842 : !torch.vtensor<[4,?,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %6843, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %none_10975 = torch.constant.none
    %none_10976 = torch.constant.none
    %int5_10977 = torch.constant.int 5
    %cpu_10978 = torch.constant.device "cpu"
    %int0_10979 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6843, %none_10975, %none_10976, %int5_10977, %cpu_10978, %int0_10979 : !torch.vtensor<[4,?,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_10980 = torch.constant.int 22
    %6844 = torch.aten.mul.Scalar %arg2, %int22_10980 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6844, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int21 = torch.constant.int 21
    %int1_10981 = torch.constant.int 1
    %6845 = torch.aten.add.Scalar %6844, %int21, %int1_10981 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6845, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_10982 = torch.constant.int 2
    %6846 = torch.aten.mul.Scalar %6845, %int2_10982 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6846, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_10983 = torch.constant.int 0
    %int1_10984 = torch.constant.int 1
    %6847 = torch.aten.add.Scalar %6846, %int0_10983, %int1_10984 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6847, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %6848 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %6849 = torch.aten.view %6847, %6848 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6849, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_10985 = torch.constant.int 4
    %int32_10986 = torch.constant.int 32
    %int4_10987 = torch.constant.int 4
    %int64_10988 = torch.constant.int 64
    %6850 = torch.prim.ListConstruct %int4_10985, %271, %int32_10986, %int4_10987, %int64_10988 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6851 = torch.aten.view %6843, %6850 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %6851, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_10989 = torch.constant.int 32
    %int4_10990 = torch.constant.int 4
    %int64_10991 = torch.constant.int 64
    %6852 = torch.prim.ListConstruct %446, %int32_10989, %int4_10990, %int64_10991 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6853 = torch.aten.view %6851, %6852 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %6853, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_10992 = torch.constant.int 1
    %int2_10993 = torch.constant.int 2
    %6854 = torch.aten.transpose.int %6853, %int1_10992, %int2_10993 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %6854, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_10994 = torch.constant.none
    %none_10995 = torch.constant.none
    %int5_10996 = torch.constant.int 5
    %cpu_10997 = torch.constant.device "cpu"
    %int0_10998 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6854, %none_10994, %none_10995, %int5_10996, %cpu_10997, %int0_10998 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_10999 = torch.constant.int 22
    %int2_11000 = torch.constant.int 2
    %int4_11001 = torch.constant.int 4
    %int32_11002 = torch.constant.int 32
    %int64_11003 = torch.constant.int 64
    %6855 = torch.prim.ListConstruct %272, %int22_10999, %int2_11000, %int4_11001, %int32_11002, %int64_11003 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6856 = torch.aten.view %6580, %6855 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6856, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_11004 = torch.constant.int 4
    %int32_11005 = torch.constant.int 32
    %int64_11006 = torch.constant.int 64
    %6857 = torch.prim.ListConstruct %439, %int4_11004, %int32_11005, %int64_11006 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6858 = torch.aten.view %6856, %6857 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %6858, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %6859 = torch.prim.ListConstruct %6849 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_11007 = torch.constant.bool false
    %6860 = torch.aten.index_put %6858, %6859, %6854, %false_11007 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %6860, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_11008 = torch.constant.int 22
    %int2_11009 = torch.constant.int 2
    %int4_11010 = torch.constant.int 4
    %int32_11011 = torch.constant.int 32
    %int64_11012 = torch.constant.int 64
    %6861 = torch.prim.ListConstruct %272, %int22_11008, %int2_11009, %int4_11010, %int32_11011, %int64_11012 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6862 = torch.aten.view %6860, %6861 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6862, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_11013 = torch.constant.int 360448
    %6863 = torch.prim.ListConstruct %272, %int360448_11013 : (!torch.int, !torch.int) -> !torch.list<int>
    %6864 = torch.aten.view %6862, %6863 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %6864, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_11014 = torch.constant.int 22
    %int2_11015 = torch.constant.int 2
    %int4_11016 = torch.constant.int 4
    %int32_11017 = torch.constant.int 32
    %int64_11018 = torch.constant.int 64
    %6865 = torch.prim.ListConstruct %272, %int22_11014, %int2_11015, %int4_11016, %int32_11017, %int64_11018 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6866 = torch.aten.view %6864, %6865 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6866, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int4_11019 = torch.constant.int 4
    %int32_11020 = torch.constant.int 32
    %int64_11021 = torch.constant.int 64
    %6867 = torch.prim.ListConstruct %439, %int4_11019, %int32_11020, %int64_11021 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6868 = torch.aten.view %6866, %6867 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %6868, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_11022 = torch.constant.int 22
    %6869 = torch.aten.mul.Scalar %arg2, %int22_11022 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6869, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int21_11023 = torch.constant.int 21
    %int1_11024 = torch.constant.int 1
    %6870 = torch.aten.add.Scalar %6869, %int21_11023, %int1_11024 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6870, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_11025 = torch.constant.int 2
    %6871 = torch.aten.mul.Scalar %6870, %int2_11025 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6871, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_11026 = torch.constant.int 1
    %int1_11027 = torch.constant.int 1
    %6872 = torch.aten.add.Scalar %6871, %int1_11026, %int1_11027 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6872, [%269], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %6873 = torch.prim.ListConstruct %446 : (!torch.int) -> !torch.list<int>
    %6874 = torch.aten.view %6872, %6873 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6874, [%269], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int4_11028 = torch.constant.int 4
    %int32_11029 = torch.constant.int 32
    %int4_11030 = torch.constant.int 4
    %int64_11031 = torch.constant.int 64
    %6875 = torch.prim.ListConstruct %int4_11028, %271, %int32_11029, %int4_11030, %int64_11031 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6876 = torch.aten.view %6765, %6875 : !torch.vtensor<[4,?,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %6876, [%269], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_11032 = torch.constant.int 32
    %int4_11033 = torch.constant.int 4
    %int64_11034 = torch.constant.int 64
    %6877 = torch.prim.ListConstruct %446, %int32_11032, %int4_11033, %int64_11034 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6878 = torch.aten.view %6876, %6877 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[?,32,4,64],f16>
    torch.bind_symbolic_shape %6878, [%269], affine_map<()[s0] -> (s0 * 4, 32, 4, 64)> : !torch.vtensor<[?,32,4,64],f16>
    %int1_11035 = torch.constant.int 1
    %int2_11036 = torch.constant.int 2
    %6879 = torch.aten.transpose.int %6878, %int1_11035, %int2_11036 : !torch.vtensor<[?,32,4,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %6879, [%269], affine_map<()[s0] -> (s0 * 4, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %none_11037 = torch.constant.none
    %none_11038 = torch.constant.none
    %int5_11039 = torch.constant.int 5
    %cpu_11040 = torch.constant.device "cpu"
    %int0_11041 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6879, %none_11037, %none_11038, %int5_11039, %cpu_11040, %int0_11041 : !torch.vtensor<[?,4,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %6880 = torch.prim.ListConstruct %6874 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_11042 = torch.constant.bool false
    %6881 = torch.aten.index_put %6868, %6880, %6879, %false_11042 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,4,32,64],f16>, !torch.bool -> !torch.vtensor<[?,4,32,64],f16>
    torch.bind_symbolic_shape %6881, [%270], affine_map<()[s0] -> (s0 * 44, 4, 32, 64)> : !torch.vtensor<[?,4,32,64],f16>
    %int22_11043 = torch.constant.int 22
    %int2_11044 = torch.constant.int 2
    %int4_11045 = torch.constant.int 4
    %int32_11046 = torch.constant.int 32
    %int64_11047 = torch.constant.int 64
    %6882 = torch.prim.ListConstruct %272, %int22_11043, %int2_11044, %int4_11045, %int32_11046, %int64_11047 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6883 = torch.aten.view %6881, %6882 : !torch.vtensor<[?,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6883, [%270], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_11048 = torch.constant.int 360448
    %6884 = torch.prim.ListConstruct %272, %int360448_11048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6885 = torch.aten.view %6883, %6884 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.overwrite.tensor.contents %6885 overwrites %arg3 : !torch.vtensor<[?,360448],f16>, !torch.tensor<[?,360448],f16>
    torch.bind_symbolic_shape %6885, [%270], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int0_11049 = torch.constant.int 0
    %int1_11050 = torch.constant.int 1
    %none_11051 = torch.constant.none
    %none_11052 = torch.constant.none
    %cpu_11053 = torch.constant.device "cpu"
    %false_11054 = torch.constant.bool false
    %6886 = torch.aten.arange.start_step %int0_11049, %273, %int1_11050, %none_11051, %none_11052, %cpu_11053, %false_11054 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6886, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_11055 = torch.constant.int -1
    %6887 = torch.aten.unsqueeze %arg1, %int-1_11055 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %6888 = torch.aten.ge.Tensor %6886, %6887 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %6888, [%269], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_11056 = torch.constant.none
    %none_11057 = torch.constant.none
    %cpu_11058 = torch.constant.device "cpu"
    %false_11059 = torch.constant.bool false
    %6889 = torch.aten.arange %273, %none_11056, %none_11057, %cpu_11058, %false_11059 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6889, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_11060 = torch.constant.int 0
    %6890 = torch.aten.unsqueeze %6889, %int0_11060 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %6890, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_11061 = torch.constant.int 1
    %6891 = torch.aten.unsqueeze %6890, %int1_11061 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %6891, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int2_11062 = torch.constant.int 2
    %6892 = torch.aten.unsqueeze %6891, %int2_11062 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,1,?],si64>
    torch.bind_symbolic_shape %6892, [%269], affine_map<()[s0] -> (1, 1, 1, s0 * 32)> : !torch.vtensor<[1,1,1,?],si64>
    %none_11063 = torch.constant.none
    %none_11064 = torch.constant.none
    %cpu_11065 = torch.constant.device "cpu"
    %false_11066 = torch.constant.bool false
    %6893 = torch.aten.arange %273, %none_11063, %none_11064, %cpu_11065, %false_11066 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6893, [%269], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int0_11067 = torch.constant.int 0
    %6894 = torch.aten.unsqueeze %6893, %int0_11067 : !torch.vtensor<[?],si64>, !torch.int -> !torch.vtensor<[1,?],si64>
    torch.bind_symbolic_shape %6894, [%269], affine_map<()[s0] -> (1, s0 * 32)> : !torch.vtensor<[1,?],si64>
    %int1_11068 = torch.constant.int 1
    %6895 = torch.aten.unsqueeze %6894, %int1_11068 : !torch.vtensor<[1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?],si64>
    torch.bind_symbolic_shape %6895, [%269], affine_map<()[s0] -> (1, 1, s0 * 32)> : !torch.vtensor<[1,1,?],si64>
    %int3_11069 = torch.constant.int 3
    %6896 = torch.aten.unsqueeze %6895, %int3_11069 : !torch.vtensor<[1,1,?],si64>, !torch.int -> !torch.vtensor<[1,1,?,1],si64>
    torch.bind_symbolic_shape %6896, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, 1)> : !torch.vtensor<[1,1,?,1],si64>
    %6897 = torch.aten.gt.Tensor %6892, %6896 : !torch.vtensor<[1,1,1,?],si64>, !torch.vtensor<[1,1,?,1],si64> -> !torch.vtensor<[1,1,?,?],i1>
    torch.bind_symbolic_shape %6897, [%269], affine_map<()[s0] -> (1, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[1,1,?,?],i1>
    %int1_11070 = torch.constant.int 1
    %6898 = torch.aten.unsqueeze %6888, %int1_11070 : !torch.vtensor<[4,?],i1>, !torch.int -> !torch.vtensor<[4,1,?],i1>
    torch.bind_symbolic_shape %6898, [%269], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],i1>
    %int2_11071 = torch.constant.int 2
    %6899 = torch.aten.unsqueeze %6898, %int2_11071 : !torch.vtensor<[4,1,?],i1>, !torch.int -> !torch.vtensor<[4,1,1,?],i1>
    torch.bind_symbolic_shape %6899, [%269], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],i1>
    %6900 = torch.aten.logical_or %6897, %6899 : !torch.vtensor<[1,1,?,?],i1>, !torch.vtensor<[4,1,1,?],i1> -> !torch.vtensor<[4,1,?,?],i1>
    torch.bind_symbolic_shape %6900, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],i1>
    %none_11072 = torch.constant.none
    %6901 = torch.aten.clone %255, %none_11072 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_11073 = torch.constant.int 0
    %6902 = torch.aten.where.ScalarOther %6900, %6901, %int0_11073 : !torch.vtensor<[4,1,?,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,1,?,?],f16>
    torch.bind_symbolic_shape %6902, [%269], affine_map<()[s0] -> (4, 1, s0 * 32, s0 * 32)> : !torch.vtensor<[4,1,?,?],f16>
    %none_11074 = torch.constant.none
    %none_11075 = torch.constant.none
    %int5_11076 = torch.constant.int 5
    %cpu_11077 = torch.constant.device "cpu"
    %int0_11078 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6902, %none_11074, %none_11075, %int5_11076, %cpu_11077, %int0_11078 : !torch.vtensor<[4,1,?,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int-2_11079 = torch.constant.int -2
    %6903 = torch.aten.unsqueeze %6843, %int-2_11079 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %6903, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_11080 = torch.constant.int 4
    %int4_11081 = torch.constant.int 4
    %int8_11082 = torch.constant.int 8
    %int64_11083 = torch.constant.int 64
    %6904 = torch.prim.ListConstruct %int4_11080, %273, %int4_11081, %int8_11082, %int64_11083 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_11084 = torch.constant.bool false
    %6905 = torch.aten.expand %6903, %6904, %false_11084 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6905, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_11085 = torch.constant.int 0
    %6906 = torch.aten.clone %6905, %int0_11085 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6906, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_11086 = torch.constant.int 4
    %int32_11087 = torch.constant.int 32
    %int64_11088 = torch.constant.int 64
    %6907 = torch.prim.ListConstruct %int4_11086, %273, %int32_11087, %int64_11088 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6908 = torch.aten._unsafe_view %6906, %6907 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6908, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_11089 = torch.constant.int -2
    %6909 = torch.aten.unsqueeze %6765, %int-2_11089 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %6909, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_11090 = torch.constant.int 4
    %int4_11091 = torch.constant.int 4
    %int8_11092 = torch.constant.int 8
    %int64_11093 = torch.constant.int 64
    %6910 = torch.prim.ListConstruct %int4_11090, %273, %int4_11091, %int8_11092, %int64_11093 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_11094 = torch.constant.bool false
    %6911 = torch.aten.expand %6909, %6910, %false_11094 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6911, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_11095 = torch.constant.int 0
    %6912 = torch.aten.clone %6911, %int0_11095 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6912, [%269], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_11096 = torch.constant.int 4
    %int32_11097 = torch.constant.int 32
    %int64_11098 = torch.constant.int 64
    %6913 = torch.prim.ListConstruct %int4_11096, %273, %int32_11097, %int64_11098 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6914 = torch.aten._unsafe_view %6912, %6913 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6914, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_11099 = torch.constant.int 1
    %int2_11100 = torch.constant.int 2
    %6915 = torch.aten.transpose.int %6804, %int1_11099, %int2_11100 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6915, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_11101 = torch.constant.int 1
    %int2_11102 = torch.constant.int 2
    %6916 = torch.aten.transpose.int %6908, %int1_11101, %int2_11102 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6916, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_11103 = torch.constant.int 1
    %int2_11104 = torch.constant.int 2
    %6917 = torch.aten.transpose.int %6914, %int1_11103, %int2_11104 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6917, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_11105 = torch.constant.float 0.000000e+00
    %false_11106 = torch.constant.bool false
    %none_11107 = torch.constant.none
    %false_11108 = torch.constant.bool false
    %6918 = torch.aten.scaled_dot_product_attention %6915, %6916, %6917, %6902, %float0.000000e00_11105, %false_11106, %none_11107, %false_11108 : !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,?,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6918, [%269], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_11109 = torch.constant.int 1
    %int2_11110 = torch.constant.int 2
    %6919 = torch.aten.transpose.int %6918, %int1_11109, %int2_11110 : !torch.vtensor<[4,32,?,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6919, [%269], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int4_11111 = torch.constant.int 4
    %int2048_11112 = torch.constant.int 2048
    %6920 = torch.prim.ListConstruct %int4_11111, %273, %int2048_11112 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6921 = torch.aten.view %6919, %6920 : !torch.vtensor<[4,?,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6921, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_11113 = torch.constant.int 2
    %6922 = torch.aten.view.dtype %260, %int2_11113 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %6923 = torch.aten.detach %6922 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_11114 = torch.constant.int -1
    %int17_11115 = torch.constant.int 17
    %6924 = torch.prim.ListConstruct %int-1_11114, %int17_11115 : (!torch.int, !torch.int) -> !torch.list<int>
    %6925 = torch.aten.view %6923, %6924 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_11116 = torch.constant.int 2048
    %int-1_11117 = torch.constant.int -1
    %int17_11118 = torch.constant.int 17
    %6926 = torch.prim.ListConstruct %int2048_11116, %int-1_11117, %int17_11118 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6927 = torch.aten.view %6925, %6926 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_11119 = torch.constant.int 2
    %int0_11120 = torch.constant.int 0
    %int1_11121 = torch.constant.int 1
    %int1_11122 = torch.constant.int 1
    %6928 = torch.aten.slice.Tensor %6927, %int2_11119, %int0_11120, %int1_11121, %int1_11122 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_11123 = torch.constant.int 5
    %6929 = torch.aten.view.dtype %6928, %int5_11123 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %6930 = torch.aten.detach %6929 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_11124 = torch.constant.int 2
    %int1_11125 = torch.constant.int 1
    %int9223372036854775807_11126 = torch.constant.int 9223372036854775807
    %int1_11127 = torch.constant.int 1
    %6931 = torch.aten.slice.Tensor %6927, %int2_11124, %int1_11125, %int9223372036854775807_11126, %int1_11127 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_11128 = torch.constant.int 1
    %6932 = torch.aten.view.dtype %6931, %int1_11128 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %6933 = torch.aten.detach %6932 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %6934 = torch_c.to_builtin_tensor %6921 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_11129 = tensor.cast %6934 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6935 = torch_c.to_builtin_tensor %6930 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %6936 = torch_c.to_builtin_tensor %6933 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %6937 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_11129, %6935, %6936) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_11130 = tensor.cast %6937 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %6938 = torch_c.from_builtin_tensor %cast_11130 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6938, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_11131 = torch.constant.none
    %none_11132 = torch.constant.none
    %int5_11133 = torch.constant.int 5
    %cpu_11134 = torch.constant.device "cpu"
    %int0_11135 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6938, %none_11131, %none_11132, %int5_11133, %cpu_11134, %int0_11135 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_11136 = torch.constant.int 1
    %6939 = torch.aten.add.Tensor %6698, %6938, %int1_11136 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6939, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_11137 = torch.constant.none
    %none_11138 = torch.constant.none
    %int5_11139 = torch.constant.int 5
    %cpu_11140 = torch.constant.device "cpu"
    %int0_11141 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6939, %none_11137, %none_11138, %int5_11139, %cpu_11140, %int0_11141 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_11142 = torch.constant.int 6
    %6940 = torch.prims.convert_element_type %6939, %int6_11142 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6940, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_11143 = torch.constant.int 2
    %6941 = torch.aten.pow.Tensor_Scalar %6940, %int2_11143 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6941, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_11144 = torch.constant.int -1
    %6942 = torch.prim.ListConstruct %int-1_11144 : (!torch.int) -> !torch.list<int>
    %true_11145 = torch.constant.bool true
    %none_11146 = torch.constant.none
    %6943 = torch.aten.mean.dim %6941, %6942, %true_11145, %none_11146 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6943, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_11147 = torch.constant.float 9.9999997473787516E-6
    %int1_11148 = torch.constant.int 1
    %6944 = torch.aten.add.Scalar %6943, %float9.999990e-06_11147, %int1_11148 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6944, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6945 = torch.aten.rsqrt %6944 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6945, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6946 = torch.aten.mul.Tensor %6940, %6945 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6946, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_11149 = torch.constant.none
    %none_11150 = torch.constant.none
    %int6_11151 = torch.constant.int 6
    %cpu_11152 = torch.constant.device "cpu"
    %int0_11153 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6946, %none_11149, %none_11150, %int6_11151, %cpu_11152, %int0_11153 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11154 = torch.constant.int 5
    %6947 = torch.prims.convert_element_type %6946, %int5_11154 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6947, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %6948 = torch.aten.mul.Tensor %261, %6947 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %6948, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_11155 = torch.constant.none
    %none_11156 = torch.constant.none
    %int6_11157 = torch.constant.int 6
    %cpu_11158 = torch.constant.device "cpu"
    %int0_11159 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6948, %none_11155, %none_11156, %int6_11157, %cpu_11158, %int0_11159 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11160 = torch.constant.int 5
    %6949 = torch.prims.convert_element_type %6948, %int5_11160 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %6949, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_11161 = torch.constant.int 2
    %6950 = torch.aten.view.dtype %262, %int2_11161 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %6951 = torch.aten.detach %6950 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_11162 = torch.constant.int -1
    %int17_11163 = torch.constant.int 17
    %6952 = torch.prim.ListConstruct %int-1_11162, %int17_11163 : (!torch.int, !torch.int) -> !torch.list<int>
    %6953 = torch.aten.view %6951, %6952 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_11164 = torch.constant.int 5632
    %int-1_11165 = torch.constant.int -1
    %int17_11166 = torch.constant.int 17
    %6954 = torch.prim.ListConstruct %int5632_11164, %int-1_11165, %int17_11166 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6955 = torch.aten.view %6953, %6954 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_11167 = torch.constant.int 2
    %int0_11168 = torch.constant.int 0
    %int1_11169 = torch.constant.int 1
    %int1_11170 = torch.constant.int 1
    %6956 = torch.aten.slice.Tensor %6955, %int2_11167, %int0_11168, %int1_11169, %int1_11170 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_11171 = torch.constant.int 5
    %6957 = torch.aten.view.dtype %6956, %int5_11171 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %6958 = torch.aten.detach %6957 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_11172 = torch.constant.int 2
    %int1_11173 = torch.constant.int 1
    %int9223372036854775807_11174 = torch.constant.int 9223372036854775807
    %int1_11175 = torch.constant.int 1
    %6959 = torch.aten.slice.Tensor %6955, %int2_11172, %int1_11173, %int9223372036854775807_11174, %int1_11175 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_11176 = torch.constant.int 1
    %6960 = torch.aten.view.dtype %6959, %int1_11176 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %6961 = torch.aten.detach %6960 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %6962 = torch_c.to_builtin_tensor %6949 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_11177 = tensor.cast %6962 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6963 = torch_c.to_builtin_tensor %6958 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %6964 = torch_c.to_builtin_tensor %6961 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %6965 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_11177, %6963, %6964) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_11178 = tensor.cast %6965 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %6966 = torch_c.from_builtin_tensor %cast_11178 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %6966, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %6967 = torch.aten.silu %6966 : !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %6967, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_11179 = torch.constant.int 2
    %6968 = torch.aten.view.dtype %263, %int2_11179 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %6969 = torch.aten.detach %6968 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_11180 = torch.constant.int -1
    %int17_11181 = torch.constant.int 17
    %6970 = torch.prim.ListConstruct %int-1_11180, %int17_11181 : (!torch.int, !torch.int) -> !torch.list<int>
    %6971 = torch.aten.view %6969, %6970 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_11182 = torch.constant.int 5632
    %int-1_11183 = torch.constant.int -1
    %int17_11184 = torch.constant.int 17
    %6972 = torch.prim.ListConstruct %int5632_11182, %int-1_11183, %int17_11184 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6973 = torch.aten.view %6971, %6972 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_11185 = torch.constant.int 2
    %int0_11186 = torch.constant.int 0
    %int1_11187 = torch.constant.int 1
    %int1_11188 = torch.constant.int 1
    %6974 = torch.aten.slice.Tensor %6973, %int2_11185, %int0_11186, %int1_11187, %int1_11188 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_11189 = torch.constant.int 5
    %6975 = torch.aten.view.dtype %6974, %int5_11189 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %6976 = torch.aten.detach %6975 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_11190 = torch.constant.int 2
    %int1_11191 = torch.constant.int 1
    %int9223372036854775807_11192 = torch.constant.int 9223372036854775807
    %int1_11193 = torch.constant.int 1
    %6977 = torch.aten.slice.Tensor %6973, %int2_11190, %int1_11191, %int9223372036854775807_11192, %int1_11193 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_11194 = torch.constant.int 1
    %6978 = torch.aten.view.dtype %6977, %int1_11194 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %6979 = torch.aten.detach %6978 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %6980 = torch_c.to_builtin_tensor %6949 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_11195 = tensor.cast %6980 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %6981 = torch_c.to_builtin_tensor %6976 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %6982 = torch_c.to_builtin_tensor %6979 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %6983 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_11195, %6981, %6982) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_11196 = tensor.cast %6983 : tensor<?x?x5632xf16> to tensor<4x?x5632xf16>
    %6984 = torch_c.from_builtin_tensor %cast_11196 : tensor<4x?x5632xf16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %6984, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %6985 = torch.aten.mul.Tensor %6967, %6984 : !torch.vtensor<[4,?,5632],f16>, !torch.vtensor<[4,?,5632],f16> -> !torch.vtensor<[4,?,5632],f16>
    torch.bind_symbolic_shape %6985, [%269], affine_map<()[s0] -> (4, s0 * 32, 5632)> : !torch.vtensor<[4,?,5632],f16>
    %int2_11197 = torch.constant.int 2
    %6986 = torch.aten.view.dtype %264, %int2_11197 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %6987 = torch.aten.detach %6986 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_11198 = torch.constant.int -1
    %int17_11199 = torch.constant.int 17
    %6988 = torch.prim.ListConstruct %int-1_11198, %int17_11199 : (!torch.int, !torch.int) -> !torch.list<int>
    %6989 = torch.aten.view %6987, %6988 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_11200 = torch.constant.int 2048
    %int-1_11201 = torch.constant.int -1
    %int17_11202 = torch.constant.int 17
    %6990 = torch.prim.ListConstruct %int2048_11200, %int-1_11201, %int17_11202 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6991 = torch.aten.view %6989, %6990 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_11203 = torch.constant.int 2
    %int0_11204 = torch.constant.int 0
    %int1_11205 = torch.constant.int 1
    %int1_11206 = torch.constant.int 1
    %6992 = torch.aten.slice.Tensor %6991, %int2_11203, %int0_11204, %int1_11205, %int1_11206 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_11207 = torch.constant.int 5
    %6993 = torch.aten.view.dtype %6992, %int5_11207 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %6994 = torch.aten.detach %6993 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_11208 = torch.constant.int 2
    %int1_11209 = torch.constant.int 1
    %int9223372036854775807_11210 = torch.constant.int 9223372036854775807
    %int1_11211 = torch.constant.int 1
    %6995 = torch.aten.slice.Tensor %6991, %int2_11208, %int1_11209, %int9223372036854775807_11210, %int1_11211 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_11212 = torch.constant.int 1
    %6996 = torch.aten.view.dtype %6995, %int1_11212 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %6997 = torch.aten.detach %6996 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %6998 = torch_c.to_builtin_tensor %6985 : !torch.vtensor<[4,?,5632],f16> -> tensor<4x?x5632xf16>
    %cast_11213 = tensor.cast %6998 : tensor<4x?x5632xf16> to tensor<?x?x5632xf16>
    %6999 = torch_c.to_builtin_tensor %6994 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %7000 = torch_c.to_builtin_tensor %6997 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %7001 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_11213, %6999, %7000) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_11214 = tensor.cast %7001 : tensor<?x?x2048xf16> to tensor<4x?x2048xf16>
    %7002 = torch_c.from_builtin_tensor %cast_11214 : tensor<4x?x2048xf16> -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %7002, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int1_11215 = torch.constant.int 1
    %7003 = torch.aten.add.Tensor %6939, %7002, %int1_11215 : !torch.vtensor<[4,?,2048],f16>, !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %7003, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %none_11216 = torch.constant.none
    %none_11217 = torch.constant.none
    %int5_11218 = torch.constant.int 5
    %cpu_11219 = torch.constant.device "cpu"
    %int0_11220 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7003, %none_11216, %none_11217, %int5_11218, %cpu_11219, %int0_11220 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_11221 = torch.constant.none
    %none_11222 = torch.constant.none
    %int5_11223 = torch.constant.int 5
    %cpu_11224 = torch.constant.device "cpu"
    %int0_11225 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7003, %none_11221, %none_11222, %int5_11223, %cpu_11224, %int0_11225 : !torch.vtensor<[4,?,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_11226 = torch.constant.int 6
    %7004 = torch.prims.convert_element_type %7003, %int6_11226 : !torch.vtensor<[4,?,2048],f16>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %7004, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int2_11227 = torch.constant.int 2
    %7005 = torch.aten.pow.Tensor_Scalar %7004, %int2_11227 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %7005, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %int-1_11228 = torch.constant.int -1
    %7006 = torch.prim.ListConstruct %int-1_11228 : (!torch.int) -> !torch.list<int>
    %true_11229 = torch.constant.bool true
    %none_11230 = torch.constant.none
    %7007 = torch.aten.mean.dim %7005, %7006, %true_11229, %none_11230 : !torch.vtensor<[4,?,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %7007, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_11231 = torch.constant.float 9.9999997473787516E-6
    %int1_11232 = torch.constant.int 1
    %7008 = torch.aten.add.Scalar %7007, %float9.999990e-06_11231, %int1_11232 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %7008, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %7009 = torch.aten.rsqrt %7008 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %7009, [%269], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %7010 = torch.aten.mul.Tensor %7004, %7009 : !torch.vtensor<[4,?,2048],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %7010, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_11233 = torch.constant.none
    %none_11234 = torch.constant.none
    %int6_11235 = torch.constant.int 6
    %cpu_11236 = torch.constant.device "cpu"
    %int0_11237 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7010, %none_11233, %none_11234, %int6_11235, %cpu_11236, %int0_11237 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11238 = torch.constant.int 5
    %7011 = torch.prims.convert_element_type %7010, %int5_11238 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %7011, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %7012 = torch.aten.mul.Tensor %265, %7011 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,?,2048],f16> -> !torch.vtensor<[4,?,2048],f32>
    torch.bind_symbolic_shape %7012, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f32>
    %none_11239 = torch.constant.none
    %none_11240 = torch.constant.none
    %int6_11241 = torch.constant.int 6
    %cpu_11242 = torch.constant.device "cpu"
    %int0_11243 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7012, %none_11239, %none_11240, %int6_11241, %cpu_11242, %int0_11243 : !torch.vtensor<[4,?,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11244 = torch.constant.int 5
    %7013 = torch.prims.convert_element_type %7012, %int5_11244 : !torch.vtensor<[4,?,2048],f32>, !torch.int -> !torch.vtensor<[4,?,2048],f16>
    torch.bind_symbolic_shape %7013, [%269], affine_map<()[s0] -> (4, s0 * 32, 2048)> : !torch.vtensor<[4,?,2048],f16>
    %int2_11245 = torch.constant.int 2
    %7014 = torch.aten.view.dtype %266, %int2_11245 : !torch.vtensor<[32000,2176],ui8>, !torch.int -> !torch.vtensor<[32000,1088],si16>
    %7015 = torch.aten.detach %7014 : !torch.vtensor<[32000,1088],si16> -> !torch.vtensor<[32000,1088],si16>
    %int-1_11246 = torch.constant.int -1
    %int17_11247 = torch.constant.int 17
    %7016 = torch.prim.ListConstruct %int-1_11246, %int17_11247 : (!torch.int, !torch.int) -> !torch.list<int>
    %7017 = torch.aten.view %7015, %7016 : !torch.vtensor<[32000,1088],si16>, !torch.list<int> -> !torch.vtensor<[2048000,17],si16>
    %int32000_11248 = torch.constant.int 32000
    %int-1_11249 = torch.constant.int -1
    %int17_11250 = torch.constant.int 17
    %7018 = torch.prim.ListConstruct %int32000_11248, %int-1_11249, %int17_11250 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7019 = torch.aten.view %7017, %7018 : !torch.vtensor<[2048000,17],si16>, !torch.list<int> -> !torch.vtensor<[32000,64,17],si16>
    %int2_11251 = torch.constant.int 2
    %int0_11252 = torch.constant.int 0
    %int1_11253 = torch.constant.int 1
    %int1_11254 = torch.constant.int 1
    %7020 = torch.aten.slice.Tensor %7019, %int2_11251, %int0_11252, %int1_11253, %int1_11254 : !torch.vtensor<[32000,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[32000,64,1],si16>
    %int5_11255 = torch.constant.int 5
    %7021 = torch.aten.view.dtype %7020, %int5_11255 : !torch.vtensor<[32000,64,1],si16>, !torch.int -> !torch.vtensor<[32000,64,1],f16>
    %7022 = torch.aten.detach %7021 : !torch.vtensor<[32000,64,1],f16> -> !torch.vtensor<[32000,64,1],f16>
    %int2_11256 = torch.constant.int 2
    %int1_11257 = torch.constant.int 1
    %int9223372036854775807_11258 = torch.constant.int 9223372036854775807
    %int1_11259 = torch.constant.int 1
    %7023 = torch.aten.slice.Tensor %7019, %int2_11256, %int1_11257, %int9223372036854775807_11258, %int1_11259 : !torch.vtensor<[32000,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[32000,64,16],si16>
    %int1_11260 = torch.constant.int 1
    %7024 = torch.aten.view.dtype %7023, %int1_11260 : !torch.vtensor<[32000,64,16],si16>, !torch.int -> !torch.vtensor<[32000,64,32],si8>
    %7025 = torch.aten.detach %7024 : !torch.vtensor<[32000,64,32],si8> -> !torch.vtensor<[32000,64,32],si8>
    %7026 = torch_c.to_builtin_tensor %7013 : !torch.vtensor<[4,?,2048],f16> -> tensor<4x?x2048xf16>
    %cast_11261 = tensor.cast %7026 : tensor<4x?x2048xf16> to tensor<?x?x2048xf16>
    %7027 = torch_c.to_builtin_tensor %7022 : !torch.vtensor<[32000,64,1],f16> -> tensor<32000x64x1xf16>
    %7028 = torch_c.to_builtin_tensor %7025 : !torch.vtensor<[32000,64,32],si8> -> tensor<32000x64x32xi8>
    %7029 = util.call @sharktank_mmt_block_scaled_q8_3d_32000_2048_32_f16(%cast_11261, %7027, %7028) : (tensor<?x?x2048xf16>, tensor<32000x64x1xf16>, tensor<32000x64x32xi8>) -> tensor<?x?x32000xf16>
    %cast_11262 = tensor.cast %7029 : tensor<?x?x32000xf16> to tensor<4x?x32000xf16>
    %7030 = torch_c.from_builtin_tensor %cast_11262 : tensor<4x?x32000xf16> -> !torch.vtensor<[4,?,32000],f16>
    torch.bind_symbolic_shape %7030, [%269], affine_map<()[s0] -> (4, s0 * 32, 32000)> : !torch.vtensor<[4,?,32000],f16>
    return %7030 : !torch.vtensor<[4,?,32000],f16>
  }
  func.func @decode_bs4(%arg0: !torch.vtensor<[4,1],si64> {iree.abi.affinity = #hal.device.promise<@__device_0>}, %arg1: !torch.vtensor<[4],si64> {iree.abi.affinity = #hal.device.promise<@__device_0>}, %arg2: !torch.vtensor<[4],si64> {iree.abi.affinity = #hal.device.promise<@__device_0>}, %arg3: !torch.vtensor<[4,?],si64> {iree.abi.affinity = #hal.device.promise<@__device_0>}, %arg4: !torch.tensor<[?,360448],f16> {iree.abi.affinity = #hal.device.promise<@__device_0>}) -> !torch.vtensor<[4,1,32000],f16> attributes {torch.assume_strict_symbolic_shapes} {
    %__auto.token_embd.weight = util.global.load @__auto.token_embd.weight : tensor<32000x2176xi8>
    %0 = torch_c.from_builtin_tensor %__auto.token_embd.weight : tensor<32000x2176xi8> -> !torch.vtensor<[32000,2176],ui8>
    %1 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %2 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %3 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %4 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %5 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %6 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %7 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %8 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.0.attn_norm.weight = util.global.load @__auto.blk.0.attn_norm.weight : tensor<2048xf32>
    %9 = torch_c.from_builtin_tensor %__auto.blk.0.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.0.attn_q.weight = util.global.load @__auto.blk.0.attn_q.weight : tensor<2048x2176xi8>
    %10 = torch_c.from_builtin_tensor %__auto.blk.0.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.0.attn_k.weight = util.global.load @__auto.blk.0.attn_k.weight : tensor<256x2176xi8>
    %11 = torch_c.from_builtin_tensor %__auto.blk.0.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.0.attn_v.weight = util.global.load @__auto.blk.0.attn_v.weight : tensor<256x2176xi8>
    %12 = torch_c.from_builtin_tensor %__auto.blk.0.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.0.attn_output.weight = util.global.load @__auto.blk.0.attn_output.weight : tensor<2048x2176xi8>
    %13 = torch_c.from_builtin_tensor %__auto.blk.0.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.0.ffn_norm.weight = util.global.load @__auto.blk.0.ffn_norm.weight : tensor<2048xf32>
    %14 = torch_c.from_builtin_tensor %__auto.blk.0.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.0.ffn_gate.weight = util.global.load @__auto.blk.0.ffn_gate.weight : tensor<5632x2176xi8>
    %15 = torch_c.from_builtin_tensor %__auto.blk.0.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.0.ffn_up.weight = util.global.load @__auto.blk.0.ffn_up.weight : tensor<5632x2176xi8>
    %16 = torch_c.from_builtin_tensor %__auto.blk.0.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.0.ffn_down.weight = util.global.load @__auto.blk.0.ffn_down.weight : tensor<2048x5984xi8>
    %17 = torch_c.from_builtin_tensor %__auto.blk.0.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %18 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %19 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %20 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %21 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %22 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %23 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %24 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %25 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.1.attn_norm.weight = util.global.load @__auto.blk.1.attn_norm.weight : tensor<2048xf32>
    %26 = torch_c.from_builtin_tensor %__auto.blk.1.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.1.attn_q.weight = util.global.load @__auto.blk.1.attn_q.weight : tensor<2048x2176xi8>
    %27 = torch_c.from_builtin_tensor %__auto.blk.1.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.1.attn_k.weight = util.global.load @__auto.blk.1.attn_k.weight : tensor<256x2176xi8>
    %28 = torch_c.from_builtin_tensor %__auto.blk.1.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.1.attn_v.weight = util.global.load @__auto.blk.1.attn_v.weight : tensor<256x2176xi8>
    %29 = torch_c.from_builtin_tensor %__auto.blk.1.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.1.attn_output.weight = util.global.load @__auto.blk.1.attn_output.weight : tensor<2048x2176xi8>
    %30 = torch_c.from_builtin_tensor %__auto.blk.1.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.1.ffn_norm.weight = util.global.load @__auto.blk.1.ffn_norm.weight : tensor<2048xf32>
    %31 = torch_c.from_builtin_tensor %__auto.blk.1.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.1.ffn_gate.weight = util.global.load @__auto.blk.1.ffn_gate.weight : tensor<5632x2176xi8>
    %32 = torch_c.from_builtin_tensor %__auto.blk.1.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.1.ffn_up.weight = util.global.load @__auto.blk.1.ffn_up.weight : tensor<5632x2176xi8>
    %33 = torch_c.from_builtin_tensor %__auto.blk.1.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.1.ffn_down.weight = util.global.load @__auto.blk.1.ffn_down.weight : tensor<2048x5984xi8>
    %34 = torch_c.from_builtin_tensor %__auto.blk.1.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %35 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %36 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %37 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %38 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %39 = torch.vtensor.literal(dense<2> : tensor<si64>) : !torch.vtensor<[],si64>
    %40 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %41 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %42 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.2.attn_norm.weight = util.global.load @__auto.blk.2.attn_norm.weight : tensor<2048xf32>
    %43 = torch_c.from_builtin_tensor %__auto.blk.2.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.2.attn_q.weight = util.global.load @__auto.blk.2.attn_q.weight : tensor<2048x2176xi8>
    %44 = torch_c.from_builtin_tensor %__auto.blk.2.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.2.attn_k.weight = util.global.load @__auto.blk.2.attn_k.weight : tensor<256x2176xi8>
    %45 = torch_c.from_builtin_tensor %__auto.blk.2.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.2.attn_v.weight = util.global.load @__auto.blk.2.attn_v.weight : tensor<256x2176xi8>
    %46 = torch_c.from_builtin_tensor %__auto.blk.2.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.2.attn_output.weight = util.global.load @__auto.blk.2.attn_output.weight : tensor<2048x2176xi8>
    %47 = torch_c.from_builtin_tensor %__auto.blk.2.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.2.ffn_norm.weight = util.global.load @__auto.blk.2.ffn_norm.weight : tensor<2048xf32>
    %48 = torch_c.from_builtin_tensor %__auto.blk.2.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.2.ffn_gate.weight = util.global.load @__auto.blk.2.ffn_gate.weight : tensor<5632x2176xi8>
    %49 = torch_c.from_builtin_tensor %__auto.blk.2.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.2.ffn_up.weight = util.global.load @__auto.blk.2.ffn_up.weight : tensor<5632x2176xi8>
    %50 = torch_c.from_builtin_tensor %__auto.blk.2.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.2.ffn_down.weight = util.global.load @__auto.blk.2.ffn_down.weight : tensor<2048x5984xi8>
    %51 = torch_c.from_builtin_tensor %__auto.blk.2.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %52 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %53 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %54 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %55 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %56 = torch.vtensor.literal(dense<3> : tensor<si64>) : !torch.vtensor<[],si64>
    %57 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %58 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %59 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.3.attn_norm.weight = util.global.load @__auto.blk.3.attn_norm.weight : tensor<2048xf32>
    %60 = torch_c.from_builtin_tensor %__auto.blk.3.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.3.attn_q.weight = util.global.load @__auto.blk.3.attn_q.weight : tensor<2048x2176xi8>
    %61 = torch_c.from_builtin_tensor %__auto.blk.3.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.3.attn_k.weight = util.global.load @__auto.blk.3.attn_k.weight : tensor<256x2176xi8>
    %62 = torch_c.from_builtin_tensor %__auto.blk.3.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.3.attn_v.weight = util.global.load @__auto.blk.3.attn_v.weight : tensor<256x2176xi8>
    %63 = torch_c.from_builtin_tensor %__auto.blk.3.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.3.attn_output.weight = util.global.load @__auto.blk.3.attn_output.weight : tensor<2048x2176xi8>
    %64 = torch_c.from_builtin_tensor %__auto.blk.3.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.3.ffn_norm.weight = util.global.load @__auto.blk.3.ffn_norm.weight : tensor<2048xf32>
    %65 = torch_c.from_builtin_tensor %__auto.blk.3.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.3.ffn_gate.weight = util.global.load @__auto.blk.3.ffn_gate.weight : tensor<5632x2176xi8>
    %66 = torch_c.from_builtin_tensor %__auto.blk.3.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.3.ffn_up.weight = util.global.load @__auto.blk.3.ffn_up.weight : tensor<5632x2176xi8>
    %67 = torch_c.from_builtin_tensor %__auto.blk.3.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.3.ffn_down.weight = util.global.load @__auto.blk.3.ffn_down.weight : tensor<2048x5984xi8>
    %68 = torch_c.from_builtin_tensor %__auto.blk.3.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %69 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %70 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %71 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %72 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %73 = torch.vtensor.literal(dense<4> : tensor<si64>) : !torch.vtensor<[],si64>
    %74 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %75 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %76 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.4.attn_norm.weight = util.global.load @__auto.blk.4.attn_norm.weight : tensor<2048xf32>
    %77 = torch_c.from_builtin_tensor %__auto.blk.4.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.4.attn_q.weight = util.global.load @__auto.blk.4.attn_q.weight : tensor<2048x2176xi8>
    %78 = torch_c.from_builtin_tensor %__auto.blk.4.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.4.attn_k.weight = util.global.load @__auto.blk.4.attn_k.weight : tensor<256x2176xi8>
    %79 = torch_c.from_builtin_tensor %__auto.blk.4.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.4.attn_v.weight = util.global.load @__auto.blk.4.attn_v.weight : tensor<256x2176xi8>
    %80 = torch_c.from_builtin_tensor %__auto.blk.4.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.4.attn_output.weight = util.global.load @__auto.blk.4.attn_output.weight : tensor<2048x2176xi8>
    %81 = torch_c.from_builtin_tensor %__auto.blk.4.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.4.ffn_norm.weight = util.global.load @__auto.blk.4.ffn_norm.weight : tensor<2048xf32>
    %82 = torch_c.from_builtin_tensor %__auto.blk.4.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.4.ffn_gate.weight = util.global.load @__auto.blk.4.ffn_gate.weight : tensor<5632x2176xi8>
    %83 = torch_c.from_builtin_tensor %__auto.blk.4.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.4.ffn_up.weight = util.global.load @__auto.blk.4.ffn_up.weight : tensor<5632x2176xi8>
    %84 = torch_c.from_builtin_tensor %__auto.blk.4.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.4.ffn_down.weight = util.global.load @__auto.blk.4.ffn_down.weight : tensor<2048x5984xi8>
    %85 = torch_c.from_builtin_tensor %__auto.blk.4.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %86 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %87 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %88 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %89 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %90 = torch.vtensor.literal(dense<5> : tensor<si64>) : !torch.vtensor<[],si64>
    %91 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %92 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %93 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.5.attn_norm.weight = util.global.load @__auto.blk.5.attn_norm.weight : tensor<2048xf32>
    %94 = torch_c.from_builtin_tensor %__auto.blk.5.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.5.attn_q.weight = util.global.load @__auto.blk.5.attn_q.weight : tensor<2048x2176xi8>
    %95 = torch_c.from_builtin_tensor %__auto.blk.5.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.5.attn_k.weight = util.global.load @__auto.blk.5.attn_k.weight : tensor<256x2176xi8>
    %96 = torch_c.from_builtin_tensor %__auto.blk.5.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.5.attn_v.weight = util.global.load @__auto.blk.5.attn_v.weight : tensor<256x2176xi8>
    %97 = torch_c.from_builtin_tensor %__auto.blk.5.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.5.attn_output.weight = util.global.load @__auto.blk.5.attn_output.weight : tensor<2048x2176xi8>
    %98 = torch_c.from_builtin_tensor %__auto.blk.5.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.5.ffn_norm.weight = util.global.load @__auto.blk.5.ffn_norm.weight : tensor<2048xf32>
    %99 = torch_c.from_builtin_tensor %__auto.blk.5.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.5.ffn_gate.weight = util.global.load @__auto.blk.5.ffn_gate.weight : tensor<5632x2176xi8>
    %100 = torch_c.from_builtin_tensor %__auto.blk.5.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.5.ffn_up.weight = util.global.load @__auto.blk.5.ffn_up.weight : tensor<5632x2176xi8>
    %101 = torch_c.from_builtin_tensor %__auto.blk.5.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.5.ffn_down.weight = util.global.load @__auto.blk.5.ffn_down.weight : tensor<2048x5984xi8>
    %102 = torch_c.from_builtin_tensor %__auto.blk.5.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %103 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %104 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %105 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %106 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %107 = torch.vtensor.literal(dense<6> : tensor<si64>) : !torch.vtensor<[],si64>
    %108 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %109 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %110 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.6.attn_norm.weight = util.global.load @__auto.blk.6.attn_norm.weight : tensor<2048xf32>
    %111 = torch_c.from_builtin_tensor %__auto.blk.6.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.6.attn_q.weight = util.global.load @__auto.blk.6.attn_q.weight : tensor<2048x2176xi8>
    %112 = torch_c.from_builtin_tensor %__auto.blk.6.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.6.attn_k.weight = util.global.load @__auto.blk.6.attn_k.weight : tensor<256x2176xi8>
    %113 = torch_c.from_builtin_tensor %__auto.blk.6.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.6.attn_v.weight = util.global.load @__auto.blk.6.attn_v.weight : tensor<256x2176xi8>
    %114 = torch_c.from_builtin_tensor %__auto.blk.6.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.6.attn_output.weight = util.global.load @__auto.blk.6.attn_output.weight : tensor<2048x2176xi8>
    %115 = torch_c.from_builtin_tensor %__auto.blk.6.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.6.ffn_norm.weight = util.global.load @__auto.blk.6.ffn_norm.weight : tensor<2048xf32>
    %116 = torch_c.from_builtin_tensor %__auto.blk.6.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.6.ffn_gate.weight = util.global.load @__auto.blk.6.ffn_gate.weight : tensor<5632x2176xi8>
    %117 = torch_c.from_builtin_tensor %__auto.blk.6.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.6.ffn_up.weight = util.global.load @__auto.blk.6.ffn_up.weight : tensor<5632x2176xi8>
    %118 = torch_c.from_builtin_tensor %__auto.blk.6.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.6.ffn_down.weight = util.global.load @__auto.blk.6.ffn_down.weight : tensor<2048x5984xi8>
    %119 = torch_c.from_builtin_tensor %__auto.blk.6.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %120 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %121 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %122 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %123 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %124 = torch.vtensor.literal(dense<7> : tensor<si64>) : !torch.vtensor<[],si64>
    %125 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %126 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %127 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.7.attn_norm.weight = util.global.load @__auto.blk.7.attn_norm.weight : tensor<2048xf32>
    %128 = torch_c.from_builtin_tensor %__auto.blk.7.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.7.attn_q.weight = util.global.load @__auto.blk.7.attn_q.weight : tensor<2048x2176xi8>
    %129 = torch_c.from_builtin_tensor %__auto.blk.7.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.7.attn_k.weight = util.global.load @__auto.blk.7.attn_k.weight : tensor<256x2176xi8>
    %130 = torch_c.from_builtin_tensor %__auto.blk.7.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.7.attn_v.weight = util.global.load @__auto.blk.7.attn_v.weight : tensor<256x2176xi8>
    %131 = torch_c.from_builtin_tensor %__auto.blk.7.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.7.attn_output.weight = util.global.load @__auto.blk.7.attn_output.weight : tensor<2048x2176xi8>
    %132 = torch_c.from_builtin_tensor %__auto.blk.7.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.7.ffn_norm.weight = util.global.load @__auto.blk.7.ffn_norm.weight : tensor<2048xf32>
    %133 = torch_c.from_builtin_tensor %__auto.blk.7.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.7.ffn_gate.weight = util.global.load @__auto.blk.7.ffn_gate.weight : tensor<5632x2176xi8>
    %134 = torch_c.from_builtin_tensor %__auto.blk.7.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.7.ffn_up.weight = util.global.load @__auto.blk.7.ffn_up.weight : tensor<5632x2176xi8>
    %135 = torch_c.from_builtin_tensor %__auto.blk.7.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.7.ffn_down.weight = util.global.load @__auto.blk.7.ffn_down.weight : tensor<2048x5984xi8>
    %136 = torch_c.from_builtin_tensor %__auto.blk.7.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %137 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %138 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %139 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %140 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %141 = torch.vtensor.literal(dense<8> : tensor<si64>) : !torch.vtensor<[],si64>
    %142 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %143 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %144 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.8.attn_norm.weight = util.global.load @__auto.blk.8.attn_norm.weight : tensor<2048xf32>
    %145 = torch_c.from_builtin_tensor %__auto.blk.8.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.8.attn_q.weight = util.global.load @__auto.blk.8.attn_q.weight : tensor<2048x2176xi8>
    %146 = torch_c.from_builtin_tensor %__auto.blk.8.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.8.attn_k.weight = util.global.load @__auto.blk.8.attn_k.weight : tensor<256x2176xi8>
    %147 = torch_c.from_builtin_tensor %__auto.blk.8.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.8.attn_v.weight = util.global.load @__auto.blk.8.attn_v.weight : tensor<256x2176xi8>
    %148 = torch_c.from_builtin_tensor %__auto.blk.8.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.8.attn_output.weight = util.global.load @__auto.blk.8.attn_output.weight : tensor<2048x2176xi8>
    %149 = torch_c.from_builtin_tensor %__auto.blk.8.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.8.ffn_norm.weight = util.global.load @__auto.blk.8.ffn_norm.weight : tensor<2048xf32>
    %150 = torch_c.from_builtin_tensor %__auto.blk.8.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.8.ffn_gate.weight = util.global.load @__auto.blk.8.ffn_gate.weight : tensor<5632x2176xi8>
    %151 = torch_c.from_builtin_tensor %__auto.blk.8.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.8.ffn_up.weight = util.global.load @__auto.blk.8.ffn_up.weight : tensor<5632x2176xi8>
    %152 = torch_c.from_builtin_tensor %__auto.blk.8.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.8.ffn_down.weight = util.global.load @__auto.blk.8.ffn_down.weight : tensor<2048x5984xi8>
    %153 = torch_c.from_builtin_tensor %__auto.blk.8.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %154 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %155 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %156 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %157 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %158 = torch.vtensor.literal(dense<9> : tensor<si64>) : !torch.vtensor<[],si64>
    %159 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %160 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %161 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.9.attn_norm.weight = util.global.load @__auto.blk.9.attn_norm.weight : tensor<2048xf32>
    %162 = torch_c.from_builtin_tensor %__auto.blk.9.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.9.attn_q.weight = util.global.load @__auto.blk.9.attn_q.weight : tensor<2048x2176xi8>
    %163 = torch_c.from_builtin_tensor %__auto.blk.9.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.9.attn_k.weight = util.global.load @__auto.blk.9.attn_k.weight : tensor<256x2176xi8>
    %164 = torch_c.from_builtin_tensor %__auto.blk.9.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.9.attn_v.weight = util.global.load @__auto.blk.9.attn_v.weight : tensor<256x2176xi8>
    %165 = torch_c.from_builtin_tensor %__auto.blk.9.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.9.attn_output.weight = util.global.load @__auto.blk.9.attn_output.weight : tensor<2048x2176xi8>
    %166 = torch_c.from_builtin_tensor %__auto.blk.9.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.9.ffn_norm.weight = util.global.load @__auto.blk.9.ffn_norm.weight : tensor<2048xf32>
    %167 = torch_c.from_builtin_tensor %__auto.blk.9.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.9.ffn_gate.weight = util.global.load @__auto.blk.9.ffn_gate.weight : tensor<5632x2176xi8>
    %168 = torch_c.from_builtin_tensor %__auto.blk.9.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.9.ffn_up.weight = util.global.load @__auto.blk.9.ffn_up.weight : tensor<5632x2176xi8>
    %169 = torch_c.from_builtin_tensor %__auto.blk.9.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.9.ffn_down.weight = util.global.load @__auto.blk.9.ffn_down.weight : tensor<2048x5984xi8>
    %170 = torch_c.from_builtin_tensor %__auto.blk.9.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %171 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %172 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %173 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %174 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %175 = torch.vtensor.literal(dense<10> : tensor<si64>) : !torch.vtensor<[],si64>
    %176 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %177 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %178 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.10.attn_norm.weight = util.global.load @__auto.blk.10.attn_norm.weight : tensor<2048xf32>
    %179 = torch_c.from_builtin_tensor %__auto.blk.10.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.10.attn_q.weight = util.global.load @__auto.blk.10.attn_q.weight : tensor<2048x2176xi8>
    %180 = torch_c.from_builtin_tensor %__auto.blk.10.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.10.attn_k.weight = util.global.load @__auto.blk.10.attn_k.weight : tensor<256x2176xi8>
    %181 = torch_c.from_builtin_tensor %__auto.blk.10.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.10.attn_v.weight = util.global.load @__auto.blk.10.attn_v.weight : tensor<256x2176xi8>
    %182 = torch_c.from_builtin_tensor %__auto.blk.10.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.10.attn_output.weight = util.global.load @__auto.blk.10.attn_output.weight : tensor<2048x2176xi8>
    %183 = torch_c.from_builtin_tensor %__auto.blk.10.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.10.ffn_norm.weight = util.global.load @__auto.blk.10.ffn_norm.weight : tensor<2048xf32>
    %184 = torch_c.from_builtin_tensor %__auto.blk.10.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.10.ffn_gate.weight = util.global.load @__auto.blk.10.ffn_gate.weight : tensor<5632x2176xi8>
    %185 = torch_c.from_builtin_tensor %__auto.blk.10.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.10.ffn_up.weight = util.global.load @__auto.blk.10.ffn_up.weight : tensor<5632x2176xi8>
    %186 = torch_c.from_builtin_tensor %__auto.blk.10.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.10.ffn_down.weight = util.global.load @__auto.blk.10.ffn_down.weight : tensor<2048x5984xi8>
    %187 = torch_c.from_builtin_tensor %__auto.blk.10.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %188 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %189 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %190 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %191 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %192 = torch.vtensor.literal(dense<11> : tensor<si64>) : !torch.vtensor<[],si64>
    %193 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %194 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %195 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.11.attn_norm.weight = util.global.load @__auto.blk.11.attn_norm.weight : tensor<2048xf32>
    %196 = torch_c.from_builtin_tensor %__auto.blk.11.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.11.attn_q.weight = util.global.load @__auto.blk.11.attn_q.weight : tensor<2048x2176xi8>
    %197 = torch_c.from_builtin_tensor %__auto.blk.11.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.11.attn_k.weight = util.global.load @__auto.blk.11.attn_k.weight : tensor<256x2176xi8>
    %198 = torch_c.from_builtin_tensor %__auto.blk.11.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.11.attn_v.weight = util.global.load @__auto.blk.11.attn_v.weight : tensor<256x2176xi8>
    %199 = torch_c.from_builtin_tensor %__auto.blk.11.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.11.attn_output.weight = util.global.load @__auto.blk.11.attn_output.weight : tensor<2048x2176xi8>
    %200 = torch_c.from_builtin_tensor %__auto.blk.11.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.11.ffn_norm.weight = util.global.load @__auto.blk.11.ffn_norm.weight : tensor<2048xf32>
    %201 = torch_c.from_builtin_tensor %__auto.blk.11.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.11.ffn_gate.weight = util.global.load @__auto.blk.11.ffn_gate.weight : tensor<5632x2176xi8>
    %202 = torch_c.from_builtin_tensor %__auto.blk.11.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.11.ffn_up.weight = util.global.load @__auto.blk.11.ffn_up.weight : tensor<5632x2176xi8>
    %203 = torch_c.from_builtin_tensor %__auto.blk.11.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.11.ffn_down.weight = util.global.load @__auto.blk.11.ffn_down.weight : tensor<2048x5984xi8>
    %204 = torch_c.from_builtin_tensor %__auto.blk.11.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %205 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %206 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %207 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %208 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %209 = torch.vtensor.literal(dense<12> : tensor<si64>) : !torch.vtensor<[],si64>
    %210 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %211 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %212 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.12.attn_norm.weight = util.global.load @__auto.blk.12.attn_norm.weight : tensor<2048xf32>
    %213 = torch_c.from_builtin_tensor %__auto.blk.12.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.12.attn_q.weight = util.global.load @__auto.blk.12.attn_q.weight : tensor<2048x2176xi8>
    %214 = torch_c.from_builtin_tensor %__auto.blk.12.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.12.attn_k.weight = util.global.load @__auto.blk.12.attn_k.weight : tensor<256x2176xi8>
    %215 = torch_c.from_builtin_tensor %__auto.blk.12.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.12.attn_v.weight = util.global.load @__auto.blk.12.attn_v.weight : tensor<256x2176xi8>
    %216 = torch_c.from_builtin_tensor %__auto.blk.12.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.12.attn_output.weight = util.global.load @__auto.blk.12.attn_output.weight : tensor<2048x2176xi8>
    %217 = torch_c.from_builtin_tensor %__auto.blk.12.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.12.ffn_norm.weight = util.global.load @__auto.blk.12.ffn_norm.weight : tensor<2048xf32>
    %218 = torch_c.from_builtin_tensor %__auto.blk.12.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.12.ffn_gate.weight = util.global.load @__auto.blk.12.ffn_gate.weight : tensor<5632x2176xi8>
    %219 = torch_c.from_builtin_tensor %__auto.blk.12.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.12.ffn_up.weight = util.global.load @__auto.blk.12.ffn_up.weight : tensor<5632x2176xi8>
    %220 = torch_c.from_builtin_tensor %__auto.blk.12.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.12.ffn_down.weight = util.global.load @__auto.blk.12.ffn_down.weight : tensor<2048x5984xi8>
    %221 = torch_c.from_builtin_tensor %__auto.blk.12.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %222 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %223 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %224 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %225 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %226 = torch.vtensor.literal(dense<13> : tensor<si64>) : !torch.vtensor<[],si64>
    %227 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %228 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %229 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.13.attn_norm.weight = util.global.load @__auto.blk.13.attn_norm.weight : tensor<2048xf32>
    %230 = torch_c.from_builtin_tensor %__auto.blk.13.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.13.attn_q.weight = util.global.load @__auto.blk.13.attn_q.weight : tensor<2048x2176xi8>
    %231 = torch_c.from_builtin_tensor %__auto.blk.13.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.13.attn_k.weight = util.global.load @__auto.blk.13.attn_k.weight : tensor<256x2176xi8>
    %232 = torch_c.from_builtin_tensor %__auto.blk.13.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.13.attn_v.weight = util.global.load @__auto.blk.13.attn_v.weight : tensor<256x2176xi8>
    %233 = torch_c.from_builtin_tensor %__auto.blk.13.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.13.attn_output.weight = util.global.load @__auto.blk.13.attn_output.weight : tensor<2048x2176xi8>
    %234 = torch_c.from_builtin_tensor %__auto.blk.13.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.13.ffn_norm.weight = util.global.load @__auto.blk.13.ffn_norm.weight : tensor<2048xf32>
    %235 = torch_c.from_builtin_tensor %__auto.blk.13.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.13.ffn_gate.weight = util.global.load @__auto.blk.13.ffn_gate.weight : tensor<5632x2176xi8>
    %236 = torch_c.from_builtin_tensor %__auto.blk.13.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.13.ffn_up.weight = util.global.load @__auto.blk.13.ffn_up.weight : tensor<5632x2176xi8>
    %237 = torch_c.from_builtin_tensor %__auto.blk.13.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.13.ffn_down.weight = util.global.load @__auto.blk.13.ffn_down.weight : tensor<2048x5984xi8>
    %238 = torch_c.from_builtin_tensor %__auto.blk.13.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %239 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %240 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %241 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %242 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %243 = torch.vtensor.literal(dense<14> : tensor<si64>) : !torch.vtensor<[],si64>
    %244 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %245 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %246 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.14.attn_norm.weight = util.global.load @__auto.blk.14.attn_norm.weight : tensor<2048xf32>
    %247 = torch_c.from_builtin_tensor %__auto.blk.14.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.14.attn_q.weight = util.global.load @__auto.blk.14.attn_q.weight : tensor<2048x2176xi8>
    %248 = torch_c.from_builtin_tensor %__auto.blk.14.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.14.attn_k.weight = util.global.load @__auto.blk.14.attn_k.weight : tensor<256x2176xi8>
    %249 = torch_c.from_builtin_tensor %__auto.blk.14.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.14.attn_v.weight = util.global.load @__auto.blk.14.attn_v.weight : tensor<256x2176xi8>
    %250 = torch_c.from_builtin_tensor %__auto.blk.14.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.14.attn_output.weight = util.global.load @__auto.blk.14.attn_output.weight : tensor<2048x2176xi8>
    %251 = torch_c.from_builtin_tensor %__auto.blk.14.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.14.ffn_norm.weight = util.global.load @__auto.blk.14.ffn_norm.weight : tensor<2048xf32>
    %252 = torch_c.from_builtin_tensor %__auto.blk.14.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.14.ffn_gate.weight = util.global.load @__auto.blk.14.ffn_gate.weight : tensor<5632x2176xi8>
    %253 = torch_c.from_builtin_tensor %__auto.blk.14.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.14.ffn_up.weight = util.global.load @__auto.blk.14.ffn_up.weight : tensor<5632x2176xi8>
    %254 = torch_c.from_builtin_tensor %__auto.blk.14.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.14.ffn_down.weight = util.global.load @__auto.blk.14.ffn_down.weight : tensor<2048x5984xi8>
    %255 = torch_c.from_builtin_tensor %__auto.blk.14.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %256 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %257 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %258 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %259 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %260 = torch.vtensor.literal(dense<15> : tensor<si64>) : !torch.vtensor<[],si64>
    %261 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %262 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %263 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.15.attn_norm.weight = util.global.load @__auto.blk.15.attn_norm.weight : tensor<2048xf32>
    %264 = torch_c.from_builtin_tensor %__auto.blk.15.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.15.attn_q.weight = util.global.load @__auto.blk.15.attn_q.weight : tensor<2048x2176xi8>
    %265 = torch_c.from_builtin_tensor %__auto.blk.15.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.15.attn_k.weight = util.global.load @__auto.blk.15.attn_k.weight : tensor<256x2176xi8>
    %266 = torch_c.from_builtin_tensor %__auto.blk.15.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.15.attn_v.weight = util.global.load @__auto.blk.15.attn_v.weight : tensor<256x2176xi8>
    %267 = torch_c.from_builtin_tensor %__auto.blk.15.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.15.attn_output.weight = util.global.load @__auto.blk.15.attn_output.weight : tensor<2048x2176xi8>
    %268 = torch_c.from_builtin_tensor %__auto.blk.15.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.15.ffn_norm.weight = util.global.load @__auto.blk.15.ffn_norm.weight : tensor<2048xf32>
    %269 = torch_c.from_builtin_tensor %__auto.blk.15.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.15.ffn_gate.weight = util.global.load @__auto.blk.15.ffn_gate.weight : tensor<5632x2176xi8>
    %270 = torch_c.from_builtin_tensor %__auto.blk.15.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.15.ffn_up.weight = util.global.load @__auto.blk.15.ffn_up.weight : tensor<5632x2176xi8>
    %271 = torch_c.from_builtin_tensor %__auto.blk.15.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.15.ffn_down.weight = util.global.load @__auto.blk.15.ffn_down.weight : tensor<2048x5984xi8>
    %272 = torch_c.from_builtin_tensor %__auto.blk.15.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %273 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %274 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %275 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %276 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %277 = torch.vtensor.literal(dense<16> : tensor<si64>) : !torch.vtensor<[],si64>
    %278 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %279 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %280 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.16.attn_norm.weight = util.global.load @__auto.blk.16.attn_norm.weight : tensor<2048xf32>
    %281 = torch_c.from_builtin_tensor %__auto.blk.16.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.16.attn_q.weight = util.global.load @__auto.blk.16.attn_q.weight : tensor<2048x2176xi8>
    %282 = torch_c.from_builtin_tensor %__auto.blk.16.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.16.attn_k.weight = util.global.load @__auto.blk.16.attn_k.weight : tensor<256x2176xi8>
    %283 = torch_c.from_builtin_tensor %__auto.blk.16.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.16.attn_v.weight = util.global.load @__auto.blk.16.attn_v.weight : tensor<256x2176xi8>
    %284 = torch_c.from_builtin_tensor %__auto.blk.16.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.16.attn_output.weight = util.global.load @__auto.blk.16.attn_output.weight : tensor<2048x2176xi8>
    %285 = torch_c.from_builtin_tensor %__auto.blk.16.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.16.ffn_norm.weight = util.global.load @__auto.blk.16.ffn_norm.weight : tensor<2048xf32>
    %286 = torch_c.from_builtin_tensor %__auto.blk.16.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.16.ffn_gate.weight = util.global.load @__auto.blk.16.ffn_gate.weight : tensor<5632x2176xi8>
    %287 = torch_c.from_builtin_tensor %__auto.blk.16.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.16.ffn_up.weight = util.global.load @__auto.blk.16.ffn_up.weight : tensor<5632x2176xi8>
    %288 = torch_c.from_builtin_tensor %__auto.blk.16.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.16.ffn_down.weight = util.global.load @__auto.blk.16.ffn_down.weight : tensor<2048x5984xi8>
    %289 = torch_c.from_builtin_tensor %__auto.blk.16.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %290 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %291 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %292 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %293 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %294 = torch.vtensor.literal(dense<17> : tensor<si64>) : !torch.vtensor<[],si64>
    %295 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %296 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %297 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.17.attn_norm.weight = util.global.load @__auto.blk.17.attn_norm.weight : tensor<2048xf32>
    %298 = torch_c.from_builtin_tensor %__auto.blk.17.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.17.attn_q.weight = util.global.load @__auto.blk.17.attn_q.weight : tensor<2048x2176xi8>
    %299 = torch_c.from_builtin_tensor %__auto.blk.17.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.17.attn_k.weight = util.global.load @__auto.blk.17.attn_k.weight : tensor<256x2176xi8>
    %300 = torch_c.from_builtin_tensor %__auto.blk.17.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.17.attn_v.weight = util.global.load @__auto.blk.17.attn_v.weight : tensor<256x2176xi8>
    %301 = torch_c.from_builtin_tensor %__auto.blk.17.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.17.attn_output.weight = util.global.load @__auto.blk.17.attn_output.weight : tensor<2048x2176xi8>
    %302 = torch_c.from_builtin_tensor %__auto.blk.17.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.17.ffn_norm.weight = util.global.load @__auto.blk.17.ffn_norm.weight : tensor<2048xf32>
    %303 = torch_c.from_builtin_tensor %__auto.blk.17.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.17.ffn_gate.weight = util.global.load @__auto.blk.17.ffn_gate.weight : tensor<5632x2176xi8>
    %304 = torch_c.from_builtin_tensor %__auto.blk.17.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.17.ffn_up.weight = util.global.load @__auto.blk.17.ffn_up.weight : tensor<5632x2176xi8>
    %305 = torch_c.from_builtin_tensor %__auto.blk.17.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.17.ffn_down.weight = util.global.load @__auto.blk.17.ffn_down.weight : tensor<2048x5984xi8>
    %306 = torch_c.from_builtin_tensor %__auto.blk.17.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %307 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %308 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %309 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %310 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %311 = torch.vtensor.literal(dense<18> : tensor<si64>) : !torch.vtensor<[],si64>
    %312 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %313 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %314 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.18.attn_norm.weight = util.global.load @__auto.blk.18.attn_norm.weight : tensor<2048xf32>
    %315 = torch_c.from_builtin_tensor %__auto.blk.18.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.18.attn_q.weight = util.global.load @__auto.blk.18.attn_q.weight : tensor<2048x2176xi8>
    %316 = torch_c.from_builtin_tensor %__auto.blk.18.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.18.attn_k.weight = util.global.load @__auto.blk.18.attn_k.weight : tensor<256x2176xi8>
    %317 = torch_c.from_builtin_tensor %__auto.blk.18.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.18.attn_v.weight = util.global.load @__auto.blk.18.attn_v.weight : tensor<256x2176xi8>
    %318 = torch_c.from_builtin_tensor %__auto.blk.18.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.18.attn_output.weight = util.global.load @__auto.blk.18.attn_output.weight : tensor<2048x2176xi8>
    %319 = torch_c.from_builtin_tensor %__auto.blk.18.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.18.ffn_norm.weight = util.global.load @__auto.blk.18.ffn_norm.weight : tensor<2048xf32>
    %320 = torch_c.from_builtin_tensor %__auto.blk.18.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.18.ffn_gate.weight = util.global.load @__auto.blk.18.ffn_gate.weight : tensor<5632x2176xi8>
    %321 = torch_c.from_builtin_tensor %__auto.blk.18.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.18.ffn_up.weight = util.global.load @__auto.blk.18.ffn_up.weight : tensor<5632x2176xi8>
    %322 = torch_c.from_builtin_tensor %__auto.blk.18.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.18.ffn_down.weight = util.global.load @__auto.blk.18.ffn_down.weight : tensor<2048x5984xi8>
    %323 = torch_c.from_builtin_tensor %__auto.blk.18.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %324 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %325 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %326 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %327 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %328 = torch.vtensor.literal(dense<19> : tensor<si64>) : !torch.vtensor<[],si64>
    %329 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %330 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %331 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.19.attn_norm.weight = util.global.load @__auto.blk.19.attn_norm.weight : tensor<2048xf32>
    %332 = torch_c.from_builtin_tensor %__auto.blk.19.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.19.attn_q.weight = util.global.load @__auto.blk.19.attn_q.weight : tensor<2048x2176xi8>
    %333 = torch_c.from_builtin_tensor %__auto.blk.19.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.19.attn_k.weight = util.global.load @__auto.blk.19.attn_k.weight : tensor<256x2176xi8>
    %334 = torch_c.from_builtin_tensor %__auto.blk.19.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.19.attn_v.weight = util.global.load @__auto.blk.19.attn_v.weight : tensor<256x2176xi8>
    %335 = torch_c.from_builtin_tensor %__auto.blk.19.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.19.attn_output.weight = util.global.load @__auto.blk.19.attn_output.weight : tensor<2048x2176xi8>
    %336 = torch_c.from_builtin_tensor %__auto.blk.19.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.19.ffn_norm.weight = util.global.load @__auto.blk.19.ffn_norm.weight : tensor<2048xf32>
    %337 = torch_c.from_builtin_tensor %__auto.blk.19.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.19.ffn_gate.weight = util.global.load @__auto.blk.19.ffn_gate.weight : tensor<5632x2176xi8>
    %338 = torch_c.from_builtin_tensor %__auto.blk.19.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.19.ffn_up.weight = util.global.load @__auto.blk.19.ffn_up.weight : tensor<5632x2176xi8>
    %339 = torch_c.from_builtin_tensor %__auto.blk.19.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.19.ffn_down.weight = util.global.load @__auto.blk.19.ffn_down.weight : tensor<2048x5984xi8>
    %340 = torch_c.from_builtin_tensor %__auto.blk.19.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %341 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %342 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %343 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %344 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %345 = torch.vtensor.literal(dense<20> : tensor<si64>) : !torch.vtensor<[],si64>
    %346 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %347 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %348 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.20.attn_norm.weight = util.global.load @__auto.blk.20.attn_norm.weight : tensor<2048xf32>
    %349 = torch_c.from_builtin_tensor %__auto.blk.20.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.20.attn_q.weight = util.global.load @__auto.blk.20.attn_q.weight : tensor<2048x2176xi8>
    %350 = torch_c.from_builtin_tensor %__auto.blk.20.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.20.attn_k.weight = util.global.load @__auto.blk.20.attn_k.weight : tensor<256x2176xi8>
    %351 = torch_c.from_builtin_tensor %__auto.blk.20.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.20.attn_v.weight = util.global.load @__auto.blk.20.attn_v.weight : tensor<256x2176xi8>
    %352 = torch_c.from_builtin_tensor %__auto.blk.20.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.20.attn_output.weight = util.global.load @__auto.blk.20.attn_output.weight : tensor<2048x2176xi8>
    %353 = torch_c.from_builtin_tensor %__auto.blk.20.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.20.ffn_norm.weight = util.global.load @__auto.blk.20.ffn_norm.weight : tensor<2048xf32>
    %354 = torch_c.from_builtin_tensor %__auto.blk.20.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.20.ffn_gate.weight = util.global.load @__auto.blk.20.ffn_gate.weight : tensor<5632x2176xi8>
    %355 = torch_c.from_builtin_tensor %__auto.blk.20.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.20.ffn_up.weight = util.global.load @__auto.blk.20.ffn_up.weight : tensor<5632x2176xi8>
    %356 = torch_c.from_builtin_tensor %__auto.blk.20.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.20.ffn_down.weight = util.global.load @__auto.blk.20.ffn_down.weight : tensor<2048x5984xi8>
    %357 = torch_c.from_builtin_tensor %__auto.blk.20.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %358 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %359 = torch.vtensor.literal(dense<1.000000e+00> : tensor<f32>) : !torch.vtensor<[],f32>
    %360 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %361 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %362 = torch.vtensor.literal(dense<21> : tensor<si64>) : !torch.vtensor<[],si64>
    %363 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %364 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %365 = torch.vtensor.literal(dense<0xFC00> : tensor<f16>) : !torch.vtensor<[],f16>
    %__auto.blk.21.attn_norm.weight = util.global.load @__auto.blk.21.attn_norm.weight : tensor<2048xf32>
    %366 = torch_c.from_builtin_tensor %__auto.blk.21.attn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.21.attn_q.weight = util.global.load @__auto.blk.21.attn_q.weight : tensor<2048x2176xi8>
    %367 = torch_c.from_builtin_tensor %__auto.blk.21.attn_q.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.21.attn_k.weight = util.global.load @__auto.blk.21.attn_k.weight : tensor<256x2176xi8>
    %368 = torch_c.from_builtin_tensor %__auto.blk.21.attn_k.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.21.attn_v.weight = util.global.load @__auto.blk.21.attn_v.weight : tensor<256x2176xi8>
    %369 = torch_c.from_builtin_tensor %__auto.blk.21.attn_v.weight : tensor<256x2176xi8> -> !torch.vtensor<[256,2176],ui8>
    %__auto.blk.21.attn_output.weight = util.global.load @__auto.blk.21.attn_output.weight : tensor<2048x2176xi8>
    %370 = torch_c.from_builtin_tensor %__auto.blk.21.attn_output.weight : tensor<2048x2176xi8> -> !torch.vtensor<[2048,2176],ui8>
    %__auto.blk.21.ffn_norm.weight = util.global.load @__auto.blk.21.ffn_norm.weight : tensor<2048xf32>
    %371 = torch_c.from_builtin_tensor %__auto.blk.21.ffn_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.blk.21.ffn_gate.weight = util.global.load @__auto.blk.21.ffn_gate.weight : tensor<5632x2176xi8>
    %372 = torch_c.from_builtin_tensor %__auto.blk.21.ffn_gate.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.21.ffn_up.weight = util.global.load @__auto.blk.21.ffn_up.weight : tensor<5632x2176xi8>
    %373 = torch_c.from_builtin_tensor %__auto.blk.21.ffn_up.weight : tensor<5632x2176xi8> -> !torch.vtensor<[5632,2176],ui8>
    %__auto.blk.21.ffn_down.weight = util.global.load @__auto.blk.21.ffn_down.weight : tensor<2048x5984xi8>
    %374 = torch_c.from_builtin_tensor %__auto.blk.21.ffn_down.weight : tensor<2048x5984xi8> -> !torch.vtensor<[2048,5984],ui8>
    %__auto.output_norm.weight = util.global.load @__auto.output_norm.weight : tensor<2048xf32>
    %375 = torch_c.from_builtin_tensor %__auto.output_norm.weight : tensor<2048xf32> -> !torch.vtensor<[2048],f32>
    %__auto.output.weight = util.global.load @__auto.output.weight : tensor<32000x2176xi8>
    %376 = torch_c.from_builtin_tensor %__auto.output.weight : tensor<32000x2176xi8> -> !torch.vtensor<[32000,2176],ui8>
    %377 = torch.copy.to_vtensor %arg4 : !torch.vtensor<[?,360448],f16>
    %378 = torch.symbolic_int "s23" {min_val = 2, max_val = 63} : !torch.int
    %379 = torch.symbolic_int "s98" {min_val = 0, max_val = 9223372036854775807} : !torch.int
    torch.bind_symbolic_shape %arg3, [%378], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %377, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int1 = torch.constant.int 1
    %380 = torch.aten.size.int %arg3, %int1 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.int
    %int0 = torch.constant.int 0
    %381 = torch.aten.size.int %377, %int0 : !torch.vtensor<[?,360448],f16>, !torch.int -> !torch.int
    %int2 = torch.constant.int 2
    %382 = torch.aten.view.dtype %0, %int2 : !torch.vtensor<[32000,2176],ui8>, !torch.int -> !torch.vtensor<[32000,1088],si16>
    %383 = torch.aten.detach %382 : !torch.vtensor<[32000,1088],si16> -> !torch.vtensor<[32000,1088],si16>
    %int-1 = torch.constant.int -1
    %int17 = torch.constant.int 17
    %384 = torch.prim.ListConstruct %int-1, %int17 : (!torch.int, !torch.int) -> !torch.list<int>
    %385 = torch.aten.view %383, %384 : !torch.vtensor<[32000,1088],si16>, !torch.list<int> -> !torch.vtensor<[2048000,17],si16>
    %int32000 = torch.constant.int 32000
    %int-1_0 = torch.constant.int -1
    %int17_1 = torch.constant.int 17
    %386 = torch.prim.ListConstruct %int32000, %int-1_0, %int17_1 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %387 = torch.aten.view %385, %386 : !torch.vtensor<[2048000,17],si16>, !torch.list<int> -> !torch.vtensor<[32000,64,17],si16>
    %int2_2 = torch.constant.int 2
    %int0_3 = torch.constant.int 0
    %int1_4 = torch.constant.int 1
    %int1_5 = torch.constant.int 1
    %388 = torch.aten.slice.Tensor %387, %int2_2, %int0_3, %int1_4, %int1_5 : !torch.vtensor<[32000,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[32000,64,1],si16>
    %int5 = torch.constant.int 5
    %389 = torch.aten.view.dtype %388, %int5 : !torch.vtensor<[32000,64,1],si16>, !torch.int -> !torch.vtensor<[32000,64,1],f16>
    %390 = torch.aten.detach %389 : !torch.vtensor<[32000,64,1],f16> -> !torch.vtensor<[32000,64,1],f16>
    %int2_6 = torch.constant.int 2
    %int1_7 = torch.constant.int 1
    %int9223372036854775807 = torch.constant.int 9223372036854775807
    %int1_8 = torch.constant.int 1
    %391 = torch.aten.slice.Tensor %387, %int2_6, %int1_7, %int9223372036854775807, %int1_8 : !torch.vtensor<[32000,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[32000,64,16],si16>
    %int1_9 = torch.constant.int 1
    %392 = torch.aten.view.dtype %391, %int1_9 : !torch.vtensor<[32000,64,16],si16>, !torch.int -> !torch.vtensor<[32000,64,32],si8>
    %393 = torch.aten.detach %392 : !torch.vtensor<[32000,64,32],si8> -> !torch.vtensor<[32000,64,32],si8>
    %none = torch.constant.none
    %none_10 = torch.constant.none
    %int5_11 = torch.constant.int 5
    %cpu = torch.constant.device "cpu"
    %int0_12 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %390, %none, %none_10, %int5_11, %cpu, %int0_12 : !torch.vtensor<[32000,64,1],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_13 = torch.constant.none
    %none_14 = torch.constant.none
    %int1_15 = torch.constant.int 1
    %cpu_16 = torch.constant.device "cpu"
    %int0_17 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %393, %none_13, %none_14, %int1_15, %cpu_16, %int0_17 : !torch.vtensor<[32000,64,32],si8>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_18 = torch.constant.int 5
    %394 = torch.prims.convert_element_type %393, %int5_18 : !torch.vtensor<[32000,64,32],si8>, !torch.int -> !torch.vtensor<[32000,64,32],f16>
    %395 = torch.aten.mul.Tensor %390, %394 : !torch.vtensor<[32000,64,1],f16>, !torch.vtensor<[32000,64,32],f16> -> !torch.vtensor<[32000,64,32],f16>
    %int32000_19 = torch.constant.int 32000
    %int2048 = torch.constant.int 2048
    %396 = torch.prim.ListConstruct %int32000_19, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %397 = torch.aten.view %395, %396 : !torch.vtensor<[32000,64,32],f16>, !torch.list<int> -> !torch.vtensor<[32000,2048],f16>
    %int-1_20 = torch.constant.int -1
    %false = torch.constant.bool false
    %false_21 = torch.constant.bool false
    %398 = torch.aten.embedding %397, %arg0, %int-1_20, %false, %false_21 : !torch.vtensor<[32000,2048],f16>, !torch.vtensor<[4,1],si64>, !torch.int, !torch.bool, !torch.bool -> !torch.vtensor<[4,1,2048],f16>
    %none_22 = torch.constant.none
    %none_23 = torch.constant.none
    %int5_24 = torch.constant.int 5
    %cpu_25 = torch.constant.device "cpu"
    %int0_26 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %398, %none_22, %none_23, %int5_24, %cpu_25, %int0_26 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6 = torch.constant.int 6
    %399 = torch.prims.convert_element_type %398, %int6 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_27 = torch.constant.int 2
    %400 = torch.aten.pow.Tensor_Scalar %399, %int2_27 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_28 = torch.constant.int -1
    %401 = torch.prim.ListConstruct %int-1_28 : (!torch.int) -> !torch.list<int>
    %true = torch.constant.bool true
    %none_29 = torch.constant.none
    %402 = torch.aten.mean.dim %400, %401, %true, %none_29 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06 = torch.constant.float 9.9999997473787516E-6
    %int1_30 = torch.constant.int 1
    %403 = torch.aten.add.Scalar %402, %float9.999990e-06, %int1_30 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %404 = torch.aten.rsqrt %403 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %405 = torch.aten.mul.Tensor %399, %404 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_31 = torch.constant.none
    %none_32 = torch.constant.none
    %int6_33 = torch.constant.int 6
    %cpu_34 = torch.constant.device "cpu"
    %int0_35 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %405, %none_31, %none_32, %int6_33, %cpu_34, %int0_35 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_36 = torch.constant.int 5
    %406 = torch.prims.convert_element_type %405, %int5_36 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %407 = torch.aten.mul.Tensor %9, %406 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_37 = torch.constant.none
    %none_38 = torch.constant.none
    %int6_39 = torch.constant.int 6
    %cpu_40 = torch.constant.device "cpu"
    %int0_41 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %407, %none_37, %none_38, %int6_39, %cpu_40, %int0_41 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_42 = torch.constant.int 5
    %408 = torch.prims.convert_element_type %407, %int5_42 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_43 = torch.constant.int 2
    %409 = torch.aten.view.dtype %10, %int2_43 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %410 = torch.aten.detach %409 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_44 = torch.constant.int -1
    %int17_45 = torch.constant.int 17
    %411 = torch.prim.ListConstruct %int-1_44, %int17_45 : (!torch.int, !torch.int) -> !torch.list<int>
    %412 = torch.aten.view %410, %411 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_46 = torch.constant.int 2048
    %int-1_47 = torch.constant.int -1
    %int17_48 = torch.constant.int 17
    %413 = torch.prim.ListConstruct %int2048_46, %int-1_47, %int17_48 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %414 = torch.aten.view %412, %413 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_49 = torch.constant.int 2
    %int0_50 = torch.constant.int 0
    %int1_51 = torch.constant.int 1
    %int1_52 = torch.constant.int 1
    %415 = torch.aten.slice.Tensor %414, %int2_49, %int0_50, %int1_51, %int1_52 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_53 = torch.constant.int 5
    %416 = torch.aten.view.dtype %415, %int5_53 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %417 = torch.aten.detach %416 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_54 = torch.constant.int 2
    %int1_55 = torch.constant.int 1
    %int9223372036854775807_56 = torch.constant.int 9223372036854775807
    %int1_57 = torch.constant.int 1
    %418 = torch.aten.slice.Tensor %414, %int2_54, %int1_55, %int9223372036854775807_56, %int1_57 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_58 = torch.constant.int 1
    %419 = torch.aten.view.dtype %418, %int1_58 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %420 = torch.aten.detach %419 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %421 = torch_c.to_builtin_tensor %408 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast = tensor.cast %421 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %422 = torch_c.to_builtin_tensor %417 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %423 = torch_c.to_builtin_tensor %420 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %424 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast, %422, %423) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_59 = tensor.cast %424 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %425 = torch_c.from_builtin_tensor %cast_59 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_60 = torch.constant.int 2
    %426 = torch.aten.view.dtype %11, %int2_60 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %427 = torch.aten.detach %426 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_61 = torch.constant.int -1
    %int17_62 = torch.constant.int 17
    %428 = torch.prim.ListConstruct %int-1_61, %int17_62 : (!torch.int, !torch.int) -> !torch.list<int>
    %429 = torch.aten.view %427, %428 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256 = torch.constant.int 256
    %int-1_63 = torch.constant.int -1
    %int17_64 = torch.constant.int 17
    %430 = torch.prim.ListConstruct %int256, %int-1_63, %int17_64 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %431 = torch.aten.view %429, %430 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_65 = torch.constant.int 2
    %int0_66 = torch.constant.int 0
    %int1_67 = torch.constant.int 1
    %int1_68 = torch.constant.int 1
    %432 = torch.aten.slice.Tensor %431, %int2_65, %int0_66, %int1_67, %int1_68 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_69 = torch.constant.int 5
    %433 = torch.aten.view.dtype %432, %int5_69 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %434 = torch.aten.detach %433 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_70 = torch.constant.int 2
    %int1_71 = torch.constant.int 1
    %int9223372036854775807_72 = torch.constant.int 9223372036854775807
    %int1_73 = torch.constant.int 1
    %435 = torch.aten.slice.Tensor %431, %int2_70, %int1_71, %int9223372036854775807_72, %int1_73 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_74 = torch.constant.int 1
    %436 = torch.aten.view.dtype %435, %int1_74 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %437 = torch.aten.detach %436 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %438 = torch_c.to_builtin_tensor %408 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_75 = tensor.cast %438 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %439 = torch_c.to_builtin_tensor %434 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %440 = torch_c.to_builtin_tensor %437 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %441 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_75, %439, %440) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_76 = tensor.cast %441 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %442 = torch_c.from_builtin_tensor %cast_76 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_77 = torch.constant.int 2
    %443 = torch.aten.view.dtype %12, %int2_77 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %444 = torch.aten.detach %443 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_78 = torch.constant.int -1
    %int17_79 = torch.constant.int 17
    %445 = torch.prim.ListConstruct %int-1_78, %int17_79 : (!torch.int, !torch.int) -> !torch.list<int>
    %446 = torch.aten.view %444, %445 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_80 = torch.constant.int 256
    %int-1_81 = torch.constant.int -1
    %int17_82 = torch.constant.int 17
    %447 = torch.prim.ListConstruct %int256_80, %int-1_81, %int17_82 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %448 = torch.aten.view %446, %447 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_83 = torch.constant.int 2
    %int0_84 = torch.constant.int 0
    %int1_85 = torch.constant.int 1
    %int1_86 = torch.constant.int 1
    %449 = torch.aten.slice.Tensor %448, %int2_83, %int0_84, %int1_85, %int1_86 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_87 = torch.constant.int 5
    %450 = torch.aten.view.dtype %449, %int5_87 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %451 = torch.aten.detach %450 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_88 = torch.constant.int 2
    %int1_89 = torch.constant.int 1
    %int9223372036854775807_90 = torch.constant.int 9223372036854775807
    %int1_91 = torch.constant.int 1
    %452 = torch.aten.slice.Tensor %448, %int2_88, %int1_89, %int9223372036854775807_90, %int1_91 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_92 = torch.constant.int 1
    %453 = torch.aten.view.dtype %452, %int1_92 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %454 = torch.aten.detach %453 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %455 = torch_c.to_builtin_tensor %408 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_93 = tensor.cast %455 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %456 = torch_c.to_builtin_tensor %451 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %457 = torch_c.to_builtin_tensor %454 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %458 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_93, %456, %457) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_94 = tensor.cast %458 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %459 = torch_c.from_builtin_tensor %cast_94 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4 = torch.constant.int 4
    %int1_95 = torch.constant.int 1
    %int32 = torch.constant.int 32
    %int64 = torch.constant.int 64
    %460 = torch.prim.ListConstruct %int4, %int1_95, %int32, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %461 = torch.aten.view %425, %460 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_96 = torch.constant.int 4
    %int1_97 = torch.constant.int 1
    %int4_98 = torch.constant.int 4
    %int64_99 = torch.constant.int 64
    %462 = torch.prim.ListConstruct %int4_96, %int1_97, %int4_98, %int64_99 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %463 = torch.aten.view %442, %462 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_100 = torch.constant.int 4
    %int1_101 = torch.constant.int 1
    %int4_102 = torch.constant.int 4
    %int64_103 = torch.constant.int 64
    %464 = torch.prim.ListConstruct %int4_100, %int1_101, %int4_102, %int64_103 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %465 = torch.aten.view %459, %464 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_104 = torch.constant.int 0
    %int1_105 = torch.constant.int 1
    %none_106 = torch.constant.none
    %none_107 = torch.constant.none
    %cpu_108 = torch.constant.device "cpu"
    %false_109 = torch.constant.bool false
    %466 = torch.aten.arange.start %int0_104, %int1_105, %none_106, %none_107, %cpu_108, %false_109 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_110 = torch.constant.int 0
    %467 = torch.aten.unsqueeze %466, %int0_110 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_111 = torch.constant.int 1
    %468 = torch.aten.unsqueeze %arg2, %int1_111 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_112 = torch.constant.int 1
    %469 = torch.aten.add.Tensor %467, %468, %int1_112 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_113 = torch.constant.int 0
    %int64_114 = torch.constant.int 64
    %int2_115 = torch.constant.int 2
    %none_116 = torch.constant.none
    %none_117 = torch.constant.none
    %cpu_118 = torch.constant.device "cpu"
    %false_119 = torch.constant.bool false
    %470 = torch.aten.arange.start_step %int0_113, %int64_114, %int2_115, %none_116, %none_117, %cpu_118, %false_119 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_120 = torch.constant.none
    %none_121 = torch.constant.none
    %int4_122 = torch.constant.int 4
    %cpu_123 = torch.constant.device "cpu"
    %int0_124 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %470, %none_120, %none_121, %int4_122, %cpu_123, %int0_124 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_125 = torch.constant.int 6
    %471 = torch.prims.convert_element_type %470, %int6_125 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_126 = torch.constant.int 64
    %472 = torch.aten.div.Scalar %471, %int64_126 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04 = torch.constant.float 1.000000e+04
    %473 = torch.aten.pow.Scalar %float1.000000e04, %472 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %474 = torch.aten.reciprocal %473 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00 = torch.constant.float 1.000000e+00
    %475 = torch.aten.mul.Scalar %474, %float1.000000e00 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_127 = torch.constant.none
    %476 = torch.aten.clone %1, %none_127 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_128 = torch.constant.int 0
    %477 = torch.aten.unsqueeze %475, %int0_128 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_129 = torch.constant.int 2
    %478 = torch.aten.unsqueeze %477, %int2_129 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_130 = torch.constant.none
    %none_131 = torch.constant.none
    %int6_132 = torch.constant.int 6
    %cpu_133 = torch.constant.device "cpu"
    %int0_134 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %478, %none_130, %none_131, %int6_132, %cpu_133, %int0_134 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_135 = torch.constant.int 4
    %int-1_136 = torch.constant.int -1
    %int1_137 = torch.constant.int 1
    %479 = torch.prim.ListConstruct %int4_135, %int-1_136, %int1_137 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_138 = torch.constant.bool false
    %480 = torch.aten.expand %478, %479, %false_138 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_139 = torch.constant.int 1
    %481 = torch.aten.unsqueeze %469, %int1_139 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_140 = torch.constant.none
    %none_141 = torch.constant.none
    %int4_142 = torch.constant.int 4
    %cpu_143 = torch.constant.device "cpu"
    %int0_144 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %481, %none_140, %none_141, %int4_142, %cpu_143, %int0_144 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_145 = torch.constant.int 6
    %482 = torch.prims.convert_element_type %481, %int6_145 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %483 = torch.aten.matmul %480, %482 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_146 = torch.constant.int 1
    %int2_147 = torch.constant.int 2
    %484 = torch.aten.transpose.int %483, %int1_146, %int2_147 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %485 = torch.aten.cos %484 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %486 = torch.aten.mul.Tensor %485, %476 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_148 = torch.constant.none
    %none_149 = torch.constant.none
    %int6_150 = torch.constant.int 6
    %cpu_151 = torch.constant.device "cpu"
    %int0_152 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %486, %none_148, %none_149, %int6_150, %cpu_151, %int0_152 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_153 = torch.constant.int 5
    %487 = torch.prims.convert_element_type %486, %int5_153 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %488 = torch.aten.sin %484 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %489 = torch.aten.mul.Tensor %488, %476 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_154 = torch.constant.none
    %none_155 = torch.constant.none
    %int6_156 = torch.constant.int 6
    %cpu_157 = torch.constant.device "cpu"
    %int0_158 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %489, %none_154, %none_155, %int6_156, %cpu_157, %int0_158 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_159 = torch.constant.int 5
    %490 = torch.prims.convert_element_type %489, %int5_159 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_160 = torch.constant.int 2
    %491 = torch.aten.unsqueeze %487, %int2_160 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_161 = torch.constant.int 2
    %492 = torch.aten.unsqueeze %490, %int2_161 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_162 = torch.constant.none
    %none_163 = torch.constant.none
    %int5_164 = torch.constant.int 5
    %cpu_165 = torch.constant.device "cpu"
    %int0_166 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %491, %none_162, %none_163, %int5_164, %cpu_165, %int0_166 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_167 = torch.constant.none
    %none_168 = torch.constant.none
    %int5_169 = torch.constant.int 5
    %cpu_170 = torch.constant.device "cpu"
    %int0_171 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %492, %none_167, %none_168, %int5_169, %cpu_170, %int0_171 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_172 = torch.constant.none
    %none_173 = torch.constant.none
    %int5_174 = torch.constant.int 5
    %cpu_175 = torch.constant.device "cpu"
    %int0_176 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %461, %none_172, %none_173, %int5_174, %cpu_175, %int0_176 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3 = torch.constant.int 3
    %int0_177 = torch.constant.int 0
    %int64_178 = torch.constant.int 64
    %int2_179 = torch.constant.int 2
    %493 = torch.aten.slice.Tensor %461, %int3, %int0_177, %int64_178, %int2_179 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_180 = torch.constant.int 3
    %int1_181 = torch.constant.int 1
    %int64_182 = torch.constant.int 64
    %int2_183 = torch.constant.int 2
    %494 = torch.aten.slice.Tensor %461, %int3_180, %int1_181, %int64_182, %int2_183 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %495 = torch.aten.mul.Tensor %493, %491 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %496 = torch.aten.mul.Tensor %494, %492 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_184 = torch.constant.int 1
    %497 = torch.aten.sub.Tensor %495, %496, %int1_184 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %498 = torch.aten.mul.Tensor %494, %491 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %499 = torch.aten.mul.Tensor %493, %492 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_185 = torch.constant.int 1
    %500 = torch.aten.add.Tensor %498, %499, %int1_185 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %501 = torch_c.to_builtin_tensor %497 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_186 = tensor.cast %501 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %502 = torch_c.to_builtin_tensor %500 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_187 = tensor.cast %502 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %503 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_186, %cast_187) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_188 = tensor.cast %503 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %504 = torch_c.from_builtin_tensor %cast_188 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_189 = torch.constant.int 4
    %int1_190 = torch.constant.int 1
    %int32_191 = torch.constant.int 32
    %int64_192 = torch.constant.int 64
    %505 = torch.prim.ListConstruct %int4_189, %int1_190, %int32_191, %int64_192 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %506 = torch.aten.view %504, %505 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_193 = torch.constant.none
    %none_194 = torch.constant.none
    %int5_195 = torch.constant.int 5
    %cpu_196 = torch.constant.device "cpu"
    %int0_197 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %506, %none_193, %none_194, %int5_195, %cpu_196, %int0_197 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_198 = torch.constant.int 0
    %int1_199 = torch.constant.int 1
    %none_200 = torch.constant.none
    %none_201 = torch.constant.none
    %cpu_202 = torch.constant.device "cpu"
    %false_203 = torch.constant.bool false
    %507 = torch.aten.arange.start %int0_198, %int1_199, %none_200, %none_201, %cpu_202, %false_203 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_204 = torch.constant.int 0
    %508 = torch.aten.unsqueeze %507, %int0_204 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_205 = torch.constant.int 1
    %509 = torch.aten.unsqueeze %arg2, %int1_205 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_206 = torch.constant.int 1
    %510 = torch.aten.add.Tensor %508, %509, %int1_206 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_207 = torch.constant.int 0
    %int64_208 = torch.constant.int 64
    %int2_209 = torch.constant.int 2
    %none_210 = torch.constant.none
    %none_211 = torch.constant.none
    %cpu_212 = torch.constant.device "cpu"
    %false_213 = torch.constant.bool false
    %511 = torch.aten.arange.start_step %int0_207, %int64_208, %int2_209, %none_210, %none_211, %cpu_212, %false_213 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_214 = torch.constant.none
    %none_215 = torch.constant.none
    %int4_216 = torch.constant.int 4
    %cpu_217 = torch.constant.device "cpu"
    %int0_218 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %511, %none_214, %none_215, %int4_216, %cpu_217, %int0_218 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_219 = torch.constant.int 6
    %512 = torch.prims.convert_element_type %511, %int6_219 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_220 = torch.constant.int 64
    %513 = torch.aten.div.Scalar %512, %int64_220 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_221 = torch.constant.float 1.000000e+04
    %514 = torch.aten.pow.Scalar %float1.000000e04_221, %513 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %515 = torch.aten.reciprocal %514 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_222 = torch.constant.float 1.000000e+00
    %516 = torch.aten.mul.Scalar %515, %float1.000000e00_222 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_223 = torch.constant.none
    %517 = torch.aten.clone %2, %none_223 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_224 = torch.constant.int 0
    %518 = torch.aten.unsqueeze %516, %int0_224 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_225 = torch.constant.int 2
    %519 = torch.aten.unsqueeze %518, %int2_225 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_226 = torch.constant.none
    %none_227 = torch.constant.none
    %int6_228 = torch.constant.int 6
    %cpu_229 = torch.constant.device "cpu"
    %int0_230 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %519, %none_226, %none_227, %int6_228, %cpu_229, %int0_230 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_231 = torch.constant.int 4
    %int-1_232 = torch.constant.int -1
    %int1_233 = torch.constant.int 1
    %520 = torch.prim.ListConstruct %int4_231, %int-1_232, %int1_233 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_234 = torch.constant.bool false
    %521 = torch.aten.expand %519, %520, %false_234 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_235 = torch.constant.int 1
    %522 = torch.aten.unsqueeze %510, %int1_235 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_236 = torch.constant.none
    %none_237 = torch.constant.none
    %int4_238 = torch.constant.int 4
    %cpu_239 = torch.constant.device "cpu"
    %int0_240 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %522, %none_236, %none_237, %int4_238, %cpu_239, %int0_240 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_241 = torch.constant.int 6
    %523 = torch.prims.convert_element_type %522, %int6_241 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %524 = torch.aten.matmul %521, %523 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_242 = torch.constant.int 1
    %int2_243 = torch.constant.int 2
    %525 = torch.aten.transpose.int %524, %int1_242, %int2_243 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %526 = torch.aten.cos %525 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %527 = torch.aten.mul.Tensor %526, %517 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_244 = torch.constant.none
    %none_245 = torch.constant.none
    %int6_246 = torch.constant.int 6
    %cpu_247 = torch.constant.device "cpu"
    %int0_248 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %527, %none_244, %none_245, %int6_246, %cpu_247, %int0_248 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_249 = torch.constant.int 5
    %528 = torch.prims.convert_element_type %527, %int5_249 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %529 = torch.aten.sin %525 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %530 = torch.aten.mul.Tensor %529, %517 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_250 = torch.constant.none
    %none_251 = torch.constant.none
    %int6_252 = torch.constant.int 6
    %cpu_253 = torch.constant.device "cpu"
    %int0_254 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %530, %none_250, %none_251, %int6_252, %cpu_253, %int0_254 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_255 = torch.constant.int 5
    %531 = torch.prims.convert_element_type %530, %int5_255 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_256 = torch.constant.int 2
    %532 = torch.aten.unsqueeze %528, %int2_256 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_257 = torch.constant.int 2
    %533 = torch.aten.unsqueeze %531, %int2_257 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_258 = torch.constant.none
    %none_259 = torch.constant.none
    %int5_260 = torch.constant.int 5
    %cpu_261 = torch.constant.device "cpu"
    %int0_262 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %532, %none_258, %none_259, %int5_260, %cpu_261, %int0_262 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_263 = torch.constant.none
    %none_264 = torch.constant.none
    %int5_265 = torch.constant.int 5
    %cpu_266 = torch.constant.device "cpu"
    %int0_267 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %533, %none_263, %none_264, %int5_265, %cpu_266, %int0_267 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_268 = torch.constant.none
    %none_269 = torch.constant.none
    %int5_270 = torch.constant.int 5
    %cpu_271 = torch.constant.device "cpu"
    %int0_272 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %463, %none_268, %none_269, %int5_270, %cpu_271, %int0_272 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_273 = torch.constant.int 3
    %int0_274 = torch.constant.int 0
    %int64_275 = torch.constant.int 64
    %int2_276 = torch.constant.int 2
    %534 = torch.aten.slice.Tensor %463, %int3_273, %int0_274, %int64_275, %int2_276 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_277 = torch.constant.int 3
    %int1_278 = torch.constant.int 1
    %int64_279 = torch.constant.int 64
    %int2_280 = torch.constant.int 2
    %535 = torch.aten.slice.Tensor %463, %int3_277, %int1_278, %int64_279, %int2_280 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %536 = torch.aten.mul.Tensor %534, %532 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %537 = torch.aten.mul.Tensor %535, %533 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_281 = torch.constant.int 1
    %538 = torch.aten.sub.Tensor %536, %537, %int1_281 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %539 = torch.aten.mul.Tensor %535, %532 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %540 = torch.aten.mul.Tensor %534, %533 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_282 = torch.constant.int 1
    %541 = torch.aten.add.Tensor %539, %540, %int1_282 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %542 = torch_c.to_builtin_tensor %538 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_283 = tensor.cast %542 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %543 = torch_c.to_builtin_tensor %541 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_284 = tensor.cast %543 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %544 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_283, %cast_284) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_285 = tensor.cast %544 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %545 = torch_c.from_builtin_tensor %cast_285 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_286 = torch.constant.int 4
    %int1_287 = torch.constant.int 1
    %int4_288 = torch.constant.int 4
    %int64_289 = torch.constant.int 64
    %546 = torch.prim.ListConstruct %int4_286, %int1_287, %int4_288, %int64_289 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %547 = torch.aten.view %545, %546 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_290 = torch.constant.none
    %none_291 = torch.constant.none
    %int5_292 = torch.constant.int 5
    %cpu_293 = torch.constant.device "cpu"
    %int0_294 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %547, %none_290, %none_291, %int5_292, %cpu_293, %int0_294 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22 = torch.constant.int 22
    %int2_295 = torch.constant.int 2
    %int4_296 = torch.constant.int 4
    %int32_297 = torch.constant.int 32
    %int64_298 = torch.constant.int 64
    %548 = torch.prim.ListConstruct %381, %int22, %int2_295, %int4_296, %int32_297, %int64_298 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %549 = torch.aten.view %377, %548 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %549, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int22_299 = torch.constant.int 22
    %550 = torch.aten.mul.int %381, %int22_299 : !torch.int, !torch.int -> !torch.int
    %int2_300 = torch.constant.int 2
    %551 = torch.aten.mul.int %550, %int2_300 : !torch.int, !torch.int -> !torch.int
    %int4_301 = torch.constant.int 4
    %552 = torch.aten.mul.int %551, %int4_301 : !torch.int, !torch.int -> !torch.int
    %int32_302 = torch.constant.int 32
    %553 = torch.aten.mul.int %552, %int32_302 : !torch.int, !torch.int -> !torch.int
    %int64_303 = torch.constant.int 64
    %554 = torch.prim.ListConstruct %553, %int64_303 : (!torch.int, !torch.int) -> !torch.list<int>
    %555 = torch.aten.view %549, %554 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %555, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int32_304 = torch.constant.int 32
    %556 = torch.aten.floor_divide.Scalar %arg2, %int32_304 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_305 = torch.constant.int 1
    %557 = torch.aten.unsqueeze %556, %int1_305 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_306 = torch.constant.int 1
    %false_307 = torch.constant.bool false
    %558 = torch.aten.gather %arg3, %int1_306, %557, %false_307 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_308 = torch.constant.int 4
    %int1_309 = torch.constant.int 1
    %int1_310 = torch.constant.int 1
    %559 = torch.prim.ListConstruct %int4_308, %int1_309, %int1_310 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %560 = torch.aten.view %558, %559 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_311 = torch.constant.int 32
    %561 = torch.aten.remainder.Scalar %arg2, %int32_311 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_312 = torch.constant.int 4
    %int1_313 = torch.constant.int 1
    %int1_314 = torch.constant.int 1
    %562 = torch.prim.ListConstruct %int4_312, %int1_313, %int1_314 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %563 = torch.aten.view %561, %562 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_315 = torch.constant.int 4
    %none_316 = torch.constant.none
    %none_317 = torch.constant.none
    %cpu_318 = torch.constant.device "cpu"
    %false_319 = torch.constant.bool false
    %564 = torch.aten.arange %int4_315, %none_316, %none_317, %cpu_318, %false_319 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_320 = torch.constant.int 1
    %int1_321 = torch.constant.int 1
    %int4_322 = torch.constant.int 4
    %565 = torch.prim.ListConstruct %int1_320, %int1_321, %int4_322 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %566 = torch.aten.view %564, %565 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_323 = torch.constant.none
    %567 = torch.aten.clone %3, %none_323 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_324 = torch.constant.int 1
    %int1_325 = torch.constant.int 1
    %int1_326 = torch.constant.int 1
    %568 = torch.prim.ListConstruct %int1_324, %int1_325, %int1_326 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %569 = torch.aten.view %567, %568 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_327 = torch.constant.int 22
    %570 = torch.aten.mul.Scalar %560, %int22_327 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int0_328 = torch.constant.int 0
    %int1_329 = torch.constant.int 1
    %571 = torch.aten.add.Scalar %570, %int0_328, %int1_329 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_330 = torch.constant.int 2
    %572 = torch.aten.mul.Scalar %571, %int2_330 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_331 = torch.constant.int 1
    %573 = torch.aten.add.Tensor %572, %569, %int1_331 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_332 = torch.constant.int 4
    %574 = torch.aten.mul.Scalar %573, %int4_332 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_333 = torch.constant.int 1
    %575 = torch.aten.add.Tensor %574, %566, %int1_333 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_334 = torch.constant.int 32
    %576 = torch.aten.mul.Scalar %575, %int32_334 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_335 = torch.constant.int 1
    %577 = torch.aten.add.Tensor %576, %563, %int1_335 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_336 = torch.constant.none
    %none_337 = torch.constant.none
    %int5_338 = torch.constant.int 5
    %cpu_339 = torch.constant.device "cpu"
    %int0_340 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %547, %none_336, %none_337, %int5_338, %cpu_339, %int0_340 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %578 = torch.prim.ListConstruct %577 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_341 = torch.constant.bool false
    %579 = torch.aten.index_put %555, %578, %547, %false_341 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %579, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_342 = torch.constant.int 22
    %int2_343 = torch.constant.int 2
    %int4_344 = torch.constant.int 4
    %int32_345 = torch.constant.int 32
    %int64_346 = torch.constant.int 64
    %580 = torch.prim.ListConstruct %381, %int22_342, %int2_343, %int4_344, %int32_345, %int64_346 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %581 = torch.aten.view %579, %580 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %581, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448 = torch.constant.int 360448
    %582 = torch.prim.ListConstruct %381, %int360448 : (!torch.int, !torch.int) -> !torch.list<int>
    %583 = torch.aten.view %581, %582 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %583, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_347 = torch.constant.int 22
    %int2_348 = torch.constant.int 2
    %int4_349 = torch.constant.int 4
    %int32_350 = torch.constant.int 32
    %int64_351 = torch.constant.int 64
    %584 = torch.prim.ListConstruct %381, %int22_347, %int2_348, %int4_349, %int32_350, %int64_351 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %585 = torch.aten.view %583, %584 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %585, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_352 = torch.constant.int 64
    %586 = torch.prim.ListConstruct %553, %int64_352 : (!torch.int, !torch.int) -> !torch.list<int>
    %587 = torch.aten.view %585, %586 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %587, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_353 = torch.constant.none
    %588 = torch.aten.clone %4, %none_353 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_354 = torch.constant.int 1
    %int1_355 = torch.constant.int 1
    %int1_356 = torch.constant.int 1
    %589 = torch.prim.ListConstruct %int1_354, %int1_355, %int1_356 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %590 = torch.aten.view %588, %589 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_357 = torch.constant.int 22
    %591 = torch.aten.mul.Scalar %560, %int22_357 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int0_358 = torch.constant.int 0
    %int1_359 = torch.constant.int 1
    %592 = torch.aten.add.Scalar %591, %int0_358, %int1_359 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_360 = torch.constant.int 2
    %593 = torch.aten.mul.Scalar %592, %int2_360 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_361 = torch.constant.int 1
    %594 = torch.aten.add.Tensor %593, %590, %int1_361 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_362 = torch.constant.int 4
    %595 = torch.aten.mul.Scalar %594, %int4_362 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_363 = torch.constant.int 1
    %596 = torch.aten.add.Tensor %595, %566, %int1_363 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_364 = torch.constant.int 32
    %597 = torch.aten.mul.Scalar %596, %int32_364 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_365 = torch.constant.int 1
    %598 = torch.aten.add.Tensor %597, %563, %int1_365 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_366 = torch.constant.none
    %none_367 = torch.constant.none
    %int5_368 = torch.constant.int 5
    %cpu_369 = torch.constant.device "cpu"
    %int0_370 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %465, %none_366, %none_367, %int5_368, %cpu_369, %int0_370 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %599 = torch.prim.ListConstruct %598 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_371 = torch.constant.bool false
    %600 = torch.aten.index_put %587, %599, %465, %false_371 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %600, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_372 = torch.constant.int 22
    %int2_373 = torch.constant.int 2
    %int4_374 = torch.constant.int 4
    %int32_375 = torch.constant.int 32
    %int64_376 = torch.constant.int 64
    %601 = torch.prim.ListConstruct %381, %int22_372, %int2_373, %int4_374, %int32_375, %int64_376 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %602 = torch.aten.view %600, %601 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %602, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_377 = torch.constant.int 360448
    %603 = torch.prim.ListConstruct %381, %int360448_377 : (!torch.int, !torch.int) -> !torch.list<int>
    %604 = torch.aten.view %602, %603 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %604, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_378 = torch.constant.none
    %605 = torch.aten.clone %5, %none_378 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_379 = torch.constant.none
    %606 = torch.aten.clone %6, %none_379 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_380 = torch.constant.none
    %607 = torch.aten.clone %7, %none_380 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_381 = torch.constant.int 22
    %int2_382 = torch.constant.int 2
    %int4_383 = torch.constant.int 4
    %int32_384 = torch.constant.int 32
    %int64_385 = torch.constant.int 64
    %608 = torch.prim.ListConstruct %381, %int22_381, %int2_382, %int4_383, %int32_384, %int64_385 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %609 = torch.aten.view %604, %608 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %609, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %610 = torch_c.to_builtin_tensor %609 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %611 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_386 = tensor.cast %611 : tensor<4x?xi64> to tensor<?x?xi64>
    %612 = torch_c.to_builtin_tensor %605 : !torch.vtensor<[],si64> -> tensor<i64>
    %613 = torch_c.to_builtin_tensor %606 : !torch.vtensor<[],si64> -> tensor<i64>
    %614 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%610, %cast_386, %612, %613) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_387 = tensor.cast %614 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %615 = torch_c.from_builtin_tensor %cast_387 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %615, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %616 = torch_c.to_builtin_tensor %609 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %617 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_388 = tensor.cast %617 : tensor<4x?xi64> to tensor<?x?xi64>
    %618 = torch_c.to_builtin_tensor %605 : !torch.vtensor<[],si64> -> tensor<i64>
    %619 = torch_c.to_builtin_tensor %607 : !torch.vtensor<[],si64> -> tensor<i64>
    %620 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%616, %cast_388, %618, %619) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_389 = tensor.cast %620 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %621 = torch_c.from_builtin_tensor %cast_389 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %621, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_390 = torch.constant.int 2
    %int3_391 = torch.constant.int 3
    %622 = torch.aten.transpose.int %615, %int2_390, %int3_391 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %622, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int32_392 = torch.constant.int 32
    %623 = torch.aten.mul.int %380, %int32_392 : !torch.int, !torch.int -> !torch.int
    %int0_393 = torch.constant.int 0
    %624 = torch.aten.clone %622, %int0_393 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %624, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_394 = torch.constant.int 4
    %int4_395 = torch.constant.int 4
    %int64_396 = torch.constant.int 64
    %625 = torch.prim.ListConstruct %int4_394, %623, %int4_395, %int64_396 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %626 = torch.aten._unsafe_view %624, %625 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %626, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_397 = torch.constant.int 2
    %int3_398 = torch.constant.int 3
    %627 = torch.aten.transpose.int %621, %int2_397, %int3_398 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %627, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_399 = torch.constant.int 0
    %628 = torch.aten.clone %627, %int0_399 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %628, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_400 = torch.constant.int 4
    %int4_401 = torch.constant.int 4
    %int64_402 = torch.constant.int 64
    %629 = torch.prim.ListConstruct %int4_400, %623, %int4_401, %int64_402 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %630 = torch.aten._unsafe_view %628, %629 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %630, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_403 = torch.constant.int 0
    %int1_404 = torch.constant.int 1
    %none_405 = torch.constant.none
    %none_406 = torch.constant.none
    %cpu_407 = torch.constant.device "cpu"
    %false_408 = torch.constant.bool false
    %631 = torch.aten.arange.start_step %int0_403, %623, %int1_404, %none_405, %none_406, %cpu_407, %false_408 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %631, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_409 = torch.constant.int -1
    %632 = torch.aten.unsqueeze %arg1, %int-1_409 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %633 = torch.aten.ge.Tensor %631, %632 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %633, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_410 = torch.constant.none
    %634 = torch.aten.clone %8, %none_410 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_411 = torch.constant.int 0
    %635 = torch.aten.where.ScalarOther %633, %634, %int0_411 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %635, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_412 = torch.constant.none
    %none_413 = torch.constant.none
    %int5_414 = torch.constant.int 5
    %cpu_415 = torch.constant.device "cpu"
    %int0_416 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %635, %none_412, %none_413, %int5_414, %cpu_415, %int0_416 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_417 = torch.constant.int 1
    %636 = torch.aten.unsqueeze %635, %int1_417 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %636, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_418 = torch.constant.int 1
    %637 = torch.aten.unsqueeze %636, %int1_418 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %637, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2 = torch.constant.int -2
    %638 = torch.aten.unsqueeze %626, %int-2 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %638, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_419 = torch.constant.int 4
    %int4_420 = torch.constant.int 4
    %int8 = torch.constant.int 8
    %int64_421 = torch.constant.int 64
    %639 = torch.prim.ListConstruct %int4_419, %623, %int4_420, %int8, %int64_421 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_422 = torch.constant.bool false
    %640 = torch.aten.expand %638, %639, %false_422 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %640, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_423 = torch.constant.int 0
    %641 = torch.aten.clone %640, %int0_423 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %641, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_424 = torch.constant.int 4
    %int32_425 = torch.constant.int 32
    %int64_426 = torch.constant.int 64
    %642 = torch.prim.ListConstruct %int4_424, %623, %int32_425, %int64_426 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %643 = torch.aten._unsafe_view %641, %642 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %643, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_427 = torch.constant.int -2
    %644 = torch.aten.unsqueeze %630, %int-2_427 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %644, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_428 = torch.constant.int 4
    %int4_429 = torch.constant.int 4
    %int8_430 = torch.constant.int 8
    %int64_431 = torch.constant.int 64
    %645 = torch.prim.ListConstruct %int4_428, %623, %int4_429, %int8_430, %int64_431 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_432 = torch.constant.bool false
    %646 = torch.aten.expand %644, %645, %false_432 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %646, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_433 = torch.constant.int 0
    %647 = torch.aten.clone %646, %int0_433 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %647, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_434 = torch.constant.int 4
    %int32_435 = torch.constant.int 32
    %int64_436 = torch.constant.int 64
    %648 = torch.prim.ListConstruct %int4_434, %623, %int32_435, %int64_436 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %649 = torch.aten._unsafe_view %647, %648 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %649, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_437 = torch.constant.int 1
    %int2_438 = torch.constant.int 2
    %650 = torch.aten.transpose.int %506, %int1_437, %int2_438 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_439 = torch.constant.int 1
    %int2_440 = torch.constant.int 2
    %651 = torch.aten.transpose.int %643, %int1_439, %int2_440 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %651, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_441 = torch.constant.int 1
    %int2_442 = torch.constant.int 2
    %652 = torch.aten.transpose.int %649, %int1_441, %int2_442 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %652, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00 = torch.constant.float 0.000000e+00
    %false_443 = torch.constant.bool false
    %none_444 = torch.constant.none
    %false_445 = torch.constant.bool false
    %653 = torch.aten.scaled_dot_product_attention %650, %651, %652, %637, %float0.000000e00, %false_443, %none_444, %false_445 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_446 = torch.constant.int 1
    %int2_447 = torch.constant.int 2
    %654 = torch.aten.transpose.int %653, %int1_446, %int2_447 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_448 = torch.constant.int 4
    %int1_449 = torch.constant.int 1
    %int2048_450 = torch.constant.int 2048
    %655 = torch.prim.ListConstruct %int4_448, %int1_449, %int2048_450 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %656 = torch.aten.view %654, %655 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_451 = torch.constant.int 2
    %657 = torch.aten.view.dtype %13, %int2_451 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %658 = torch.aten.detach %657 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_452 = torch.constant.int -1
    %int17_453 = torch.constant.int 17
    %659 = torch.prim.ListConstruct %int-1_452, %int17_453 : (!torch.int, !torch.int) -> !torch.list<int>
    %660 = torch.aten.view %658, %659 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_454 = torch.constant.int 2048
    %int-1_455 = torch.constant.int -1
    %int17_456 = torch.constant.int 17
    %661 = torch.prim.ListConstruct %int2048_454, %int-1_455, %int17_456 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %662 = torch.aten.view %660, %661 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_457 = torch.constant.int 2
    %int0_458 = torch.constant.int 0
    %int1_459 = torch.constant.int 1
    %int1_460 = torch.constant.int 1
    %663 = torch.aten.slice.Tensor %662, %int2_457, %int0_458, %int1_459, %int1_460 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_461 = torch.constant.int 5
    %664 = torch.aten.view.dtype %663, %int5_461 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %665 = torch.aten.detach %664 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_462 = torch.constant.int 2
    %int1_463 = torch.constant.int 1
    %int9223372036854775807_464 = torch.constant.int 9223372036854775807
    %int1_465 = torch.constant.int 1
    %666 = torch.aten.slice.Tensor %662, %int2_462, %int1_463, %int9223372036854775807_464, %int1_465 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_466 = torch.constant.int 1
    %667 = torch.aten.view.dtype %666, %int1_466 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %668 = torch.aten.detach %667 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %669 = torch_c.to_builtin_tensor %656 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_467 = tensor.cast %669 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %670 = torch_c.to_builtin_tensor %665 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %671 = torch_c.to_builtin_tensor %668 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %672 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_467, %670, %671) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_468 = tensor.cast %672 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %673 = torch_c.from_builtin_tensor %cast_468 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_469 = torch.constant.none
    %none_470 = torch.constant.none
    %int5_471 = torch.constant.int 5
    %cpu_472 = torch.constant.device "cpu"
    %int0_473 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %673, %none_469, %none_470, %int5_471, %cpu_472, %int0_473 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_474 = torch.constant.int 1
    %674 = torch.aten.add.Tensor %398, %673, %int1_474 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_475 = torch.constant.none
    %none_476 = torch.constant.none
    %int5_477 = torch.constant.int 5
    %cpu_478 = torch.constant.device "cpu"
    %int0_479 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %674, %none_475, %none_476, %int5_477, %cpu_478, %int0_479 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_480 = torch.constant.int 6
    %675 = torch.prims.convert_element_type %674, %int6_480 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_481 = torch.constant.int 2
    %676 = torch.aten.pow.Tensor_Scalar %675, %int2_481 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_482 = torch.constant.int -1
    %677 = torch.prim.ListConstruct %int-1_482 : (!torch.int) -> !torch.list<int>
    %true_483 = torch.constant.bool true
    %none_484 = torch.constant.none
    %678 = torch.aten.mean.dim %676, %677, %true_483, %none_484 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_485 = torch.constant.float 9.9999997473787516E-6
    %int1_486 = torch.constant.int 1
    %679 = torch.aten.add.Scalar %678, %float9.999990e-06_485, %int1_486 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %680 = torch.aten.rsqrt %679 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %681 = torch.aten.mul.Tensor %675, %680 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_487 = torch.constant.none
    %none_488 = torch.constant.none
    %int6_489 = torch.constant.int 6
    %cpu_490 = torch.constant.device "cpu"
    %int0_491 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %681, %none_487, %none_488, %int6_489, %cpu_490, %int0_491 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_492 = torch.constant.int 5
    %682 = torch.prims.convert_element_type %681, %int5_492 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %683 = torch.aten.mul.Tensor %14, %682 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_493 = torch.constant.none
    %none_494 = torch.constant.none
    %int6_495 = torch.constant.int 6
    %cpu_496 = torch.constant.device "cpu"
    %int0_497 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %683, %none_493, %none_494, %int6_495, %cpu_496, %int0_497 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_498 = torch.constant.int 5
    %684 = torch.prims.convert_element_type %683, %int5_498 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_499 = torch.constant.int 2
    %685 = torch.aten.view.dtype %15, %int2_499 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %686 = torch.aten.detach %685 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_500 = torch.constant.int -1
    %int17_501 = torch.constant.int 17
    %687 = torch.prim.ListConstruct %int-1_500, %int17_501 : (!torch.int, !torch.int) -> !torch.list<int>
    %688 = torch.aten.view %686, %687 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632 = torch.constant.int 5632
    %int-1_502 = torch.constant.int -1
    %int17_503 = torch.constant.int 17
    %689 = torch.prim.ListConstruct %int5632, %int-1_502, %int17_503 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %690 = torch.aten.view %688, %689 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_504 = torch.constant.int 2
    %int0_505 = torch.constant.int 0
    %int1_506 = torch.constant.int 1
    %int1_507 = torch.constant.int 1
    %691 = torch.aten.slice.Tensor %690, %int2_504, %int0_505, %int1_506, %int1_507 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_508 = torch.constant.int 5
    %692 = torch.aten.view.dtype %691, %int5_508 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %693 = torch.aten.detach %692 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_509 = torch.constant.int 2
    %int1_510 = torch.constant.int 1
    %int9223372036854775807_511 = torch.constant.int 9223372036854775807
    %int1_512 = torch.constant.int 1
    %694 = torch.aten.slice.Tensor %690, %int2_509, %int1_510, %int9223372036854775807_511, %int1_512 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_513 = torch.constant.int 1
    %695 = torch.aten.view.dtype %694, %int1_513 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %696 = torch.aten.detach %695 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %697 = torch_c.to_builtin_tensor %684 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_514 = tensor.cast %697 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %698 = torch_c.to_builtin_tensor %693 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %699 = torch_c.to_builtin_tensor %696 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %700 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_514, %698, %699) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_515 = tensor.cast %700 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %701 = torch_c.from_builtin_tensor %cast_515 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %702 = torch.aten.silu %701 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_516 = torch.constant.int 2
    %703 = torch.aten.view.dtype %16, %int2_516 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %704 = torch.aten.detach %703 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_517 = torch.constant.int -1
    %int17_518 = torch.constant.int 17
    %705 = torch.prim.ListConstruct %int-1_517, %int17_518 : (!torch.int, !torch.int) -> !torch.list<int>
    %706 = torch.aten.view %704, %705 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_519 = torch.constant.int 5632
    %int-1_520 = torch.constant.int -1
    %int17_521 = torch.constant.int 17
    %707 = torch.prim.ListConstruct %int5632_519, %int-1_520, %int17_521 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %708 = torch.aten.view %706, %707 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_522 = torch.constant.int 2
    %int0_523 = torch.constant.int 0
    %int1_524 = torch.constant.int 1
    %int1_525 = torch.constant.int 1
    %709 = torch.aten.slice.Tensor %708, %int2_522, %int0_523, %int1_524, %int1_525 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_526 = torch.constant.int 5
    %710 = torch.aten.view.dtype %709, %int5_526 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %711 = torch.aten.detach %710 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_527 = torch.constant.int 2
    %int1_528 = torch.constant.int 1
    %int9223372036854775807_529 = torch.constant.int 9223372036854775807
    %int1_530 = torch.constant.int 1
    %712 = torch.aten.slice.Tensor %708, %int2_527, %int1_528, %int9223372036854775807_529, %int1_530 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_531 = torch.constant.int 1
    %713 = torch.aten.view.dtype %712, %int1_531 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %714 = torch.aten.detach %713 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %715 = torch_c.to_builtin_tensor %684 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_532 = tensor.cast %715 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %716 = torch_c.to_builtin_tensor %711 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %717 = torch_c.to_builtin_tensor %714 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %718 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_532, %716, %717) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_533 = tensor.cast %718 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %719 = torch_c.from_builtin_tensor %cast_533 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %720 = torch.aten.mul.Tensor %702, %719 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_534 = torch.constant.int 2
    %721 = torch.aten.view.dtype %17, %int2_534 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %722 = torch.aten.detach %721 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_535 = torch.constant.int -1
    %int17_536 = torch.constant.int 17
    %723 = torch.prim.ListConstruct %int-1_535, %int17_536 : (!torch.int, !torch.int) -> !torch.list<int>
    %724 = torch.aten.view %722, %723 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_537 = torch.constant.int 2048
    %int-1_538 = torch.constant.int -1
    %int17_539 = torch.constant.int 17
    %725 = torch.prim.ListConstruct %int2048_537, %int-1_538, %int17_539 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %726 = torch.aten.view %724, %725 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_540 = torch.constant.int 2
    %int0_541 = torch.constant.int 0
    %int1_542 = torch.constant.int 1
    %int1_543 = torch.constant.int 1
    %727 = torch.aten.slice.Tensor %726, %int2_540, %int0_541, %int1_542, %int1_543 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_544 = torch.constant.int 5
    %728 = torch.aten.view.dtype %727, %int5_544 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %729 = torch.aten.detach %728 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_545 = torch.constant.int 2
    %int1_546 = torch.constant.int 1
    %int9223372036854775807_547 = torch.constant.int 9223372036854775807
    %int1_548 = torch.constant.int 1
    %730 = torch.aten.slice.Tensor %726, %int2_545, %int1_546, %int9223372036854775807_547, %int1_548 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_549 = torch.constant.int 1
    %731 = torch.aten.view.dtype %730, %int1_549 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %732 = torch.aten.detach %731 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %733 = torch_c.to_builtin_tensor %720 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_550 = tensor.cast %733 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %734 = torch_c.to_builtin_tensor %729 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %735 = torch_c.to_builtin_tensor %732 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %736 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_550, %734, %735) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_551 = tensor.cast %736 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %737 = torch_c.from_builtin_tensor %cast_551 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_552 = torch.constant.int 1
    %738 = torch.aten.add.Tensor %674, %737, %int1_552 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_553 = torch.constant.none
    %none_554 = torch.constant.none
    %int5_555 = torch.constant.int 5
    %cpu_556 = torch.constant.device "cpu"
    %int0_557 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %738, %none_553, %none_554, %int5_555, %cpu_556, %int0_557 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_558 = torch.constant.int 6
    %739 = torch.prims.convert_element_type %738, %int6_558 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_559 = torch.constant.int 2
    %740 = torch.aten.pow.Tensor_Scalar %739, %int2_559 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_560 = torch.constant.int -1
    %741 = torch.prim.ListConstruct %int-1_560 : (!torch.int) -> !torch.list<int>
    %true_561 = torch.constant.bool true
    %none_562 = torch.constant.none
    %742 = torch.aten.mean.dim %740, %741, %true_561, %none_562 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_563 = torch.constant.float 9.9999997473787516E-6
    %int1_564 = torch.constant.int 1
    %743 = torch.aten.add.Scalar %742, %float9.999990e-06_563, %int1_564 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %744 = torch.aten.rsqrt %743 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %745 = torch.aten.mul.Tensor %739, %744 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_565 = torch.constant.none
    %none_566 = torch.constant.none
    %int6_567 = torch.constant.int 6
    %cpu_568 = torch.constant.device "cpu"
    %int0_569 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %745, %none_565, %none_566, %int6_567, %cpu_568, %int0_569 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_570 = torch.constant.int 5
    %746 = torch.prims.convert_element_type %745, %int5_570 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %747 = torch.aten.mul.Tensor %26, %746 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_571 = torch.constant.none
    %none_572 = torch.constant.none
    %int6_573 = torch.constant.int 6
    %cpu_574 = torch.constant.device "cpu"
    %int0_575 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %747, %none_571, %none_572, %int6_573, %cpu_574, %int0_575 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_576 = torch.constant.int 5
    %748 = torch.prims.convert_element_type %747, %int5_576 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_577 = torch.constant.int 2
    %749 = torch.aten.view.dtype %27, %int2_577 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %750 = torch.aten.detach %749 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_578 = torch.constant.int -1
    %int17_579 = torch.constant.int 17
    %751 = torch.prim.ListConstruct %int-1_578, %int17_579 : (!torch.int, !torch.int) -> !torch.list<int>
    %752 = torch.aten.view %750, %751 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_580 = torch.constant.int 2048
    %int-1_581 = torch.constant.int -1
    %int17_582 = torch.constant.int 17
    %753 = torch.prim.ListConstruct %int2048_580, %int-1_581, %int17_582 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %754 = torch.aten.view %752, %753 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_583 = torch.constant.int 2
    %int0_584 = torch.constant.int 0
    %int1_585 = torch.constant.int 1
    %int1_586 = torch.constant.int 1
    %755 = torch.aten.slice.Tensor %754, %int2_583, %int0_584, %int1_585, %int1_586 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_587 = torch.constant.int 5
    %756 = torch.aten.view.dtype %755, %int5_587 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %757 = torch.aten.detach %756 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_588 = torch.constant.int 2
    %int1_589 = torch.constant.int 1
    %int9223372036854775807_590 = torch.constant.int 9223372036854775807
    %int1_591 = torch.constant.int 1
    %758 = torch.aten.slice.Tensor %754, %int2_588, %int1_589, %int9223372036854775807_590, %int1_591 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_592 = torch.constant.int 1
    %759 = torch.aten.view.dtype %758, %int1_592 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %760 = torch.aten.detach %759 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %761 = torch_c.to_builtin_tensor %748 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_593 = tensor.cast %761 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %762 = torch_c.to_builtin_tensor %757 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %763 = torch_c.to_builtin_tensor %760 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %764 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_593, %762, %763) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_594 = tensor.cast %764 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %765 = torch_c.from_builtin_tensor %cast_594 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_595 = torch.constant.int 2
    %766 = torch.aten.view.dtype %28, %int2_595 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %767 = torch.aten.detach %766 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_596 = torch.constant.int -1
    %int17_597 = torch.constant.int 17
    %768 = torch.prim.ListConstruct %int-1_596, %int17_597 : (!torch.int, !torch.int) -> !torch.list<int>
    %769 = torch.aten.view %767, %768 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_598 = torch.constant.int 256
    %int-1_599 = torch.constant.int -1
    %int17_600 = torch.constant.int 17
    %770 = torch.prim.ListConstruct %int256_598, %int-1_599, %int17_600 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %771 = torch.aten.view %769, %770 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_601 = torch.constant.int 2
    %int0_602 = torch.constant.int 0
    %int1_603 = torch.constant.int 1
    %int1_604 = torch.constant.int 1
    %772 = torch.aten.slice.Tensor %771, %int2_601, %int0_602, %int1_603, %int1_604 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_605 = torch.constant.int 5
    %773 = torch.aten.view.dtype %772, %int5_605 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %774 = torch.aten.detach %773 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_606 = torch.constant.int 2
    %int1_607 = torch.constant.int 1
    %int9223372036854775807_608 = torch.constant.int 9223372036854775807
    %int1_609 = torch.constant.int 1
    %775 = torch.aten.slice.Tensor %771, %int2_606, %int1_607, %int9223372036854775807_608, %int1_609 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_610 = torch.constant.int 1
    %776 = torch.aten.view.dtype %775, %int1_610 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %777 = torch.aten.detach %776 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %778 = torch_c.to_builtin_tensor %748 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_611 = tensor.cast %778 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %779 = torch_c.to_builtin_tensor %774 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %780 = torch_c.to_builtin_tensor %777 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %781 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_611, %779, %780) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_612 = tensor.cast %781 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %782 = torch_c.from_builtin_tensor %cast_612 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_613 = torch.constant.int 2
    %783 = torch.aten.view.dtype %29, %int2_613 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %784 = torch.aten.detach %783 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_614 = torch.constant.int -1
    %int17_615 = torch.constant.int 17
    %785 = torch.prim.ListConstruct %int-1_614, %int17_615 : (!torch.int, !torch.int) -> !torch.list<int>
    %786 = torch.aten.view %784, %785 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_616 = torch.constant.int 256
    %int-1_617 = torch.constant.int -1
    %int17_618 = torch.constant.int 17
    %787 = torch.prim.ListConstruct %int256_616, %int-1_617, %int17_618 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %788 = torch.aten.view %786, %787 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_619 = torch.constant.int 2
    %int0_620 = torch.constant.int 0
    %int1_621 = torch.constant.int 1
    %int1_622 = torch.constant.int 1
    %789 = torch.aten.slice.Tensor %788, %int2_619, %int0_620, %int1_621, %int1_622 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_623 = torch.constant.int 5
    %790 = torch.aten.view.dtype %789, %int5_623 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %791 = torch.aten.detach %790 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_624 = torch.constant.int 2
    %int1_625 = torch.constant.int 1
    %int9223372036854775807_626 = torch.constant.int 9223372036854775807
    %int1_627 = torch.constant.int 1
    %792 = torch.aten.slice.Tensor %788, %int2_624, %int1_625, %int9223372036854775807_626, %int1_627 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_628 = torch.constant.int 1
    %793 = torch.aten.view.dtype %792, %int1_628 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %794 = torch.aten.detach %793 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %795 = torch_c.to_builtin_tensor %748 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_629 = tensor.cast %795 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %796 = torch_c.to_builtin_tensor %791 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %797 = torch_c.to_builtin_tensor %794 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %798 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_629, %796, %797) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_630 = tensor.cast %798 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %799 = torch_c.from_builtin_tensor %cast_630 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_631 = torch.constant.int 4
    %int1_632 = torch.constant.int 1
    %int32_633 = torch.constant.int 32
    %int64_634 = torch.constant.int 64
    %800 = torch.prim.ListConstruct %int4_631, %int1_632, %int32_633, %int64_634 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %801 = torch.aten.view %765, %800 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_635 = torch.constant.int 4
    %int1_636 = torch.constant.int 1
    %int4_637 = torch.constant.int 4
    %int64_638 = torch.constant.int 64
    %802 = torch.prim.ListConstruct %int4_635, %int1_636, %int4_637, %int64_638 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %803 = torch.aten.view %782, %802 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_639 = torch.constant.int 4
    %int1_640 = torch.constant.int 1
    %int4_641 = torch.constant.int 4
    %int64_642 = torch.constant.int 64
    %804 = torch.prim.ListConstruct %int4_639, %int1_640, %int4_641, %int64_642 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %805 = torch.aten.view %799, %804 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_643 = torch.constant.int 0
    %int1_644 = torch.constant.int 1
    %none_645 = torch.constant.none
    %none_646 = torch.constant.none
    %cpu_647 = torch.constant.device "cpu"
    %false_648 = torch.constant.bool false
    %806 = torch.aten.arange.start %int0_643, %int1_644, %none_645, %none_646, %cpu_647, %false_648 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_649 = torch.constant.int 0
    %807 = torch.aten.unsqueeze %806, %int0_649 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_650 = torch.constant.int 1
    %808 = torch.aten.unsqueeze %arg2, %int1_650 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_651 = torch.constant.int 1
    %809 = torch.aten.add.Tensor %807, %808, %int1_651 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_652 = torch.constant.int 0
    %int64_653 = torch.constant.int 64
    %int2_654 = torch.constant.int 2
    %none_655 = torch.constant.none
    %none_656 = torch.constant.none
    %cpu_657 = torch.constant.device "cpu"
    %false_658 = torch.constant.bool false
    %810 = torch.aten.arange.start_step %int0_652, %int64_653, %int2_654, %none_655, %none_656, %cpu_657, %false_658 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_659 = torch.constant.none
    %none_660 = torch.constant.none
    %int4_661 = torch.constant.int 4
    %cpu_662 = torch.constant.device "cpu"
    %int0_663 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %810, %none_659, %none_660, %int4_661, %cpu_662, %int0_663 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_664 = torch.constant.int 6
    %811 = torch.prims.convert_element_type %810, %int6_664 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_665 = torch.constant.int 64
    %812 = torch.aten.div.Scalar %811, %int64_665 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_666 = torch.constant.float 1.000000e+04
    %813 = torch.aten.pow.Scalar %float1.000000e04_666, %812 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %814 = torch.aten.reciprocal %813 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_667 = torch.constant.float 1.000000e+00
    %815 = torch.aten.mul.Scalar %814, %float1.000000e00_667 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_668 = torch.constant.none
    %816 = torch.aten.clone %18, %none_668 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_669 = torch.constant.int 0
    %817 = torch.aten.unsqueeze %815, %int0_669 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_670 = torch.constant.int 2
    %818 = torch.aten.unsqueeze %817, %int2_670 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_671 = torch.constant.none
    %none_672 = torch.constant.none
    %int6_673 = torch.constant.int 6
    %cpu_674 = torch.constant.device "cpu"
    %int0_675 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %818, %none_671, %none_672, %int6_673, %cpu_674, %int0_675 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_676 = torch.constant.int 4
    %int-1_677 = torch.constant.int -1
    %int1_678 = torch.constant.int 1
    %819 = torch.prim.ListConstruct %int4_676, %int-1_677, %int1_678 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_679 = torch.constant.bool false
    %820 = torch.aten.expand %818, %819, %false_679 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_680 = torch.constant.int 1
    %821 = torch.aten.unsqueeze %809, %int1_680 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_681 = torch.constant.none
    %none_682 = torch.constant.none
    %int4_683 = torch.constant.int 4
    %cpu_684 = torch.constant.device "cpu"
    %int0_685 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %821, %none_681, %none_682, %int4_683, %cpu_684, %int0_685 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_686 = torch.constant.int 6
    %822 = torch.prims.convert_element_type %821, %int6_686 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %823 = torch.aten.matmul %820, %822 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_687 = torch.constant.int 1
    %int2_688 = torch.constant.int 2
    %824 = torch.aten.transpose.int %823, %int1_687, %int2_688 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %825 = torch.aten.cos %824 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %826 = torch.aten.mul.Tensor %825, %816 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_689 = torch.constant.none
    %none_690 = torch.constant.none
    %int6_691 = torch.constant.int 6
    %cpu_692 = torch.constant.device "cpu"
    %int0_693 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %826, %none_689, %none_690, %int6_691, %cpu_692, %int0_693 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_694 = torch.constant.int 5
    %827 = torch.prims.convert_element_type %826, %int5_694 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %828 = torch.aten.sin %824 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %829 = torch.aten.mul.Tensor %828, %816 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_695 = torch.constant.none
    %none_696 = torch.constant.none
    %int6_697 = torch.constant.int 6
    %cpu_698 = torch.constant.device "cpu"
    %int0_699 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %829, %none_695, %none_696, %int6_697, %cpu_698, %int0_699 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_700 = torch.constant.int 5
    %830 = torch.prims.convert_element_type %829, %int5_700 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_701 = torch.constant.int 2
    %831 = torch.aten.unsqueeze %827, %int2_701 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_702 = torch.constant.int 2
    %832 = torch.aten.unsqueeze %830, %int2_702 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_703 = torch.constant.none
    %none_704 = torch.constant.none
    %int5_705 = torch.constant.int 5
    %cpu_706 = torch.constant.device "cpu"
    %int0_707 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %831, %none_703, %none_704, %int5_705, %cpu_706, %int0_707 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_708 = torch.constant.none
    %none_709 = torch.constant.none
    %int5_710 = torch.constant.int 5
    %cpu_711 = torch.constant.device "cpu"
    %int0_712 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %832, %none_708, %none_709, %int5_710, %cpu_711, %int0_712 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_713 = torch.constant.none
    %none_714 = torch.constant.none
    %int5_715 = torch.constant.int 5
    %cpu_716 = torch.constant.device "cpu"
    %int0_717 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %801, %none_713, %none_714, %int5_715, %cpu_716, %int0_717 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_718 = torch.constant.int 3
    %int0_719 = torch.constant.int 0
    %int64_720 = torch.constant.int 64
    %int2_721 = torch.constant.int 2
    %833 = torch.aten.slice.Tensor %801, %int3_718, %int0_719, %int64_720, %int2_721 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_722 = torch.constant.int 3
    %int1_723 = torch.constant.int 1
    %int64_724 = torch.constant.int 64
    %int2_725 = torch.constant.int 2
    %834 = torch.aten.slice.Tensor %801, %int3_722, %int1_723, %int64_724, %int2_725 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %835 = torch.aten.mul.Tensor %833, %831 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %836 = torch.aten.mul.Tensor %834, %832 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_726 = torch.constant.int 1
    %837 = torch.aten.sub.Tensor %835, %836, %int1_726 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %838 = torch.aten.mul.Tensor %834, %831 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %839 = torch.aten.mul.Tensor %833, %832 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_727 = torch.constant.int 1
    %840 = torch.aten.add.Tensor %838, %839, %int1_727 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %841 = torch_c.to_builtin_tensor %837 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_728 = tensor.cast %841 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %842 = torch_c.to_builtin_tensor %840 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_729 = tensor.cast %842 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %843 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_728, %cast_729) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_730 = tensor.cast %843 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %844 = torch_c.from_builtin_tensor %cast_730 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_731 = torch.constant.int 4
    %int1_732 = torch.constant.int 1
    %int32_733 = torch.constant.int 32
    %int64_734 = torch.constant.int 64
    %845 = torch.prim.ListConstruct %int4_731, %int1_732, %int32_733, %int64_734 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %846 = torch.aten.view %844, %845 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_735 = torch.constant.none
    %none_736 = torch.constant.none
    %int5_737 = torch.constant.int 5
    %cpu_738 = torch.constant.device "cpu"
    %int0_739 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %846, %none_735, %none_736, %int5_737, %cpu_738, %int0_739 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_740 = torch.constant.int 0
    %int1_741 = torch.constant.int 1
    %none_742 = torch.constant.none
    %none_743 = torch.constant.none
    %cpu_744 = torch.constant.device "cpu"
    %false_745 = torch.constant.bool false
    %847 = torch.aten.arange.start %int0_740, %int1_741, %none_742, %none_743, %cpu_744, %false_745 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_746 = torch.constant.int 0
    %848 = torch.aten.unsqueeze %847, %int0_746 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_747 = torch.constant.int 1
    %849 = torch.aten.unsqueeze %arg2, %int1_747 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_748 = torch.constant.int 1
    %850 = torch.aten.add.Tensor %848, %849, %int1_748 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_749 = torch.constant.int 0
    %int64_750 = torch.constant.int 64
    %int2_751 = torch.constant.int 2
    %none_752 = torch.constant.none
    %none_753 = torch.constant.none
    %cpu_754 = torch.constant.device "cpu"
    %false_755 = torch.constant.bool false
    %851 = torch.aten.arange.start_step %int0_749, %int64_750, %int2_751, %none_752, %none_753, %cpu_754, %false_755 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_756 = torch.constant.none
    %none_757 = torch.constant.none
    %int4_758 = torch.constant.int 4
    %cpu_759 = torch.constant.device "cpu"
    %int0_760 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %851, %none_756, %none_757, %int4_758, %cpu_759, %int0_760 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_761 = torch.constant.int 6
    %852 = torch.prims.convert_element_type %851, %int6_761 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_762 = torch.constant.int 64
    %853 = torch.aten.div.Scalar %852, %int64_762 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_763 = torch.constant.float 1.000000e+04
    %854 = torch.aten.pow.Scalar %float1.000000e04_763, %853 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %855 = torch.aten.reciprocal %854 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_764 = torch.constant.float 1.000000e+00
    %856 = torch.aten.mul.Scalar %855, %float1.000000e00_764 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_765 = torch.constant.none
    %857 = torch.aten.clone %19, %none_765 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_766 = torch.constant.int 0
    %858 = torch.aten.unsqueeze %856, %int0_766 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_767 = torch.constant.int 2
    %859 = torch.aten.unsqueeze %858, %int2_767 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_768 = torch.constant.none
    %none_769 = torch.constant.none
    %int6_770 = torch.constant.int 6
    %cpu_771 = torch.constant.device "cpu"
    %int0_772 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %859, %none_768, %none_769, %int6_770, %cpu_771, %int0_772 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_773 = torch.constant.int 4
    %int-1_774 = torch.constant.int -1
    %int1_775 = torch.constant.int 1
    %860 = torch.prim.ListConstruct %int4_773, %int-1_774, %int1_775 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_776 = torch.constant.bool false
    %861 = torch.aten.expand %859, %860, %false_776 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_777 = torch.constant.int 1
    %862 = torch.aten.unsqueeze %850, %int1_777 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_778 = torch.constant.none
    %none_779 = torch.constant.none
    %int4_780 = torch.constant.int 4
    %cpu_781 = torch.constant.device "cpu"
    %int0_782 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %862, %none_778, %none_779, %int4_780, %cpu_781, %int0_782 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_783 = torch.constant.int 6
    %863 = torch.prims.convert_element_type %862, %int6_783 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %864 = torch.aten.matmul %861, %863 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_784 = torch.constant.int 1
    %int2_785 = torch.constant.int 2
    %865 = torch.aten.transpose.int %864, %int1_784, %int2_785 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %866 = torch.aten.cos %865 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %867 = torch.aten.mul.Tensor %866, %857 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_786 = torch.constant.none
    %none_787 = torch.constant.none
    %int6_788 = torch.constant.int 6
    %cpu_789 = torch.constant.device "cpu"
    %int0_790 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %867, %none_786, %none_787, %int6_788, %cpu_789, %int0_790 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_791 = torch.constant.int 5
    %868 = torch.prims.convert_element_type %867, %int5_791 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %869 = torch.aten.sin %865 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %870 = torch.aten.mul.Tensor %869, %857 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_792 = torch.constant.none
    %none_793 = torch.constant.none
    %int6_794 = torch.constant.int 6
    %cpu_795 = torch.constant.device "cpu"
    %int0_796 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %870, %none_792, %none_793, %int6_794, %cpu_795, %int0_796 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_797 = torch.constant.int 5
    %871 = torch.prims.convert_element_type %870, %int5_797 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_798 = torch.constant.int 2
    %872 = torch.aten.unsqueeze %868, %int2_798 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_799 = torch.constant.int 2
    %873 = torch.aten.unsqueeze %871, %int2_799 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_800 = torch.constant.none
    %none_801 = torch.constant.none
    %int5_802 = torch.constant.int 5
    %cpu_803 = torch.constant.device "cpu"
    %int0_804 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %872, %none_800, %none_801, %int5_802, %cpu_803, %int0_804 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_805 = torch.constant.none
    %none_806 = torch.constant.none
    %int5_807 = torch.constant.int 5
    %cpu_808 = torch.constant.device "cpu"
    %int0_809 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %873, %none_805, %none_806, %int5_807, %cpu_808, %int0_809 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_810 = torch.constant.none
    %none_811 = torch.constant.none
    %int5_812 = torch.constant.int 5
    %cpu_813 = torch.constant.device "cpu"
    %int0_814 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %803, %none_810, %none_811, %int5_812, %cpu_813, %int0_814 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_815 = torch.constant.int 3
    %int0_816 = torch.constant.int 0
    %int64_817 = torch.constant.int 64
    %int2_818 = torch.constant.int 2
    %874 = torch.aten.slice.Tensor %803, %int3_815, %int0_816, %int64_817, %int2_818 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_819 = torch.constant.int 3
    %int1_820 = torch.constant.int 1
    %int64_821 = torch.constant.int 64
    %int2_822 = torch.constant.int 2
    %875 = torch.aten.slice.Tensor %803, %int3_819, %int1_820, %int64_821, %int2_822 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %876 = torch.aten.mul.Tensor %874, %872 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %877 = torch.aten.mul.Tensor %875, %873 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_823 = torch.constant.int 1
    %878 = torch.aten.sub.Tensor %876, %877, %int1_823 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %879 = torch.aten.mul.Tensor %875, %872 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %880 = torch.aten.mul.Tensor %874, %873 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_824 = torch.constant.int 1
    %881 = torch.aten.add.Tensor %879, %880, %int1_824 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %882 = torch_c.to_builtin_tensor %878 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_825 = tensor.cast %882 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %883 = torch_c.to_builtin_tensor %881 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_826 = tensor.cast %883 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %884 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_825, %cast_826) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_827 = tensor.cast %884 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %885 = torch_c.from_builtin_tensor %cast_827 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_828 = torch.constant.int 4
    %int1_829 = torch.constant.int 1
    %int4_830 = torch.constant.int 4
    %int64_831 = torch.constant.int 64
    %886 = torch.prim.ListConstruct %int4_828, %int1_829, %int4_830, %int64_831 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %887 = torch.aten.view %885, %886 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_832 = torch.constant.none
    %none_833 = torch.constant.none
    %int5_834 = torch.constant.int 5
    %cpu_835 = torch.constant.device "cpu"
    %int0_836 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %887, %none_832, %none_833, %int5_834, %cpu_835, %int0_836 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_837 = torch.constant.int 32
    %888 = torch.aten.floor_divide.Scalar %arg2, %int32_837 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_838 = torch.constant.int 1
    %889 = torch.aten.unsqueeze %888, %int1_838 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_839 = torch.constant.int 1
    %false_840 = torch.constant.bool false
    %890 = torch.aten.gather %arg3, %int1_839, %889, %false_840 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_841 = torch.constant.int 4
    %int1_842 = torch.constant.int 1
    %int1_843 = torch.constant.int 1
    %891 = torch.prim.ListConstruct %int4_841, %int1_842, %int1_843 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %892 = torch.aten.view %890, %891 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_844 = torch.constant.int 32
    %893 = torch.aten.remainder.Scalar %arg2, %int32_844 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_845 = torch.constant.int 4
    %int1_846 = torch.constant.int 1
    %int1_847 = torch.constant.int 1
    %894 = torch.prim.ListConstruct %int4_845, %int1_846, %int1_847 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %895 = torch.aten.view %893, %894 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_848 = torch.constant.int 4
    %none_849 = torch.constant.none
    %none_850 = torch.constant.none
    %cpu_851 = torch.constant.device "cpu"
    %false_852 = torch.constant.bool false
    %896 = torch.aten.arange %int4_848, %none_849, %none_850, %cpu_851, %false_852 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_853 = torch.constant.int 1
    %int1_854 = torch.constant.int 1
    %int4_855 = torch.constant.int 4
    %897 = torch.prim.ListConstruct %int1_853, %int1_854, %int4_855 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %898 = torch.aten.view %896, %897 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_856 = torch.constant.none
    %899 = torch.aten.clone %20, %none_856 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_857 = torch.constant.int 1
    %int1_858 = torch.constant.int 1
    %int1_859 = torch.constant.int 1
    %900 = torch.prim.ListConstruct %int1_857, %int1_858, %int1_859 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %901 = torch.aten.view %899, %900 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_860 = torch.constant.int 22
    %902 = torch.aten.mul.Scalar %892, %int22_860 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_861 = torch.constant.int 1
    %int1_862 = torch.constant.int 1
    %903 = torch.aten.add.Scalar %902, %int1_861, %int1_862 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_863 = torch.constant.int 2
    %904 = torch.aten.mul.Scalar %903, %int2_863 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_864 = torch.constant.int 1
    %905 = torch.aten.add.Tensor %904, %901, %int1_864 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_865 = torch.constant.int 4
    %906 = torch.aten.mul.Scalar %905, %int4_865 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_866 = torch.constant.int 1
    %907 = torch.aten.add.Tensor %906, %898, %int1_866 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_867 = torch.constant.int 32
    %908 = torch.aten.mul.Scalar %907, %int32_867 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_868 = torch.constant.int 1
    %909 = torch.aten.add.Tensor %908, %895, %int1_868 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_869 = torch.constant.none
    %none_870 = torch.constant.none
    %int5_871 = torch.constant.int 5
    %cpu_872 = torch.constant.device "cpu"
    %int0_873 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %887, %none_869, %none_870, %int5_871, %cpu_872, %int0_873 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_874 = torch.constant.int 22
    %int2_875 = torch.constant.int 2
    %int4_876 = torch.constant.int 4
    %int32_877 = torch.constant.int 32
    %int64_878 = torch.constant.int 64
    %910 = torch.prim.ListConstruct %381, %int22_874, %int2_875, %int4_876, %int32_877, %int64_878 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %911 = torch.aten.view %604, %910 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %911, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_879 = torch.constant.int 64
    %912 = torch.prim.ListConstruct %553, %int64_879 : (!torch.int, !torch.int) -> !torch.list<int>
    %913 = torch.aten.view %911, %912 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %913, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %914 = torch.prim.ListConstruct %909 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_880 = torch.constant.bool false
    %915 = torch.aten.index_put %913, %914, %887, %false_880 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %915, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_881 = torch.constant.int 22
    %int2_882 = torch.constant.int 2
    %int4_883 = torch.constant.int 4
    %int32_884 = torch.constant.int 32
    %int64_885 = torch.constant.int 64
    %916 = torch.prim.ListConstruct %381, %int22_881, %int2_882, %int4_883, %int32_884, %int64_885 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %917 = torch.aten.view %915, %916 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %917, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_886 = torch.constant.int 360448
    %918 = torch.prim.ListConstruct %381, %int360448_886 : (!torch.int, !torch.int) -> !torch.list<int>
    %919 = torch.aten.view %917, %918 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %919, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_887 = torch.constant.int 22
    %int2_888 = torch.constant.int 2
    %int4_889 = torch.constant.int 4
    %int32_890 = torch.constant.int 32
    %int64_891 = torch.constant.int 64
    %920 = torch.prim.ListConstruct %381, %int22_887, %int2_888, %int4_889, %int32_890, %int64_891 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %921 = torch.aten.view %919, %920 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %921, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_892 = torch.constant.int 64
    %922 = torch.prim.ListConstruct %553, %int64_892 : (!torch.int, !torch.int) -> !torch.list<int>
    %923 = torch.aten.view %921, %922 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %923, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_893 = torch.constant.none
    %924 = torch.aten.clone %21, %none_893 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_894 = torch.constant.int 1
    %int1_895 = torch.constant.int 1
    %int1_896 = torch.constant.int 1
    %925 = torch.prim.ListConstruct %int1_894, %int1_895, %int1_896 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %926 = torch.aten.view %924, %925 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_897 = torch.constant.int 22
    %927 = torch.aten.mul.Scalar %892, %int22_897 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_898 = torch.constant.int 1
    %int1_899 = torch.constant.int 1
    %928 = torch.aten.add.Scalar %927, %int1_898, %int1_899 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_900 = torch.constant.int 2
    %929 = torch.aten.mul.Scalar %928, %int2_900 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_901 = torch.constant.int 1
    %930 = torch.aten.add.Tensor %929, %926, %int1_901 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_902 = torch.constant.int 4
    %931 = torch.aten.mul.Scalar %930, %int4_902 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_903 = torch.constant.int 1
    %932 = torch.aten.add.Tensor %931, %898, %int1_903 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_904 = torch.constant.int 32
    %933 = torch.aten.mul.Scalar %932, %int32_904 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_905 = torch.constant.int 1
    %934 = torch.aten.add.Tensor %933, %895, %int1_905 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_906 = torch.constant.none
    %none_907 = torch.constant.none
    %int5_908 = torch.constant.int 5
    %cpu_909 = torch.constant.device "cpu"
    %int0_910 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %805, %none_906, %none_907, %int5_908, %cpu_909, %int0_910 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %935 = torch.prim.ListConstruct %934 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_911 = torch.constant.bool false
    %936 = torch.aten.index_put %923, %935, %805, %false_911 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %936, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_912 = torch.constant.int 22
    %int2_913 = torch.constant.int 2
    %int4_914 = torch.constant.int 4
    %int32_915 = torch.constant.int 32
    %int64_916 = torch.constant.int 64
    %937 = torch.prim.ListConstruct %381, %int22_912, %int2_913, %int4_914, %int32_915, %int64_916 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %938 = torch.aten.view %936, %937 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %938, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_917 = torch.constant.int 360448
    %939 = torch.prim.ListConstruct %381, %int360448_917 : (!torch.int, !torch.int) -> !torch.list<int>
    %940 = torch.aten.view %938, %939 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %940, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_918 = torch.constant.none
    %941 = torch.aten.clone %22, %none_918 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_919 = torch.constant.none
    %942 = torch.aten.clone %23, %none_919 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_920 = torch.constant.none
    %943 = torch.aten.clone %24, %none_920 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_921 = torch.constant.int 22
    %int2_922 = torch.constant.int 2
    %int4_923 = torch.constant.int 4
    %int32_924 = torch.constant.int 32
    %int64_925 = torch.constant.int 64
    %944 = torch.prim.ListConstruct %381, %int22_921, %int2_922, %int4_923, %int32_924, %int64_925 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %945 = torch.aten.view %940, %944 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %945, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %946 = torch_c.to_builtin_tensor %945 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %947 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_926 = tensor.cast %947 : tensor<4x?xi64> to tensor<?x?xi64>
    %948 = torch_c.to_builtin_tensor %941 : !torch.vtensor<[],si64> -> tensor<i64>
    %949 = torch_c.to_builtin_tensor %942 : !torch.vtensor<[],si64> -> tensor<i64>
    %950 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%946, %cast_926, %948, %949) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_927 = tensor.cast %950 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %951 = torch_c.from_builtin_tensor %cast_927 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %951, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %952 = torch_c.to_builtin_tensor %945 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %953 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_928 = tensor.cast %953 : tensor<4x?xi64> to tensor<?x?xi64>
    %954 = torch_c.to_builtin_tensor %941 : !torch.vtensor<[],si64> -> tensor<i64>
    %955 = torch_c.to_builtin_tensor %943 : !torch.vtensor<[],si64> -> tensor<i64>
    %956 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%952, %cast_928, %954, %955) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_929 = tensor.cast %956 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %957 = torch_c.from_builtin_tensor %cast_929 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %957, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_930 = torch.constant.int 2
    %int3_931 = torch.constant.int 3
    %958 = torch.aten.transpose.int %951, %int2_930, %int3_931 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %958, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_932 = torch.constant.int 0
    %959 = torch.aten.clone %958, %int0_932 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %959, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_933 = torch.constant.int 4
    %int4_934 = torch.constant.int 4
    %int64_935 = torch.constant.int 64
    %960 = torch.prim.ListConstruct %int4_933, %623, %int4_934, %int64_935 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %961 = torch.aten._unsafe_view %959, %960 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %961, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_936 = torch.constant.int 2
    %int3_937 = torch.constant.int 3
    %962 = torch.aten.transpose.int %957, %int2_936, %int3_937 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %962, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_938 = torch.constant.int 0
    %963 = torch.aten.clone %962, %int0_938 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %963, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_939 = torch.constant.int 4
    %int4_940 = torch.constant.int 4
    %int64_941 = torch.constant.int 64
    %964 = torch.prim.ListConstruct %int4_939, %623, %int4_940, %int64_941 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %965 = torch.aten._unsafe_view %963, %964 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %965, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_942 = torch.constant.int 0
    %int1_943 = torch.constant.int 1
    %none_944 = torch.constant.none
    %none_945 = torch.constant.none
    %cpu_946 = torch.constant.device "cpu"
    %false_947 = torch.constant.bool false
    %966 = torch.aten.arange.start_step %int0_942, %623, %int1_943, %none_944, %none_945, %cpu_946, %false_947 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %966, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_948 = torch.constant.int -1
    %967 = torch.aten.unsqueeze %arg1, %int-1_948 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %968 = torch.aten.ge.Tensor %966, %967 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %968, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_949 = torch.constant.none
    %969 = torch.aten.clone %25, %none_949 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_950 = torch.constant.int 0
    %970 = torch.aten.where.ScalarOther %968, %969, %int0_950 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %970, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_951 = torch.constant.none
    %none_952 = torch.constant.none
    %int5_953 = torch.constant.int 5
    %cpu_954 = torch.constant.device "cpu"
    %int0_955 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %970, %none_951, %none_952, %int5_953, %cpu_954, %int0_955 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_956 = torch.constant.int 1
    %971 = torch.aten.unsqueeze %970, %int1_956 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %971, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_957 = torch.constant.int 1
    %972 = torch.aten.unsqueeze %971, %int1_957 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %972, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_958 = torch.constant.int -2
    %973 = torch.aten.unsqueeze %961, %int-2_958 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %973, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_959 = torch.constant.int 4
    %int4_960 = torch.constant.int 4
    %int8_961 = torch.constant.int 8
    %int64_962 = torch.constant.int 64
    %974 = torch.prim.ListConstruct %int4_959, %623, %int4_960, %int8_961, %int64_962 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_963 = torch.constant.bool false
    %975 = torch.aten.expand %973, %974, %false_963 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %975, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_964 = torch.constant.int 0
    %976 = torch.aten.clone %975, %int0_964 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %976, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_965 = torch.constant.int 4
    %int32_966 = torch.constant.int 32
    %int64_967 = torch.constant.int 64
    %977 = torch.prim.ListConstruct %int4_965, %623, %int32_966, %int64_967 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %978 = torch.aten._unsafe_view %976, %977 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %978, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_968 = torch.constant.int -2
    %979 = torch.aten.unsqueeze %965, %int-2_968 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %979, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_969 = torch.constant.int 4
    %int4_970 = torch.constant.int 4
    %int8_971 = torch.constant.int 8
    %int64_972 = torch.constant.int 64
    %980 = torch.prim.ListConstruct %int4_969, %623, %int4_970, %int8_971, %int64_972 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_973 = torch.constant.bool false
    %981 = torch.aten.expand %979, %980, %false_973 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %981, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_974 = torch.constant.int 0
    %982 = torch.aten.clone %981, %int0_974 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %982, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_975 = torch.constant.int 4
    %int32_976 = torch.constant.int 32
    %int64_977 = torch.constant.int 64
    %983 = torch.prim.ListConstruct %int4_975, %623, %int32_976, %int64_977 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %984 = torch.aten._unsafe_view %982, %983 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %984, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_978 = torch.constant.int 1
    %int2_979 = torch.constant.int 2
    %985 = torch.aten.transpose.int %846, %int1_978, %int2_979 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_980 = torch.constant.int 1
    %int2_981 = torch.constant.int 2
    %986 = torch.aten.transpose.int %978, %int1_980, %int2_981 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %986, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_982 = torch.constant.int 1
    %int2_983 = torch.constant.int 2
    %987 = torch.aten.transpose.int %984, %int1_982, %int2_983 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %987, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_984 = torch.constant.float 0.000000e+00
    %false_985 = torch.constant.bool false
    %none_986 = torch.constant.none
    %false_987 = torch.constant.bool false
    %988 = torch.aten.scaled_dot_product_attention %985, %986, %987, %972, %float0.000000e00_984, %false_985, %none_986, %false_987 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_988 = torch.constant.int 1
    %int2_989 = torch.constant.int 2
    %989 = torch.aten.transpose.int %988, %int1_988, %int2_989 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_990 = torch.constant.int 4
    %int1_991 = torch.constant.int 1
    %int2048_992 = torch.constant.int 2048
    %990 = torch.prim.ListConstruct %int4_990, %int1_991, %int2048_992 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %991 = torch.aten.view %989, %990 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_993 = torch.constant.int 2
    %992 = torch.aten.view.dtype %30, %int2_993 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %993 = torch.aten.detach %992 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_994 = torch.constant.int -1
    %int17_995 = torch.constant.int 17
    %994 = torch.prim.ListConstruct %int-1_994, %int17_995 : (!torch.int, !torch.int) -> !torch.list<int>
    %995 = torch.aten.view %993, %994 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_996 = torch.constant.int 2048
    %int-1_997 = torch.constant.int -1
    %int17_998 = torch.constant.int 17
    %996 = torch.prim.ListConstruct %int2048_996, %int-1_997, %int17_998 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %997 = torch.aten.view %995, %996 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_999 = torch.constant.int 2
    %int0_1000 = torch.constant.int 0
    %int1_1001 = torch.constant.int 1
    %int1_1002 = torch.constant.int 1
    %998 = torch.aten.slice.Tensor %997, %int2_999, %int0_1000, %int1_1001, %int1_1002 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_1003 = torch.constant.int 5
    %999 = torch.aten.view.dtype %998, %int5_1003 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %1000 = torch.aten.detach %999 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_1004 = torch.constant.int 2
    %int1_1005 = torch.constant.int 1
    %int9223372036854775807_1006 = torch.constant.int 9223372036854775807
    %int1_1007 = torch.constant.int 1
    %1001 = torch.aten.slice.Tensor %997, %int2_1004, %int1_1005, %int9223372036854775807_1006, %int1_1007 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_1008 = torch.constant.int 1
    %1002 = torch.aten.view.dtype %1001, %int1_1008 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %1003 = torch.aten.detach %1002 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %1004 = torch_c.to_builtin_tensor %991 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_1009 = tensor.cast %1004 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %1005 = torch_c.to_builtin_tensor %1000 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %1006 = torch_c.to_builtin_tensor %1003 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %1007 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_1009, %1005, %1006) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_1010 = tensor.cast %1007 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %1008 = torch_c.from_builtin_tensor %cast_1010 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_1011 = torch.constant.none
    %none_1012 = torch.constant.none
    %int5_1013 = torch.constant.int 5
    %cpu_1014 = torch.constant.device "cpu"
    %int0_1015 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1008, %none_1011, %none_1012, %int5_1013, %cpu_1014, %int0_1015 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_1016 = torch.constant.int 1
    %1009 = torch.aten.add.Tensor %738, %1008, %int1_1016 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_1017 = torch.constant.none
    %none_1018 = torch.constant.none
    %int5_1019 = torch.constant.int 5
    %cpu_1020 = torch.constant.device "cpu"
    %int0_1021 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1009, %none_1017, %none_1018, %int5_1019, %cpu_1020, %int0_1021 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1022 = torch.constant.int 6
    %1010 = torch.prims.convert_element_type %1009, %int6_1022 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_1023 = torch.constant.int 2
    %1011 = torch.aten.pow.Tensor_Scalar %1010, %int2_1023 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_1024 = torch.constant.int -1
    %1012 = torch.prim.ListConstruct %int-1_1024 : (!torch.int) -> !torch.list<int>
    %true_1025 = torch.constant.bool true
    %none_1026 = torch.constant.none
    %1013 = torch.aten.mean.dim %1011, %1012, %true_1025, %none_1026 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_1027 = torch.constant.float 9.9999997473787516E-6
    %int1_1028 = torch.constant.int 1
    %1014 = torch.aten.add.Scalar %1013, %float9.999990e-06_1027, %int1_1028 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1015 = torch.aten.rsqrt %1014 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %1016 = torch.aten.mul.Tensor %1010, %1015 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_1029 = torch.constant.none
    %none_1030 = torch.constant.none
    %int6_1031 = torch.constant.int 6
    %cpu_1032 = torch.constant.device "cpu"
    %int0_1033 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1016, %none_1029, %none_1030, %int6_1031, %cpu_1032, %int0_1033 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1034 = torch.constant.int 5
    %1017 = torch.prims.convert_element_type %1016, %int5_1034 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %1018 = torch.aten.mul.Tensor %31, %1017 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_1035 = torch.constant.none
    %none_1036 = torch.constant.none
    %int6_1037 = torch.constant.int 6
    %cpu_1038 = torch.constant.device "cpu"
    %int0_1039 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1018, %none_1035, %none_1036, %int6_1037, %cpu_1038, %int0_1039 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1040 = torch.constant.int 5
    %1019 = torch.prims.convert_element_type %1018, %int5_1040 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_1041 = torch.constant.int 2
    %1020 = torch.aten.view.dtype %32, %int2_1041 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %1021 = torch.aten.detach %1020 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_1042 = torch.constant.int -1
    %int17_1043 = torch.constant.int 17
    %1022 = torch.prim.ListConstruct %int-1_1042, %int17_1043 : (!torch.int, !torch.int) -> !torch.list<int>
    %1023 = torch.aten.view %1021, %1022 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_1044 = torch.constant.int 5632
    %int-1_1045 = torch.constant.int -1
    %int17_1046 = torch.constant.int 17
    %1024 = torch.prim.ListConstruct %int5632_1044, %int-1_1045, %int17_1046 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1025 = torch.aten.view %1023, %1024 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_1047 = torch.constant.int 2
    %int0_1048 = torch.constant.int 0
    %int1_1049 = torch.constant.int 1
    %int1_1050 = torch.constant.int 1
    %1026 = torch.aten.slice.Tensor %1025, %int2_1047, %int0_1048, %int1_1049, %int1_1050 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_1051 = torch.constant.int 5
    %1027 = torch.aten.view.dtype %1026, %int5_1051 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %1028 = torch.aten.detach %1027 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_1052 = torch.constant.int 2
    %int1_1053 = torch.constant.int 1
    %int9223372036854775807_1054 = torch.constant.int 9223372036854775807
    %int1_1055 = torch.constant.int 1
    %1029 = torch.aten.slice.Tensor %1025, %int2_1052, %int1_1053, %int9223372036854775807_1054, %int1_1055 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_1056 = torch.constant.int 1
    %1030 = torch.aten.view.dtype %1029, %int1_1056 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %1031 = torch.aten.detach %1030 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %1032 = torch_c.to_builtin_tensor %1019 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_1057 = tensor.cast %1032 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %1033 = torch_c.to_builtin_tensor %1028 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %1034 = torch_c.to_builtin_tensor %1031 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %1035 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_1057, %1033, %1034) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_1058 = tensor.cast %1035 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %1036 = torch_c.from_builtin_tensor %cast_1058 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %1037 = torch.aten.silu %1036 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_1059 = torch.constant.int 2
    %1038 = torch.aten.view.dtype %33, %int2_1059 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %1039 = torch.aten.detach %1038 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_1060 = torch.constant.int -1
    %int17_1061 = torch.constant.int 17
    %1040 = torch.prim.ListConstruct %int-1_1060, %int17_1061 : (!torch.int, !torch.int) -> !torch.list<int>
    %1041 = torch.aten.view %1039, %1040 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_1062 = torch.constant.int 5632
    %int-1_1063 = torch.constant.int -1
    %int17_1064 = torch.constant.int 17
    %1042 = torch.prim.ListConstruct %int5632_1062, %int-1_1063, %int17_1064 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1043 = torch.aten.view %1041, %1042 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_1065 = torch.constant.int 2
    %int0_1066 = torch.constant.int 0
    %int1_1067 = torch.constant.int 1
    %int1_1068 = torch.constant.int 1
    %1044 = torch.aten.slice.Tensor %1043, %int2_1065, %int0_1066, %int1_1067, %int1_1068 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_1069 = torch.constant.int 5
    %1045 = torch.aten.view.dtype %1044, %int5_1069 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %1046 = torch.aten.detach %1045 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_1070 = torch.constant.int 2
    %int1_1071 = torch.constant.int 1
    %int9223372036854775807_1072 = torch.constant.int 9223372036854775807
    %int1_1073 = torch.constant.int 1
    %1047 = torch.aten.slice.Tensor %1043, %int2_1070, %int1_1071, %int9223372036854775807_1072, %int1_1073 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_1074 = torch.constant.int 1
    %1048 = torch.aten.view.dtype %1047, %int1_1074 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %1049 = torch.aten.detach %1048 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %1050 = torch_c.to_builtin_tensor %1019 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_1075 = tensor.cast %1050 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %1051 = torch_c.to_builtin_tensor %1046 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %1052 = torch_c.to_builtin_tensor %1049 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %1053 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_1075, %1051, %1052) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_1076 = tensor.cast %1053 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %1054 = torch_c.from_builtin_tensor %cast_1076 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %1055 = torch.aten.mul.Tensor %1037, %1054 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_1077 = torch.constant.int 2
    %1056 = torch.aten.view.dtype %34, %int2_1077 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %1057 = torch.aten.detach %1056 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_1078 = torch.constant.int -1
    %int17_1079 = torch.constant.int 17
    %1058 = torch.prim.ListConstruct %int-1_1078, %int17_1079 : (!torch.int, !torch.int) -> !torch.list<int>
    %1059 = torch.aten.view %1057, %1058 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_1080 = torch.constant.int 2048
    %int-1_1081 = torch.constant.int -1
    %int17_1082 = torch.constant.int 17
    %1060 = torch.prim.ListConstruct %int2048_1080, %int-1_1081, %int17_1082 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1061 = torch.aten.view %1059, %1060 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_1083 = torch.constant.int 2
    %int0_1084 = torch.constant.int 0
    %int1_1085 = torch.constant.int 1
    %int1_1086 = torch.constant.int 1
    %1062 = torch.aten.slice.Tensor %1061, %int2_1083, %int0_1084, %int1_1085, %int1_1086 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_1087 = torch.constant.int 5
    %1063 = torch.aten.view.dtype %1062, %int5_1087 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %1064 = torch.aten.detach %1063 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_1088 = torch.constant.int 2
    %int1_1089 = torch.constant.int 1
    %int9223372036854775807_1090 = torch.constant.int 9223372036854775807
    %int1_1091 = torch.constant.int 1
    %1065 = torch.aten.slice.Tensor %1061, %int2_1088, %int1_1089, %int9223372036854775807_1090, %int1_1091 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_1092 = torch.constant.int 1
    %1066 = torch.aten.view.dtype %1065, %int1_1092 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %1067 = torch.aten.detach %1066 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %1068 = torch_c.to_builtin_tensor %1055 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_1093 = tensor.cast %1068 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %1069 = torch_c.to_builtin_tensor %1064 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %1070 = torch_c.to_builtin_tensor %1067 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %1071 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_1093, %1069, %1070) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_1094 = tensor.cast %1071 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %1072 = torch_c.from_builtin_tensor %cast_1094 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_1095 = torch.constant.int 1
    %1073 = torch.aten.add.Tensor %1009, %1072, %int1_1095 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_1096 = torch.constant.none
    %none_1097 = torch.constant.none
    %int5_1098 = torch.constant.int 5
    %cpu_1099 = torch.constant.device "cpu"
    %int0_1100 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1073, %none_1096, %none_1097, %int5_1098, %cpu_1099, %int0_1100 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1101 = torch.constant.int 6
    %1074 = torch.prims.convert_element_type %1073, %int6_1101 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_1102 = torch.constant.int 2
    %1075 = torch.aten.pow.Tensor_Scalar %1074, %int2_1102 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_1103 = torch.constant.int -1
    %1076 = torch.prim.ListConstruct %int-1_1103 : (!torch.int) -> !torch.list<int>
    %true_1104 = torch.constant.bool true
    %none_1105 = torch.constant.none
    %1077 = torch.aten.mean.dim %1075, %1076, %true_1104, %none_1105 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_1106 = torch.constant.float 9.9999997473787516E-6
    %int1_1107 = torch.constant.int 1
    %1078 = torch.aten.add.Scalar %1077, %float9.999990e-06_1106, %int1_1107 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1079 = torch.aten.rsqrt %1078 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %1080 = torch.aten.mul.Tensor %1074, %1079 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_1108 = torch.constant.none
    %none_1109 = torch.constant.none
    %int6_1110 = torch.constant.int 6
    %cpu_1111 = torch.constant.device "cpu"
    %int0_1112 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1080, %none_1108, %none_1109, %int6_1110, %cpu_1111, %int0_1112 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1113 = torch.constant.int 5
    %1081 = torch.prims.convert_element_type %1080, %int5_1113 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %1082 = torch.aten.mul.Tensor %43, %1081 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_1114 = torch.constant.none
    %none_1115 = torch.constant.none
    %int6_1116 = torch.constant.int 6
    %cpu_1117 = torch.constant.device "cpu"
    %int0_1118 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1082, %none_1114, %none_1115, %int6_1116, %cpu_1117, %int0_1118 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1119 = torch.constant.int 5
    %1083 = torch.prims.convert_element_type %1082, %int5_1119 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_1120 = torch.constant.int 2
    %1084 = torch.aten.view.dtype %44, %int2_1120 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %1085 = torch.aten.detach %1084 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_1121 = torch.constant.int -1
    %int17_1122 = torch.constant.int 17
    %1086 = torch.prim.ListConstruct %int-1_1121, %int17_1122 : (!torch.int, !torch.int) -> !torch.list<int>
    %1087 = torch.aten.view %1085, %1086 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_1123 = torch.constant.int 2048
    %int-1_1124 = torch.constant.int -1
    %int17_1125 = torch.constant.int 17
    %1088 = torch.prim.ListConstruct %int2048_1123, %int-1_1124, %int17_1125 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1089 = torch.aten.view %1087, %1088 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_1126 = torch.constant.int 2
    %int0_1127 = torch.constant.int 0
    %int1_1128 = torch.constant.int 1
    %int1_1129 = torch.constant.int 1
    %1090 = torch.aten.slice.Tensor %1089, %int2_1126, %int0_1127, %int1_1128, %int1_1129 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_1130 = torch.constant.int 5
    %1091 = torch.aten.view.dtype %1090, %int5_1130 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %1092 = torch.aten.detach %1091 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_1131 = torch.constant.int 2
    %int1_1132 = torch.constant.int 1
    %int9223372036854775807_1133 = torch.constant.int 9223372036854775807
    %int1_1134 = torch.constant.int 1
    %1093 = torch.aten.slice.Tensor %1089, %int2_1131, %int1_1132, %int9223372036854775807_1133, %int1_1134 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_1135 = torch.constant.int 1
    %1094 = torch.aten.view.dtype %1093, %int1_1135 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %1095 = torch.aten.detach %1094 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %1096 = torch_c.to_builtin_tensor %1083 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_1136 = tensor.cast %1096 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %1097 = torch_c.to_builtin_tensor %1092 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %1098 = torch_c.to_builtin_tensor %1095 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %1099 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_1136, %1097, %1098) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_1137 = tensor.cast %1099 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %1100 = torch_c.from_builtin_tensor %cast_1137 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_1138 = torch.constant.int 2
    %1101 = torch.aten.view.dtype %45, %int2_1138 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %1102 = torch.aten.detach %1101 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_1139 = torch.constant.int -1
    %int17_1140 = torch.constant.int 17
    %1103 = torch.prim.ListConstruct %int-1_1139, %int17_1140 : (!torch.int, !torch.int) -> !torch.list<int>
    %1104 = torch.aten.view %1102, %1103 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_1141 = torch.constant.int 256
    %int-1_1142 = torch.constant.int -1
    %int17_1143 = torch.constant.int 17
    %1105 = torch.prim.ListConstruct %int256_1141, %int-1_1142, %int17_1143 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1106 = torch.aten.view %1104, %1105 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_1144 = torch.constant.int 2
    %int0_1145 = torch.constant.int 0
    %int1_1146 = torch.constant.int 1
    %int1_1147 = torch.constant.int 1
    %1107 = torch.aten.slice.Tensor %1106, %int2_1144, %int0_1145, %int1_1146, %int1_1147 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_1148 = torch.constant.int 5
    %1108 = torch.aten.view.dtype %1107, %int5_1148 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %1109 = torch.aten.detach %1108 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_1149 = torch.constant.int 2
    %int1_1150 = torch.constant.int 1
    %int9223372036854775807_1151 = torch.constant.int 9223372036854775807
    %int1_1152 = torch.constant.int 1
    %1110 = torch.aten.slice.Tensor %1106, %int2_1149, %int1_1150, %int9223372036854775807_1151, %int1_1152 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_1153 = torch.constant.int 1
    %1111 = torch.aten.view.dtype %1110, %int1_1153 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %1112 = torch.aten.detach %1111 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %1113 = torch_c.to_builtin_tensor %1083 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_1154 = tensor.cast %1113 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %1114 = torch_c.to_builtin_tensor %1109 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %1115 = torch_c.to_builtin_tensor %1112 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %1116 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_1154, %1114, %1115) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_1155 = tensor.cast %1116 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %1117 = torch_c.from_builtin_tensor %cast_1155 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_1156 = torch.constant.int 2
    %1118 = torch.aten.view.dtype %46, %int2_1156 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %1119 = torch.aten.detach %1118 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_1157 = torch.constant.int -1
    %int17_1158 = torch.constant.int 17
    %1120 = torch.prim.ListConstruct %int-1_1157, %int17_1158 : (!torch.int, !torch.int) -> !torch.list<int>
    %1121 = torch.aten.view %1119, %1120 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_1159 = torch.constant.int 256
    %int-1_1160 = torch.constant.int -1
    %int17_1161 = torch.constant.int 17
    %1122 = torch.prim.ListConstruct %int256_1159, %int-1_1160, %int17_1161 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1123 = torch.aten.view %1121, %1122 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_1162 = torch.constant.int 2
    %int0_1163 = torch.constant.int 0
    %int1_1164 = torch.constant.int 1
    %int1_1165 = torch.constant.int 1
    %1124 = torch.aten.slice.Tensor %1123, %int2_1162, %int0_1163, %int1_1164, %int1_1165 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_1166 = torch.constant.int 5
    %1125 = torch.aten.view.dtype %1124, %int5_1166 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %1126 = torch.aten.detach %1125 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_1167 = torch.constant.int 2
    %int1_1168 = torch.constant.int 1
    %int9223372036854775807_1169 = torch.constant.int 9223372036854775807
    %int1_1170 = torch.constant.int 1
    %1127 = torch.aten.slice.Tensor %1123, %int2_1167, %int1_1168, %int9223372036854775807_1169, %int1_1170 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_1171 = torch.constant.int 1
    %1128 = torch.aten.view.dtype %1127, %int1_1171 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %1129 = torch.aten.detach %1128 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %1130 = torch_c.to_builtin_tensor %1083 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_1172 = tensor.cast %1130 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %1131 = torch_c.to_builtin_tensor %1126 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %1132 = torch_c.to_builtin_tensor %1129 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %1133 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_1172, %1131, %1132) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_1173 = tensor.cast %1133 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %1134 = torch_c.from_builtin_tensor %cast_1173 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_1174 = torch.constant.int 4
    %int1_1175 = torch.constant.int 1
    %int32_1176 = torch.constant.int 32
    %int64_1177 = torch.constant.int 64
    %1135 = torch.prim.ListConstruct %int4_1174, %int1_1175, %int32_1176, %int64_1177 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1136 = torch.aten.view %1100, %1135 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_1178 = torch.constant.int 4
    %int1_1179 = torch.constant.int 1
    %int4_1180 = torch.constant.int 4
    %int64_1181 = torch.constant.int 64
    %1137 = torch.prim.ListConstruct %int4_1178, %int1_1179, %int4_1180, %int64_1181 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1138 = torch.aten.view %1117, %1137 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_1182 = torch.constant.int 4
    %int1_1183 = torch.constant.int 1
    %int4_1184 = torch.constant.int 4
    %int64_1185 = torch.constant.int 64
    %1139 = torch.prim.ListConstruct %int4_1182, %int1_1183, %int4_1184, %int64_1185 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1140 = torch.aten.view %1134, %1139 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_1186 = torch.constant.int 0
    %int1_1187 = torch.constant.int 1
    %none_1188 = torch.constant.none
    %none_1189 = torch.constant.none
    %cpu_1190 = torch.constant.device "cpu"
    %false_1191 = torch.constant.bool false
    %1141 = torch.aten.arange.start %int0_1186, %int1_1187, %none_1188, %none_1189, %cpu_1190, %false_1191 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_1192 = torch.constant.int 0
    %1142 = torch.aten.unsqueeze %1141, %int0_1192 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_1193 = torch.constant.int 1
    %1143 = torch.aten.unsqueeze %arg2, %int1_1193 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1194 = torch.constant.int 1
    %1144 = torch.aten.add.Tensor %1142, %1143, %int1_1194 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_1195 = torch.constant.int 0
    %int64_1196 = torch.constant.int 64
    %int2_1197 = torch.constant.int 2
    %none_1198 = torch.constant.none
    %none_1199 = torch.constant.none
    %cpu_1200 = torch.constant.device "cpu"
    %false_1201 = torch.constant.bool false
    %1145 = torch.aten.arange.start_step %int0_1195, %int64_1196, %int2_1197, %none_1198, %none_1199, %cpu_1200, %false_1201 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_1202 = torch.constant.none
    %none_1203 = torch.constant.none
    %int4_1204 = torch.constant.int 4
    %cpu_1205 = torch.constant.device "cpu"
    %int0_1206 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1145, %none_1202, %none_1203, %int4_1204, %cpu_1205, %int0_1206 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1207 = torch.constant.int 6
    %1146 = torch.prims.convert_element_type %1145, %int6_1207 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_1208 = torch.constant.int 64
    %1147 = torch.aten.div.Scalar %1146, %int64_1208 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_1209 = torch.constant.float 1.000000e+04
    %1148 = torch.aten.pow.Scalar %float1.000000e04_1209, %1147 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %1149 = torch.aten.reciprocal %1148 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_1210 = torch.constant.float 1.000000e+00
    %1150 = torch.aten.mul.Scalar %1149, %float1.000000e00_1210 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_1211 = torch.constant.none
    %1151 = torch.aten.clone %35, %none_1211 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_1212 = torch.constant.int 0
    %1152 = torch.aten.unsqueeze %1150, %int0_1212 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_1213 = torch.constant.int 2
    %1153 = torch.aten.unsqueeze %1152, %int2_1213 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_1214 = torch.constant.none
    %none_1215 = torch.constant.none
    %int6_1216 = torch.constant.int 6
    %cpu_1217 = torch.constant.device "cpu"
    %int0_1218 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1153, %none_1214, %none_1215, %int6_1216, %cpu_1217, %int0_1218 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_1219 = torch.constant.int 4
    %int-1_1220 = torch.constant.int -1
    %int1_1221 = torch.constant.int 1
    %1154 = torch.prim.ListConstruct %int4_1219, %int-1_1220, %int1_1221 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1222 = torch.constant.bool false
    %1155 = torch.aten.expand %1153, %1154, %false_1222 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_1223 = torch.constant.int 1
    %1156 = torch.aten.unsqueeze %1144, %int1_1223 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_1224 = torch.constant.none
    %none_1225 = torch.constant.none
    %int4_1226 = torch.constant.int 4
    %cpu_1227 = torch.constant.device "cpu"
    %int0_1228 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1156, %none_1224, %none_1225, %int4_1226, %cpu_1227, %int0_1228 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1229 = torch.constant.int 6
    %1157 = torch.prims.convert_element_type %1156, %int6_1229 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1158 = torch.aten.matmul %1155, %1157 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_1230 = torch.constant.int 1
    %int2_1231 = torch.constant.int 2
    %1159 = torch.aten.transpose.int %1158, %int1_1230, %int2_1231 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %1160 = torch.aten.cos %1159 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %1161 = torch.aten.mul.Tensor %1160, %1151 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_1232 = torch.constant.none
    %none_1233 = torch.constant.none
    %int6_1234 = torch.constant.int 6
    %cpu_1235 = torch.constant.device "cpu"
    %int0_1236 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1161, %none_1232, %none_1233, %int6_1234, %cpu_1235, %int0_1236 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1237 = torch.constant.int 5
    %1162 = torch.prims.convert_element_type %1161, %int5_1237 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %1163 = torch.aten.sin %1159 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %1164 = torch.aten.mul.Tensor %1163, %1151 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_1238 = torch.constant.none
    %none_1239 = torch.constant.none
    %int6_1240 = torch.constant.int 6
    %cpu_1241 = torch.constant.device "cpu"
    %int0_1242 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1164, %none_1238, %none_1239, %int6_1240, %cpu_1241, %int0_1242 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1243 = torch.constant.int 5
    %1165 = torch.prims.convert_element_type %1164, %int5_1243 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_1244 = torch.constant.int 2
    %1166 = torch.aten.unsqueeze %1162, %int2_1244 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_1245 = torch.constant.int 2
    %1167 = torch.aten.unsqueeze %1165, %int2_1245 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_1246 = torch.constant.none
    %none_1247 = torch.constant.none
    %int5_1248 = torch.constant.int 5
    %cpu_1249 = torch.constant.device "cpu"
    %int0_1250 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1166, %none_1246, %none_1247, %int5_1248, %cpu_1249, %int0_1250 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_1251 = torch.constant.none
    %none_1252 = torch.constant.none
    %int5_1253 = torch.constant.int 5
    %cpu_1254 = torch.constant.device "cpu"
    %int0_1255 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1167, %none_1251, %none_1252, %int5_1253, %cpu_1254, %int0_1255 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_1256 = torch.constant.none
    %none_1257 = torch.constant.none
    %int5_1258 = torch.constant.int 5
    %cpu_1259 = torch.constant.device "cpu"
    %int0_1260 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1136, %none_1256, %none_1257, %int5_1258, %cpu_1259, %int0_1260 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_1261 = torch.constant.int 3
    %int0_1262 = torch.constant.int 0
    %int64_1263 = torch.constant.int 64
    %int2_1264 = torch.constant.int 2
    %1168 = torch.aten.slice.Tensor %1136, %int3_1261, %int0_1262, %int64_1263, %int2_1264 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_1265 = torch.constant.int 3
    %int1_1266 = torch.constant.int 1
    %int64_1267 = torch.constant.int 64
    %int2_1268 = torch.constant.int 2
    %1169 = torch.aten.slice.Tensor %1136, %int3_1265, %int1_1266, %int64_1267, %int2_1268 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %1170 = torch.aten.mul.Tensor %1168, %1166 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %1171 = torch.aten.mul.Tensor %1169, %1167 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_1269 = torch.constant.int 1
    %1172 = torch.aten.sub.Tensor %1170, %1171, %int1_1269 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %1173 = torch.aten.mul.Tensor %1169, %1166 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %1174 = torch.aten.mul.Tensor %1168, %1167 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_1270 = torch.constant.int 1
    %1175 = torch.aten.add.Tensor %1173, %1174, %int1_1270 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %1176 = torch_c.to_builtin_tensor %1172 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_1271 = tensor.cast %1176 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %1177 = torch_c.to_builtin_tensor %1175 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_1272 = tensor.cast %1177 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %1178 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_1271, %cast_1272) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_1273 = tensor.cast %1178 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %1179 = torch_c.from_builtin_tensor %cast_1273 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_1274 = torch.constant.int 4
    %int1_1275 = torch.constant.int 1
    %int32_1276 = torch.constant.int 32
    %int64_1277 = torch.constant.int 64
    %1180 = torch.prim.ListConstruct %int4_1274, %int1_1275, %int32_1276, %int64_1277 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1181 = torch.aten.view %1179, %1180 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_1278 = torch.constant.none
    %none_1279 = torch.constant.none
    %int5_1280 = torch.constant.int 5
    %cpu_1281 = torch.constant.device "cpu"
    %int0_1282 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1181, %none_1278, %none_1279, %int5_1280, %cpu_1281, %int0_1282 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_1283 = torch.constant.int 0
    %int1_1284 = torch.constant.int 1
    %none_1285 = torch.constant.none
    %none_1286 = torch.constant.none
    %cpu_1287 = torch.constant.device "cpu"
    %false_1288 = torch.constant.bool false
    %1182 = torch.aten.arange.start %int0_1283, %int1_1284, %none_1285, %none_1286, %cpu_1287, %false_1288 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_1289 = torch.constant.int 0
    %1183 = torch.aten.unsqueeze %1182, %int0_1289 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_1290 = torch.constant.int 1
    %1184 = torch.aten.unsqueeze %arg2, %int1_1290 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1291 = torch.constant.int 1
    %1185 = torch.aten.add.Tensor %1183, %1184, %int1_1291 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_1292 = torch.constant.int 0
    %int64_1293 = torch.constant.int 64
    %int2_1294 = torch.constant.int 2
    %none_1295 = torch.constant.none
    %none_1296 = torch.constant.none
    %cpu_1297 = torch.constant.device "cpu"
    %false_1298 = torch.constant.bool false
    %1186 = torch.aten.arange.start_step %int0_1292, %int64_1293, %int2_1294, %none_1295, %none_1296, %cpu_1297, %false_1298 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_1299 = torch.constant.none
    %none_1300 = torch.constant.none
    %int4_1301 = torch.constant.int 4
    %cpu_1302 = torch.constant.device "cpu"
    %int0_1303 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1186, %none_1299, %none_1300, %int4_1301, %cpu_1302, %int0_1303 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1304 = torch.constant.int 6
    %1187 = torch.prims.convert_element_type %1186, %int6_1304 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_1305 = torch.constant.int 64
    %1188 = torch.aten.div.Scalar %1187, %int64_1305 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_1306 = torch.constant.float 1.000000e+04
    %1189 = torch.aten.pow.Scalar %float1.000000e04_1306, %1188 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %1190 = torch.aten.reciprocal %1189 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_1307 = torch.constant.float 1.000000e+00
    %1191 = torch.aten.mul.Scalar %1190, %float1.000000e00_1307 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_1308 = torch.constant.none
    %1192 = torch.aten.clone %36, %none_1308 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_1309 = torch.constant.int 0
    %1193 = torch.aten.unsqueeze %1191, %int0_1309 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_1310 = torch.constant.int 2
    %1194 = torch.aten.unsqueeze %1193, %int2_1310 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_1311 = torch.constant.none
    %none_1312 = torch.constant.none
    %int6_1313 = torch.constant.int 6
    %cpu_1314 = torch.constant.device "cpu"
    %int0_1315 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1194, %none_1311, %none_1312, %int6_1313, %cpu_1314, %int0_1315 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_1316 = torch.constant.int 4
    %int-1_1317 = torch.constant.int -1
    %int1_1318 = torch.constant.int 1
    %1195 = torch.prim.ListConstruct %int4_1316, %int-1_1317, %int1_1318 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1319 = torch.constant.bool false
    %1196 = torch.aten.expand %1194, %1195, %false_1319 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_1320 = torch.constant.int 1
    %1197 = torch.aten.unsqueeze %1185, %int1_1320 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_1321 = torch.constant.none
    %none_1322 = torch.constant.none
    %int4_1323 = torch.constant.int 4
    %cpu_1324 = torch.constant.device "cpu"
    %int0_1325 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1197, %none_1321, %none_1322, %int4_1323, %cpu_1324, %int0_1325 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1326 = torch.constant.int 6
    %1198 = torch.prims.convert_element_type %1197, %int6_1326 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1199 = torch.aten.matmul %1196, %1198 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_1327 = torch.constant.int 1
    %int2_1328 = torch.constant.int 2
    %1200 = torch.aten.transpose.int %1199, %int1_1327, %int2_1328 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %1201 = torch.aten.cos %1200 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %1202 = torch.aten.mul.Tensor %1201, %1192 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_1329 = torch.constant.none
    %none_1330 = torch.constant.none
    %int6_1331 = torch.constant.int 6
    %cpu_1332 = torch.constant.device "cpu"
    %int0_1333 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1202, %none_1329, %none_1330, %int6_1331, %cpu_1332, %int0_1333 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1334 = torch.constant.int 5
    %1203 = torch.prims.convert_element_type %1202, %int5_1334 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %1204 = torch.aten.sin %1200 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %1205 = torch.aten.mul.Tensor %1204, %1192 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_1335 = torch.constant.none
    %none_1336 = torch.constant.none
    %int6_1337 = torch.constant.int 6
    %cpu_1338 = torch.constant.device "cpu"
    %int0_1339 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1205, %none_1335, %none_1336, %int6_1337, %cpu_1338, %int0_1339 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1340 = torch.constant.int 5
    %1206 = torch.prims.convert_element_type %1205, %int5_1340 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_1341 = torch.constant.int 2
    %1207 = torch.aten.unsqueeze %1203, %int2_1341 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_1342 = torch.constant.int 2
    %1208 = torch.aten.unsqueeze %1206, %int2_1342 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_1343 = torch.constant.none
    %none_1344 = torch.constant.none
    %int5_1345 = torch.constant.int 5
    %cpu_1346 = torch.constant.device "cpu"
    %int0_1347 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1207, %none_1343, %none_1344, %int5_1345, %cpu_1346, %int0_1347 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_1348 = torch.constant.none
    %none_1349 = torch.constant.none
    %int5_1350 = torch.constant.int 5
    %cpu_1351 = torch.constant.device "cpu"
    %int0_1352 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1208, %none_1348, %none_1349, %int5_1350, %cpu_1351, %int0_1352 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_1353 = torch.constant.none
    %none_1354 = torch.constant.none
    %int5_1355 = torch.constant.int 5
    %cpu_1356 = torch.constant.device "cpu"
    %int0_1357 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1138, %none_1353, %none_1354, %int5_1355, %cpu_1356, %int0_1357 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_1358 = torch.constant.int 3
    %int0_1359 = torch.constant.int 0
    %int64_1360 = torch.constant.int 64
    %int2_1361 = torch.constant.int 2
    %1209 = torch.aten.slice.Tensor %1138, %int3_1358, %int0_1359, %int64_1360, %int2_1361 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_1362 = torch.constant.int 3
    %int1_1363 = torch.constant.int 1
    %int64_1364 = torch.constant.int 64
    %int2_1365 = torch.constant.int 2
    %1210 = torch.aten.slice.Tensor %1138, %int3_1362, %int1_1363, %int64_1364, %int2_1365 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %1211 = torch.aten.mul.Tensor %1209, %1207 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %1212 = torch.aten.mul.Tensor %1210, %1208 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_1366 = torch.constant.int 1
    %1213 = torch.aten.sub.Tensor %1211, %1212, %int1_1366 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %1214 = torch.aten.mul.Tensor %1210, %1207 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %1215 = torch.aten.mul.Tensor %1209, %1208 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_1367 = torch.constant.int 1
    %1216 = torch.aten.add.Tensor %1214, %1215, %int1_1367 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %1217 = torch_c.to_builtin_tensor %1213 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_1368 = tensor.cast %1217 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %1218 = torch_c.to_builtin_tensor %1216 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_1369 = tensor.cast %1218 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %1219 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_1368, %cast_1369) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_1370 = tensor.cast %1219 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %1220 = torch_c.from_builtin_tensor %cast_1370 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_1371 = torch.constant.int 4
    %int1_1372 = torch.constant.int 1
    %int4_1373 = torch.constant.int 4
    %int64_1374 = torch.constant.int 64
    %1221 = torch.prim.ListConstruct %int4_1371, %int1_1372, %int4_1373, %int64_1374 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1222 = torch.aten.view %1220, %1221 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_1375 = torch.constant.none
    %none_1376 = torch.constant.none
    %int5_1377 = torch.constant.int 5
    %cpu_1378 = torch.constant.device "cpu"
    %int0_1379 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1222, %none_1375, %none_1376, %int5_1377, %cpu_1378, %int0_1379 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_1380 = torch.constant.int 32
    %1223 = torch.aten.floor_divide.Scalar %arg2, %int32_1380 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_1381 = torch.constant.int 1
    %1224 = torch.aten.unsqueeze %1223, %int1_1381 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1382 = torch.constant.int 1
    %false_1383 = torch.constant.bool false
    %1225 = torch.aten.gather %arg3, %int1_1382, %1224, %false_1383 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_1384 = torch.constant.int 4
    %int1_1385 = torch.constant.int 1
    %int1_1386 = torch.constant.int 1
    %1226 = torch.prim.ListConstruct %int4_1384, %int1_1385, %int1_1386 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1227 = torch.aten.view %1225, %1226 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_1387 = torch.constant.int 32
    %1228 = torch.aten.remainder.Scalar %arg2, %int32_1387 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_1388 = torch.constant.int 4
    %int1_1389 = torch.constant.int 1
    %int1_1390 = torch.constant.int 1
    %1229 = torch.prim.ListConstruct %int4_1388, %int1_1389, %int1_1390 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1230 = torch.aten.view %1228, %1229 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_1391 = torch.constant.int 4
    %none_1392 = torch.constant.none
    %none_1393 = torch.constant.none
    %cpu_1394 = torch.constant.device "cpu"
    %false_1395 = torch.constant.bool false
    %1231 = torch.aten.arange %int4_1391, %none_1392, %none_1393, %cpu_1394, %false_1395 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_1396 = torch.constant.int 1
    %int1_1397 = torch.constant.int 1
    %int4_1398 = torch.constant.int 4
    %1232 = torch.prim.ListConstruct %int1_1396, %int1_1397, %int4_1398 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1233 = torch.aten.view %1231, %1232 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_1399 = torch.constant.none
    %1234 = torch.aten.clone %37, %none_1399 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_1400 = torch.constant.int 1
    %int1_1401 = torch.constant.int 1
    %int1_1402 = torch.constant.int 1
    %1235 = torch.prim.ListConstruct %int1_1400, %int1_1401, %int1_1402 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1236 = torch.aten.view %1234, %1235 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_1403 = torch.constant.int 22
    %1237 = torch.aten.mul.Scalar %1227, %int22_1403 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_1404 = torch.constant.int 2
    %int1_1405 = torch.constant.int 1
    %1238 = torch.aten.add.Scalar %1237, %int2_1404, %int1_1405 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_1406 = torch.constant.int 2
    %1239 = torch.aten.mul.Scalar %1238, %int2_1406 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_1407 = torch.constant.int 1
    %1240 = torch.aten.add.Tensor %1239, %1236, %int1_1407 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_1408 = torch.constant.int 4
    %1241 = torch.aten.mul.Scalar %1240, %int4_1408 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_1409 = torch.constant.int 1
    %1242 = torch.aten.add.Tensor %1241, %1233, %int1_1409 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_1410 = torch.constant.int 32
    %1243 = torch.aten.mul.Scalar %1242, %int32_1410 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_1411 = torch.constant.int 1
    %1244 = torch.aten.add.Tensor %1243, %1230, %int1_1411 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_1412 = torch.constant.none
    %none_1413 = torch.constant.none
    %int5_1414 = torch.constant.int 5
    %cpu_1415 = torch.constant.device "cpu"
    %int0_1416 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1222, %none_1412, %none_1413, %int5_1414, %cpu_1415, %int0_1416 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_1417 = torch.constant.int 22
    %int2_1418 = torch.constant.int 2
    %int4_1419 = torch.constant.int 4
    %int32_1420 = torch.constant.int 32
    %int64_1421 = torch.constant.int 64
    %1245 = torch.prim.ListConstruct %381, %int22_1417, %int2_1418, %int4_1419, %int32_1420, %int64_1421 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1246 = torch.aten.view %940, %1245 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1246, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_1422 = torch.constant.int 64
    %1247 = torch.prim.ListConstruct %553, %int64_1422 : (!torch.int, !torch.int) -> !torch.list<int>
    %1248 = torch.aten.view %1246, %1247 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %1248, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %1249 = torch.prim.ListConstruct %1244 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_1423 = torch.constant.bool false
    %1250 = torch.aten.index_put %1248, %1249, %1222, %false_1423 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %1250, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_1424 = torch.constant.int 22
    %int2_1425 = torch.constant.int 2
    %int4_1426 = torch.constant.int 4
    %int32_1427 = torch.constant.int 32
    %int64_1428 = torch.constant.int 64
    %1251 = torch.prim.ListConstruct %381, %int22_1424, %int2_1425, %int4_1426, %int32_1427, %int64_1428 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1252 = torch.aten.view %1250, %1251 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1252, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_1429 = torch.constant.int 360448
    %1253 = torch.prim.ListConstruct %381, %int360448_1429 : (!torch.int, !torch.int) -> !torch.list<int>
    %1254 = torch.aten.view %1252, %1253 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %1254, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_1430 = torch.constant.int 22
    %int2_1431 = torch.constant.int 2
    %int4_1432 = torch.constant.int 4
    %int32_1433 = torch.constant.int 32
    %int64_1434 = torch.constant.int 64
    %1255 = torch.prim.ListConstruct %381, %int22_1430, %int2_1431, %int4_1432, %int32_1433, %int64_1434 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1256 = torch.aten.view %1254, %1255 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1256, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_1435 = torch.constant.int 64
    %1257 = torch.prim.ListConstruct %553, %int64_1435 : (!torch.int, !torch.int) -> !torch.list<int>
    %1258 = torch.aten.view %1256, %1257 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %1258, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_1436 = torch.constant.none
    %1259 = torch.aten.clone %38, %none_1436 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_1437 = torch.constant.int 1
    %int1_1438 = torch.constant.int 1
    %int1_1439 = torch.constant.int 1
    %1260 = torch.prim.ListConstruct %int1_1437, %int1_1438, %int1_1439 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1261 = torch.aten.view %1259, %1260 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_1440 = torch.constant.int 22
    %1262 = torch.aten.mul.Scalar %1227, %int22_1440 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_1441 = torch.constant.int 2
    %int1_1442 = torch.constant.int 1
    %1263 = torch.aten.add.Scalar %1262, %int2_1441, %int1_1442 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_1443 = torch.constant.int 2
    %1264 = torch.aten.mul.Scalar %1263, %int2_1443 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_1444 = torch.constant.int 1
    %1265 = torch.aten.add.Tensor %1264, %1261, %int1_1444 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_1445 = torch.constant.int 4
    %1266 = torch.aten.mul.Scalar %1265, %int4_1445 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_1446 = torch.constant.int 1
    %1267 = torch.aten.add.Tensor %1266, %1233, %int1_1446 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_1447 = torch.constant.int 32
    %1268 = torch.aten.mul.Scalar %1267, %int32_1447 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_1448 = torch.constant.int 1
    %1269 = torch.aten.add.Tensor %1268, %1230, %int1_1448 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_1449 = torch.constant.none
    %none_1450 = torch.constant.none
    %int5_1451 = torch.constant.int 5
    %cpu_1452 = torch.constant.device "cpu"
    %int0_1453 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1140, %none_1449, %none_1450, %int5_1451, %cpu_1452, %int0_1453 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %1270 = torch.prim.ListConstruct %1269 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_1454 = torch.constant.bool false
    %1271 = torch.aten.index_put %1258, %1270, %1140, %false_1454 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %1271, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_1455 = torch.constant.int 22
    %int2_1456 = torch.constant.int 2
    %int4_1457 = torch.constant.int 4
    %int32_1458 = torch.constant.int 32
    %int64_1459 = torch.constant.int 64
    %1272 = torch.prim.ListConstruct %381, %int22_1455, %int2_1456, %int4_1457, %int32_1458, %int64_1459 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1273 = torch.aten.view %1271, %1272 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1273, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_1460 = torch.constant.int 360448
    %1274 = torch.prim.ListConstruct %381, %int360448_1460 : (!torch.int, !torch.int) -> !torch.list<int>
    %1275 = torch.aten.view %1273, %1274 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %1275, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_1461 = torch.constant.none
    %1276 = torch.aten.clone %39, %none_1461 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_1462 = torch.constant.none
    %1277 = torch.aten.clone %40, %none_1462 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_1463 = torch.constant.none
    %1278 = torch.aten.clone %41, %none_1463 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_1464 = torch.constant.int 22
    %int2_1465 = torch.constant.int 2
    %int4_1466 = torch.constant.int 4
    %int32_1467 = torch.constant.int 32
    %int64_1468 = torch.constant.int 64
    %1279 = torch.prim.ListConstruct %381, %int22_1464, %int2_1465, %int4_1466, %int32_1467, %int64_1468 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1280 = torch.aten.view %1275, %1279 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1280, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %1281 = torch_c.to_builtin_tensor %1280 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %1282 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_1469 = tensor.cast %1282 : tensor<4x?xi64> to tensor<?x?xi64>
    %1283 = torch_c.to_builtin_tensor %1276 : !torch.vtensor<[],si64> -> tensor<i64>
    %1284 = torch_c.to_builtin_tensor %1277 : !torch.vtensor<[],si64> -> tensor<i64>
    %1285 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%1281, %cast_1469, %1283, %1284) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_1470 = tensor.cast %1285 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %1286 = torch_c.from_builtin_tensor %cast_1470 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %1286, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %1287 = torch_c.to_builtin_tensor %1280 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %1288 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_1471 = tensor.cast %1288 : tensor<4x?xi64> to tensor<?x?xi64>
    %1289 = torch_c.to_builtin_tensor %1276 : !torch.vtensor<[],si64> -> tensor<i64>
    %1290 = torch_c.to_builtin_tensor %1278 : !torch.vtensor<[],si64> -> tensor<i64>
    %1291 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%1287, %cast_1471, %1289, %1290) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_1472 = tensor.cast %1291 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %1292 = torch_c.from_builtin_tensor %cast_1472 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %1292, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_1473 = torch.constant.int 2
    %int3_1474 = torch.constant.int 3
    %1293 = torch.aten.transpose.int %1286, %int2_1473, %int3_1474 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1293, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_1475 = torch.constant.int 0
    %1294 = torch.aten.clone %1293, %int0_1475 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1294, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_1476 = torch.constant.int 4
    %int4_1477 = torch.constant.int 4
    %int64_1478 = torch.constant.int 64
    %1295 = torch.prim.ListConstruct %int4_1476, %623, %int4_1477, %int64_1478 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1296 = torch.aten._unsafe_view %1294, %1295 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %1296, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_1479 = torch.constant.int 2
    %int3_1480 = torch.constant.int 3
    %1297 = torch.aten.transpose.int %1292, %int2_1479, %int3_1480 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1297, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_1481 = torch.constant.int 0
    %1298 = torch.aten.clone %1297, %int0_1481 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1298, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_1482 = torch.constant.int 4
    %int4_1483 = torch.constant.int 4
    %int64_1484 = torch.constant.int 64
    %1299 = torch.prim.ListConstruct %int4_1482, %623, %int4_1483, %int64_1484 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1300 = torch.aten._unsafe_view %1298, %1299 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %1300, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_1485 = torch.constant.int 0
    %int1_1486 = torch.constant.int 1
    %none_1487 = torch.constant.none
    %none_1488 = torch.constant.none
    %cpu_1489 = torch.constant.device "cpu"
    %false_1490 = torch.constant.bool false
    %1301 = torch.aten.arange.start_step %int0_1485, %623, %int1_1486, %none_1487, %none_1488, %cpu_1489, %false_1490 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1301, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_1491 = torch.constant.int -1
    %1302 = torch.aten.unsqueeze %arg1, %int-1_1491 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %1303 = torch.aten.ge.Tensor %1301, %1302 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %1303, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_1492 = torch.constant.none
    %1304 = torch.aten.clone %42, %none_1492 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_1493 = torch.constant.int 0
    %1305 = torch.aten.where.ScalarOther %1303, %1304, %int0_1493 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %1305, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_1494 = torch.constant.none
    %none_1495 = torch.constant.none
    %int5_1496 = torch.constant.int 5
    %cpu_1497 = torch.constant.device "cpu"
    %int0_1498 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1305, %none_1494, %none_1495, %int5_1496, %cpu_1497, %int0_1498 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_1499 = torch.constant.int 1
    %1306 = torch.aten.unsqueeze %1305, %int1_1499 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %1306, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_1500 = torch.constant.int 1
    %1307 = torch.aten.unsqueeze %1306, %int1_1500 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %1307, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_1501 = torch.constant.int -2
    %1308 = torch.aten.unsqueeze %1296, %int-2_1501 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %1308, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_1502 = torch.constant.int 4
    %int4_1503 = torch.constant.int 4
    %int8_1504 = torch.constant.int 8
    %int64_1505 = torch.constant.int 64
    %1309 = torch.prim.ListConstruct %int4_1502, %623, %int4_1503, %int8_1504, %int64_1505 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1506 = torch.constant.bool false
    %1310 = torch.aten.expand %1308, %1309, %false_1506 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1310, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_1507 = torch.constant.int 0
    %1311 = torch.aten.clone %1310, %int0_1507 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1311, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_1508 = torch.constant.int 4
    %int32_1509 = torch.constant.int 32
    %int64_1510 = torch.constant.int 64
    %1312 = torch.prim.ListConstruct %int4_1508, %623, %int32_1509, %int64_1510 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1313 = torch.aten._unsafe_view %1311, %1312 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1313, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_1511 = torch.constant.int -2
    %1314 = torch.aten.unsqueeze %1300, %int-2_1511 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %1314, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_1512 = torch.constant.int 4
    %int4_1513 = torch.constant.int 4
    %int8_1514 = torch.constant.int 8
    %int64_1515 = torch.constant.int 64
    %1315 = torch.prim.ListConstruct %int4_1512, %623, %int4_1513, %int8_1514, %int64_1515 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1516 = torch.constant.bool false
    %1316 = torch.aten.expand %1314, %1315, %false_1516 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1316, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_1517 = torch.constant.int 0
    %1317 = torch.aten.clone %1316, %int0_1517 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1317, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_1518 = torch.constant.int 4
    %int32_1519 = torch.constant.int 32
    %int64_1520 = torch.constant.int 64
    %1318 = torch.prim.ListConstruct %int4_1518, %623, %int32_1519, %int64_1520 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1319 = torch.aten._unsafe_view %1317, %1318 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1319, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_1521 = torch.constant.int 1
    %int2_1522 = torch.constant.int 2
    %1320 = torch.aten.transpose.int %1181, %int1_1521, %int2_1522 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_1523 = torch.constant.int 1
    %int2_1524 = torch.constant.int 2
    %1321 = torch.aten.transpose.int %1313, %int1_1523, %int2_1524 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %1321, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_1525 = torch.constant.int 1
    %int2_1526 = torch.constant.int 2
    %1322 = torch.aten.transpose.int %1319, %int1_1525, %int2_1526 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %1322, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_1527 = torch.constant.float 0.000000e+00
    %false_1528 = torch.constant.bool false
    %none_1529 = torch.constant.none
    %false_1530 = torch.constant.bool false
    %1323 = torch.aten.scaled_dot_product_attention %1320, %1321, %1322, %1307, %float0.000000e00_1527, %false_1528, %none_1529, %false_1530 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_1531 = torch.constant.int 1
    %int2_1532 = torch.constant.int 2
    %1324 = torch.aten.transpose.int %1323, %int1_1531, %int2_1532 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_1533 = torch.constant.int 4
    %int1_1534 = torch.constant.int 1
    %int2048_1535 = torch.constant.int 2048
    %1325 = torch.prim.ListConstruct %int4_1533, %int1_1534, %int2048_1535 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1326 = torch.aten.view %1324, %1325 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_1536 = torch.constant.int 2
    %1327 = torch.aten.view.dtype %47, %int2_1536 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %1328 = torch.aten.detach %1327 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_1537 = torch.constant.int -1
    %int17_1538 = torch.constant.int 17
    %1329 = torch.prim.ListConstruct %int-1_1537, %int17_1538 : (!torch.int, !torch.int) -> !torch.list<int>
    %1330 = torch.aten.view %1328, %1329 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_1539 = torch.constant.int 2048
    %int-1_1540 = torch.constant.int -1
    %int17_1541 = torch.constant.int 17
    %1331 = torch.prim.ListConstruct %int2048_1539, %int-1_1540, %int17_1541 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1332 = torch.aten.view %1330, %1331 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_1542 = torch.constant.int 2
    %int0_1543 = torch.constant.int 0
    %int1_1544 = torch.constant.int 1
    %int1_1545 = torch.constant.int 1
    %1333 = torch.aten.slice.Tensor %1332, %int2_1542, %int0_1543, %int1_1544, %int1_1545 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_1546 = torch.constant.int 5
    %1334 = torch.aten.view.dtype %1333, %int5_1546 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %1335 = torch.aten.detach %1334 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_1547 = torch.constant.int 2
    %int1_1548 = torch.constant.int 1
    %int9223372036854775807_1549 = torch.constant.int 9223372036854775807
    %int1_1550 = torch.constant.int 1
    %1336 = torch.aten.slice.Tensor %1332, %int2_1547, %int1_1548, %int9223372036854775807_1549, %int1_1550 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_1551 = torch.constant.int 1
    %1337 = torch.aten.view.dtype %1336, %int1_1551 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %1338 = torch.aten.detach %1337 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %1339 = torch_c.to_builtin_tensor %1326 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_1552 = tensor.cast %1339 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %1340 = torch_c.to_builtin_tensor %1335 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %1341 = torch_c.to_builtin_tensor %1338 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %1342 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_1552, %1340, %1341) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_1553 = tensor.cast %1342 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %1343 = torch_c.from_builtin_tensor %cast_1553 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_1554 = torch.constant.none
    %none_1555 = torch.constant.none
    %int5_1556 = torch.constant.int 5
    %cpu_1557 = torch.constant.device "cpu"
    %int0_1558 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1343, %none_1554, %none_1555, %int5_1556, %cpu_1557, %int0_1558 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_1559 = torch.constant.int 1
    %1344 = torch.aten.add.Tensor %1073, %1343, %int1_1559 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_1560 = torch.constant.none
    %none_1561 = torch.constant.none
    %int5_1562 = torch.constant.int 5
    %cpu_1563 = torch.constant.device "cpu"
    %int0_1564 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1344, %none_1560, %none_1561, %int5_1562, %cpu_1563, %int0_1564 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1565 = torch.constant.int 6
    %1345 = torch.prims.convert_element_type %1344, %int6_1565 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_1566 = torch.constant.int 2
    %1346 = torch.aten.pow.Tensor_Scalar %1345, %int2_1566 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_1567 = torch.constant.int -1
    %1347 = torch.prim.ListConstruct %int-1_1567 : (!torch.int) -> !torch.list<int>
    %true_1568 = torch.constant.bool true
    %none_1569 = torch.constant.none
    %1348 = torch.aten.mean.dim %1346, %1347, %true_1568, %none_1569 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_1570 = torch.constant.float 9.9999997473787516E-6
    %int1_1571 = torch.constant.int 1
    %1349 = torch.aten.add.Scalar %1348, %float9.999990e-06_1570, %int1_1571 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1350 = torch.aten.rsqrt %1349 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %1351 = torch.aten.mul.Tensor %1345, %1350 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_1572 = torch.constant.none
    %none_1573 = torch.constant.none
    %int6_1574 = torch.constant.int 6
    %cpu_1575 = torch.constant.device "cpu"
    %int0_1576 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1351, %none_1572, %none_1573, %int6_1574, %cpu_1575, %int0_1576 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1577 = torch.constant.int 5
    %1352 = torch.prims.convert_element_type %1351, %int5_1577 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %1353 = torch.aten.mul.Tensor %48, %1352 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_1578 = torch.constant.none
    %none_1579 = torch.constant.none
    %int6_1580 = torch.constant.int 6
    %cpu_1581 = torch.constant.device "cpu"
    %int0_1582 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1353, %none_1578, %none_1579, %int6_1580, %cpu_1581, %int0_1582 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1583 = torch.constant.int 5
    %1354 = torch.prims.convert_element_type %1353, %int5_1583 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_1584 = torch.constant.int 2
    %1355 = torch.aten.view.dtype %49, %int2_1584 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %1356 = torch.aten.detach %1355 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_1585 = torch.constant.int -1
    %int17_1586 = torch.constant.int 17
    %1357 = torch.prim.ListConstruct %int-1_1585, %int17_1586 : (!torch.int, !torch.int) -> !torch.list<int>
    %1358 = torch.aten.view %1356, %1357 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_1587 = torch.constant.int 5632
    %int-1_1588 = torch.constant.int -1
    %int17_1589 = torch.constant.int 17
    %1359 = torch.prim.ListConstruct %int5632_1587, %int-1_1588, %int17_1589 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1360 = torch.aten.view %1358, %1359 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_1590 = torch.constant.int 2
    %int0_1591 = torch.constant.int 0
    %int1_1592 = torch.constant.int 1
    %int1_1593 = torch.constant.int 1
    %1361 = torch.aten.slice.Tensor %1360, %int2_1590, %int0_1591, %int1_1592, %int1_1593 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_1594 = torch.constant.int 5
    %1362 = torch.aten.view.dtype %1361, %int5_1594 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %1363 = torch.aten.detach %1362 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_1595 = torch.constant.int 2
    %int1_1596 = torch.constant.int 1
    %int9223372036854775807_1597 = torch.constant.int 9223372036854775807
    %int1_1598 = torch.constant.int 1
    %1364 = torch.aten.slice.Tensor %1360, %int2_1595, %int1_1596, %int9223372036854775807_1597, %int1_1598 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_1599 = torch.constant.int 1
    %1365 = torch.aten.view.dtype %1364, %int1_1599 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %1366 = torch.aten.detach %1365 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %1367 = torch_c.to_builtin_tensor %1354 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_1600 = tensor.cast %1367 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %1368 = torch_c.to_builtin_tensor %1363 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %1369 = torch_c.to_builtin_tensor %1366 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %1370 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_1600, %1368, %1369) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_1601 = tensor.cast %1370 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %1371 = torch_c.from_builtin_tensor %cast_1601 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %1372 = torch.aten.silu %1371 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_1602 = torch.constant.int 2
    %1373 = torch.aten.view.dtype %50, %int2_1602 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %1374 = torch.aten.detach %1373 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_1603 = torch.constant.int -1
    %int17_1604 = torch.constant.int 17
    %1375 = torch.prim.ListConstruct %int-1_1603, %int17_1604 : (!torch.int, !torch.int) -> !torch.list<int>
    %1376 = torch.aten.view %1374, %1375 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_1605 = torch.constant.int 5632
    %int-1_1606 = torch.constant.int -1
    %int17_1607 = torch.constant.int 17
    %1377 = torch.prim.ListConstruct %int5632_1605, %int-1_1606, %int17_1607 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1378 = torch.aten.view %1376, %1377 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_1608 = torch.constant.int 2
    %int0_1609 = torch.constant.int 0
    %int1_1610 = torch.constant.int 1
    %int1_1611 = torch.constant.int 1
    %1379 = torch.aten.slice.Tensor %1378, %int2_1608, %int0_1609, %int1_1610, %int1_1611 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_1612 = torch.constant.int 5
    %1380 = torch.aten.view.dtype %1379, %int5_1612 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %1381 = torch.aten.detach %1380 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_1613 = torch.constant.int 2
    %int1_1614 = torch.constant.int 1
    %int9223372036854775807_1615 = torch.constant.int 9223372036854775807
    %int1_1616 = torch.constant.int 1
    %1382 = torch.aten.slice.Tensor %1378, %int2_1613, %int1_1614, %int9223372036854775807_1615, %int1_1616 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_1617 = torch.constant.int 1
    %1383 = torch.aten.view.dtype %1382, %int1_1617 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %1384 = torch.aten.detach %1383 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %1385 = torch_c.to_builtin_tensor %1354 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_1618 = tensor.cast %1385 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %1386 = torch_c.to_builtin_tensor %1381 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %1387 = torch_c.to_builtin_tensor %1384 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %1388 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_1618, %1386, %1387) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_1619 = tensor.cast %1388 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %1389 = torch_c.from_builtin_tensor %cast_1619 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %1390 = torch.aten.mul.Tensor %1372, %1389 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_1620 = torch.constant.int 2
    %1391 = torch.aten.view.dtype %51, %int2_1620 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %1392 = torch.aten.detach %1391 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_1621 = torch.constant.int -1
    %int17_1622 = torch.constant.int 17
    %1393 = torch.prim.ListConstruct %int-1_1621, %int17_1622 : (!torch.int, !torch.int) -> !torch.list<int>
    %1394 = torch.aten.view %1392, %1393 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_1623 = torch.constant.int 2048
    %int-1_1624 = torch.constant.int -1
    %int17_1625 = torch.constant.int 17
    %1395 = torch.prim.ListConstruct %int2048_1623, %int-1_1624, %int17_1625 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1396 = torch.aten.view %1394, %1395 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_1626 = torch.constant.int 2
    %int0_1627 = torch.constant.int 0
    %int1_1628 = torch.constant.int 1
    %int1_1629 = torch.constant.int 1
    %1397 = torch.aten.slice.Tensor %1396, %int2_1626, %int0_1627, %int1_1628, %int1_1629 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_1630 = torch.constant.int 5
    %1398 = torch.aten.view.dtype %1397, %int5_1630 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %1399 = torch.aten.detach %1398 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_1631 = torch.constant.int 2
    %int1_1632 = torch.constant.int 1
    %int9223372036854775807_1633 = torch.constant.int 9223372036854775807
    %int1_1634 = torch.constant.int 1
    %1400 = torch.aten.slice.Tensor %1396, %int2_1631, %int1_1632, %int9223372036854775807_1633, %int1_1634 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_1635 = torch.constant.int 1
    %1401 = torch.aten.view.dtype %1400, %int1_1635 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %1402 = torch.aten.detach %1401 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %1403 = torch_c.to_builtin_tensor %1390 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_1636 = tensor.cast %1403 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %1404 = torch_c.to_builtin_tensor %1399 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %1405 = torch_c.to_builtin_tensor %1402 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %1406 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_1636, %1404, %1405) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_1637 = tensor.cast %1406 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %1407 = torch_c.from_builtin_tensor %cast_1637 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_1638 = torch.constant.int 1
    %1408 = torch.aten.add.Tensor %1344, %1407, %int1_1638 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_1639 = torch.constant.none
    %none_1640 = torch.constant.none
    %int5_1641 = torch.constant.int 5
    %cpu_1642 = torch.constant.device "cpu"
    %int0_1643 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1408, %none_1639, %none_1640, %int5_1641, %cpu_1642, %int0_1643 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1644 = torch.constant.int 6
    %1409 = torch.prims.convert_element_type %1408, %int6_1644 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_1645 = torch.constant.int 2
    %1410 = torch.aten.pow.Tensor_Scalar %1409, %int2_1645 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_1646 = torch.constant.int -1
    %1411 = torch.prim.ListConstruct %int-1_1646 : (!torch.int) -> !torch.list<int>
    %true_1647 = torch.constant.bool true
    %none_1648 = torch.constant.none
    %1412 = torch.aten.mean.dim %1410, %1411, %true_1647, %none_1648 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_1649 = torch.constant.float 9.9999997473787516E-6
    %int1_1650 = torch.constant.int 1
    %1413 = torch.aten.add.Scalar %1412, %float9.999990e-06_1649, %int1_1650 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1414 = torch.aten.rsqrt %1413 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %1415 = torch.aten.mul.Tensor %1409, %1414 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_1651 = torch.constant.none
    %none_1652 = torch.constant.none
    %int6_1653 = torch.constant.int 6
    %cpu_1654 = torch.constant.device "cpu"
    %int0_1655 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1415, %none_1651, %none_1652, %int6_1653, %cpu_1654, %int0_1655 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1656 = torch.constant.int 5
    %1416 = torch.prims.convert_element_type %1415, %int5_1656 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %1417 = torch.aten.mul.Tensor %60, %1416 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_1657 = torch.constant.none
    %none_1658 = torch.constant.none
    %int6_1659 = torch.constant.int 6
    %cpu_1660 = torch.constant.device "cpu"
    %int0_1661 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1417, %none_1657, %none_1658, %int6_1659, %cpu_1660, %int0_1661 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1662 = torch.constant.int 5
    %1418 = torch.prims.convert_element_type %1417, %int5_1662 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_1663 = torch.constant.int 2
    %1419 = torch.aten.view.dtype %61, %int2_1663 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %1420 = torch.aten.detach %1419 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_1664 = torch.constant.int -1
    %int17_1665 = torch.constant.int 17
    %1421 = torch.prim.ListConstruct %int-1_1664, %int17_1665 : (!torch.int, !torch.int) -> !torch.list<int>
    %1422 = torch.aten.view %1420, %1421 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_1666 = torch.constant.int 2048
    %int-1_1667 = torch.constant.int -1
    %int17_1668 = torch.constant.int 17
    %1423 = torch.prim.ListConstruct %int2048_1666, %int-1_1667, %int17_1668 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1424 = torch.aten.view %1422, %1423 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_1669 = torch.constant.int 2
    %int0_1670 = torch.constant.int 0
    %int1_1671 = torch.constant.int 1
    %int1_1672 = torch.constant.int 1
    %1425 = torch.aten.slice.Tensor %1424, %int2_1669, %int0_1670, %int1_1671, %int1_1672 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_1673 = torch.constant.int 5
    %1426 = torch.aten.view.dtype %1425, %int5_1673 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %1427 = torch.aten.detach %1426 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_1674 = torch.constant.int 2
    %int1_1675 = torch.constant.int 1
    %int9223372036854775807_1676 = torch.constant.int 9223372036854775807
    %int1_1677 = torch.constant.int 1
    %1428 = torch.aten.slice.Tensor %1424, %int2_1674, %int1_1675, %int9223372036854775807_1676, %int1_1677 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_1678 = torch.constant.int 1
    %1429 = torch.aten.view.dtype %1428, %int1_1678 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %1430 = torch.aten.detach %1429 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %1431 = torch_c.to_builtin_tensor %1418 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_1679 = tensor.cast %1431 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %1432 = torch_c.to_builtin_tensor %1427 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %1433 = torch_c.to_builtin_tensor %1430 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %1434 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_1679, %1432, %1433) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_1680 = tensor.cast %1434 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %1435 = torch_c.from_builtin_tensor %cast_1680 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_1681 = torch.constant.int 2
    %1436 = torch.aten.view.dtype %62, %int2_1681 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %1437 = torch.aten.detach %1436 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_1682 = torch.constant.int -1
    %int17_1683 = torch.constant.int 17
    %1438 = torch.prim.ListConstruct %int-1_1682, %int17_1683 : (!torch.int, !torch.int) -> !torch.list<int>
    %1439 = torch.aten.view %1437, %1438 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_1684 = torch.constant.int 256
    %int-1_1685 = torch.constant.int -1
    %int17_1686 = torch.constant.int 17
    %1440 = torch.prim.ListConstruct %int256_1684, %int-1_1685, %int17_1686 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1441 = torch.aten.view %1439, %1440 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_1687 = torch.constant.int 2
    %int0_1688 = torch.constant.int 0
    %int1_1689 = torch.constant.int 1
    %int1_1690 = torch.constant.int 1
    %1442 = torch.aten.slice.Tensor %1441, %int2_1687, %int0_1688, %int1_1689, %int1_1690 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_1691 = torch.constant.int 5
    %1443 = torch.aten.view.dtype %1442, %int5_1691 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %1444 = torch.aten.detach %1443 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_1692 = torch.constant.int 2
    %int1_1693 = torch.constant.int 1
    %int9223372036854775807_1694 = torch.constant.int 9223372036854775807
    %int1_1695 = torch.constant.int 1
    %1445 = torch.aten.slice.Tensor %1441, %int2_1692, %int1_1693, %int9223372036854775807_1694, %int1_1695 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_1696 = torch.constant.int 1
    %1446 = torch.aten.view.dtype %1445, %int1_1696 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %1447 = torch.aten.detach %1446 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %1448 = torch_c.to_builtin_tensor %1418 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_1697 = tensor.cast %1448 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %1449 = torch_c.to_builtin_tensor %1444 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %1450 = torch_c.to_builtin_tensor %1447 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %1451 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_1697, %1449, %1450) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_1698 = tensor.cast %1451 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %1452 = torch_c.from_builtin_tensor %cast_1698 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_1699 = torch.constant.int 2
    %1453 = torch.aten.view.dtype %63, %int2_1699 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %1454 = torch.aten.detach %1453 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_1700 = torch.constant.int -1
    %int17_1701 = torch.constant.int 17
    %1455 = torch.prim.ListConstruct %int-1_1700, %int17_1701 : (!torch.int, !torch.int) -> !torch.list<int>
    %1456 = torch.aten.view %1454, %1455 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_1702 = torch.constant.int 256
    %int-1_1703 = torch.constant.int -1
    %int17_1704 = torch.constant.int 17
    %1457 = torch.prim.ListConstruct %int256_1702, %int-1_1703, %int17_1704 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1458 = torch.aten.view %1456, %1457 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_1705 = torch.constant.int 2
    %int0_1706 = torch.constant.int 0
    %int1_1707 = torch.constant.int 1
    %int1_1708 = torch.constant.int 1
    %1459 = torch.aten.slice.Tensor %1458, %int2_1705, %int0_1706, %int1_1707, %int1_1708 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_1709 = torch.constant.int 5
    %1460 = torch.aten.view.dtype %1459, %int5_1709 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %1461 = torch.aten.detach %1460 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_1710 = torch.constant.int 2
    %int1_1711 = torch.constant.int 1
    %int9223372036854775807_1712 = torch.constant.int 9223372036854775807
    %int1_1713 = torch.constant.int 1
    %1462 = torch.aten.slice.Tensor %1458, %int2_1710, %int1_1711, %int9223372036854775807_1712, %int1_1713 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_1714 = torch.constant.int 1
    %1463 = torch.aten.view.dtype %1462, %int1_1714 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %1464 = torch.aten.detach %1463 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %1465 = torch_c.to_builtin_tensor %1418 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_1715 = tensor.cast %1465 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %1466 = torch_c.to_builtin_tensor %1461 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %1467 = torch_c.to_builtin_tensor %1464 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %1468 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_1715, %1466, %1467) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_1716 = tensor.cast %1468 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %1469 = torch_c.from_builtin_tensor %cast_1716 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_1717 = torch.constant.int 4
    %int1_1718 = torch.constant.int 1
    %int32_1719 = torch.constant.int 32
    %int64_1720 = torch.constant.int 64
    %1470 = torch.prim.ListConstruct %int4_1717, %int1_1718, %int32_1719, %int64_1720 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1471 = torch.aten.view %1435, %1470 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_1721 = torch.constant.int 4
    %int1_1722 = torch.constant.int 1
    %int4_1723 = torch.constant.int 4
    %int64_1724 = torch.constant.int 64
    %1472 = torch.prim.ListConstruct %int4_1721, %int1_1722, %int4_1723, %int64_1724 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1473 = torch.aten.view %1452, %1472 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_1725 = torch.constant.int 4
    %int1_1726 = torch.constant.int 1
    %int4_1727 = torch.constant.int 4
    %int64_1728 = torch.constant.int 64
    %1474 = torch.prim.ListConstruct %int4_1725, %int1_1726, %int4_1727, %int64_1728 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1475 = torch.aten.view %1469, %1474 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_1729 = torch.constant.int 0
    %int1_1730 = torch.constant.int 1
    %none_1731 = torch.constant.none
    %none_1732 = torch.constant.none
    %cpu_1733 = torch.constant.device "cpu"
    %false_1734 = torch.constant.bool false
    %1476 = torch.aten.arange.start %int0_1729, %int1_1730, %none_1731, %none_1732, %cpu_1733, %false_1734 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_1735 = torch.constant.int 0
    %1477 = torch.aten.unsqueeze %1476, %int0_1735 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_1736 = torch.constant.int 1
    %1478 = torch.aten.unsqueeze %arg2, %int1_1736 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1737 = torch.constant.int 1
    %1479 = torch.aten.add.Tensor %1477, %1478, %int1_1737 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_1738 = torch.constant.int 0
    %int64_1739 = torch.constant.int 64
    %int2_1740 = torch.constant.int 2
    %none_1741 = torch.constant.none
    %none_1742 = torch.constant.none
    %cpu_1743 = torch.constant.device "cpu"
    %false_1744 = torch.constant.bool false
    %1480 = torch.aten.arange.start_step %int0_1738, %int64_1739, %int2_1740, %none_1741, %none_1742, %cpu_1743, %false_1744 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_1745 = torch.constant.none
    %none_1746 = torch.constant.none
    %int4_1747 = torch.constant.int 4
    %cpu_1748 = torch.constant.device "cpu"
    %int0_1749 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1480, %none_1745, %none_1746, %int4_1747, %cpu_1748, %int0_1749 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1750 = torch.constant.int 6
    %1481 = torch.prims.convert_element_type %1480, %int6_1750 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_1751 = torch.constant.int 64
    %1482 = torch.aten.div.Scalar %1481, %int64_1751 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_1752 = torch.constant.float 1.000000e+04
    %1483 = torch.aten.pow.Scalar %float1.000000e04_1752, %1482 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %1484 = torch.aten.reciprocal %1483 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_1753 = torch.constant.float 1.000000e+00
    %1485 = torch.aten.mul.Scalar %1484, %float1.000000e00_1753 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_1754 = torch.constant.none
    %1486 = torch.aten.clone %52, %none_1754 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_1755 = torch.constant.int 0
    %1487 = torch.aten.unsqueeze %1485, %int0_1755 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_1756 = torch.constant.int 2
    %1488 = torch.aten.unsqueeze %1487, %int2_1756 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_1757 = torch.constant.none
    %none_1758 = torch.constant.none
    %int6_1759 = torch.constant.int 6
    %cpu_1760 = torch.constant.device "cpu"
    %int0_1761 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1488, %none_1757, %none_1758, %int6_1759, %cpu_1760, %int0_1761 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_1762 = torch.constant.int 4
    %int-1_1763 = torch.constant.int -1
    %int1_1764 = torch.constant.int 1
    %1489 = torch.prim.ListConstruct %int4_1762, %int-1_1763, %int1_1764 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1765 = torch.constant.bool false
    %1490 = torch.aten.expand %1488, %1489, %false_1765 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_1766 = torch.constant.int 1
    %1491 = torch.aten.unsqueeze %1479, %int1_1766 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_1767 = torch.constant.none
    %none_1768 = torch.constant.none
    %int4_1769 = torch.constant.int 4
    %cpu_1770 = torch.constant.device "cpu"
    %int0_1771 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1491, %none_1767, %none_1768, %int4_1769, %cpu_1770, %int0_1771 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1772 = torch.constant.int 6
    %1492 = torch.prims.convert_element_type %1491, %int6_1772 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1493 = torch.aten.matmul %1490, %1492 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_1773 = torch.constant.int 1
    %int2_1774 = torch.constant.int 2
    %1494 = torch.aten.transpose.int %1493, %int1_1773, %int2_1774 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %1495 = torch.aten.cos %1494 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %1496 = torch.aten.mul.Tensor %1495, %1486 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_1775 = torch.constant.none
    %none_1776 = torch.constant.none
    %int6_1777 = torch.constant.int 6
    %cpu_1778 = torch.constant.device "cpu"
    %int0_1779 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1496, %none_1775, %none_1776, %int6_1777, %cpu_1778, %int0_1779 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1780 = torch.constant.int 5
    %1497 = torch.prims.convert_element_type %1496, %int5_1780 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %1498 = torch.aten.sin %1494 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %1499 = torch.aten.mul.Tensor %1498, %1486 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_1781 = torch.constant.none
    %none_1782 = torch.constant.none
    %int6_1783 = torch.constant.int 6
    %cpu_1784 = torch.constant.device "cpu"
    %int0_1785 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1499, %none_1781, %none_1782, %int6_1783, %cpu_1784, %int0_1785 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1786 = torch.constant.int 5
    %1500 = torch.prims.convert_element_type %1499, %int5_1786 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_1787 = torch.constant.int 2
    %1501 = torch.aten.unsqueeze %1497, %int2_1787 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_1788 = torch.constant.int 2
    %1502 = torch.aten.unsqueeze %1500, %int2_1788 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_1789 = torch.constant.none
    %none_1790 = torch.constant.none
    %int5_1791 = torch.constant.int 5
    %cpu_1792 = torch.constant.device "cpu"
    %int0_1793 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1501, %none_1789, %none_1790, %int5_1791, %cpu_1792, %int0_1793 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_1794 = torch.constant.none
    %none_1795 = torch.constant.none
    %int5_1796 = torch.constant.int 5
    %cpu_1797 = torch.constant.device "cpu"
    %int0_1798 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1502, %none_1794, %none_1795, %int5_1796, %cpu_1797, %int0_1798 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_1799 = torch.constant.none
    %none_1800 = torch.constant.none
    %int5_1801 = torch.constant.int 5
    %cpu_1802 = torch.constant.device "cpu"
    %int0_1803 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1471, %none_1799, %none_1800, %int5_1801, %cpu_1802, %int0_1803 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_1804 = torch.constant.int 3
    %int0_1805 = torch.constant.int 0
    %int64_1806 = torch.constant.int 64
    %int2_1807 = torch.constant.int 2
    %1503 = torch.aten.slice.Tensor %1471, %int3_1804, %int0_1805, %int64_1806, %int2_1807 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_1808 = torch.constant.int 3
    %int1_1809 = torch.constant.int 1
    %int64_1810 = torch.constant.int 64
    %int2_1811 = torch.constant.int 2
    %1504 = torch.aten.slice.Tensor %1471, %int3_1808, %int1_1809, %int64_1810, %int2_1811 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %1505 = torch.aten.mul.Tensor %1503, %1501 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %1506 = torch.aten.mul.Tensor %1504, %1502 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_1812 = torch.constant.int 1
    %1507 = torch.aten.sub.Tensor %1505, %1506, %int1_1812 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %1508 = torch.aten.mul.Tensor %1504, %1501 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %1509 = torch.aten.mul.Tensor %1503, %1502 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_1813 = torch.constant.int 1
    %1510 = torch.aten.add.Tensor %1508, %1509, %int1_1813 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %1511 = torch_c.to_builtin_tensor %1507 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_1814 = tensor.cast %1511 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %1512 = torch_c.to_builtin_tensor %1510 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_1815 = tensor.cast %1512 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %1513 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_1814, %cast_1815) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_1816 = tensor.cast %1513 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %1514 = torch_c.from_builtin_tensor %cast_1816 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_1817 = torch.constant.int 4
    %int1_1818 = torch.constant.int 1
    %int32_1819 = torch.constant.int 32
    %int64_1820 = torch.constant.int 64
    %1515 = torch.prim.ListConstruct %int4_1817, %int1_1818, %int32_1819, %int64_1820 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1516 = torch.aten.view %1514, %1515 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_1821 = torch.constant.none
    %none_1822 = torch.constant.none
    %int5_1823 = torch.constant.int 5
    %cpu_1824 = torch.constant.device "cpu"
    %int0_1825 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1516, %none_1821, %none_1822, %int5_1823, %cpu_1824, %int0_1825 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_1826 = torch.constant.int 0
    %int1_1827 = torch.constant.int 1
    %none_1828 = torch.constant.none
    %none_1829 = torch.constant.none
    %cpu_1830 = torch.constant.device "cpu"
    %false_1831 = torch.constant.bool false
    %1517 = torch.aten.arange.start %int0_1826, %int1_1827, %none_1828, %none_1829, %cpu_1830, %false_1831 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_1832 = torch.constant.int 0
    %1518 = torch.aten.unsqueeze %1517, %int0_1832 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_1833 = torch.constant.int 1
    %1519 = torch.aten.unsqueeze %arg2, %int1_1833 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1834 = torch.constant.int 1
    %1520 = torch.aten.add.Tensor %1518, %1519, %int1_1834 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_1835 = torch.constant.int 0
    %int64_1836 = torch.constant.int 64
    %int2_1837 = torch.constant.int 2
    %none_1838 = torch.constant.none
    %none_1839 = torch.constant.none
    %cpu_1840 = torch.constant.device "cpu"
    %false_1841 = torch.constant.bool false
    %1521 = torch.aten.arange.start_step %int0_1835, %int64_1836, %int2_1837, %none_1838, %none_1839, %cpu_1840, %false_1841 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_1842 = torch.constant.none
    %none_1843 = torch.constant.none
    %int4_1844 = torch.constant.int 4
    %cpu_1845 = torch.constant.device "cpu"
    %int0_1846 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1521, %none_1842, %none_1843, %int4_1844, %cpu_1845, %int0_1846 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1847 = torch.constant.int 6
    %1522 = torch.prims.convert_element_type %1521, %int6_1847 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_1848 = torch.constant.int 64
    %1523 = torch.aten.div.Scalar %1522, %int64_1848 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_1849 = torch.constant.float 1.000000e+04
    %1524 = torch.aten.pow.Scalar %float1.000000e04_1849, %1523 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %1525 = torch.aten.reciprocal %1524 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_1850 = torch.constant.float 1.000000e+00
    %1526 = torch.aten.mul.Scalar %1525, %float1.000000e00_1850 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_1851 = torch.constant.none
    %1527 = torch.aten.clone %53, %none_1851 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_1852 = torch.constant.int 0
    %1528 = torch.aten.unsqueeze %1526, %int0_1852 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_1853 = torch.constant.int 2
    %1529 = torch.aten.unsqueeze %1528, %int2_1853 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_1854 = torch.constant.none
    %none_1855 = torch.constant.none
    %int6_1856 = torch.constant.int 6
    %cpu_1857 = torch.constant.device "cpu"
    %int0_1858 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1529, %none_1854, %none_1855, %int6_1856, %cpu_1857, %int0_1858 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_1859 = torch.constant.int 4
    %int-1_1860 = torch.constant.int -1
    %int1_1861 = torch.constant.int 1
    %1530 = torch.prim.ListConstruct %int4_1859, %int-1_1860, %int1_1861 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1862 = torch.constant.bool false
    %1531 = torch.aten.expand %1529, %1530, %false_1862 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_1863 = torch.constant.int 1
    %1532 = torch.aten.unsqueeze %1520, %int1_1863 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_1864 = torch.constant.none
    %none_1865 = torch.constant.none
    %int4_1866 = torch.constant.int 4
    %cpu_1867 = torch.constant.device "cpu"
    %int0_1868 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1532, %none_1864, %none_1865, %int4_1866, %cpu_1867, %int0_1868 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_1869 = torch.constant.int 6
    %1533 = torch.prims.convert_element_type %1532, %int6_1869 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1534 = torch.aten.matmul %1531, %1533 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_1870 = torch.constant.int 1
    %int2_1871 = torch.constant.int 2
    %1535 = torch.aten.transpose.int %1534, %int1_1870, %int2_1871 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %1536 = torch.aten.cos %1535 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %1537 = torch.aten.mul.Tensor %1536, %1527 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_1872 = torch.constant.none
    %none_1873 = torch.constant.none
    %int6_1874 = torch.constant.int 6
    %cpu_1875 = torch.constant.device "cpu"
    %int0_1876 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1537, %none_1872, %none_1873, %int6_1874, %cpu_1875, %int0_1876 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1877 = torch.constant.int 5
    %1538 = torch.prims.convert_element_type %1537, %int5_1877 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %1539 = torch.aten.sin %1535 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %1540 = torch.aten.mul.Tensor %1539, %1527 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_1878 = torch.constant.none
    %none_1879 = torch.constant.none
    %int6_1880 = torch.constant.int 6
    %cpu_1881 = torch.constant.device "cpu"
    %int0_1882 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1540, %none_1878, %none_1879, %int6_1880, %cpu_1881, %int0_1882 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_1883 = torch.constant.int 5
    %1541 = torch.prims.convert_element_type %1540, %int5_1883 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_1884 = torch.constant.int 2
    %1542 = torch.aten.unsqueeze %1538, %int2_1884 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_1885 = torch.constant.int 2
    %1543 = torch.aten.unsqueeze %1541, %int2_1885 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_1886 = torch.constant.none
    %none_1887 = torch.constant.none
    %int5_1888 = torch.constant.int 5
    %cpu_1889 = torch.constant.device "cpu"
    %int0_1890 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1542, %none_1886, %none_1887, %int5_1888, %cpu_1889, %int0_1890 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_1891 = torch.constant.none
    %none_1892 = torch.constant.none
    %int5_1893 = torch.constant.int 5
    %cpu_1894 = torch.constant.device "cpu"
    %int0_1895 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1543, %none_1891, %none_1892, %int5_1893, %cpu_1894, %int0_1895 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_1896 = torch.constant.none
    %none_1897 = torch.constant.none
    %int5_1898 = torch.constant.int 5
    %cpu_1899 = torch.constant.device "cpu"
    %int0_1900 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1473, %none_1896, %none_1897, %int5_1898, %cpu_1899, %int0_1900 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_1901 = torch.constant.int 3
    %int0_1902 = torch.constant.int 0
    %int64_1903 = torch.constant.int 64
    %int2_1904 = torch.constant.int 2
    %1544 = torch.aten.slice.Tensor %1473, %int3_1901, %int0_1902, %int64_1903, %int2_1904 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_1905 = torch.constant.int 3
    %int1_1906 = torch.constant.int 1
    %int64_1907 = torch.constant.int 64
    %int2_1908 = torch.constant.int 2
    %1545 = torch.aten.slice.Tensor %1473, %int3_1905, %int1_1906, %int64_1907, %int2_1908 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %1546 = torch.aten.mul.Tensor %1544, %1542 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %1547 = torch.aten.mul.Tensor %1545, %1543 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_1909 = torch.constant.int 1
    %1548 = torch.aten.sub.Tensor %1546, %1547, %int1_1909 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %1549 = torch.aten.mul.Tensor %1545, %1542 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %1550 = torch.aten.mul.Tensor %1544, %1543 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_1910 = torch.constant.int 1
    %1551 = torch.aten.add.Tensor %1549, %1550, %int1_1910 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %1552 = torch_c.to_builtin_tensor %1548 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_1911 = tensor.cast %1552 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %1553 = torch_c.to_builtin_tensor %1551 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_1912 = tensor.cast %1553 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %1554 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_1911, %cast_1912) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_1913 = tensor.cast %1554 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %1555 = torch_c.from_builtin_tensor %cast_1913 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_1914 = torch.constant.int 4
    %int1_1915 = torch.constant.int 1
    %int4_1916 = torch.constant.int 4
    %int64_1917 = torch.constant.int 64
    %1556 = torch.prim.ListConstruct %int4_1914, %int1_1915, %int4_1916, %int64_1917 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1557 = torch.aten.view %1555, %1556 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_1918 = torch.constant.none
    %none_1919 = torch.constant.none
    %int5_1920 = torch.constant.int 5
    %cpu_1921 = torch.constant.device "cpu"
    %int0_1922 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1557, %none_1918, %none_1919, %int5_1920, %cpu_1921, %int0_1922 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_1923 = torch.constant.int 32
    %1558 = torch.aten.floor_divide.Scalar %arg2, %int32_1923 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_1924 = torch.constant.int 1
    %1559 = torch.aten.unsqueeze %1558, %int1_1924 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1925 = torch.constant.int 1
    %false_1926 = torch.constant.bool false
    %1560 = torch.aten.gather %arg3, %int1_1925, %1559, %false_1926 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_1927 = torch.constant.int 4
    %int1_1928 = torch.constant.int 1
    %int1_1929 = torch.constant.int 1
    %1561 = torch.prim.ListConstruct %int4_1927, %int1_1928, %int1_1929 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1562 = torch.aten.view %1560, %1561 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_1930 = torch.constant.int 32
    %1563 = torch.aten.remainder.Scalar %arg2, %int32_1930 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_1931 = torch.constant.int 4
    %int1_1932 = torch.constant.int 1
    %int1_1933 = torch.constant.int 1
    %1564 = torch.prim.ListConstruct %int4_1931, %int1_1932, %int1_1933 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1565 = torch.aten.view %1563, %1564 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_1934 = torch.constant.int 4
    %none_1935 = torch.constant.none
    %none_1936 = torch.constant.none
    %cpu_1937 = torch.constant.device "cpu"
    %false_1938 = torch.constant.bool false
    %1566 = torch.aten.arange %int4_1934, %none_1935, %none_1936, %cpu_1937, %false_1938 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_1939 = torch.constant.int 1
    %int1_1940 = torch.constant.int 1
    %int4_1941 = torch.constant.int 4
    %1567 = torch.prim.ListConstruct %int1_1939, %int1_1940, %int4_1941 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1568 = torch.aten.view %1566, %1567 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_1942 = torch.constant.none
    %1569 = torch.aten.clone %54, %none_1942 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_1943 = torch.constant.int 1
    %int1_1944 = torch.constant.int 1
    %int1_1945 = torch.constant.int 1
    %1570 = torch.prim.ListConstruct %int1_1943, %int1_1944, %int1_1945 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1571 = torch.aten.view %1569, %1570 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_1946 = torch.constant.int 22
    %1572 = torch.aten.mul.Scalar %1562, %int22_1946 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int3_1947 = torch.constant.int 3
    %int1_1948 = torch.constant.int 1
    %1573 = torch.aten.add.Scalar %1572, %int3_1947, %int1_1948 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_1949 = torch.constant.int 2
    %1574 = torch.aten.mul.Scalar %1573, %int2_1949 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_1950 = torch.constant.int 1
    %1575 = torch.aten.add.Tensor %1574, %1571, %int1_1950 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_1951 = torch.constant.int 4
    %1576 = torch.aten.mul.Scalar %1575, %int4_1951 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_1952 = torch.constant.int 1
    %1577 = torch.aten.add.Tensor %1576, %1568, %int1_1952 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_1953 = torch.constant.int 32
    %1578 = torch.aten.mul.Scalar %1577, %int32_1953 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_1954 = torch.constant.int 1
    %1579 = torch.aten.add.Tensor %1578, %1565, %int1_1954 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_1955 = torch.constant.none
    %none_1956 = torch.constant.none
    %int5_1957 = torch.constant.int 5
    %cpu_1958 = torch.constant.device "cpu"
    %int0_1959 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1557, %none_1955, %none_1956, %int5_1957, %cpu_1958, %int0_1959 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_1960 = torch.constant.int 22
    %int2_1961 = torch.constant.int 2
    %int4_1962 = torch.constant.int 4
    %int32_1963 = torch.constant.int 32
    %int64_1964 = torch.constant.int 64
    %1580 = torch.prim.ListConstruct %381, %int22_1960, %int2_1961, %int4_1962, %int32_1963, %int64_1964 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1581 = torch.aten.view %1275, %1580 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1581, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_1965 = torch.constant.int 64
    %1582 = torch.prim.ListConstruct %553, %int64_1965 : (!torch.int, !torch.int) -> !torch.list<int>
    %1583 = torch.aten.view %1581, %1582 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %1583, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %1584 = torch.prim.ListConstruct %1579 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_1966 = torch.constant.bool false
    %1585 = torch.aten.index_put %1583, %1584, %1557, %false_1966 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %1585, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_1967 = torch.constant.int 22
    %int2_1968 = torch.constant.int 2
    %int4_1969 = torch.constant.int 4
    %int32_1970 = torch.constant.int 32
    %int64_1971 = torch.constant.int 64
    %1586 = torch.prim.ListConstruct %381, %int22_1967, %int2_1968, %int4_1969, %int32_1970, %int64_1971 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1587 = torch.aten.view %1585, %1586 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1587, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_1972 = torch.constant.int 360448
    %1588 = torch.prim.ListConstruct %381, %int360448_1972 : (!torch.int, !torch.int) -> !torch.list<int>
    %1589 = torch.aten.view %1587, %1588 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %1589, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_1973 = torch.constant.int 22
    %int2_1974 = torch.constant.int 2
    %int4_1975 = torch.constant.int 4
    %int32_1976 = torch.constant.int 32
    %int64_1977 = torch.constant.int 64
    %1590 = torch.prim.ListConstruct %381, %int22_1973, %int2_1974, %int4_1975, %int32_1976, %int64_1977 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1591 = torch.aten.view %1589, %1590 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1591, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_1978 = torch.constant.int 64
    %1592 = torch.prim.ListConstruct %553, %int64_1978 : (!torch.int, !torch.int) -> !torch.list<int>
    %1593 = torch.aten.view %1591, %1592 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %1593, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_1979 = torch.constant.none
    %1594 = torch.aten.clone %55, %none_1979 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_1980 = torch.constant.int 1
    %int1_1981 = torch.constant.int 1
    %int1_1982 = torch.constant.int 1
    %1595 = torch.prim.ListConstruct %int1_1980, %int1_1981, %int1_1982 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1596 = torch.aten.view %1594, %1595 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_1983 = torch.constant.int 22
    %1597 = torch.aten.mul.Scalar %1562, %int22_1983 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int3_1984 = torch.constant.int 3
    %int1_1985 = torch.constant.int 1
    %1598 = torch.aten.add.Scalar %1597, %int3_1984, %int1_1985 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_1986 = torch.constant.int 2
    %1599 = torch.aten.mul.Scalar %1598, %int2_1986 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_1987 = torch.constant.int 1
    %1600 = torch.aten.add.Tensor %1599, %1596, %int1_1987 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_1988 = torch.constant.int 4
    %1601 = torch.aten.mul.Scalar %1600, %int4_1988 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_1989 = torch.constant.int 1
    %1602 = torch.aten.add.Tensor %1601, %1568, %int1_1989 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_1990 = torch.constant.int 32
    %1603 = torch.aten.mul.Scalar %1602, %int32_1990 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_1991 = torch.constant.int 1
    %1604 = torch.aten.add.Tensor %1603, %1565, %int1_1991 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_1992 = torch.constant.none
    %none_1993 = torch.constant.none
    %int5_1994 = torch.constant.int 5
    %cpu_1995 = torch.constant.device "cpu"
    %int0_1996 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1475, %none_1992, %none_1993, %int5_1994, %cpu_1995, %int0_1996 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %1605 = torch.prim.ListConstruct %1604 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_1997 = torch.constant.bool false
    %1606 = torch.aten.index_put %1593, %1605, %1475, %false_1997 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %1606, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_1998 = torch.constant.int 22
    %int2_1999 = torch.constant.int 2
    %int4_2000 = torch.constant.int 4
    %int32_2001 = torch.constant.int 32
    %int64_2002 = torch.constant.int 64
    %1607 = torch.prim.ListConstruct %381, %int22_1998, %int2_1999, %int4_2000, %int32_2001, %int64_2002 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1608 = torch.aten.view %1606, %1607 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1608, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_2003 = torch.constant.int 360448
    %1609 = torch.prim.ListConstruct %381, %int360448_2003 : (!torch.int, !torch.int) -> !torch.list<int>
    %1610 = torch.aten.view %1608, %1609 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %1610, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_2004 = torch.constant.none
    %1611 = torch.aten.clone %56, %none_2004 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_2005 = torch.constant.none
    %1612 = torch.aten.clone %57, %none_2005 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_2006 = torch.constant.none
    %1613 = torch.aten.clone %58, %none_2006 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_2007 = torch.constant.int 22
    %int2_2008 = torch.constant.int 2
    %int4_2009 = torch.constant.int 4
    %int32_2010 = torch.constant.int 32
    %int64_2011 = torch.constant.int 64
    %1614 = torch.prim.ListConstruct %381, %int22_2007, %int2_2008, %int4_2009, %int32_2010, %int64_2011 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1615 = torch.aten.view %1610, %1614 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1615, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %1616 = torch_c.to_builtin_tensor %1615 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %1617 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_2012 = tensor.cast %1617 : tensor<4x?xi64> to tensor<?x?xi64>
    %1618 = torch_c.to_builtin_tensor %1611 : !torch.vtensor<[],si64> -> tensor<i64>
    %1619 = torch_c.to_builtin_tensor %1612 : !torch.vtensor<[],si64> -> tensor<i64>
    %1620 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%1616, %cast_2012, %1618, %1619) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_2013 = tensor.cast %1620 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %1621 = torch_c.from_builtin_tensor %cast_2013 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %1621, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %1622 = torch_c.to_builtin_tensor %1615 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %1623 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_2014 = tensor.cast %1623 : tensor<4x?xi64> to tensor<?x?xi64>
    %1624 = torch_c.to_builtin_tensor %1611 : !torch.vtensor<[],si64> -> tensor<i64>
    %1625 = torch_c.to_builtin_tensor %1613 : !torch.vtensor<[],si64> -> tensor<i64>
    %1626 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%1622, %cast_2014, %1624, %1625) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_2015 = tensor.cast %1626 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %1627 = torch_c.from_builtin_tensor %cast_2015 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %1627, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_2016 = torch.constant.int 2
    %int3_2017 = torch.constant.int 3
    %1628 = torch.aten.transpose.int %1621, %int2_2016, %int3_2017 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1628, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_2018 = torch.constant.int 0
    %1629 = torch.aten.clone %1628, %int0_2018 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1629, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_2019 = torch.constant.int 4
    %int4_2020 = torch.constant.int 4
    %int64_2021 = torch.constant.int 64
    %1630 = torch.prim.ListConstruct %int4_2019, %623, %int4_2020, %int64_2021 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1631 = torch.aten._unsafe_view %1629, %1630 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %1631, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_2022 = torch.constant.int 2
    %int3_2023 = torch.constant.int 3
    %1632 = torch.aten.transpose.int %1627, %int2_2022, %int3_2023 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1632, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_2024 = torch.constant.int 0
    %1633 = torch.aten.clone %1632, %int0_2024 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1633, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_2025 = torch.constant.int 4
    %int4_2026 = torch.constant.int 4
    %int64_2027 = torch.constant.int 64
    %1634 = torch.prim.ListConstruct %int4_2025, %623, %int4_2026, %int64_2027 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1635 = torch.aten._unsafe_view %1633, %1634 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %1635, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_2028 = torch.constant.int 0
    %int1_2029 = torch.constant.int 1
    %none_2030 = torch.constant.none
    %none_2031 = torch.constant.none
    %cpu_2032 = torch.constant.device "cpu"
    %false_2033 = torch.constant.bool false
    %1636 = torch.aten.arange.start_step %int0_2028, %623, %int1_2029, %none_2030, %none_2031, %cpu_2032, %false_2033 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1636, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_2034 = torch.constant.int -1
    %1637 = torch.aten.unsqueeze %arg1, %int-1_2034 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %1638 = torch.aten.ge.Tensor %1636, %1637 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %1638, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_2035 = torch.constant.none
    %1639 = torch.aten.clone %59, %none_2035 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_2036 = torch.constant.int 0
    %1640 = torch.aten.where.ScalarOther %1638, %1639, %int0_2036 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %1640, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_2037 = torch.constant.none
    %none_2038 = torch.constant.none
    %int5_2039 = torch.constant.int 5
    %cpu_2040 = torch.constant.device "cpu"
    %int0_2041 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1640, %none_2037, %none_2038, %int5_2039, %cpu_2040, %int0_2041 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_2042 = torch.constant.int 1
    %1641 = torch.aten.unsqueeze %1640, %int1_2042 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %1641, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_2043 = torch.constant.int 1
    %1642 = torch.aten.unsqueeze %1641, %int1_2043 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %1642, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_2044 = torch.constant.int -2
    %1643 = torch.aten.unsqueeze %1631, %int-2_2044 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %1643, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_2045 = torch.constant.int 4
    %int4_2046 = torch.constant.int 4
    %int8_2047 = torch.constant.int 8
    %int64_2048 = torch.constant.int 64
    %1644 = torch.prim.ListConstruct %int4_2045, %623, %int4_2046, %int8_2047, %int64_2048 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2049 = torch.constant.bool false
    %1645 = torch.aten.expand %1643, %1644, %false_2049 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1645, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_2050 = torch.constant.int 0
    %1646 = torch.aten.clone %1645, %int0_2050 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1646, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_2051 = torch.constant.int 4
    %int32_2052 = torch.constant.int 32
    %int64_2053 = torch.constant.int 64
    %1647 = torch.prim.ListConstruct %int4_2051, %623, %int32_2052, %int64_2053 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1648 = torch.aten._unsafe_view %1646, %1647 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1648, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_2054 = torch.constant.int -2
    %1649 = torch.aten.unsqueeze %1635, %int-2_2054 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %1649, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_2055 = torch.constant.int 4
    %int4_2056 = torch.constant.int 4
    %int8_2057 = torch.constant.int 8
    %int64_2058 = torch.constant.int 64
    %1650 = torch.prim.ListConstruct %int4_2055, %623, %int4_2056, %int8_2057, %int64_2058 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2059 = torch.constant.bool false
    %1651 = torch.aten.expand %1649, %1650, %false_2059 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1651, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_2060 = torch.constant.int 0
    %1652 = torch.aten.clone %1651, %int0_2060 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1652, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_2061 = torch.constant.int 4
    %int32_2062 = torch.constant.int 32
    %int64_2063 = torch.constant.int 64
    %1653 = torch.prim.ListConstruct %int4_2061, %623, %int32_2062, %int64_2063 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1654 = torch.aten._unsafe_view %1652, %1653 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1654, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_2064 = torch.constant.int 1
    %int2_2065 = torch.constant.int 2
    %1655 = torch.aten.transpose.int %1516, %int1_2064, %int2_2065 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_2066 = torch.constant.int 1
    %int2_2067 = torch.constant.int 2
    %1656 = torch.aten.transpose.int %1648, %int1_2066, %int2_2067 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %1656, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_2068 = torch.constant.int 1
    %int2_2069 = torch.constant.int 2
    %1657 = torch.aten.transpose.int %1654, %int1_2068, %int2_2069 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %1657, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_2070 = torch.constant.float 0.000000e+00
    %false_2071 = torch.constant.bool false
    %none_2072 = torch.constant.none
    %false_2073 = torch.constant.bool false
    %1658 = torch.aten.scaled_dot_product_attention %1655, %1656, %1657, %1642, %float0.000000e00_2070, %false_2071, %none_2072, %false_2073 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_2074 = torch.constant.int 1
    %int2_2075 = torch.constant.int 2
    %1659 = torch.aten.transpose.int %1658, %int1_2074, %int2_2075 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_2076 = torch.constant.int 4
    %int1_2077 = torch.constant.int 1
    %int2048_2078 = torch.constant.int 2048
    %1660 = torch.prim.ListConstruct %int4_2076, %int1_2077, %int2048_2078 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1661 = torch.aten.view %1659, %1660 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_2079 = torch.constant.int 2
    %1662 = torch.aten.view.dtype %64, %int2_2079 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %1663 = torch.aten.detach %1662 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_2080 = torch.constant.int -1
    %int17_2081 = torch.constant.int 17
    %1664 = torch.prim.ListConstruct %int-1_2080, %int17_2081 : (!torch.int, !torch.int) -> !torch.list<int>
    %1665 = torch.aten.view %1663, %1664 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_2082 = torch.constant.int 2048
    %int-1_2083 = torch.constant.int -1
    %int17_2084 = torch.constant.int 17
    %1666 = torch.prim.ListConstruct %int2048_2082, %int-1_2083, %int17_2084 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1667 = torch.aten.view %1665, %1666 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_2085 = torch.constant.int 2
    %int0_2086 = torch.constant.int 0
    %int1_2087 = torch.constant.int 1
    %int1_2088 = torch.constant.int 1
    %1668 = torch.aten.slice.Tensor %1667, %int2_2085, %int0_2086, %int1_2087, %int1_2088 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_2089 = torch.constant.int 5
    %1669 = torch.aten.view.dtype %1668, %int5_2089 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %1670 = torch.aten.detach %1669 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_2090 = torch.constant.int 2
    %int1_2091 = torch.constant.int 1
    %int9223372036854775807_2092 = torch.constant.int 9223372036854775807
    %int1_2093 = torch.constant.int 1
    %1671 = torch.aten.slice.Tensor %1667, %int2_2090, %int1_2091, %int9223372036854775807_2092, %int1_2093 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_2094 = torch.constant.int 1
    %1672 = torch.aten.view.dtype %1671, %int1_2094 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %1673 = torch.aten.detach %1672 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %1674 = torch_c.to_builtin_tensor %1661 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_2095 = tensor.cast %1674 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %1675 = torch_c.to_builtin_tensor %1670 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %1676 = torch_c.to_builtin_tensor %1673 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %1677 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_2095, %1675, %1676) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_2096 = tensor.cast %1677 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %1678 = torch_c.from_builtin_tensor %cast_2096 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_2097 = torch.constant.none
    %none_2098 = torch.constant.none
    %int5_2099 = torch.constant.int 5
    %cpu_2100 = torch.constant.device "cpu"
    %int0_2101 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1678, %none_2097, %none_2098, %int5_2099, %cpu_2100, %int0_2101 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_2102 = torch.constant.int 1
    %1679 = torch.aten.add.Tensor %1408, %1678, %int1_2102 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_2103 = torch.constant.none
    %none_2104 = torch.constant.none
    %int5_2105 = torch.constant.int 5
    %cpu_2106 = torch.constant.device "cpu"
    %int0_2107 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1679, %none_2103, %none_2104, %int5_2105, %cpu_2106, %int0_2107 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2108 = torch.constant.int 6
    %1680 = torch.prims.convert_element_type %1679, %int6_2108 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_2109 = torch.constant.int 2
    %1681 = torch.aten.pow.Tensor_Scalar %1680, %int2_2109 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_2110 = torch.constant.int -1
    %1682 = torch.prim.ListConstruct %int-1_2110 : (!torch.int) -> !torch.list<int>
    %true_2111 = torch.constant.bool true
    %none_2112 = torch.constant.none
    %1683 = torch.aten.mean.dim %1681, %1682, %true_2111, %none_2112 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_2113 = torch.constant.float 9.9999997473787516E-6
    %int1_2114 = torch.constant.int 1
    %1684 = torch.aten.add.Scalar %1683, %float9.999990e-06_2113, %int1_2114 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1685 = torch.aten.rsqrt %1684 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %1686 = torch.aten.mul.Tensor %1680, %1685 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_2115 = torch.constant.none
    %none_2116 = torch.constant.none
    %int6_2117 = torch.constant.int 6
    %cpu_2118 = torch.constant.device "cpu"
    %int0_2119 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1686, %none_2115, %none_2116, %int6_2117, %cpu_2118, %int0_2119 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2120 = torch.constant.int 5
    %1687 = torch.prims.convert_element_type %1686, %int5_2120 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %1688 = torch.aten.mul.Tensor %65, %1687 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_2121 = torch.constant.none
    %none_2122 = torch.constant.none
    %int6_2123 = torch.constant.int 6
    %cpu_2124 = torch.constant.device "cpu"
    %int0_2125 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1688, %none_2121, %none_2122, %int6_2123, %cpu_2124, %int0_2125 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2126 = torch.constant.int 5
    %1689 = torch.prims.convert_element_type %1688, %int5_2126 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_2127 = torch.constant.int 2
    %1690 = torch.aten.view.dtype %66, %int2_2127 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %1691 = torch.aten.detach %1690 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_2128 = torch.constant.int -1
    %int17_2129 = torch.constant.int 17
    %1692 = torch.prim.ListConstruct %int-1_2128, %int17_2129 : (!torch.int, !torch.int) -> !torch.list<int>
    %1693 = torch.aten.view %1691, %1692 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_2130 = torch.constant.int 5632
    %int-1_2131 = torch.constant.int -1
    %int17_2132 = torch.constant.int 17
    %1694 = torch.prim.ListConstruct %int5632_2130, %int-1_2131, %int17_2132 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1695 = torch.aten.view %1693, %1694 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_2133 = torch.constant.int 2
    %int0_2134 = torch.constant.int 0
    %int1_2135 = torch.constant.int 1
    %int1_2136 = torch.constant.int 1
    %1696 = torch.aten.slice.Tensor %1695, %int2_2133, %int0_2134, %int1_2135, %int1_2136 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_2137 = torch.constant.int 5
    %1697 = torch.aten.view.dtype %1696, %int5_2137 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %1698 = torch.aten.detach %1697 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_2138 = torch.constant.int 2
    %int1_2139 = torch.constant.int 1
    %int9223372036854775807_2140 = torch.constant.int 9223372036854775807
    %int1_2141 = torch.constant.int 1
    %1699 = torch.aten.slice.Tensor %1695, %int2_2138, %int1_2139, %int9223372036854775807_2140, %int1_2141 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_2142 = torch.constant.int 1
    %1700 = torch.aten.view.dtype %1699, %int1_2142 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %1701 = torch.aten.detach %1700 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %1702 = torch_c.to_builtin_tensor %1689 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_2143 = tensor.cast %1702 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %1703 = torch_c.to_builtin_tensor %1698 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %1704 = torch_c.to_builtin_tensor %1701 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %1705 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_2143, %1703, %1704) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_2144 = tensor.cast %1705 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %1706 = torch_c.from_builtin_tensor %cast_2144 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %1707 = torch.aten.silu %1706 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_2145 = torch.constant.int 2
    %1708 = torch.aten.view.dtype %67, %int2_2145 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %1709 = torch.aten.detach %1708 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_2146 = torch.constant.int -1
    %int17_2147 = torch.constant.int 17
    %1710 = torch.prim.ListConstruct %int-1_2146, %int17_2147 : (!torch.int, !torch.int) -> !torch.list<int>
    %1711 = torch.aten.view %1709, %1710 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_2148 = torch.constant.int 5632
    %int-1_2149 = torch.constant.int -1
    %int17_2150 = torch.constant.int 17
    %1712 = torch.prim.ListConstruct %int5632_2148, %int-1_2149, %int17_2150 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1713 = torch.aten.view %1711, %1712 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_2151 = torch.constant.int 2
    %int0_2152 = torch.constant.int 0
    %int1_2153 = torch.constant.int 1
    %int1_2154 = torch.constant.int 1
    %1714 = torch.aten.slice.Tensor %1713, %int2_2151, %int0_2152, %int1_2153, %int1_2154 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_2155 = torch.constant.int 5
    %1715 = torch.aten.view.dtype %1714, %int5_2155 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %1716 = torch.aten.detach %1715 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_2156 = torch.constant.int 2
    %int1_2157 = torch.constant.int 1
    %int9223372036854775807_2158 = torch.constant.int 9223372036854775807
    %int1_2159 = torch.constant.int 1
    %1717 = torch.aten.slice.Tensor %1713, %int2_2156, %int1_2157, %int9223372036854775807_2158, %int1_2159 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_2160 = torch.constant.int 1
    %1718 = torch.aten.view.dtype %1717, %int1_2160 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %1719 = torch.aten.detach %1718 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %1720 = torch_c.to_builtin_tensor %1689 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_2161 = tensor.cast %1720 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %1721 = torch_c.to_builtin_tensor %1716 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %1722 = torch_c.to_builtin_tensor %1719 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %1723 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_2161, %1721, %1722) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_2162 = tensor.cast %1723 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %1724 = torch_c.from_builtin_tensor %cast_2162 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %1725 = torch.aten.mul.Tensor %1707, %1724 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_2163 = torch.constant.int 2
    %1726 = torch.aten.view.dtype %68, %int2_2163 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %1727 = torch.aten.detach %1726 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_2164 = torch.constant.int -1
    %int17_2165 = torch.constant.int 17
    %1728 = torch.prim.ListConstruct %int-1_2164, %int17_2165 : (!torch.int, !torch.int) -> !torch.list<int>
    %1729 = torch.aten.view %1727, %1728 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_2166 = torch.constant.int 2048
    %int-1_2167 = torch.constant.int -1
    %int17_2168 = torch.constant.int 17
    %1730 = torch.prim.ListConstruct %int2048_2166, %int-1_2167, %int17_2168 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1731 = torch.aten.view %1729, %1730 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_2169 = torch.constant.int 2
    %int0_2170 = torch.constant.int 0
    %int1_2171 = torch.constant.int 1
    %int1_2172 = torch.constant.int 1
    %1732 = torch.aten.slice.Tensor %1731, %int2_2169, %int0_2170, %int1_2171, %int1_2172 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_2173 = torch.constant.int 5
    %1733 = torch.aten.view.dtype %1732, %int5_2173 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %1734 = torch.aten.detach %1733 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_2174 = torch.constant.int 2
    %int1_2175 = torch.constant.int 1
    %int9223372036854775807_2176 = torch.constant.int 9223372036854775807
    %int1_2177 = torch.constant.int 1
    %1735 = torch.aten.slice.Tensor %1731, %int2_2174, %int1_2175, %int9223372036854775807_2176, %int1_2177 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_2178 = torch.constant.int 1
    %1736 = torch.aten.view.dtype %1735, %int1_2178 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %1737 = torch.aten.detach %1736 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %1738 = torch_c.to_builtin_tensor %1725 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_2179 = tensor.cast %1738 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %1739 = torch_c.to_builtin_tensor %1734 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %1740 = torch_c.to_builtin_tensor %1737 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %1741 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_2179, %1739, %1740) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_2180 = tensor.cast %1741 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %1742 = torch_c.from_builtin_tensor %cast_2180 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_2181 = torch.constant.int 1
    %1743 = torch.aten.add.Tensor %1679, %1742, %int1_2181 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_2182 = torch.constant.none
    %none_2183 = torch.constant.none
    %int5_2184 = torch.constant.int 5
    %cpu_2185 = torch.constant.device "cpu"
    %int0_2186 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1743, %none_2182, %none_2183, %int5_2184, %cpu_2185, %int0_2186 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2187 = torch.constant.int 6
    %1744 = torch.prims.convert_element_type %1743, %int6_2187 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_2188 = torch.constant.int 2
    %1745 = torch.aten.pow.Tensor_Scalar %1744, %int2_2188 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_2189 = torch.constant.int -1
    %1746 = torch.prim.ListConstruct %int-1_2189 : (!torch.int) -> !torch.list<int>
    %true_2190 = torch.constant.bool true
    %none_2191 = torch.constant.none
    %1747 = torch.aten.mean.dim %1745, %1746, %true_2190, %none_2191 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_2192 = torch.constant.float 9.9999997473787516E-6
    %int1_2193 = torch.constant.int 1
    %1748 = torch.aten.add.Scalar %1747, %float9.999990e-06_2192, %int1_2193 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1749 = torch.aten.rsqrt %1748 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %1750 = torch.aten.mul.Tensor %1744, %1749 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_2194 = torch.constant.none
    %none_2195 = torch.constant.none
    %int6_2196 = torch.constant.int 6
    %cpu_2197 = torch.constant.device "cpu"
    %int0_2198 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1750, %none_2194, %none_2195, %int6_2196, %cpu_2197, %int0_2198 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2199 = torch.constant.int 5
    %1751 = torch.prims.convert_element_type %1750, %int5_2199 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %1752 = torch.aten.mul.Tensor %77, %1751 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_2200 = torch.constant.none
    %none_2201 = torch.constant.none
    %int6_2202 = torch.constant.int 6
    %cpu_2203 = torch.constant.device "cpu"
    %int0_2204 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1752, %none_2200, %none_2201, %int6_2202, %cpu_2203, %int0_2204 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2205 = torch.constant.int 5
    %1753 = torch.prims.convert_element_type %1752, %int5_2205 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_2206 = torch.constant.int 2
    %1754 = torch.aten.view.dtype %78, %int2_2206 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %1755 = torch.aten.detach %1754 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_2207 = torch.constant.int -1
    %int17_2208 = torch.constant.int 17
    %1756 = torch.prim.ListConstruct %int-1_2207, %int17_2208 : (!torch.int, !torch.int) -> !torch.list<int>
    %1757 = torch.aten.view %1755, %1756 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_2209 = torch.constant.int 2048
    %int-1_2210 = torch.constant.int -1
    %int17_2211 = torch.constant.int 17
    %1758 = torch.prim.ListConstruct %int2048_2209, %int-1_2210, %int17_2211 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1759 = torch.aten.view %1757, %1758 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_2212 = torch.constant.int 2
    %int0_2213 = torch.constant.int 0
    %int1_2214 = torch.constant.int 1
    %int1_2215 = torch.constant.int 1
    %1760 = torch.aten.slice.Tensor %1759, %int2_2212, %int0_2213, %int1_2214, %int1_2215 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_2216 = torch.constant.int 5
    %1761 = torch.aten.view.dtype %1760, %int5_2216 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %1762 = torch.aten.detach %1761 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_2217 = torch.constant.int 2
    %int1_2218 = torch.constant.int 1
    %int9223372036854775807_2219 = torch.constant.int 9223372036854775807
    %int1_2220 = torch.constant.int 1
    %1763 = torch.aten.slice.Tensor %1759, %int2_2217, %int1_2218, %int9223372036854775807_2219, %int1_2220 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_2221 = torch.constant.int 1
    %1764 = torch.aten.view.dtype %1763, %int1_2221 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %1765 = torch.aten.detach %1764 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %1766 = torch_c.to_builtin_tensor %1753 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_2222 = tensor.cast %1766 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %1767 = torch_c.to_builtin_tensor %1762 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %1768 = torch_c.to_builtin_tensor %1765 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %1769 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_2222, %1767, %1768) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_2223 = tensor.cast %1769 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %1770 = torch_c.from_builtin_tensor %cast_2223 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_2224 = torch.constant.int 2
    %1771 = torch.aten.view.dtype %79, %int2_2224 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %1772 = torch.aten.detach %1771 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_2225 = torch.constant.int -1
    %int17_2226 = torch.constant.int 17
    %1773 = torch.prim.ListConstruct %int-1_2225, %int17_2226 : (!torch.int, !torch.int) -> !torch.list<int>
    %1774 = torch.aten.view %1772, %1773 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_2227 = torch.constant.int 256
    %int-1_2228 = torch.constant.int -1
    %int17_2229 = torch.constant.int 17
    %1775 = torch.prim.ListConstruct %int256_2227, %int-1_2228, %int17_2229 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1776 = torch.aten.view %1774, %1775 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_2230 = torch.constant.int 2
    %int0_2231 = torch.constant.int 0
    %int1_2232 = torch.constant.int 1
    %int1_2233 = torch.constant.int 1
    %1777 = torch.aten.slice.Tensor %1776, %int2_2230, %int0_2231, %int1_2232, %int1_2233 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_2234 = torch.constant.int 5
    %1778 = torch.aten.view.dtype %1777, %int5_2234 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %1779 = torch.aten.detach %1778 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_2235 = torch.constant.int 2
    %int1_2236 = torch.constant.int 1
    %int9223372036854775807_2237 = torch.constant.int 9223372036854775807
    %int1_2238 = torch.constant.int 1
    %1780 = torch.aten.slice.Tensor %1776, %int2_2235, %int1_2236, %int9223372036854775807_2237, %int1_2238 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_2239 = torch.constant.int 1
    %1781 = torch.aten.view.dtype %1780, %int1_2239 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %1782 = torch.aten.detach %1781 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %1783 = torch_c.to_builtin_tensor %1753 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_2240 = tensor.cast %1783 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %1784 = torch_c.to_builtin_tensor %1779 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %1785 = torch_c.to_builtin_tensor %1782 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %1786 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_2240, %1784, %1785) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_2241 = tensor.cast %1786 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %1787 = torch_c.from_builtin_tensor %cast_2241 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_2242 = torch.constant.int 2
    %1788 = torch.aten.view.dtype %80, %int2_2242 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %1789 = torch.aten.detach %1788 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_2243 = torch.constant.int -1
    %int17_2244 = torch.constant.int 17
    %1790 = torch.prim.ListConstruct %int-1_2243, %int17_2244 : (!torch.int, !torch.int) -> !torch.list<int>
    %1791 = torch.aten.view %1789, %1790 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_2245 = torch.constant.int 256
    %int-1_2246 = torch.constant.int -1
    %int17_2247 = torch.constant.int 17
    %1792 = torch.prim.ListConstruct %int256_2245, %int-1_2246, %int17_2247 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1793 = torch.aten.view %1791, %1792 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_2248 = torch.constant.int 2
    %int0_2249 = torch.constant.int 0
    %int1_2250 = torch.constant.int 1
    %int1_2251 = torch.constant.int 1
    %1794 = torch.aten.slice.Tensor %1793, %int2_2248, %int0_2249, %int1_2250, %int1_2251 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_2252 = torch.constant.int 5
    %1795 = torch.aten.view.dtype %1794, %int5_2252 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %1796 = torch.aten.detach %1795 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_2253 = torch.constant.int 2
    %int1_2254 = torch.constant.int 1
    %int9223372036854775807_2255 = torch.constant.int 9223372036854775807
    %int1_2256 = torch.constant.int 1
    %1797 = torch.aten.slice.Tensor %1793, %int2_2253, %int1_2254, %int9223372036854775807_2255, %int1_2256 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_2257 = torch.constant.int 1
    %1798 = torch.aten.view.dtype %1797, %int1_2257 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %1799 = torch.aten.detach %1798 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %1800 = torch_c.to_builtin_tensor %1753 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_2258 = tensor.cast %1800 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %1801 = torch_c.to_builtin_tensor %1796 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %1802 = torch_c.to_builtin_tensor %1799 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %1803 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_2258, %1801, %1802) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_2259 = tensor.cast %1803 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %1804 = torch_c.from_builtin_tensor %cast_2259 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_2260 = torch.constant.int 4
    %int1_2261 = torch.constant.int 1
    %int32_2262 = torch.constant.int 32
    %int64_2263 = torch.constant.int 64
    %1805 = torch.prim.ListConstruct %int4_2260, %int1_2261, %int32_2262, %int64_2263 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1806 = torch.aten.view %1770, %1805 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_2264 = torch.constant.int 4
    %int1_2265 = torch.constant.int 1
    %int4_2266 = torch.constant.int 4
    %int64_2267 = torch.constant.int 64
    %1807 = torch.prim.ListConstruct %int4_2264, %int1_2265, %int4_2266, %int64_2267 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1808 = torch.aten.view %1787, %1807 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_2268 = torch.constant.int 4
    %int1_2269 = torch.constant.int 1
    %int4_2270 = torch.constant.int 4
    %int64_2271 = torch.constant.int 64
    %1809 = torch.prim.ListConstruct %int4_2268, %int1_2269, %int4_2270, %int64_2271 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1810 = torch.aten.view %1804, %1809 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_2272 = torch.constant.int 0
    %int1_2273 = torch.constant.int 1
    %none_2274 = torch.constant.none
    %none_2275 = torch.constant.none
    %cpu_2276 = torch.constant.device "cpu"
    %false_2277 = torch.constant.bool false
    %1811 = torch.aten.arange.start %int0_2272, %int1_2273, %none_2274, %none_2275, %cpu_2276, %false_2277 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_2278 = torch.constant.int 0
    %1812 = torch.aten.unsqueeze %1811, %int0_2278 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_2279 = torch.constant.int 1
    %1813 = torch.aten.unsqueeze %arg2, %int1_2279 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2280 = torch.constant.int 1
    %1814 = torch.aten.add.Tensor %1812, %1813, %int1_2280 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_2281 = torch.constant.int 0
    %int64_2282 = torch.constant.int 64
    %int2_2283 = torch.constant.int 2
    %none_2284 = torch.constant.none
    %none_2285 = torch.constant.none
    %cpu_2286 = torch.constant.device "cpu"
    %false_2287 = torch.constant.bool false
    %1815 = torch.aten.arange.start_step %int0_2281, %int64_2282, %int2_2283, %none_2284, %none_2285, %cpu_2286, %false_2287 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_2288 = torch.constant.none
    %none_2289 = torch.constant.none
    %int4_2290 = torch.constant.int 4
    %cpu_2291 = torch.constant.device "cpu"
    %int0_2292 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1815, %none_2288, %none_2289, %int4_2290, %cpu_2291, %int0_2292 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2293 = torch.constant.int 6
    %1816 = torch.prims.convert_element_type %1815, %int6_2293 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_2294 = torch.constant.int 64
    %1817 = torch.aten.div.Scalar %1816, %int64_2294 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_2295 = torch.constant.float 1.000000e+04
    %1818 = torch.aten.pow.Scalar %float1.000000e04_2295, %1817 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %1819 = torch.aten.reciprocal %1818 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_2296 = torch.constant.float 1.000000e+00
    %1820 = torch.aten.mul.Scalar %1819, %float1.000000e00_2296 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_2297 = torch.constant.none
    %1821 = torch.aten.clone %69, %none_2297 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_2298 = torch.constant.int 0
    %1822 = torch.aten.unsqueeze %1820, %int0_2298 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_2299 = torch.constant.int 2
    %1823 = torch.aten.unsqueeze %1822, %int2_2299 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_2300 = torch.constant.none
    %none_2301 = torch.constant.none
    %int6_2302 = torch.constant.int 6
    %cpu_2303 = torch.constant.device "cpu"
    %int0_2304 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1823, %none_2300, %none_2301, %int6_2302, %cpu_2303, %int0_2304 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_2305 = torch.constant.int 4
    %int-1_2306 = torch.constant.int -1
    %int1_2307 = torch.constant.int 1
    %1824 = torch.prim.ListConstruct %int4_2305, %int-1_2306, %int1_2307 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2308 = torch.constant.bool false
    %1825 = torch.aten.expand %1823, %1824, %false_2308 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_2309 = torch.constant.int 1
    %1826 = torch.aten.unsqueeze %1814, %int1_2309 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_2310 = torch.constant.none
    %none_2311 = torch.constant.none
    %int4_2312 = torch.constant.int 4
    %cpu_2313 = torch.constant.device "cpu"
    %int0_2314 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1826, %none_2310, %none_2311, %int4_2312, %cpu_2313, %int0_2314 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2315 = torch.constant.int 6
    %1827 = torch.prims.convert_element_type %1826, %int6_2315 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1828 = torch.aten.matmul %1825, %1827 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_2316 = torch.constant.int 1
    %int2_2317 = torch.constant.int 2
    %1829 = torch.aten.transpose.int %1828, %int1_2316, %int2_2317 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %1830 = torch.aten.cos %1829 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %1831 = torch.aten.mul.Tensor %1830, %1821 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_2318 = torch.constant.none
    %none_2319 = torch.constant.none
    %int6_2320 = torch.constant.int 6
    %cpu_2321 = torch.constant.device "cpu"
    %int0_2322 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1831, %none_2318, %none_2319, %int6_2320, %cpu_2321, %int0_2322 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2323 = torch.constant.int 5
    %1832 = torch.prims.convert_element_type %1831, %int5_2323 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %1833 = torch.aten.sin %1829 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %1834 = torch.aten.mul.Tensor %1833, %1821 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_2324 = torch.constant.none
    %none_2325 = torch.constant.none
    %int6_2326 = torch.constant.int 6
    %cpu_2327 = torch.constant.device "cpu"
    %int0_2328 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1834, %none_2324, %none_2325, %int6_2326, %cpu_2327, %int0_2328 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2329 = torch.constant.int 5
    %1835 = torch.prims.convert_element_type %1834, %int5_2329 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_2330 = torch.constant.int 2
    %1836 = torch.aten.unsqueeze %1832, %int2_2330 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_2331 = torch.constant.int 2
    %1837 = torch.aten.unsqueeze %1835, %int2_2331 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_2332 = torch.constant.none
    %none_2333 = torch.constant.none
    %int5_2334 = torch.constant.int 5
    %cpu_2335 = torch.constant.device "cpu"
    %int0_2336 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1836, %none_2332, %none_2333, %int5_2334, %cpu_2335, %int0_2336 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_2337 = torch.constant.none
    %none_2338 = torch.constant.none
    %int5_2339 = torch.constant.int 5
    %cpu_2340 = torch.constant.device "cpu"
    %int0_2341 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1837, %none_2337, %none_2338, %int5_2339, %cpu_2340, %int0_2341 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_2342 = torch.constant.none
    %none_2343 = torch.constant.none
    %int5_2344 = torch.constant.int 5
    %cpu_2345 = torch.constant.device "cpu"
    %int0_2346 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1806, %none_2342, %none_2343, %int5_2344, %cpu_2345, %int0_2346 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_2347 = torch.constant.int 3
    %int0_2348 = torch.constant.int 0
    %int64_2349 = torch.constant.int 64
    %int2_2350 = torch.constant.int 2
    %1838 = torch.aten.slice.Tensor %1806, %int3_2347, %int0_2348, %int64_2349, %int2_2350 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_2351 = torch.constant.int 3
    %int1_2352 = torch.constant.int 1
    %int64_2353 = torch.constant.int 64
    %int2_2354 = torch.constant.int 2
    %1839 = torch.aten.slice.Tensor %1806, %int3_2351, %int1_2352, %int64_2353, %int2_2354 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %1840 = torch.aten.mul.Tensor %1838, %1836 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %1841 = torch.aten.mul.Tensor %1839, %1837 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_2355 = torch.constant.int 1
    %1842 = torch.aten.sub.Tensor %1840, %1841, %int1_2355 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %1843 = torch.aten.mul.Tensor %1839, %1836 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %1844 = torch.aten.mul.Tensor %1838, %1837 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_2356 = torch.constant.int 1
    %1845 = torch.aten.add.Tensor %1843, %1844, %int1_2356 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %1846 = torch_c.to_builtin_tensor %1842 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_2357 = tensor.cast %1846 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %1847 = torch_c.to_builtin_tensor %1845 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_2358 = tensor.cast %1847 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %1848 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_2357, %cast_2358) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_2359 = tensor.cast %1848 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %1849 = torch_c.from_builtin_tensor %cast_2359 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_2360 = torch.constant.int 4
    %int1_2361 = torch.constant.int 1
    %int32_2362 = torch.constant.int 32
    %int64_2363 = torch.constant.int 64
    %1850 = torch.prim.ListConstruct %int4_2360, %int1_2361, %int32_2362, %int64_2363 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1851 = torch.aten.view %1849, %1850 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_2364 = torch.constant.none
    %none_2365 = torch.constant.none
    %int5_2366 = torch.constant.int 5
    %cpu_2367 = torch.constant.device "cpu"
    %int0_2368 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1851, %none_2364, %none_2365, %int5_2366, %cpu_2367, %int0_2368 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_2369 = torch.constant.int 0
    %int1_2370 = torch.constant.int 1
    %none_2371 = torch.constant.none
    %none_2372 = torch.constant.none
    %cpu_2373 = torch.constant.device "cpu"
    %false_2374 = torch.constant.bool false
    %1852 = torch.aten.arange.start %int0_2369, %int1_2370, %none_2371, %none_2372, %cpu_2373, %false_2374 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_2375 = torch.constant.int 0
    %1853 = torch.aten.unsqueeze %1852, %int0_2375 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_2376 = torch.constant.int 1
    %1854 = torch.aten.unsqueeze %arg2, %int1_2376 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2377 = torch.constant.int 1
    %1855 = torch.aten.add.Tensor %1853, %1854, %int1_2377 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_2378 = torch.constant.int 0
    %int64_2379 = torch.constant.int 64
    %int2_2380 = torch.constant.int 2
    %none_2381 = torch.constant.none
    %none_2382 = torch.constant.none
    %cpu_2383 = torch.constant.device "cpu"
    %false_2384 = torch.constant.bool false
    %1856 = torch.aten.arange.start_step %int0_2378, %int64_2379, %int2_2380, %none_2381, %none_2382, %cpu_2383, %false_2384 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_2385 = torch.constant.none
    %none_2386 = torch.constant.none
    %int4_2387 = torch.constant.int 4
    %cpu_2388 = torch.constant.device "cpu"
    %int0_2389 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1856, %none_2385, %none_2386, %int4_2387, %cpu_2388, %int0_2389 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2390 = torch.constant.int 6
    %1857 = torch.prims.convert_element_type %1856, %int6_2390 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_2391 = torch.constant.int 64
    %1858 = torch.aten.div.Scalar %1857, %int64_2391 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_2392 = torch.constant.float 1.000000e+04
    %1859 = torch.aten.pow.Scalar %float1.000000e04_2392, %1858 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %1860 = torch.aten.reciprocal %1859 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_2393 = torch.constant.float 1.000000e+00
    %1861 = torch.aten.mul.Scalar %1860, %float1.000000e00_2393 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_2394 = torch.constant.none
    %1862 = torch.aten.clone %70, %none_2394 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_2395 = torch.constant.int 0
    %1863 = torch.aten.unsqueeze %1861, %int0_2395 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_2396 = torch.constant.int 2
    %1864 = torch.aten.unsqueeze %1863, %int2_2396 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_2397 = torch.constant.none
    %none_2398 = torch.constant.none
    %int6_2399 = torch.constant.int 6
    %cpu_2400 = torch.constant.device "cpu"
    %int0_2401 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1864, %none_2397, %none_2398, %int6_2399, %cpu_2400, %int0_2401 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_2402 = torch.constant.int 4
    %int-1_2403 = torch.constant.int -1
    %int1_2404 = torch.constant.int 1
    %1865 = torch.prim.ListConstruct %int4_2402, %int-1_2403, %int1_2404 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2405 = torch.constant.bool false
    %1866 = torch.aten.expand %1864, %1865, %false_2405 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_2406 = torch.constant.int 1
    %1867 = torch.aten.unsqueeze %1855, %int1_2406 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_2407 = torch.constant.none
    %none_2408 = torch.constant.none
    %int4_2409 = torch.constant.int 4
    %cpu_2410 = torch.constant.device "cpu"
    %int0_2411 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1867, %none_2407, %none_2408, %int4_2409, %cpu_2410, %int0_2411 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2412 = torch.constant.int 6
    %1868 = torch.prims.convert_element_type %1867, %int6_2412 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1869 = torch.aten.matmul %1866, %1868 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_2413 = torch.constant.int 1
    %int2_2414 = torch.constant.int 2
    %1870 = torch.aten.transpose.int %1869, %int1_2413, %int2_2414 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %1871 = torch.aten.cos %1870 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %1872 = torch.aten.mul.Tensor %1871, %1862 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_2415 = torch.constant.none
    %none_2416 = torch.constant.none
    %int6_2417 = torch.constant.int 6
    %cpu_2418 = torch.constant.device "cpu"
    %int0_2419 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1872, %none_2415, %none_2416, %int6_2417, %cpu_2418, %int0_2419 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2420 = torch.constant.int 5
    %1873 = torch.prims.convert_element_type %1872, %int5_2420 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %1874 = torch.aten.sin %1870 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %1875 = torch.aten.mul.Tensor %1874, %1862 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_2421 = torch.constant.none
    %none_2422 = torch.constant.none
    %int6_2423 = torch.constant.int 6
    %cpu_2424 = torch.constant.device "cpu"
    %int0_2425 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1875, %none_2421, %none_2422, %int6_2423, %cpu_2424, %int0_2425 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2426 = torch.constant.int 5
    %1876 = torch.prims.convert_element_type %1875, %int5_2426 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_2427 = torch.constant.int 2
    %1877 = torch.aten.unsqueeze %1873, %int2_2427 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_2428 = torch.constant.int 2
    %1878 = torch.aten.unsqueeze %1876, %int2_2428 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_2429 = torch.constant.none
    %none_2430 = torch.constant.none
    %int5_2431 = torch.constant.int 5
    %cpu_2432 = torch.constant.device "cpu"
    %int0_2433 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1877, %none_2429, %none_2430, %int5_2431, %cpu_2432, %int0_2433 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_2434 = torch.constant.none
    %none_2435 = torch.constant.none
    %int5_2436 = torch.constant.int 5
    %cpu_2437 = torch.constant.device "cpu"
    %int0_2438 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1878, %none_2434, %none_2435, %int5_2436, %cpu_2437, %int0_2438 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_2439 = torch.constant.none
    %none_2440 = torch.constant.none
    %int5_2441 = torch.constant.int 5
    %cpu_2442 = torch.constant.device "cpu"
    %int0_2443 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1808, %none_2439, %none_2440, %int5_2441, %cpu_2442, %int0_2443 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_2444 = torch.constant.int 3
    %int0_2445 = torch.constant.int 0
    %int64_2446 = torch.constant.int 64
    %int2_2447 = torch.constant.int 2
    %1879 = torch.aten.slice.Tensor %1808, %int3_2444, %int0_2445, %int64_2446, %int2_2447 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_2448 = torch.constant.int 3
    %int1_2449 = torch.constant.int 1
    %int64_2450 = torch.constant.int 64
    %int2_2451 = torch.constant.int 2
    %1880 = torch.aten.slice.Tensor %1808, %int3_2448, %int1_2449, %int64_2450, %int2_2451 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %1881 = torch.aten.mul.Tensor %1879, %1877 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %1882 = torch.aten.mul.Tensor %1880, %1878 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_2452 = torch.constant.int 1
    %1883 = torch.aten.sub.Tensor %1881, %1882, %int1_2452 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %1884 = torch.aten.mul.Tensor %1880, %1877 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %1885 = torch.aten.mul.Tensor %1879, %1878 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_2453 = torch.constant.int 1
    %1886 = torch.aten.add.Tensor %1884, %1885, %int1_2453 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %1887 = torch_c.to_builtin_tensor %1883 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_2454 = tensor.cast %1887 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %1888 = torch_c.to_builtin_tensor %1886 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_2455 = tensor.cast %1888 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %1889 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_2454, %cast_2455) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_2456 = tensor.cast %1889 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %1890 = torch_c.from_builtin_tensor %cast_2456 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_2457 = torch.constant.int 4
    %int1_2458 = torch.constant.int 1
    %int4_2459 = torch.constant.int 4
    %int64_2460 = torch.constant.int 64
    %1891 = torch.prim.ListConstruct %int4_2457, %int1_2458, %int4_2459, %int64_2460 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1892 = torch.aten.view %1890, %1891 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_2461 = torch.constant.none
    %none_2462 = torch.constant.none
    %int5_2463 = torch.constant.int 5
    %cpu_2464 = torch.constant.device "cpu"
    %int0_2465 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1892, %none_2461, %none_2462, %int5_2463, %cpu_2464, %int0_2465 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_2466 = torch.constant.int 32
    %1893 = torch.aten.floor_divide.Scalar %arg2, %int32_2466 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_2467 = torch.constant.int 1
    %1894 = torch.aten.unsqueeze %1893, %int1_2467 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2468 = torch.constant.int 1
    %false_2469 = torch.constant.bool false
    %1895 = torch.aten.gather %arg3, %int1_2468, %1894, %false_2469 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_2470 = torch.constant.int 4
    %int1_2471 = torch.constant.int 1
    %int1_2472 = torch.constant.int 1
    %1896 = torch.prim.ListConstruct %int4_2470, %int1_2471, %int1_2472 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1897 = torch.aten.view %1895, %1896 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_2473 = torch.constant.int 32
    %1898 = torch.aten.remainder.Scalar %arg2, %int32_2473 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_2474 = torch.constant.int 4
    %int1_2475 = torch.constant.int 1
    %int1_2476 = torch.constant.int 1
    %1899 = torch.prim.ListConstruct %int4_2474, %int1_2475, %int1_2476 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1900 = torch.aten.view %1898, %1899 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_2477 = torch.constant.int 4
    %none_2478 = torch.constant.none
    %none_2479 = torch.constant.none
    %cpu_2480 = torch.constant.device "cpu"
    %false_2481 = torch.constant.bool false
    %1901 = torch.aten.arange %int4_2477, %none_2478, %none_2479, %cpu_2480, %false_2481 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_2482 = torch.constant.int 1
    %int1_2483 = torch.constant.int 1
    %int4_2484 = torch.constant.int 4
    %1902 = torch.prim.ListConstruct %int1_2482, %int1_2483, %int4_2484 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1903 = torch.aten.view %1901, %1902 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_2485 = torch.constant.none
    %1904 = torch.aten.clone %71, %none_2485 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_2486 = torch.constant.int 1
    %int1_2487 = torch.constant.int 1
    %int1_2488 = torch.constant.int 1
    %1905 = torch.prim.ListConstruct %int1_2486, %int1_2487, %int1_2488 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1906 = torch.aten.view %1904, %1905 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_2489 = torch.constant.int 22
    %1907 = torch.aten.mul.Scalar %1897, %int22_2489 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_2490 = torch.constant.int 4
    %int1_2491 = torch.constant.int 1
    %1908 = torch.aten.add.Scalar %1907, %int4_2490, %int1_2491 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_2492 = torch.constant.int 2
    %1909 = torch.aten.mul.Scalar %1908, %int2_2492 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_2493 = torch.constant.int 1
    %1910 = torch.aten.add.Tensor %1909, %1906, %int1_2493 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_2494 = torch.constant.int 4
    %1911 = torch.aten.mul.Scalar %1910, %int4_2494 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_2495 = torch.constant.int 1
    %1912 = torch.aten.add.Tensor %1911, %1903, %int1_2495 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_2496 = torch.constant.int 32
    %1913 = torch.aten.mul.Scalar %1912, %int32_2496 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_2497 = torch.constant.int 1
    %1914 = torch.aten.add.Tensor %1913, %1900, %int1_2497 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_2498 = torch.constant.none
    %none_2499 = torch.constant.none
    %int5_2500 = torch.constant.int 5
    %cpu_2501 = torch.constant.device "cpu"
    %int0_2502 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1892, %none_2498, %none_2499, %int5_2500, %cpu_2501, %int0_2502 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_2503 = torch.constant.int 22
    %int2_2504 = torch.constant.int 2
    %int4_2505 = torch.constant.int 4
    %int32_2506 = torch.constant.int 32
    %int64_2507 = torch.constant.int 64
    %1915 = torch.prim.ListConstruct %381, %int22_2503, %int2_2504, %int4_2505, %int32_2506, %int64_2507 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1916 = torch.aten.view %1610, %1915 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1916, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_2508 = torch.constant.int 64
    %1917 = torch.prim.ListConstruct %553, %int64_2508 : (!torch.int, !torch.int) -> !torch.list<int>
    %1918 = torch.aten.view %1916, %1917 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %1918, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %1919 = torch.prim.ListConstruct %1914 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_2509 = torch.constant.bool false
    %1920 = torch.aten.index_put %1918, %1919, %1892, %false_2509 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %1920, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_2510 = torch.constant.int 22
    %int2_2511 = torch.constant.int 2
    %int4_2512 = torch.constant.int 4
    %int32_2513 = torch.constant.int 32
    %int64_2514 = torch.constant.int 64
    %1921 = torch.prim.ListConstruct %381, %int22_2510, %int2_2511, %int4_2512, %int32_2513, %int64_2514 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1922 = torch.aten.view %1920, %1921 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1922, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_2515 = torch.constant.int 360448
    %1923 = torch.prim.ListConstruct %381, %int360448_2515 : (!torch.int, !torch.int) -> !torch.list<int>
    %1924 = torch.aten.view %1922, %1923 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %1924, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_2516 = torch.constant.int 22
    %int2_2517 = torch.constant.int 2
    %int4_2518 = torch.constant.int 4
    %int32_2519 = torch.constant.int 32
    %int64_2520 = torch.constant.int 64
    %1925 = torch.prim.ListConstruct %381, %int22_2516, %int2_2517, %int4_2518, %int32_2519, %int64_2520 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1926 = torch.aten.view %1924, %1925 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1926, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_2521 = torch.constant.int 64
    %1927 = torch.prim.ListConstruct %553, %int64_2521 : (!torch.int, !torch.int) -> !torch.list<int>
    %1928 = torch.aten.view %1926, %1927 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %1928, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_2522 = torch.constant.none
    %1929 = torch.aten.clone %72, %none_2522 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_2523 = torch.constant.int 1
    %int1_2524 = torch.constant.int 1
    %int1_2525 = torch.constant.int 1
    %1930 = torch.prim.ListConstruct %int1_2523, %int1_2524, %int1_2525 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1931 = torch.aten.view %1929, %1930 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_2526 = torch.constant.int 22
    %1932 = torch.aten.mul.Scalar %1897, %int22_2526 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_2527 = torch.constant.int 4
    %int1_2528 = torch.constant.int 1
    %1933 = torch.aten.add.Scalar %1932, %int4_2527, %int1_2528 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_2529 = torch.constant.int 2
    %1934 = torch.aten.mul.Scalar %1933, %int2_2529 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_2530 = torch.constant.int 1
    %1935 = torch.aten.add.Tensor %1934, %1931, %int1_2530 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_2531 = torch.constant.int 4
    %1936 = torch.aten.mul.Scalar %1935, %int4_2531 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_2532 = torch.constant.int 1
    %1937 = torch.aten.add.Tensor %1936, %1903, %int1_2532 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_2533 = torch.constant.int 32
    %1938 = torch.aten.mul.Scalar %1937, %int32_2533 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_2534 = torch.constant.int 1
    %1939 = torch.aten.add.Tensor %1938, %1900, %int1_2534 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_2535 = torch.constant.none
    %none_2536 = torch.constant.none
    %int5_2537 = torch.constant.int 5
    %cpu_2538 = torch.constant.device "cpu"
    %int0_2539 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1810, %none_2535, %none_2536, %int5_2537, %cpu_2538, %int0_2539 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %1940 = torch.prim.ListConstruct %1939 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_2540 = torch.constant.bool false
    %1941 = torch.aten.index_put %1928, %1940, %1810, %false_2540 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %1941, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_2541 = torch.constant.int 22
    %int2_2542 = torch.constant.int 2
    %int4_2543 = torch.constant.int 4
    %int32_2544 = torch.constant.int 32
    %int64_2545 = torch.constant.int 64
    %1942 = torch.prim.ListConstruct %381, %int22_2541, %int2_2542, %int4_2543, %int32_2544, %int64_2545 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1943 = torch.aten.view %1941, %1942 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1943, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_2546 = torch.constant.int 360448
    %1944 = torch.prim.ListConstruct %381, %int360448_2546 : (!torch.int, !torch.int) -> !torch.list<int>
    %1945 = torch.aten.view %1943, %1944 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %1945, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_2547 = torch.constant.none
    %1946 = torch.aten.clone %73, %none_2547 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_2548 = torch.constant.none
    %1947 = torch.aten.clone %74, %none_2548 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_2549 = torch.constant.none
    %1948 = torch.aten.clone %75, %none_2549 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_2550 = torch.constant.int 22
    %int2_2551 = torch.constant.int 2
    %int4_2552 = torch.constant.int 4
    %int32_2553 = torch.constant.int 32
    %int64_2554 = torch.constant.int 64
    %1949 = torch.prim.ListConstruct %381, %int22_2550, %int2_2551, %int4_2552, %int32_2553, %int64_2554 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1950 = torch.aten.view %1945, %1949 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %1950, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %1951 = torch_c.to_builtin_tensor %1950 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %1952 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_2555 = tensor.cast %1952 : tensor<4x?xi64> to tensor<?x?xi64>
    %1953 = torch_c.to_builtin_tensor %1946 : !torch.vtensor<[],si64> -> tensor<i64>
    %1954 = torch_c.to_builtin_tensor %1947 : !torch.vtensor<[],si64> -> tensor<i64>
    %1955 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%1951, %cast_2555, %1953, %1954) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_2556 = tensor.cast %1955 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %1956 = torch_c.from_builtin_tensor %cast_2556 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %1956, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %1957 = torch_c.to_builtin_tensor %1950 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %1958 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_2557 = tensor.cast %1958 : tensor<4x?xi64> to tensor<?x?xi64>
    %1959 = torch_c.to_builtin_tensor %1946 : !torch.vtensor<[],si64> -> tensor<i64>
    %1960 = torch_c.to_builtin_tensor %1948 : !torch.vtensor<[],si64> -> tensor<i64>
    %1961 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%1957, %cast_2557, %1959, %1960) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_2558 = tensor.cast %1961 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %1962 = torch_c.from_builtin_tensor %cast_2558 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %1962, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_2559 = torch.constant.int 2
    %int3_2560 = torch.constant.int 3
    %1963 = torch.aten.transpose.int %1956, %int2_2559, %int3_2560 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1963, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_2561 = torch.constant.int 0
    %1964 = torch.aten.clone %1963, %int0_2561 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1964, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_2562 = torch.constant.int 4
    %int4_2563 = torch.constant.int 4
    %int64_2564 = torch.constant.int 64
    %1965 = torch.prim.ListConstruct %int4_2562, %623, %int4_2563, %int64_2564 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1966 = torch.aten._unsafe_view %1964, %1965 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %1966, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_2565 = torch.constant.int 2
    %int3_2566 = torch.constant.int 3
    %1967 = torch.aten.transpose.int %1962, %int2_2565, %int3_2566 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1967, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_2567 = torch.constant.int 0
    %1968 = torch.aten.clone %1967, %int0_2567 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %1968, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_2568 = torch.constant.int 4
    %int4_2569 = torch.constant.int 4
    %int64_2570 = torch.constant.int 64
    %1969 = torch.prim.ListConstruct %int4_2568, %623, %int4_2569, %int64_2570 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1970 = torch.aten._unsafe_view %1968, %1969 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %1970, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_2571 = torch.constant.int 0
    %int1_2572 = torch.constant.int 1
    %none_2573 = torch.constant.none
    %none_2574 = torch.constant.none
    %cpu_2575 = torch.constant.device "cpu"
    %false_2576 = torch.constant.bool false
    %1971 = torch.aten.arange.start_step %int0_2571, %623, %int1_2572, %none_2573, %none_2574, %cpu_2575, %false_2576 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1971, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_2577 = torch.constant.int -1
    %1972 = torch.aten.unsqueeze %arg1, %int-1_2577 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %1973 = torch.aten.ge.Tensor %1971, %1972 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %1973, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_2578 = torch.constant.none
    %1974 = torch.aten.clone %76, %none_2578 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_2579 = torch.constant.int 0
    %1975 = torch.aten.where.ScalarOther %1973, %1974, %int0_2579 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %1975, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_2580 = torch.constant.none
    %none_2581 = torch.constant.none
    %int5_2582 = torch.constant.int 5
    %cpu_2583 = torch.constant.device "cpu"
    %int0_2584 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %1975, %none_2580, %none_2581, %int5_2582, %cpu_2583, %int0_2584 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_2585 = torch.constant.int 1
    %1976 = torch.aten.unsqueeze %1975, %int1_2585 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %1976, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_2586 = torch.constant.int 1
    %1977 = torch.aten.unsqueeze %1976, %int1_2586 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %1977, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_2587 = torch.constant.int -2
    %1978 = torch.aten.unsqueeze %1966, %int-2_2587 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %1978, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_2588 = torch.constant.int 4
    %int4_2589 = torch.constant.int 4
    %int8_2590 = torch.constant.int 8
    %int64_2591 = torch.constant.int 64
    %1979 = torch.prim.ListConstruct %int4_2588, %623, %int4_2589, %int8_2590, %int64_2591 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2592 = torch.constant.bool false
    %1980 = torch.aten.expand %1978, %1979, %false_2592 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1980, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_2593 = torch.constant.int 0
    %1981 = torch.aten.clone %1980, %int0_2593 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1981, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_2594 = torch.constant.int 4
    %int32_2595 = torch.constant.int 32
    %int64_2596 = torch.constant.int 64
    %1982 = torch.prim.ListConstruct %int4_2594, %623, %int32_2595, %int64_2596 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1983 = torch.aten._unsafe_view %1981, %1982 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1983, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_2597 = torch.constant.int -2
    %1984 = torch.aten.unsqueeze %1970, %int-2_2597 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %1984, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_2598 = torch.constant.int 4
    %int4_2599 = torch.constant.int 4
    %int8_2600 = torch.constant.int 8
    %int64_2601 = torch.constant.int 64
    %1985 = torch.prim.ListConstruct %int4_2598, %623, %int4_2599, %int8_2600, %int64_2601 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2602 = torch.constant.bool false
    %1986 = torch.aten.expand %1984, %1985, %false_2602 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1986, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_2603 = torch.constant.int 0
    %1987 = torch.aten.clone %1986, %int0_2603 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %1987, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_2604 = torch.constant.int 4
    %int32_2605 = torch.constant.int 32
    %int64_2606 = torch.constant.int 64
    %1988 = torch.prim.ListConstruct %int4_2604, %623, %int32_2605, %int64_2606 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1989 = torch.aten._unsafe_view %1987, %1988 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %1989, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_2607 = torch.constant.int 1
    %int2_2608 = torch.constant.int 2
    %1990 = torch.aten.transpose.int %1851, %int1_2607, %int2_2608 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_2609 = torch.constant.int 1
    %int2_2610 = torch.constant.int 2
    %1991 = torch.aten.transpose.int %1983, %int1_2609, %int2_2610 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %1991, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_2611 = torch.constant.int 1
    %int2_2612 = torch.constant.int 2
    %1992 = torch.aten.transpose.int %1989, %int1_2611, %int2_2612 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %1992, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_2613 = torch.constant.float 0.000000e+00
    %false_2614 = torch.constant.bool false
    %none_2615 = torch.constant.none
    %false_2616 = torch.constant.bool false
    %1993 = torch.aten.scaled_dot_product_attention %1990, %1991, %1992, %1977, %float0.000000e00_2613, %false_2614, %none_2615, %false_2616 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_2617 = torch.constant.int 1
    %int2_2618 = torch.constant.int 2
    %1994 = torch.aten.transpose.int %1993, %int1_2617, %int2_2618 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_2619 = torch.constant.int 4
    %int1_2620 = torch.constant.int 1
    %int2048_2621 = torch.constant.int 2048
    %1995 = torch.prim.ListConstruct %int4_2619, %int1_2620, %int2048_2621 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1996 = torch.aten.view %1994, %1995 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_2622 = torch.constant.int 2
    %1997 = torch.aten.view.dtype %81, %int2_2622 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %1998 = torch.aten.detach %1997 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_2623 = torch.constant.int -1
    %int17_2624 = torch.constant.int 17
    %1999 = torch.prim.ListConstruct %int-1_2623, %int17_2624 : (!torch.int, !torch.int) -> !torch.list<int>
    %2000 = torch.aten.view %1998, %1999 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_2625 = torch.constant.int 2048
    %int-1_2626 = torch.constant.int -1
    %int17_2627 = torch.constant.int 17
    %2001 = torch.prim.ListConstruct %int2048_2625, %int-1_2626, %int17_2627 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2002 = torch.aten.view %2000, %2001 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_2628 = torch.constant.int 2
    %int0_2629 = torch.constant.int 0
    %int1_2630 = torch.constant.int 1
    %int1_2631 = torch.constant.int 1
    %2003 = torch.aten.slice.Tensor %2002, %int2_2628, %int0_2629, %int1_2630, %int1_2631 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_2632 = torch.constant.int 5
    %2004 = torch.aten.view.dtype %2003, %int5_2632 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %2005 = torch.aten.detach %2004 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_2633 = torch.constant.int 2
    %int1_2634 = torch.constant.int 1
    %int9223372036854775807_2635 = torch.constant.int 9223372036854775807
    %int1_2636 = torch.constant.int 1
    %2006 = torch.aten.slice.Tensor %2002, %int2_2633, %int1_2634, %int9223372036854775807_2635, %int1_2636 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_2637 = torch.constant.int 1
    %2007 = torch.aten.view.dtype %2006, %int1_2637 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %2008 = torch.aten.detach %2007 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %2009 = torch_c.to_builtin_tensor %1996 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_2638 = tensor.cast %2009 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %2010 = torch_c.to_builtin_tensor %2005 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %2011 = torch_c.to_builtin_tensor %2008 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %2012 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_2638, %2010, %2011) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_2639 = tensor.cast %2012 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %2013 = torch_c.from_builtin_tensor %cast_2639 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_2640 = torch.constant.none
    %none_2641 = torch.constant.none
    %int5_2642 = torch.constant.int 5
    %cpu_2643 = torch.constant.device "cpu"
    %int0_2644 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2013, %none_2640, %none_2641, %int5_2642, %cpu_2643, %int0_2644 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_2645 = torch.constant.int 1
    %2014 = torch.aten.add.Tensor %1743, %2013, %int1_2645 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_2646 = torch.constant.none
    %none_2647 = torch.constant.none
    %int5_2648 = torch.constant.int 5
    %cpu_2649 = torch.constant.device "cpu"
    %int0_2650 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2014, %none_2646, %none_2647, %int5_2648, %cpu_2649, %int0_2650 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2651 = torch.constant.int 6
    %2015 = torch.prims.convert_element_type %2014, %int6_2651 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_2652 = torch.constant.int 2
    %2016 = torch.aten.pow.Tensor_Scalar %2015, %int2_2652 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_2653 = torch.constant.int -1
    %2017 = torch.prim.ListConstruct %int-1_2653 : (!torch.int) -> !torch.list<int>
    %true_2654 = torch.constant.bool true
    %none_2655 = torch.constant.none
    %2018 = torch.aten.mean.dim %2016, %2017, %true_2654, %none_2655 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_2656 = torch.constant.float 9.9999997473787516E-6
    %int1_2657 = torch.constant.int 1
    %2019 = torch.aten.add.Scalar %2018, %float9.999990e-06_2656, %int1_2657 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2020 = torch.aten.rsqrt %2019 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %2021 = torch.aten.mul.Tensor %2015, %2020 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_2658 = torch.constant.none
    %none_2659 = torch.constant.none
    %int6_2660 = torch.constant.int 6
    %cpu_2661 = torch.constant.device "cpu"
    %int0_2662 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2021, %none_2658, %none_2659, %int6_2660, %cpu_2661, %int0_2662 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2663 = torch.constant.int 5
    %2022 = torch.prims.convert_element_type %2021, %int5_2663 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %2023 = torch.aten.mul.Tensor %82, %2022 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_2664 = torch.constant.none
    %none_2665 = torch.constant.none
    %int6_2666 = torch.constant.int 6
    %cpu_2667 = torch.constant.device "cpu"
    %int0_2668 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2023, %none_2664, %none_2665, %int6_2666, %cpu_2667, %int0_2668 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2669 = torch.constant.int 5
    %2024 = torch.prims.convert_element_type %2023, %int5_2669 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_2670 = torch.constant.int 2
    %2025 = torch.aten.view.dtype %83, %int2_2670 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %2026 = torch.aten.detach %2025 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_2671 = torch.constant.int -1
    %int17_2672 = torch.constant.int 17
    %2027 = torch.prim.ListConstruct %int-1_2671, %int17_2672 : (!torch.int, !torch.int) -> !torch.list<int>
    %2028 = torch.aten.view %2026, %2027 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_2673 = torch.constant.int 5632
    %int-1_2674 = torch.constant.int -1
    %int17_2675 = torch.constant.int 17
    %2029 = torch.prim.ListConstruct %int5632_2673, %int-1_2674, %int17_2675 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2030 = torch.aten.view %2028, %2029 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_2676 = torch.constant.int 2
    %int0_2677 = torch.constant.int 0
    %int1_2678 = torch.constant.int 1
    %int1_2679 = torch.constant.int 1
    %2031 = torch.aten.slice.Tensor %2030, %int2_2676, %int0_2677, %int1_2678, %int1_2679 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_2680 = torch.constant.int 5
    %2032 = torch.aten.view.dtype %2031, %int5_2680 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %2033 = torch.aten.detach %2032 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_2681 = torch.constant.int 2
    %int1_2682 = torch.constant.int 1
    %int9223372036854775807_2683 = torch.constant.int 9223372036854775807
    %int1_2684 = torch.constant.int 1
    %2034 = torch.aten.slice.Tensor %2030, %int2_2681, %int1_2682, %int9223372036854775807_2683, %int1_2684 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_2685 = torch.constant.int 1
    %2035 = torch.aten.view.dtype %2034, %int1_2685 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %2036 = torch.aten.detach %2035 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %2037 = torch_c.to_builtin_tensor %2024 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_2686 = tensor.cast %2037 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %2038 = torch_c.to_builtin_tensor %2033 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %2039 = torch_c.to_builtin_tensor %2036 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %2040 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_2686, %2038, %2039) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_2687 = tensor.cast %2040 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %2041 = torch_c.from_builtin_tensor %cast_2687 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %2042 = torch.aten.silu %2041 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_2688 = torch.constant.int 2
    %2043 = torch.aten.view.dtype %84, %int2_2688 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %2044 = torch.aten.detach %2043 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_2689 = torch.constant.int -1
    %int17_2690 = torch.constant.int 17
    %2045 = torch.prim.ListConstruct %int-1_2689, %int17_2690 : (!torch.int, !torch.int) -> !torch.list<int>
    %2046 = torch.aten.view %2044, %2045 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_2691 = torch.constant.int 5632
    %int-1_2692 = torch.constant.int -1
    %int17_2693 = torch.constant.int 17
    %2047 = torch.prim.ListConstruct %int5632_2691, %int-1_2692, %int17_2693 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2048 = torch.aten.view %2046, %2047 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_2694 = torch.constant.int 2
    %int0_2695 = torch.constant.int 0
    %int1_2696 = torch.constant.int 1
    %int1_2697 = torch.constant.int 1
    %2049 = torch.aten.slice.Tensor %2048, %int2_2694, %int0_2695, %int1_2696, %int1_2697 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_2698 = torch.constant.int 5
    %2050 = torch.aten.view.dtype %2049, %int5_2698 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %2051 = torch.aten.detach %2050 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_2699 = torch.constant.int 2
    %int1_2700 = torch.constant.int 1
    %int9223372036854775807_2701 = torch.constant.int 9223372036854775807
    %int1_2702 = torch.constant.int 1
    %2052 = torch.aten.slice.Tensor %2048, %int2_2699, %int1_2700, %int9223372036854775807_2701, %int1_2702 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_2703 = torch.constant.int 1
    %2053 = torch.aten.view.dtype %2052, %int1_2703 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %2054 = torch.aten.detach %2053 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %2055 = torch_c.to_builtin_tensor %2024 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_2704 = tensor.cast %2055 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %2056 = torch_c.to_builtin_tensor %2051 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %2057 = torch_c.to_builtin_tensor %2054 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %2058 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_2704, %2056, %2057) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_2705 = tensor.cast %2058 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %2059 = torch_c.from_builtin_tensor %cast_2705 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %2060 = torch.aten.mul.Tensor %2042, %2059 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_2706 = torch.constant.int 2
    %2061 = torch.aten.view.dtype %85, %int2_2706 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %2062 = torch.aten.detach %2061 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_2707 = torch.constant.int -1
    %int17_2708 = torch.constant.int 17
    %2063 = torch.prim.ListConstruct %int-1_2707, %int17_2708 : (!torch.int, !torch.int) -> !torch.list<int>
    %2064 = torch.aten.view %2062, %2063 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_2709 = torch.constant.int 2048
    %int-1_2710 = torch.constant.int -1
    %int17_2711 = torch.constant.int 17
    %2065 = torch.prim.ListConstruct %int2048_2709, %int-1_2710, %int17_2711 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2066 = torch.aten.view %2064, %2065 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_2712 = torch.constant.int 2
    %int0_2713 = torch.constant.int 0
    %int1_2714 = torch.constant.int 1
    %int1_2715 = torch.constant.int 1
    %2067 = torch.aten.slice.Tensor %2066, %int2_2712, %int0_2713, %int1_2714, %int1_2715 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_2716 = torch.constant.int 5
    %2068 = torch.aten.view.dtype %2067, %int5_2716 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %2069 = torch.aten.detach %2068 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_2717 = torch.constant.int 2
    %int1_2718 = torch.constant.int 1
    %int9223372036854775807_2719 = torch.constant.int 9223372036854775807
    %int1_2720 = torch.constant.int 1
    %2070 = torch.aten.slice.Tensor %2066, %int2_2717, %int1_2718, %int9223372036854775807_2719, %int1_2720 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_2721 = torch.constant.int 1
    %2071 = torch.aten.view.dtype %2070, %int1_2721 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %2072 = torch.aten.detach %2071 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %2073 = torch_c.to_builtin_tensor %2060 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_2722 = tensor.cast %2073 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %2074 = torch_c.to_builtin_tensor %2069 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %2075 = torch_c.to_builtin_tensor %2072 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %2076 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_2722, %2074, %2075) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_2723 = tensor.cast %2076 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %2077 = torch_c.from_builtin_tensor %cast_2723 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_2724 = torch.constant.int 1
    %2078 = torch.aten.add.Tensor %2014, %2077, %int1_2724 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_2725 = torch.constant.none
    %none_2726 = torch.constant.none
    %int5_2727 = torch.constant.int 5
    %cpu_2728 = torch.constant.device "cpu"
    %int0_2729 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2078, %none_2725, %none_2726, %int5_2727, %cpu_2728, %int0_2729 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2730 = torch.constant.int 6
    %2079 = torch.prims.convert_element_type %2078, %int6_2730 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_2731 = torch.constant.int 2
    %2080 = torch.aten.pow.Tensor_Scalar %2079, %int2_2731 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_2732 = torch.constant.int -1
    %2081 = torch.prim.ListConstruct %int-1_2732 : (!torch.int) -> !torch.list<int>
    %true_2733 = torch.constant.bool true
    %none_2734 = torch.constant.none
    %2082 = torch.aten.mean.dim %2080, %2081, %true_2733, %none_2734 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_2735 = torch.constant.float 9.9999997473787516E-6
    %int1_2736 = torch.constant.int 1
    %2083 = torch.aten.add.Scalar %2082, %float9.999990e-06_2735, %int1_2736 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2084 = torch.aten.rsqrt %2083 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %2085 = torch.aten.mul.Tensor %2079, %2084 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_2737 = torch.constant.none
    %none_2738 = torch.constant.none
    %int6_2739 = torch.constant.int 6
    %cpu_2740 = torch.constant.device "cpu"
    %int0_2741 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2085, %none_2737, %none_2738, %int6_2739, %cpu_2740, %int0_2741 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2742 = torch.constant.int 5
    %2086 = torch.prims.convert_element_type %2085, %int5_2742 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %2087 = torch.aten.mul.Tensor %94, %2086 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_2743 = torch.constant.none
    %none_2744 = torch.constant.none
    %int6_2745 = torch.constant.int 6
    %cpu_2746 = torch.constant.device "cpu"
    %int0_2747 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2087, %none_2743, %none_2744, %int6_2745, %cpu_2746, %int0_2747 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2748 = torch.constant.int 5
    %2088 = torch.prims.convert_element_type %2087, %int5_2748 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_2749 = torch.constant.int 2
    %2089 = torch.aten.view.dtype %95, %int2_2749 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %2090 = torch.aten.detach %2089 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_2750 = torch.constant.int -1
    %int17_2751 = torch.constant.int 17
    %2091 = torch.prim.ListConstruct %int-1_2750, %int17_2751 : (!torch.int, !torch.int) -> !torch.list<int>
    %2092 = torch.aten.view %2090, %2091 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_2752 = torch.constant.int 2048
    %int-1_2753 = torch.constant.int -1
    %int17_2754 = torch.constant.int 17
    %2093 = torch.prim.ListConstruct %int2048_2752, %int-1_2753, %int17_2754 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2094 = torch.aten.view %2092, %2093 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_2755 = torch.constant.int 2
    %int0_2756 = torch.constant.int 0
    %int1_2757 = torch.constant.int 1
    %int1_2758 = torch.constant.int 1
    %2095 = torch.aten.slice.Tensor %2094, %int2_2755, %int0_2756, %int1_2757, %int1_2758 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_2759 = torch.constant.int 5
    %2096 = torch.aten.view.dtype %2095, %int5_2759 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %2097 = torch.aten.detach %2096 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_2760 = torch.constant.int 2
    %int1_2761 = torch.constant.int 1
    %int9223372036854775807_2762 = torch.constant.int 9223372036854775807
    %int1_2763 = torch.constant.int 1
    %2098 = torch.aten.slice.Tensor %2094, %int2_2760, %int1_2761, %int9223372036854775807_2762, %int1_2763 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_2764 = torch.constant.int 1
    %2099 = torch.aten.view.dtype %2098, %int1_2764 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %2100 = torch.aten.detach %2099 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %2101 = torch_c.to_builtin_tensor %2088 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_2765 = tensor.cast %2101 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %2102 = torch_c.to_builtin_tensor %2097 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %2103 = torch_c.to_builtin_tensor %2100 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %2104 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_2765, %2102, %2103) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_2766 = tensor.cast %2104 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %2105 = torch_c.from_builtin_tensor %cast_2766 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_2767 = torch.constant.int 2
    %2106 = torch.aten.view.dtype %96, %int2_2767 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %2107 = torch.aten.detach %2106 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_2768 = torch.constant.int -1
    %int17_2769 = torch.constant.int 17
    %2108 = torch.prim.ListConstruct %int-1_2768, %int17_2769 : (!torch.int, !torch.int) -> !torch.list<int>
    %2109 = torch.aten.view %2107, %2108 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_2770 = torch.constant.int 256
    %int-1_2771 = torch.constant.int -1
    %int17_2772 = torch.constant.int 17
    %2110 = torch.prim.ListConstruct %int256_2770, %int-1_2771, %int17_2772 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2111 = torch.aten.view %2109, %2110 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_2773 = torch.constant.int 2
    %int0_2774 = torch.constant.int 0
    %int1_2775 = torch.constant.int 1
    %int1_2776 = torch.constant.int 1
    %2112 = torch.aten.slice.Tensor %2111, %int2_2773, %int0_2774, %int1_2775, %int1_2776 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_2777 = torch.constant.int 5
    %2113 = torch.aten.view.dtype %2112, %int5_2777 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %2114 = torch.aten.detach %2113 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_2778 = torch.constant.int 2
    %int1_2779 = torch.constant.int 1
    %int9223372036854775807_2780 = torch.constant.int 9223372036854775807
    %int1_2781 = torch.constant.int 1
    %2115 = torch.aten.slice.Tensor %2111, %int2_2778, %int1_2779, %int9223372036854775807_2780, %int1_2781 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_2782 = torch.constant.int 1
    %2116 = torch.aten.view.dtype %2115, %int1_2782 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %2117 = torch.aten.detach %2116 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %2118 = torch_c.to_builtin_tensor %2088 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_2783 = tensor.cast %2118 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %2119 = torch_c.to_builtin_tensor %2114 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %2120 = torch_c.to_builtin_tensor %2117 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %2121 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_2783, %2119, %2120) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_2784 = tensor.cast %2121 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %2122 = torch_c.from_builtin_tensor %cast_2784 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_2785 = torch.constant.int 2
    %2123 = torch.aten.view.dtype %97, %int2_2785 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %2124 = torch.aten.detach %2123 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_2786 = torch.constant.int -1
    %int17_2787 = torch.constant.int 17
    %2125 = torch.prim.ListConstruct %int-1_2786, %int17_2787 : (!torch.int, !torch.int) -> !torch.list<int>
    %2126 = torch.aten.view %2124, %2125 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_2788 = torch.constant.int 256
    %int-1_2789 = torch.constant.int -1
    %int17_2790 = torch.constant.int 17
    %2127 = torch.prim.ListConstruct %int256_2788, %int-1_2789, %int17_2790 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2128 = torch.aten.view %2126, %2127 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_2791 = torch.constant.int 2
    %int0_2792 = torch.constant.int 0
    %int1_2793 = torch.constant.int 1
    %int1_2794 = torch.constant.int 1
    %2129 = torch.aten.slice.Tensor %2128, %int2_2791, %int0_2792, %int1_2793, %int1_2794 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_2795 = torch.constant.int 5
    %2130 = torch.aten.view.dtype %2129, %int5_2795 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %2131 = torch.aten.detach %2130 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_2796 = torch.constant.int 2
    %int1_2797 = torch.constant.int 1
    %int9223372036854775807_2798 = torch.constant.int 9223372036854775807
    %int1_2799 = torch.constant.int 1
    %2132 = torch.aten.slice.Tensor %2128, %int2_2796, %int1_2797, %int9223372036854775807_2798, %int1_2799 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_2800 = torch.constant.int 1
    %2133 = torch.aten.view.dtype %2132, %int1_2800 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %2134 = torch.aten.detach %2133 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %2135 = torch_c.to_builtin_tensor %2088 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_2801 = tensor.cast %2135 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %2136 = torch_c.to_builtin_tensor %2131 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %2137 = torch_c.to_builtin_tensor %2134 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %2138 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_2801, %2136, %2137) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_2802 = tensor.cast %2138 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %2139 = torch_c.from_builtin_tensor %cast_2802 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_2803 = torch.constant.int 4
    %int1_2804 = torch.constant.int 1
    %int32_2805 = torch.constant.int 32
    %int64_2806 = torch.constant.int 64
    %2140 = torch.prim.ListConstruct %int4_2803, %int1_2804, %int32_2805, %int64_2806 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2141 = torch.aten.view %2105, %2140 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_2807 = torch.constant.int 4
    %int1_2808 = torch.constant.int 1
    %int4_2809 = torch.constant.int 4
    %int64_2810 = torch.constant.int 64
    %2142 = torch.prim.ListConstruct %int4_2807, %int1_2808, %int4_2809, %int64_2810 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2143 = torch.aten.view %2122, %2142 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_2811 = torch.constant.int 4
    %int1_2812 = torch.constant.int 1
    %int4_2813 = torch.constant.int 4
    %int64_2814 = torch.constant.int 64
    %2144 = torch.prim.ListConstruct %int4_2811, %int1_2812, %int4_2813, %int64_2814 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2145 = torch.aten.view %2139, %2144 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_2815 = torch.constant.int 0
    %int1_2816 = torch.constant.int 1
    %none_2817 = torch.constant.none
    %none_2818 = torch.constant.none
    %cpu_2819 = torch.constant.device "cpu"
    %false_2820 = torch.constant.bool false
    %2146 = torch.aten.arange.start %int0_2815, %int1_2816, %none_2817, %none_2818, %cpu_2819, %false_2820 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_2821 = torch.constant.int 0
    %2147 = torch.aten.unsqueeze %2146, %int0_2821 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_2822 = torch.constant.int 1
    %2148 = torch.aten.unsqueeze %arg2, %int1_2822 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2823 = torch.constant.int 1
    %2149 = torch.aten.add.Tensor %2147, %2148, %int1_2823 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_2824 = torch.constant.int 0
    %int64_2825 = torch.constant.int 64
    %int2_2826 = torch.constant.int 2
    %none_2827 = torch.constant.none
    %none_2828 = torch.constant.none
    %cpu_2829 = torch.constant.device "cpu"
    %false_2830 = torch.constant.bool false
    %2150 = torch.aten.arange.start_step %int0_2824, %int64_2825, %int2_2826, %none_2827, %none_2828, %cpu_2829, %false_2830 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_2831 = torch.constant.none
    %none_2832 = torch.constant.none
    %int4_2833 = torch.constant.int 4
    %cpu_2834 = torch.constant.device "cpu"
    %int0_2835 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2150, %none_2831, %none_2832, %int4_2833, %cpu_2834, %int0_2835 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2836 = torch.constant.int 6
    %2151 = torch.prims.convert_element_type %2150, %int6_2836 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_2837 = torch.constant.int 64
    %2152 = torch.aten.div.Scalar %2151, %int64_2837 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_2838 = torch.constant.float 1.000000e+04
    %2153 = torch.aten.pow.Scalar %float1.000000e04_2838, %2152 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %2154 = torch.aten.reciprocal %2153 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_2839 = torch.constant.float 1.000000e+00
    %2155 = torch.aten.mul.Scalar %2154, %float1.000000e00_2839 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_2840 = torch.constant.none
    %2156 = torch.aten.clone %86, %none_2840 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_2841 = torch.constant.int 0
    %2157 = torch.aten.unsqueeze %2155, %int0_2841 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_2842 = torch.constant.int 2
    %2158 = torch.aten.unsqueeze %2157, %int2_2842 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_2843 = torch.constant.none
    %none_2844 = torch.constant.none
    %int6_2845 = torch.constant.int 6
    %cpu_2846 = torch.constant.device "cpu"
    %int0_2847 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2158, %none_2843, %none_2844, %int6_2845, %cpu_2846, %int0_2847 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_2848 = torch.constant.int 4
    %int-1_2849 = torch.constant.int -1
    %int1_2850 = torch.constant.int 1
    %2159 = torch.prim.ListConstruct %int4_2848, %int-1_2849, %int1_2850 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2851 = torch.constant.bool false
    %2160 = torch.aten.expand %2158, %2159, %false_2851 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_2852 = torch.constant.int 1
    %2161 = torch.aten.unsqueeze %2149, %int1_2852 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_2853 = torch.constant.none
    %none_2854 = torch.constant.none
    %int4_2855 = torch.constant.int 4
    %cpu_2856 = torch.constant.device "cpu"
    %int0_2857 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2161, %none_2853, %none_2854, %int4_2855, %cpu_2856, %int0_2857 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2858 = torch.constant.int 6
    %2162 = torch.prims.convert_element_type %2161, %int6_2858 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2163 = torch.aten.matmul %2160, %2162 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_2859 = torch.constant.int 1
    %int2_2860 = torch.constant.int 2
    %2164 = torch.aten.transpose.int %2163, %int1_2859, %int2_2860 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %2165 = torch.aten.cos %2164 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %2166 = torch.aten.mul.Tensor %2165, %2156 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_2861 = torch.constant.none
    %none_2862 = torch.constant.none
    %int6_2863 = torch.constant.int 6
    %cpu_2864 = torch.constant.device "cpu"
    %int0_2865 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2166, %none_2861, %none_2862, %int6_2863, %cpu_2864, %int0_2865 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2866 = torch.constant.int 5
    %2167 = torch.prims.convert_element_type %2166, %int5_2866 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %2168 = torch.aten.sin %2164 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %2169 = torch.aten.mul.Tensor %2168, %2156 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_2867 = torch.constant.none
    %none_2868 = torch.constant.none
    %int6_2869 = torch.constant.int 6
    %cpu_2870 = torch.constant.device "cpu"
    %int0_2871 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2169, %none_2867, %none_2868, %int6_2869, %cpu_2870, %int0_2871 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2872 = torch.constant.int 5
    %2170 = torch.prims.convert_element_type %2169, %int5_2872 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_2873 = torch.constant.int 2
    %2171 = torch.aten.unsqueeze %2167, %int2_2873 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_2874 = torch.constant.int 2
    %2172 = torch.aten.unsqueeze %2170, %int2_2874 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_2875 = torch.constant.none
    %none_2876 = torch.constant.none
    %int5_2877 = torch.constant.int 5
    %cpu_2878 = torch.constant.device "cpu"
    %int0_2879 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2171, %none_2875, %none_2876, %int5_2877, %cpu_2878, %int0_2879 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_2880 = torch.constant.none
    %none_2881 = torch.constant.none
    %int5_2882 = torch.constant.int 5
    %cpu_2883 = torch.constant.device "cpu"
    %int0_2884 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2172, %none_2880, %none_2881, %int5_2882, %cpu_2883, %int0_2884 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_2885 = torch.constant.none
    %none_2886 = torch.constant.none
    %int5_2887 = torch.constant.int 5
    %cpu_2888 = torch.constant.device "cpu"
    %int0_2889 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2141, %none_2885, %none_2886, %int5_2887, %cpu_2888, %int0_2889 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_2890 = torch.constant.int 3
    %int0_2891 = torch.constant.int 0
    %int64_2892 = torch.constant.int 64
    %int2_2893 = torch.constant.int 2
    %2173 = torch.aten.slice.Tensor %2141, %int3_2890, %int0_2891, %int64_2892, %int2_2893 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_2894 = torch.constant.int 3
    %int1_2895 = torch.constant.int 1
    %int64_2896 = torch.constant.int 64
    %int2_2897 = torch.constant.int 2
    %2174 = torch.aten.slice.Tensor %2141, %int3_2894, %int1_2895, %int64_2896, %int2_2897 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %2175 = torch.aten.mul.Tensor %2173, %2171 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %2176 = torch.aten.mul.Tensor %2174, %2172 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_2898 = torch.constant.int 1
    %2177 = torch.aten.sub.Tensor %2175, %2176, %int1_2898 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %2178 = torch.aten.mul.Tensor %2174, %2171 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %2179 = torch.aten.mul.Tensor %2173, %2172 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_2899 = torch.constant.int 1
    %2180 = torch.aten.add.Tensor %2178, %2179, %int1_2899 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %2181 = torch_c.to_builtin_tensor %2177 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_2900 = tensor.cast %2181 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %2182 = torch_c.to_builtin_tensor %2180 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_2901 = tensor.cast %2182 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %2183 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_2900, %cast_2901) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_2902 = tensor.cast %2183 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %2184 = torch_c.from_builtin_tensor %cast_2902 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_2903 = torch.constant.int 4
    %int1_2904 = torch.constant.int 1
    %int32_2905 = torch.constant.int 32
    %int64_2906 = torch.constant.int 64
    %2185 = torch.prim.ListConstruct %int4_2903, %int1_2904, %int32_2905, %int64_2906 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2186 = torch.aten.view %2184, %2185 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_2907 = torch.constant.none
    %none_2908 = torch.constant.none
    %int5_2909 = torch.constant.int 5
    %cpu_2910 = torch.constant.device "cpu"
    %int0_2911 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2186, %none_2907, %none_2908, %int5_2909, %cpu_2910, %int0_2911 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_2912 = torch.constant.int 0
    %int1_2913 = torch.constant.int 1
    %none_2914 = torch.constant.none
    %none_2915 = torch.constant.none
    %cpu_2916 = torch.constant.device "cpu"
    %false_2917 = torch.constant.bool false
    %2187 = torch.aten.arange.start %int0_2912, %int1_2913, %none_2914, %none_2915, %cpu_2916, %false_2917 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_2918 = torch.constant.int 0
    %2188 = torch.aten.unsqueeze %2187, %int0_2918 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_2919 = torch.constant.int 1
    %2189 = torch.aten.unsqueeze %arg2, %int1_2919 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2920 = torch.constant.int 1
    %2190 = torch.aten.add.Tensor %2188, %2189, %int1_2920 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_2921 = torch.constant.int 0
    %int64_2922 = torch.constant.int 64
    %int2_2923 = torch.constant.int 2
    %none_2924 = torch.constant.none
    %none_2925 = torch.constant.none
    %cpu_2926 = torch.constant.device "cpu"
    %false_2927 = torch.constant.bool false
    %2191 = torch.aten.arange.start_step %int0_2921, %int64_2922, %int2_2923, %none_2924, %none_2925, %cpu_2926, %false_2927 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_2928 = torch.constant.none
    %none_2929 = torch.constant.none
    %int4_2930 = torch.constant.int 4
    %cpu_2931 = torch.constant.device "cpu"
    %int0_2932 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2191, %none_2928, %none_2929, %int4_2930, %cpu_2931, %int0_2932 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2933 = torch.constant.int 6
    %2192 = torch.prims.convert_element_type %2191, %int6_2933 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_2934 = torch.constant.int 64
    %2193 = torch.aten.div.Scalar %2192, %int64_2934 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_2935 = torch.constant.float 1.000000e+04
    %2194 = torch.aten.pow.Scalar %float1.000000e04_2935, %2193 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %2195 = torch.aten.reciprocal %2194 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_2936 = torch.constant.float 1.000000e+00
    %2196 = torch.aten.mul.Scalar %2195, %float1.000000e00_2936 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_2937 = torch.constant.none
    %2197 = torch.aten.clone %87, %none_2937 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_2938 = torch.constant.int 0
    %2198 = torch.aten.unsqueeze %2196, %int0_2938 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_2939 = torch.constant.int 2
    %2199 = torch.aten.unsqueeze %2198, %int2_2939 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_2940 = torch.constant.none
    %none_2941 = torch.constant.none
    %int6_2942 = torch.constant.int 6
    %cpu_2943 = torch.constant.device "cpu"
    %int0_2944 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2199, %none_2940, %none_2941, %int6_2942, %cpu_2943, %int0_2944 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_2945 = torch.constant.int 4
    %int-1_2946 = torch.constant.int -1
    %int1_2947 = torch.constant.int 1
    %2200 = torch.prim.ListConstruct %int4_2945, %int-1_2946, %int1_2947 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2948 = torch.constant.bool false
    %2201 = torch.aten.expand %2199, %2200, %false_2948 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_2949 = torch.constant.int 1
    %2202 = torch.aten.unsqueeze %2190, %int1_2949 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_2950 = torch.constant.none
    %none_2951 = torch.constant.none
    %int4_2952 = torch.constant.int 4
    %cpu_2953 = torch.constant.device "cpu"
    %int0_2954 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2202, %none_2950, %none_2951, %int4_2952, %cpu_2953, %int0_2954 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_2955 = torch.constant.int 6
    %2203 = torch.prims.convert_element_type %2202, %int6_2955 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2204 = torch.aten.matmul %2201, %2203 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_2956 = torch.constant.int 1
    %int2_2957 = torch.constant.int 2
    %2205 = torch.aten.transpose.int %2204, %int1_2956, %int2_2957 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %2206 = torch.aten.cos %2205 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %2207 = torch.aten.mul.Tensor %2206, %2197 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_2958 = torch.constant.none
    %none_2959 = torch.constant.none
    %int6_2960 = torch.constant.int 6
    %cpu_2961 = torch.constant.device "cpu"
    %int0_2962 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2207, %none_2958, %none_2959, %int6_2960, %cpu_2961, %int0_2962 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2963 = torch.constant.int 5
    %2208 = torch.prims.convert_element_type %2207, %int5_2963 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %2209 = torch.aten.sin %2205 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %2210 = torch.aten.mul.Tensor %2209, %2197 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_2964 = torch.constant.none
    %none_2965 = torch.constant.none
    %int6_2966 = torch.constant.int 6
    %cpu_2967 = torch.constant.device "cpu"
    %int0_2968 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2210, %none_2964, %none_2965, %int6_2966, %cpu_2967, %int0_2968 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_2969 = torch.constant.int 5
    %2211 = torch.prims.convert_element_type %2210, %int5_2969 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_2970 = torch.constant.int 2
    %2212 = torch.aten.unsqueeze %2208, %int2_2970 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_2971 = torch.constant.int 2
    %2213 = torch.aten.unsqueeze %2211, %int2_2971 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_2972 = torch.constant.none
    %none_2973 = torch.constant.none
    %int5_2974 = torch.constant.int 5
    %cpu_2975 = torch.constant.device "cpu"
    %int0_2976 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2212, %none_2972, %none_2973, %int5_2974, %cpu_2975, %int0_2976 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_2977 = torch.constant.none
    %none_2978 = torch.constant.none
    %int5_2979 = torch.constant.int 5
    %cpu_2980 = torch.constant.device "cpu"
    %int0_2981 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2213, %none_2977, %none_2978, %int5_2979, %cpu_2980, %int0_2981 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_2982 = torch.constant.none
    %none_2983 = torch.constant.none
    %int5_2984 = torch.constant.int 5
    %cpu_2985 = torch.constant.device "cpu"
    %int0_2986 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2143, %none_2982, %none_2983, %int5_2984, %cpu_2985, %int0_2986 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_2987 = torch.constant.int 3
    %int0_2988 = torch.constant.int 0
    %int64_2989 = torch.constant.int 64
    %int2_2990 = torch.constant.int 2
    %2214 = torch.aten.slice.Tensor %2143, %int3_2987, %int0_2988, %int64_2989, %int2_2990 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_2991 = torch.constant.int 3
    %int1_2992 = torch.constant.int 1
    %int64_2993 = torch.constant.int 64
    %int2_2994 = torch.constant.int 2
    %2215 = torch.aten.slice.Tensor %2143, %int3_2991, %int1_2992, %int64_2993, %int2_2994 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %2216 = torch.aten.mul.Tensor %2214, %2212 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %2217 = torch.aten.mul.Tensor %2215, %2213 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_2995 = torch.constant.int 1
    %2218 = torch.aten.sub.Tensor %2216, %2217, %int1_2995 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %2219 = torch.aten.mul.Tensor %2215, %2212 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %2220 = torch.aten.mul.Tensor %2214, %2213 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_2996 = torch.constant.int 1
    %2221 = torch.aten.add.Tensor %2219, %2220, %int1_2996 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %2222 = torch_c.to_builtin_tensor %2218 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_2997 = tensor.cast %2222 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %2223 = torch_c.to_builtin_tensor %2221 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_2998 = tensor.cast %2223 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %2224 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_2997, %cast_2998) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_2999 = tensor.cast %2224 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %2225 = torch_c.from_builtin_tensor %cast_2999 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_3000 = torch.constant.int 4
    %int1_3001 = torch.constant.int 1
    %int4_3002 = torch.constant.int 4
    %int64_3003 = torch.constant.int 64
    %2226 = torch.prim.ListConstruct %int4_3000, %int1_3001, %int4_3002, %int64_3003 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2227 = torch.aten.view %2225, %2226 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_3004 = torch.constant.none
    %none_3005 = torch.constant.none
    %int5_3006 = torch.constant.int 5
    %cpu_3007 = torch.constant.device "cpu"
    %int0_3008 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2227, %none_3004, %none_3005, %int5_3006, %cpu_3007, %int0_3008 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_3009 = torch.constant.int 32
    %2228 = torch.aten.floor_divide.Scalar %arg2, %int32_3009 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_3010 = torch.constant.int 1
    %2229 = torch.aten.unsqueeze %2228, %int1_3010 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3011 = torch.constant.int 1
    %false_3012 = torch.constant.bool false
    %2230 = torch.aten.gather %arg3, %int1_3011, %2229, %false_3012 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_3013 = torch.constant.int 4
    %int1_3014 = torch.constant.int 1
    %int1_3015 = torch.constant.int 1
    %2231 = torch.prim.ListConstruct %int4_3013, %int1_3014, %int1_3015 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2232 = torch.aten.view %2230, %2231 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_3016 = torch.constant.int 32
    %2233 = torch.aten.remainder.Scalar %arg2, %int32_3016 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_3017 = torch.constant.int 4
    %int1_3018 = torch.constant.int 1
    %int1_3019 = torch.constant.int 1
    %2234 = torch.prim.ListConstruct %int4_3017, %int1_3018, %int1_3019 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2235 = torch.aten.view %2233, %2234 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_3020 = torch.constant.int 4
    %none_3021 = torch.constant.none
    %none_3022 = torch.constant.none
    %cpu_3023 = torch.constant.device "cpu"
    %false_3024 = torch.constant.bool false
    %2236 = torch.aten.arange %int4_3020, %none_3021, %none_3022, %cpu_3023, %false_3024 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_3025 = torch.constant.int 1
    %int1_3026 = torch.constant.int 1
    %int4_3027 = torch.constant.int 4
    %2237 = torch.prim.ListConstruct %int1_3025, %int1_3026, %int4_3027 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2238 = torch.aten.view %2236, %2237 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_3028 = torch.constant.none
    %2239 = torch.aten.clone %88, %none_3028 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_3029 = torch.constant.int 1
    %int1_3030 = torch.constant.int 1
    %int1_3031 = torch.constant.int 1
    %2240 = torch.prim.ListConstruct %int1_3029, %int1_3030, %int1_3031 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2241 = torch.aten.view %2239, %2240 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_3032 = torch.constant.int 22
    %2242 = torch.aten.mul.Scalar %2232, %int22_3032 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int5_3033 = torch.constant.int 5
    %int1_3034 = torch.constant.int 1
    %2243 = torch.aten.add.Scalar %2242, %int5_3033, %int1_3034 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_3035 = torch.constant.int 2
    %2244 = torch.aten.mul.Scalar %2243, %int2_3035 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_3036 = torch.constant.int 1
    %2245 = torch.aten.add.Tensor %2244, %2241, %int1_3036 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_3037 = torch.constant.int 4
    %2246 = torch.aten.mul.Scalar %2245, %int4_3037 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_3038 = torch.constant.int 1
    %2247 = torch.aten.add.Tensor %2246, %2238, %int1_3038 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_3039 = torch.constant.int 32
    %2248 = torch.aten.mul.Scalar %2247, %int32_3039 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_3040 = torch.constant.int 1
    %2249 = torch.aten.add.Tensor %2248, %2235, %int1_3040 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_3041 = torch.constant.none
    %none_3042 = torch.constant.none
    %int5_3043 = torch.constant.int 5
    %cpu_3044 = torch.constant.device "cpu"
    %int0_3045 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2227, %none_3041, %none_3042, %int5_3043, %cpu_3044, %int0_3045 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_3046 = torch.constant.int 22
    %int2_3047 = torch.constant.int 2
    %int4_3048 = torch.constant.int 4
    %int32_3049 = torch.constant.int 32
    %int64_3050 = torch.constant.int 64
    %2250 = torch.prim.ListConstruct %381, %int22_3046, %int2_3047, %int4_3048, %int32_3049, %int64_3050 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2251 = torch.aten.view %1945, %2250 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2251, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_3051 = torch.constant.int 64
    %2252 = torch.prim.ListConstruct %553, %int64_3051 : (!torch.int, !torch.int) -> !torch.list<int>
    %2253 = torch.aten.view %2251, %2252 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %2253, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %2254 = torch.prim.ListConstruct %2249 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_3052 = torch.constant.bool false
    %2255 = torch.aten.index_put %2253, %2254, %2227, %false_3052 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %2255, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_3053 = torch.constant.int 22
    %int2_3054 = torch.constant.int 2
    %int4_3055 = torch.constant.int 4
    %int32_3056 = torch.constant.int 32
    %int64_3057 = torch.constant.int 64
    %2256 = torch.prim.ListConstruct %381, %int22_3053, %int2_3054, %int4_3055, %int32_3056, %int64_3057 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2257 = torch.aten.view %2255, %2256 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2257, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_3058 = torch.constant.int 360448
    %2258 = torch.prim.ListConstruct %381, %int360448_3058 : (!torch.int, !torch.int) -> !torch.list<int>
    %2259 = torch.aten.view %2257, %2258 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %2259, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_3059 = torch.constant.int 22
    %int2_3060 = torch.constant.int 2
    %int4_3061 = torch.constant.int 4
    %int32_3062 = torch.constant.int 32
    %int64_3063 = torch.constant.int 64
    %2260 = torch.prim.ListConstruct %381, %int22_3059, %int2_3060, %int4_3061, %int32_3062, %int64_3063 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2261 = torch.aten.view %2259, %2260 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2261, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_3064 = torch.constant.int 64
    %2262 = torch.prim.ListConstruct %553, %int64_3064 : (!torch.int, !torch.int) -> !torch.list<int>
    %2263 = torch.aten.view %2261, %2262 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %2263, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_3065 = torch.constant.none
    %2264 = torch.aten.clone %89, %none_3065 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_3066 = torch.constant.int 1
    %int1_3067 = torch.constant.int 1
    %int1_3068 = torch.constant.int 1
    %2265 = torch.prim.ListConstruct %int1_3066, %int1_3067, %int1_3068 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2266 = torch.aten.view %2264, %2265 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_3069 = torch.constant.int 22
    %2267 = torch.aten.mul.Scalar %2232, %int22_3069 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int5_3070 = torch.constant.int 5
    %int1_3071 = torch.constant.int 1
    %2268 = torch.aten.add.Scalar %2267, %int5_3070, %int1_3071 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_3072 = torch.constant.int 2
    %2269 = torch.aten.mul.Scalar %2268, %int2_3072 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_3073 = torch.constant.int 1
    %2270 = torch.aten.add.Tensor %2269, %2266, %int1_3073 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_3074 = torch.constant.int 4
    %2271 = torch.aten.mul.Scalar %2270, %int4_3074 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_3075 = torch.constant.int 1
    %2272 = torch.aten.add.Tensor %2271, %2238, %int1_3075 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_3076 = torch.constant.int 32
    %2273 = torch.aten.mul.Scalar %2272, %int32_3076 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_3077 = torch.constant.int 1
    %2274 = torch.aten.add.Tensor %2273, %2235, %int1_3077 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_3078 = torch.constant.none
    %none_3079 = torch.constant.none
    %int5_3080 = torch.constant.int 5
    %cpu_3081 = torch.constant.device "cpu"
    %int0_3082 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2145, %none_3078, %none_3079, %int5_3080, %cpu_3081, %int0_3082 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %2275 = torch.prim.ListConstruct %2274 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_3083 = torch.constant.bool false
    %2276 = torch.aten.index_put %2263, %2275, %2145, %false_3083 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %2276, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_3084 = torch.constant.int 22
    %int2_3085 = torch.constant.int 2
    %int4_3086 = torch.constant.int 4
    %int32_3087 = torch.constant.int 32
    %int64_3088 = torch.constant.int 64
    %2277 = torch.prim.ListConstruct %381, %int22_3084, %int2_3085, %int4_3086, %int32_3087, %int64_3088 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2278 = torch.aten.view %2276, %2277 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2278, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_3089 = torch.constant.int 360448
    %2279 = torch.prim.ListConstruct %381, %int360448_3089 : (!torch.int, !torch.int) -> !torch.list<int>
    %2280 = torch.aten.view %2278, %2279 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %2280, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_3090 = torch.constant.none
    %2281 = torch.aten.clone %90, %none_3090 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_3091 = torch.constant.none
    %2282 = torch.aten.clone %91, %none_3091 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_3092 = torch.constant.none
    %2283 = torch.aten.clone %92, %none_3092 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_3093 = torch.constant.int 22
    %int2_3094 = torch.constant.int 2
    %int4_3095 = torch.constant.int 4
    %int32_3096 = torch.constant.int 32
    %int64_3097 = torch.constant.int 64
    %2284 = torch.prim.ListConstruct %381, %int22_3093, %int2_3094, %int4_3095, %int32_3096, %int64_3097 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2285 = torch.aten.view %2280, %2284 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2285, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %2286 = torch_c.to_builtin_tensor %2285 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %2287 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_3098 = tensor.cast %2287 : tensor<4x?xi64> to tensor<?x?xi64>
    %2288 = torch_c.to_builtin_tensor %2281 : !torch.vtensor<[],si64> -> tensor<i64>
    %2289 = torch_c.to_builtin_tensor %2282 : !torch.vtensor<[],si64> -> tensor<i64>
    %2290 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%2286, %cast_3098, %2288, %2289) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_3099 = tensor.cast %2290 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %2291 = torch_c.from_builtin_tensor %cast_3099 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %2291, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %2292 = torch_c.to_builtin_tensor %2285 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %2293 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_3100 = tensor.cast %2293 : tensor<4x?xi64> to tensor<?x?xi64>
    %2294 = torch_c.to_builtin_tensor %2281 : !torch.vtensor<[],si64> -> tensor<i64>
    %2295 = torch_c.to_builtin_tensor %2283 : !torch.vtensor<[],si64> -> tensor<i64>
    %2296 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%2292, %cast_3100, %2294, %2295) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_3101 = tensor.cast %2296 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %2297 = torch_c.from_builtin_tensor %cast_3101 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %2297, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_3102 = torch.constant.int 2
    %int3_3103 = torch.constant.int 3
    %2298 = torch.aten.transpose.int %2291, %int2_3102, %int3_3103 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %2298, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_3104 = torch.constant.int 0
    %2299 = torch.aten.clone %2298, %int0_3104 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %2299, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_3105 = torch.constant.int 4
    %int4_3106 = torch.constant.int 4
    %int64_3107 = torch.constant.int 64
    %2300 = torch.prim.ListConstruct %int4_3105, %623, %int4_3106, %int64_3107 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2301 = torch.aten._unsafe_view %2299, %2300 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %2301, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_3108 = torch.constant.int 2
    %int3_3109 = torch.constant.int 3
    %2302 = torch.aten.transpose.int %2297, %int2_3108, %int3_3109 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %2302, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_3110 = torch.constant.int 0
    %2303 = torch.aten.clone %2302, %int0_3110 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %2303, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_3111 = torch.constant.int 4
    %int4_3112 = torch.constant.int 4
    %int64_3113 = torch.constant.int 64
    %2304 = torch.prim.ListConstruct %int4_3111, %623, %int4_3112, %int64_3113 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2305 = torch.aten._unsafe_view %2303, %2304 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %2305, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_3114 = torch.constant.int 0
    %int1_3115 = torch.constant.int 1
    %none_3116 = torch.constant.none
    %none_3117 = torch.constant.none
    %cpu_3118 = torch.constant.device "cpu"
    %false_3119 = torch.constant.bool false
    %2306 = torch.aten.arange.start_step %int0_3114, %623, %int1_3115, %none_3116, %none_3117, %cpu_3118, %false_3119 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2306, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_3120 = torch.constant.int -1
    %2307 = torch.aten.unsqueeze %arg1, %int-1_3120 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %2308 = torch.aten.ge.Tensor %2306, %2307 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %2308, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_3121 = torch.constant.none
    %2309 = torch.aten.clone %93, %none_3121 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_3122 = torch.constant.int 0
    %2310 = torch.aten.where.ScalarOther %2308, %2309, %int0_3122 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %2310, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_3123 = torch.constant.none
    %none_3124 = torch.constant.none
    %int5_3125 = torch.constant.int 5
    %cpu_3126 = torch.constant.device "cpu"
    %int0_3127 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2310, %none_3123, %none_3124, %int5_3125, %cpu_3126, %int0_3127 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_3128 = torch.constant.int 1
    %2311 = torch.aten.unsqueeze %2310, %int1_3128 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %2311, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_3129 = torch.constant.int 1
    %2312 = torch.aten.unsqueeze %2311, %int1_3129 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %2312, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_3130 = torch.constant.int -2
    %2313 = torch.aten.unsqueeze %2301, %int-2_3130 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %2313, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_3131 = torch.constant.int 4
    %int4_3132 = torch.constant.int 4
    %int8_3133 = torch.constant.int 8
    %int64_3134 = torch.constant.int 64
    %2314 = torch.prim.ListConstruct %int4_3131, %623, %int4_3132, %int8_3133, %int64_3134 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3135 = torch.constant.bool false
    %2315 = torch.aten.expand %2313, %2314, %false_3135 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2315, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_3136 = torch.constant.int 0
    %2316 = torch.aten.clone %2315, %int0_3136 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2316, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_3137 = torch.constant.int 4
    %int32_3138 = torch.constant.int 32
    %int64_3139 = torch.constant.int 64
    %2317 = torch.prim.ListConstruct %int4_3137, %623, %int32_3138, %int64_3139 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2318 = torch.aten._unsafe_view %2316, %2317 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2318, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_3140 = torch.constant.int -2
    %2319 = torch.aten.unsqueeze %2305, %int-2_3140 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %2319, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_3141 = torch.constant.int 4
    %int4_3142 = torch.constant.int 4
    %int8_3143 = torch.constant.int 8
    %int64_3144 = torch.constant.int 64
    %2320 = torch.prim.ListConstruct %int4_3141, %623, %int4_3142, %int8_3143, %int64_3144 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3145 = torch.constant.bool false
    %2321 = torch.aten.expand %2319, %2320, %false_3145 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2321, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_3146 = torch.constant.int 0
    %2322 = torch.aten.clone %2321, %int0_3146 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2322, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_3147 = torch.constant.int 4
    %int32_3148 = torch.constant.int 32
    %int64_3149 = torch.constant.int 64
    %2323 = torch.prim.ListConstruct %int4_3147, %623, %int32_3148, %int64_3149 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2324 = torch.aten._unsafe_view %2322, %2323 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2324, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_3150 = torch.constant.int 1
    %int2_3151 = torch.constant.int 2
    %2325 = torch.aten.transpose.int %2186, %int1_3150, %int2_3151 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_3152 = torch.constant.int 1
    %int2_3153 = torch.constant.int 2
    %2326 = torch.aten.transpose.int %2318, %int1_3152, %int2_3153 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2326, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_3154 = torch.constant.int 1
    %int2_3155 = torch.constant.int 2
    %2327 = torch.aten.transpose.int %2324, %int1_3154, %int2_3155 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2327, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_3156 = torch.constant.float 0.000000e+00
    %false_3157 = torch.constant.bool false
    %none_3158 = torch.constant.none
    %false_3159 = torch.constant.bool false
    %2328 = torch.aten.scaled_dot_product_attention %2325, %2326, %2327, %2312, %float0.000000e00_3156, %false_3157, %none_3158, %false_3159 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_3160 = torch.constant.int 1
    %int2_3161 = torch.constant.int 2
    %2329 = torch.aten.transpose.int %2328, %int1_3160, %int2_3161 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_3162 = torch.constant.int 4
    %int1_3163 = torch.constant.int 1
    %int2048_3164 = torch.constant.int 2048
    %2330 = torch.prim.ListConstruct %int4_3162, %int1_3163, %int2048_3164 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2331 = torch.aten.view %2329, %2330 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_3165 = torch.constant.int 2
    %2332 = torch.aten.view.dtype %98, %int2_3165 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %2333 = torch.aten.detach %2332 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_3166 = torch.constant.int -1
    %int17_3167 = torch.constant.int 17
    %2334 = torch.prim.ListConstruct %int-1_3166, %int17_3167 : (!torch.int, !torch.int) -> !torch.list<int>
    %2335 = torch.aten.view %2333, %2334 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_3168 = torch.constant.int 2048
    %int-1_3169 = torch.constant.int -1
    %int17_3170 = torch.constant.int 17
    %2336 = torch.prim.ListConstruct %int2048_3168, %int-1_3169, %int17_3170 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2337 = torch.aten.view %2335, %2336 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_3171 = torch.constant.int 2
    %int0_3172 = torch.constant.int 0
    %int1_3173 = torch.constant.int 1
    %int1_3174 = torch.constant.int 1
    %2338 = torch.aten.slice.Tensor %2337, %int2_3171, %int0_3172, %int1_3173, %int1_3174 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_3175 = torch.constant.int 5
    %2339 = torch.aten.view.dtype %2338, %int5_3175 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %2340 = torch.aten.detach %2339 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_3176 = torch.constant.int 2
    %int1_3177 = torch.constant.int 1
    %int9223372036854775807_3178 = torch.constant.int 9223372036854775807
    %int1_3179 = torch.constant.int 1
    %2341 = torch.aten.slice.Tensor %2337, %int2_3176, %int1_3177, %int9223372036854775807_3178, %int1_3179 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_3180 = torch.constant.int 1
    %2342 = torch.aten.view.dtype %2341, %int1_3180 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %2343 = torch.aten.detach %2342 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %2344 = torch_c.to_builtin_tensor %2331 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_3181 = tensor.cast %2344 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %2345 = torch_c.to_builtin_tensor %2340 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %2346 = torch_c.to_builtin_tensor %2343 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %2347 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_3181, %2345, %2346) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_3182 = tensor.cast %2347 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %2348 = torch_c.from_builtin_tensor %cast_3182 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_3183 = torch.constant.none
    %none_3184 = torch.constant.none
    %int5_3185 = torch.constant.int 5
    %cpu_3186 = torch.constant.device "cpu"
    %int0_3187 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2348, %none_3183, %none_3184, %int5_3185, %cpu_3186, %int0_3187 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_3188 = torch.constant.int 1
    %2349 = torch.aten.add.Tensor %2078, %2348, %int1_3188 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_3189 = torch.constant.none
    %none_3190 = torch.constant.none
    %int5_3191 = torch.constant.int 5
    %cpu_3192 = torch.constant.device "cpu"
    %int0_3193 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2349, %none_3189, %none_3190, %int5_3191, %cpu_3192, %int0_3193 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3194 = torch.constant.int 6
    %2350 = torch.prims.convert_element_type %2349, %int6_3194 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_3195 = torch.constant.int 2
    %2351 = torch.aten.pow.Tensor_Scalar %2350, %int2_3195 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_3196 = torch.constant.int -1
    %2352 = torch.prim.ListConstruct %int-1_3196 : (!torch.int) -> !torch.list<int>
    %true_3197 = torch.constant.bool true
    %none_3198 = torch.constant.none
    %2353 = torch.aten.mean.dim %2351, %2352, %true_3197, %none_3198 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_3199 = torch.constant.float 9.9999997473787516E-6
    %int1_3200 = torch.constant.int 1
    %2354 = torch.aten.add.Scalar %2353, %float9.999990e-06_3199, %int1_3200 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2355 = torch.aten.rsqrt %2354 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %2356 = torch.aten.mul.Tensor %2350, %2355 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_3201 = torch.constant.none
    %none_3202 = torch.constant.none
    %int6_3203 = torch.constant.int 6
    %cpu_3204 = torch.constant.device "cpu"
    %int0_3205 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2356, %none_3201, %none_3202, %int6_3203, %cpu_3204, %int0_3205 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3206 = torch.constant.int 5
    %2357 = torch.prims.convert_element_type %2356, %int5_3206 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %2358 = torch.aten.mul.Tensor %99, %2357 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_3207 = torch.constant.none
    %none_3208 = torch.constant.none
    %int6_3209 = torch.constant.int 6
    %cpu_3210 = torch.constant.device "cpu"
    %int0_3211 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2358, %none_3207, %none_3208, %int6_3209, %cpu_3210, %int0_3211 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3212 = torch.constant.int 5
    %2359 = torch.prims.convert_element_type %2358, %int5_3212 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_3213 = torch.constant.int 2
    %2360 = torch.aten.view.dtype %100, %int2_3213 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %2361 = torch.aten.detach %2360 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_3214 = torch.constant.int -1
    %int17_3215 = torch.constant.int 17
    %2362 = torch.prim.ListConstruct %int-1_3214, %int17_3215 : (!torch.int, !torch.int) -> !torch.list<int>
    %2363 = torch.aten.view %2361, %2362 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_3216 = torch.constant.int 5632
    %int-1_3217 = torch.constant.int -1
    %int17_3218 = torch.constant.int 17
    %2364 = torch.prim.ListConstruct %int5632_3216, %int-1_3217, %int17_3218 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2365 = torch.aten.view %2363, %2364 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_3219 = torch.constant.int 2
    %int0_3220 = torch.constant.int 0
    %int1_3221 = torch.constant.int 1
    %int1_3222 = torch.constant.int 1
    %2366 = torch.aten.slice.Tensor %2365, %int2_3219, %int0_3220, %int1_3221, %int1_3222 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_3223 = torch.constant.int 5
    %2367 = torch.aten.view.dtype %2366, %int5_3223 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %2368 = torch.aten.detach %2367 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_3224 = torch.constant.int 2
    %int1_3225 = torch.constant.int 1
    %int9223372036854775807_3226 = torch.constant.int 9223372036854775807
    %int1_3227 = torch.constant.int 1
    %2369 = torch.aten.slice.Tensor %2365, %int2_3224, %int1_3225, %int9223372036854775807_3226, %int1_3227 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_3228 = torch.constant.int 1
    %2370 = torch.aten.view.dtype %2369, %int1_3228 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %2371 = torch.aten.detach %2370 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %2372 = torch_c.to_builtin_tensor %2359 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_3229 = tensor.cast %2372 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %2373 = torch_c.to_builtin_tensor %2368 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %2374 = torch_c.to_builtin_tensor %2371 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %2375 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_3229, %2373, %2374) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_3230 = tensor.cast %2375 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %2376 = torch_c.from_builtin_tensor %cast_3230 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %2377 = torch.aten.silu %2376 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_3231 = torch.constant.int 2
    %2378 = torch.aten.view.dtype %101, %int2_3231 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %2379 = torch.aten.detach %2378 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_3232 = torch.constant.int -1
    %int17_3233 = torch.constant.int 17
    %2380 = torch.prim.ListConstruct %int-1_3232, %int17_3233 : (!torch.int, !torch.int) -> !torch.list<int>
    %2381 = torch.aten.view %2379, %2380 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_3234 = torch.constant.int 5632
    %int-1_3235 = torch.constant.int -1
    %int17_3236 = torch.constant.int 17
    %2382 = torch.prim.ListConstruct %int5632_3234, %int-1_3235, %int17_3236 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2383 = torch.aten.view %2381, %2382 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_3237 = torch.constant.int 2
    %int0_3238 = torch.constant.int 0
    %int1_3239 = torch.constant.int 1
    %int1_3240 = torch.constant.int 1
    %2384 = torch.aten.slice.Tensor %2383, %int2_3237, %int0_3238, %int1_3239, %int1_3240 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_3241 = torch.constant.int 5
    %2385 = torch.aten.view.dtype %2384, %int5_3241 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %2386 = torch.aten.detach %2385 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_3242 = torch.constant.int 2
    %int1_3243 = torch.constant.int 1
    %int9223372036854775807_3244 = torch.constant.int 9223372036854775807
    %int1_3245 = torch.constant.int 1
    %2387 = torch.aten.slice.Tensor %2383, %int2_3242, %int1_3243, %int9223372036854775807_3244, %int1_3245 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_3246 = torch.constant.int 1
    %2388 = torch.aten.view.dtype %2387, %int1_3246 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %2389 = torch.aten.detach %2388 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %2390 = torch_c.to_builtin_tensor %2359 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_3247 = tensor.cast %2390 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %2391 = torch_c.to_builtin_tensor %2386 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %2392 = torch_c.to_builtin_tensor %2389 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %2393 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_3247, %2391, %2392) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_3248 = tensor.cast %2393 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %2394 = torch_c.from_builtin_tensor %cast_3248 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %2395 = torch.aten.mul.Tensor %2377, %2394 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_3249 = torch.constant.int 2
    %2396 = torch.aten.view.dtype %102, %int2_3249 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %2397 = torch.aten.detach %2396 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_3250 = torch.constant.int -1
    %int17_3251 = torch.constant.int 17
    %2398 = torch.prim.ListConstruct %int-1_3250, %int17_3251 : (!torch.int, !torch.int) -> !torch.list<int>
    %2399 = torch.aten.view %2397, %2398 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_3252 = torch.constant.int 2048
    %int-1_3253 = torch.constant.int -1
    %int17_3254 = torch.constant.int 17
    %2400 = torch.prim.ListConstruct %int2048_3252, %int-1_3253, %int17_3254 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2401 = torch.aten.view %2399, %2400 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_3255 = torch.constant.int 2
    %int0_3256 = torch.constant.int 0
    %int1_3257 = torch.constant.int 1
    %int1_3258 = torch.constant.int 1
    %2402 = torch.aten.slice.Tensor %2401, %int2_3255, %int0_3256, %int1_3257, %int1_3258 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_3259 = torch.constant.int 5
    %2403 = torch.aten.view.dtype %2402, %int5_3259 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %2404 = torch.aten.detach %2403 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_3260 = torch.constant.int 2
    %int1_3261 = torch.constant.int 1
    %int9223372036854775807_3262 = torch.constant.int 9223372036854775807
    %int1_3263 = torch.constant.int 1
    %2405 = torch.aten.slice.Tensor %2401, %int2_3260, %int1_3261, %int9223372036854775807_3262, %int1_3263 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_3264 = torch.constant.int 1
    %2406 = torch.aten.view.dtype %2405, %int1_3264 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %2407 = torch.aten.detach %2406 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %2408 = torch_c.to_builtin_tensor %2395 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_3265 = tensor.cast %2408 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %2409 = torch_c.to_builtin_tensor %2404 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %2410 = torch_c.to_builtin_tensor %2407 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %2411 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_3265, %2409, %2410) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_3266 = tensor.cast %2411 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %2412 = torch_c.from_builtin_tensor %cast_3266 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_3267 = torch.constant.int 1
    %2413 = torch.aten.add.Tensor %2349, %2412, %int1_3267 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_3268 = torch.constant.none
    %none_3269 = torch.constant.none
    %int5_3270 = torch.constant.int 5
    %cpu_3271 = torch.constant.device "cpu"
    %int0_3272 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2413, %none_3268, %none_3269, %int5_3270, %cpu_3271, %int0_3272 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3273 = torch.constant.int 6
    %2414 = torch.prims.convert_element_type %2413, %int6_3273 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_3274 = torch.constant.int 2
    %2415 = torch.aten.pow.Tensor_Scalar %2414, %int2_3274 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_3275 = torch.constant.int -1
    %2416 = torch.prim.ListConstruct %int-1_3275 : (!torch.int) -> !torch.list<int>
    %true_3276 = torch.constant.bool true
    %none_3277 = torch.constant.none
    %2417 = torch.aten.mean.dim %2415, %2416, %true_3276, %none_3277 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_3278 = torch.constant.float 9.9999997473787516E-6
    %int1_3279 = torch.constant.int 1
    %2418 = torch.aten.add.Scalar %2417, %float9.999990e-06_3278, %int1_3279 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2419 = torch.aten.rsqrt %2418 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %2420 = torch.aten.mul.Tensor %2414, %2419 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_3280 = torch.constant.none
    %none_3281 = torch.constant.none
    %int6_3282 = torch.constant.int 6
    %cpu_3283 = torch.constant.device "cpu"
    %int0_3284 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2420, %none_3280, %none_3281, %int6_3282, %cpu_3283, %int0_3284 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3285 = torch.constant.int 5
    %2421 = torch.prims.convert_element_type %2420, %int5_3285 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %2422 = torch.aten.mul.Tensor %111, %2421 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_3286 = torch.constant.none
    %none_3287 = torch.constant.none
    %int6_3288 = torch.constant.int 6
    %cpu_3289 = torch.constant.device "cpu"
    %int0_3290 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2422, %none_3286, %none_3287, %int6_3288, %cpu_3289, %int0_3290 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3291 = torch.constant.int 5
    %2423 = torch.prims.convert_element_type %2422, %int5_3291 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_3292 = torch.constant.int 2
    %2424 = torch.aten.view.dtype %112, %int2_3292 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %2425 = torch.aten.detach %2424 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_3293 = torch.constant.int -1
    %int17_3294 = torch.constant.int 17
    %2426 = torch.prim.ListConstruct %int-1_3293, %int17_3294 : (!torch.int, !torch.int) -> !torch.list<int>
    %2427 = torch.aten.view %2425, %2426 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_3295 = torch.constant.int 2048
    %int-1_3296 = torch.constant.int -1
    %int17_3297 = torch.constant.int 17
    %2428 = torch.prim.ListConstruct %int2048_3295, %int-1_3296, %int17_3297 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2429 = torch.aten.view %2427, %2428 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_3298 = torch.constant.int 2
    %int0_3299 = torch.constant.int 0
    %int1_3300 = torch.constant.int 1
    %int1_3301 = torch.constant.int 1
    %2430 = torch.aten.slice.Tensor %2429, %int2_3298, %int0_3299, %int1_3300, %int1_3301 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_3302 = torch.constant.int 5
    %2431 = torch.aten.view.dtype %2430, %int5_3302 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %2432 = torch.aten.detach %2431 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_3303 = torch.constant.int 2
    %int1_3304 = torch.constant.int 1
    %int9223372036854775807_3305 = torch.constant.int 9223372036854775807
    %int1_3306 = torch.constant.int 1
    %2433 = torch.aten.slice.Tensor %2429, %int2_3303, %int1_3304, %int9223372036854775807_3305, %int1_3306 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_3307 = torch.constant.int 1
    %2434 = torch.aten.view.dtype %2433, %int1_3307 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %2435 = torch.aten.detach %2434 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %2436 = torch_c.to_builtin_tensor %2423 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_3308 = tensor.cast %2436 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %2437 = torch_c.to_builtin_tensor %2432 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %2438 = torch_c.to_builtin_tensor %2435 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %2439 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_3308, %2437, %2438) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_3309 = tensor.cast %2439 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %2440 = torch_c.from_builtin_tensor %cast_3309 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_3310 = torch.constant.int 2
    %2441 = torch.aten.view.dtype %113, %int2_3310 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %2442 = torch.aten.detach %2441 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_3311 = torch.constant.int -1
    %int17_3312 = torch.constant.int 17
    %2443 = torch.prim.ListConstruct %int-1_3311, %int17_3312 : (!torch.int, !torch.int) -> !torch.list<int>
    %2444 = torch.aten.view %2442, %2443 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_3313 = torch.constant.int 256
    %int-1_3314 = torch.constant.int -1
    %int17_3315 = torch.constant.int 17
    %2445 = torch.prim.ListConstruct %int256_3313, %int-1_3314, %int17_3315 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2446 = torch.aten.view %2444, %2445 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_3316 = torch.constant.int 2
    %int0_3317 = torch.constant.int 0
    %int1_3318 = torch.constant.int 1
    %int1_3319 = torch.constant.int 1
    %2447 = torch.aten.slice.Tensor %2446, %int2_3316, %int0_3317, %int1_3318, %int1_3319 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_3320 = torch.constant.int 5
    %2448 = torch.aten.view.dtype %2447, %int5_3320 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %2449 = torch.aten.detach %2448 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_3321 = torch.constant.int 2
    %int1_3322 = torch.constant.int 1
    %int9223372036854775807_3323 = torch.constant.int 9223372036854775807
    %int1_3324 = torch.constant.int 1
    %2450 = torch.aten.slice.Tensor %2446, %int2_3321, %int1_3322, %int9223372036854775807_3323, %int1_3324 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_3325 = torch.constant.int 1
    %2451 = torch.aten.view.dtype %2450, %int1_3325 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %2452 = torch.aten.detach %2451 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %2453 = torch_c.to_builtin_tensor %2423 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_3326 = tensor.cast %2453 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %2454 = torch_c.to_builtin_tensor %2449 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %2455 = torch_c.to_builtin_tensor %2452 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %2456 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_3326, %2454, %2455) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_3327 = tensor.cast %2456 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %2457 = torch_c.from_builtin_tensor %cast_3327 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_3328 = torch.constant.int 2
    %2458 = torch.aten.view.dtype %114, %int2_3328 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %2459 = torch.aten.detach %2458 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_3329 = torch.constant.int -1
    %int17_3330 = torch.constant.int 17
    %2460 = torch.prim.ListConstruct %int-1_3329, %int17_3330 : (!torch.int, !torch.int) -> !torch.list<int>
    %2461 = torch.aten.view %2459, %2460 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_3331 = torch.constant.int 256
    %int-1_3332 = torch.constant.int -1
    %int17_3333 = torch.constant.int 17
    %2462 = torch.prim.ListConstruct %int256_3331, %int-1_3332, %int17_3333 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2463 = torch.aten.view %2461, %2462 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_3334 = torch.constant.int 2
    %int0_3335 = torch.constant.int 0
    %int1_3336 = torch.constant.int 1
    %int1_3337 = torch.constant.int 1
    %2464 = torch.aten.slice.Tensor %2463, %int2_3334, %int0_3335, %int1_3336, %int1_3337 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_3338 = torch.constant.int 5
    %2465 = torch.aten.view.dtype %2464, %int5_3338 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %2466 = torch.aten.detach %2465 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_3339 = torch.constant.int 2
    %int1_3340 = torch.constant.int 1
    %int9223372036854775807_3341 = torch.constant.int 9223372036854775807
    %int1_3342 = torch.constant.int 1
    %2467 = torch.aten.slice.Tensor %2463, %int2_3339, %int1_3340, %int9223372036854775807_3341, %int1_3342 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_3343 = torch.constant.int 1
    %2468 = torch.aten.view.dtype %2467, %int1_3343 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %2469 = torch.aten.detach %2468 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %2470 = torch_c.to_builtin_tensor %2423 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_3344 = tensor.cast %2470 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %2471 = torch_c.to_builtin_tensor %2466 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %2472 = torch_c.to_builtin_tensor %2469 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %2473 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_3344, %2471, %2472) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_3345 = tensor.cast %2473 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %2474 = torch_c.from_builtin_tensor %cast_3345 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_3346 = torch.constant.int 4
    %int1_3347 = torch.constant.int 1
    %int32_3348 = torch.constant.int 32
    %int64_3349 = torch.constant.int 64
    %2475 = torch.prim.ListConstruct %int4_3346, %int1_3347, %int32_3348, %int64_3349 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2476 = torch.aten.view %2440, %2475 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_3350 = torch.constant.int 4
    %int1_3351 = torch.constant.int 1
    %int4_3352 = torch.constant.int 4
    %int64_3353 = torch.constant.int 64
    %2477 = torch.prim.ListConstruct %int4_3350, %int1_3351, %int4_3352, %int64_3353 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2478 = torch.aten.view %2457, %2477 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_3354 = torch.constant.int 4
    %int1_3355 = torch.constant.int 1
    %int4_3356 = torch.constant.int 4
    %int64_3357 = torch.constant.int 64
    %2479 = torch.prim.ListConstruct %int4_3354, %int1_3355, %int4_3356, %int64_3357 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2480 = torch.aten.view %2474, %2479 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_3358 = torch.constant.int 0
    %int1_3359 = torch.constant.int 1
    %none_3360 = torch.constant.none
    %none_3361 = torch.constant.none
    %cpu_3362 = torch.constant.device "cpu"
    %false_3363 = torch.constant.bool false
    %2481 = torch.aten.arange.start %int0_3358, %int1_3359, %none_3360, %none_3361, %cpu_3362, %false_3363 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_3364 = torch.constant.int 0
    %2482 = torch.aten.unsqueeze %2481, %int0_3364 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_3365 = torch.constant.int 1
    %2483 = torch.aten.unsqueeze %arg2, %int1_3365 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3366 = torch.constant.int 1
    %2484 = torch.aten.add.Tensor %2482, %2483, %int1_3366 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_3367 = torch.constant.int 0
    %int64_3368 = torch.constant.int 64
    %int2_3369 = torch.constant.int 2
    %none_3370 = torch.constant.none
    %none_3371 = torch.constant.none
    %cpu_3372 = torch.constant.device "cpu"
    %false_3373 = torch.constant.bool false
    %2485 = torch.aten.arange.start_step %int0_3367, %int64_3368, %int2_3369, %none_3370, %none_3371, %cpu_3372, %false_3373 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_3374 = torch.constant.none
    %none_3375 = torch.constant.none
    %int4_3376 = torch.constant.int 4
    %cpu_3377 = torch.constant.device "cpu"
    %int0_3378 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2485, %none_3374, %none_3375, %int4_3376, %cpu_3377, %int0_3378 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3379 = torch.constant.int 6
    %2486 = torch.prims.convert_element_type %2485, %int6_3379 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_3380 = torch.constant.int 64
    %2487 = torch.aten.div.Scalar %2486, %int64_3380 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_3381 = torch.constant.float 1.000000e+04
    %2488 = torch.aten.pow.Scalar %float1.000000e04_3381, %2487 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %2489 = torch.aten.reciprocal %2488 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_3382 = torch.constant.float 1.000000e+00
    %2490 = torch.aten.mul.Scalar %2489, %float1.000000e00_3382 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_3383 = torch.constant.none
    %2491 = torch.aten.clone %103, %none_3383 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_3384 = torch.constant.int 0
    %2492 = torch.aten.unsqueeze %2490, %int0_3384 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_3385 = torch.constant.int 2
    %2493 = torch.aten.unsqueeze %2492, %int2_3385 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_3386 = torch.constant.none
    %none_3387 = torch.constant.none
    %int6_3388 = torch.constant.int 6
    %cpu_3389 = torch.constant.device "cpu"
    %int0_3390 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2493, %none_3386, %none_3387, %int6_3388, %cpu_3389, %int0_3390 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_3391 = torch.constant.int 4
    %int-1_3392 = torch.constant.int -1
    %int1_3393 = torch.constant.int 1
    %2494 = torch.prim.ListConstruct %int4_3391, %int-1_3392, %int1_3393 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3394 = torch.constant.bool false
    %2495 = torch.aten.expand %2493, %2494, %false_3394 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_3395 = torch.constant.int 1
    %2496 = torch.aten.unsqueeze %2484, %int1_3395 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_3396 = torch.constant.none
    %none_3397 = torch.constant.none
    %int4_3398 = torch.constant.int 4
    %cpu_3399 = torch.constant.device "cpu"
    %int0_3400 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2496, %none_3396, %none_3397, %int4_3398, %cpu_3399, %int0_3400 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3401 = torch.constant.int 6
    %2497 = torch.prims.convert_element_type %2496, %int6_3401 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2498 = torch.aten.matmul %2495, %2497 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_3402 = torch.constant.int 1
    %int2_3403 = torch.constant.int 2
    %2499 = torch.aten.transpose.int %2498, %int1_3402, %int2_3403 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %2500 = torch.aten.cos %2499 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %2501 = torch.aten.mul.Tensor %2500, %2491 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_3404 = torch.constant.none
    %none_3405 = torch.constant.none
    %int6_3406 = torch.constant.int 6
    %cpu_3407 = torch.constant.device "cpu"
    %int0_3408 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2501, %none_3404, %none_3405, %int6_3406, %cpu_3407, %int0_3408 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3409 = torch.constant.int 5
    %2502 = torch.prims.convert_element_type %2501, %int5_3409 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %2503 = torch.aten.sin %2499 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %2504 = torch.aten.mul.Tensor %2503, %2491 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_3410 = torch.constant.none
    %none_3411 = torch.constant.none
    %int6_3412 = torch.constant.int 6
    %cpu_3413 = torch.constant.device "cpu"
    %int0_3414 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2504, %none_3410, %none_3411, %int6_3412, %cpu_3413, %int0_3414 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3415 = torch.constant.int 5
    %2505 = torch.prims.convert_element_type %2504, %int5_3415 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_3416 = torch.constant.int 2
    %2506 = torch.aten.unsqueeze %2502, %int2_3416 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_3417 = torch.constant.int 2
    %2507 = torch.aten.unsqueeze %2505, %int2_3417 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_3418 = torch.constant.none
    %none_3419 = torch.constant.none
    %int5_3420 = torch.constant.int 5
    %cpu_3421 = torch.constant.device "cpu"
    %int0_3422 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2506, %none_3418, %none_3419, %int5_3420, %cpu_3421, %int0_3422 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_3423 = torch.constant.none
    %none_3424 = torch.constant.none
    %int5_3425 = torch.constant.int 5
    %cpu_3426 = torch.constant.device "cpu"
    %int0_3427 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2507, %none_3423, %none_3424, %int5_3425, %cpu_3426, %int0_3427 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_3428 = torch.constant.none
    %none_3429 = torch.constant.none
    %int5_3430 = torch.constant.int 5
    %cpu_3431 = torch.constant.device "cpu"
    %int0_3432 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2476, %none_3428, %none_3429, %int5_3430, %cpu_3431, %int0_3432 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_3433 = torch.constant.int 3
    %int0_3434 = torch.constant.int 0
    %int64_3435 = torch.constant.int 64
    %int2_3436 = torch.constant.int 2
    %2508 = torch.aten.slice.Tensor %2476, %int3_3433, %int0_3434, %int64_3435, %int2_3436 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_3437 = torch.constant.int 3
    %int1_3438 = torch.constant.int 1
    %int64_3439 = torch.constant.int 64
    %int2_3440 = torch.constant.int 2
    %2509 = torch.aten.slice.Tensor %2476, %int3_3437, %int1_3438, %int64_3439, %int2_3440 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %2510 = torch.aten.mul.Tensor %2508, %2506 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %2511 = torch.aten.mul.Tensor %2509, %2507 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_3441 = torch.constant.int 1
    %2512 = torch.aten.sub.Tensor %2510, %2511, %int1_3441 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %2513 = torch.aten.mul.Tensor %2509, %2506 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %2514 = torch.aten.mul.Tensor %2508, %2507 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_3442 = torch.constant.int 1
    %2515 = torch.aten.add.Tensor %2513, %2514, %int1_3442 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %2516 = torch_c.to_builtin_tensor %2512 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_3443 = tensor.cast %2516 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %2517 = torch_c.to_builtin_tensor %2515 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_3444 = tensor.cast %2517 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %2518 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_3443, %cast_3444) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_3445 = tensor.cast %2518 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %2519 = torch_c.from_builtin_tensor %cast_3445 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_3446 = torch.constant.int 4
    %int1_3447 = torch.constant.int 1
    %int32_3448 = torch.constant.int 32
    %int64_3449 = torch.constant.int 64
    %2520 = torch.prim.ListConstruct %int4_3446, %int1_3447, %int32_3448, %int64_3449 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2521 = torch.aten.view %2519, %2520 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_3450 = torch.constant.none
    %none_3451 = torch.constant.none
    %int5_3452 = torch.constant.int 5
    %cpu_3453 = torch.constant.device "cpu"
    %int0_3454 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2521, %none_3450, %none_3451, %int5_3452, %cpu_3453, %int0_3454 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_3455 = torch.constant.int 0
    %int1_3456 = torch.constant.int 1
    %none_3457 = torch.constant.none
    %none_3458 = torch.constant.none
    %cpu_3459 = torch.constant.device "cpu"
    %false_3460 = torch.constant.bool false
    %2522 = torch.aten.arange.start %int0_3455, %int1_3456, %none_3457, %none_3458, %cpu_3459, %false_3460 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_3461 = torch.constant.int 0
    %2523 = torch.aten.unsqueeze %2522, %int0_3461 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_3462 = torch.constant.int 1
    %2524 = torch.aten.unsqueeze %arg2, %int1_3462 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3463 = torch.constant.int 1
    %2525 = torch.aten.add.Tensor %2523, %2524, %int1_3463 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_3464 = torch.constant.int 0
    %int64_3465 = torch.constant.int 64
    %int2_3466 = torch.constant.int 2
    %none_3467 = torch.constant.none
    %none_3468 = torch.constant.none
    %cpu_3469 = torch.constant.device "cpu"
    %false_3470 = torch.constant.bool false
    %2526 = torch.aten.arange.start_step %int0_3464, %int64_3465, %int2_3466, %none_3467, %none_3468, %cpu_3469, %false_3470 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_3471 = torch.constant.none
    %none_3472 = torch.constant.none
    %int4_3473 = torch.constant.int 4
    %cpu_3474 = torch.constant.device "cpu"
    %int0_3475 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2526, %none_3471, %none_3472, %int4_3473, %cpu_3474, %int0_3475 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3476 = torch.constant.int 6
    %2527 = torch.prims.convert_element_type %2526, %int6_3476 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_3477 = torch.constant.int 64
    %2528 = torch.aten.div.Scalar %2527, %int64_3477 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_3478 = torch.constant.float 1.000000e+04
    %2529 = torch.aten.pow.Scalar %float1.000000e04_3478, %2528 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %2530 = torch.aten.reciprocal %2529 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_3479 = torch.constant.float 1.000000e+00
    %2531 = torch.aten.mul.Scalar %2530, %float1.000000e00_3479 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_3480 = torch.constant.none
    %2532 = torch.aten.clone %104, %none_3480 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_3481 = torch.constant.int 0
    %2533 = torch.aten.unsqueeze %2531, %int0_3481 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_3482 = torch.constant.int 2
    %2534 = torch.aten.unsqueeze %2533, %int2_3482 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_3483 = torch.constant.none
    %none_3484 = torch.constant.none
    %int6_3485 = torch.constant.int 6
    %cpu_3486 = torch.constant.device "cpu"
    %int0_3487 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2534, %none_3483, %none_3484, %int6_3485, %cpu_3486, %int0_3487 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_3488 = torch.constant.int 4
    %int-1_3489 = torch.constant.int -1
    %int1_3490 = torch.constant.int 1
    %2535 = torch.prim.ListConstruct %int4_3488, %int-1_3489, %int1_3490 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3491 = torch.constant.bool false
    %2536 = torch.aten.expand %2534, %2535, %false_3491 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_3492 = torch.constant.int 1
    %2537 = torch.aten.unsqueeze %2525, %int1_3492 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_3493 = torch.constant.none
    %none_3494 = torch.constant.none
    %int4_3495 = torch.constant.int 4
    %cpu_3496 = torch.constant.device "cpu"
    %int0_3497 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2537, %none_3493, %none_3494, %int4_3495, %cpu_3496, %int0_3497 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3498 = torch.constant.int 6
    %2538 = torch.prims.convert_element_type %2537, %int6_3498 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2539 = torch.aten.matmul %2536, %2538 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_3499 = torch.constant.int 1
    %int2_3500 = torch.constant.int 2
    %2540 = torch.aten.transpose.int %2539, %int1_3499, %int2_3500 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %2541 = torch.aten.cos %2540 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %2542 = torch.aten.mul.Tensor %2541, %2532 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_3501 = torch.constant.none
    %none_3502 = torch.constant.none
    %int6_3503 = torch.constant.int 6
    %cpu_3504 = torch.constant.device "cpu"
    %int0_3505 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2542, %none_3501, %none_3502, %int6_3503, %cpu_3504, %int0_3505 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3506 = torch.constant.int 5
    %2543 = torch.prims.convert_element_type %2542, %int5_3506 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %2544 = torch.aten.sin %2540 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %2545 = torch.aten.mul.Tensor %2544, %2532 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_3507 = torch.constant.none
    %none_3508 = torch.constant.none
    %int6_3509 = torch.constant.int 6
    %cpu_3510 = torch.constant.device "cpu"
    %int0_3511 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2545, %none_3507, %none_3508, %int6_3509, %cpu_3510, %int0_3511 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3512 = torch.constant.int 5
    %2546 = torch.prims.convert_element_type %2545, %int5_3512 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_3513 = torch.constant.int 2
    %2547 = torch.aten.unsqueeze %2543, %int2_3513 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_3514 = torch.constant.int 2
    %2548 = torch.aten.unsqueeze %2546, %int2_3514 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_3515 = torch.constant.none
    %none_3516 = torch.constant.none
    %int5_3517 = torch.constant.int 5
    %cpu_3518 = torch.constant.device "cpu"
    %int0_3519 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2547, %none_3515, %none_3516, %int5_3517, %cpu_3518, %int0_3519 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_3520 = torch.constant.none
    %none_3521 = torch.constant.none
    %int5_3522 = torch.constant.int 5
    %cpu_3523 = torch.constant.device "cpu"
    %int0_3524 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2548, %none_3520, %none_3521, %int5_3522, %cpu_3523, %int0_3524 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_3525 = torch.constant.none
    %none_3526 = torch.constant.none
    %int5_3527 = torch.constant.int 5
    %cpu_3528 = torch.constant.device "cpu"
    %int0_3529 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2478, %none_3525, %none_3526, %int5_3527, %cpu_3528, %int0_3529 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_3530 = torch.constant.int 3
    %int0_3531 = torch.constant.int 0
    %int64_3532 = torch.constant.int 64
    %int2_3533 = torch.constant.int 2
    %2549 = torch.aten.slice.Tensor %2478, %int3_3530, %int0_3531, %int64_3532, %int2_3533 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_3534 = torch.constant.int 3
    %int1_3535 = torch.constant.int 1
    %int64_3536 = torch.constant.int 64
    %int2_3537 = torch.constant.int 2
    %2550 = torch.aten.slice.Tensor %2478, %int3_3534, %int1_3535, %int64_3536, %int2_3537 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %2551 = torch.aten.mul.Tensor %2549, %2547 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %2552 = torch.aten.mul.Tensor %2550, %2548 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_3538 = torch.constant.int 1
    %2553 = torch.aten.sub.Tensor %2551, %2552, %int1_3538 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %2554 = torch.aten.mul.Tensor %2550, %2547 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %2555 = torch.aten.mul.Tensor %2549, %2548 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_3539 = torch.constant.int 1
    %2556 = torch.aten.add.Tensor %2554, %2555, %int1_3539 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %2557 = torch_c.to_builtin_tensor %2553 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_3540 = tensor.cast %2557 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %2558 = torch_c.to_builtin_tensor %2556 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_3541 = tensor.cast %2558 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %2559 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_3540, %cast_3541) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_3542 = tensor.cast %2559 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %2560 = torch_c.from_builtin_tensor %cast_3542 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_3543 = torch.constant.int 4
    %int1_3544 = torch.constant.int 1
    %int4_3545 = torch.constant.int 4
    %int64_3546 = torch.constant.int 64
    %2561 = torch.prim.ListConstruct %int4_3543, %int1_3544, %int4_3545, %int64_3546 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2562 = torch.aten.view %2560, %2561 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_3547 = torch.constant.none
    %none_3548 = torch.constant.none
    %int5_3549 = torch.constant.int 5
    %cpu_3550 = torch.constant.device "cpu"
    %int0_3551 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2562, %none_3547, %none_3548, %int5_3549, %cpu_3550, %int0_3551 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_3552 = torch.constant.int 32
    %2563 = torch.aten.floor_divide.Scalar %arg2, %int32_3552 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_3553 = torch.constant.int 1
    %2564 = torch.aten.unsqueeze %2563, %int1_3553 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3554 = torch.constant.int 1
    %false_3555 = torch.constant.bool false
    %2565 = torch.aten.gather %arg3, %int1_3554, %2564, %false_3555 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_3556 = torch.constant.int 4
    %int1_3557 = torch.constant.int 1
    %int1_3558 = torch.constant.int 1
    %2566 = torch.prim.ListConstruct %int4_3556, %int1_3557, %int1_3558 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2567 = torch.aten.view %2565, %2566 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_3559 = torch.constant.int 32
    %2568 = torch.aten.remainder.Scalar %arg2, %int32_3559 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_3560 = torch.constant.int 4
    %int1_3561 = torch.constant.int 1
    %int1_3562 = torch.constant.int 1
    %2569 = torch.prim.ListConstruct %int4_3560, %int1_3561, %int1_3562 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2570 = torch.aten.view %2568, %2569 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_3563 = torch.constant.int 4
    %none_3564 = torch.constant.none
    %none_3565 = torch.constant.none
    %cpu_3566 = torch.constant.device "cpu"
    %false_3567 = torch.constant.bool false
    %2571 = torch.aten.arange %int4_3563, %none_3564, %none_3565, %cpu_3566, %false_3567 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_3568 = torch.constant.int 1
    %int1_3569 = torch.constant.int 1
    %int4_3570 = torch.constant.int 4
    %2572 = torch.prim.ListConstruct %int1_3568, %int1_3569, %int4_3570 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2573 = torch.aten.view %2571, %2572 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_3571 = torch.constant.none
    %2574 = torch.aten.clone %105, %none_3571 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_3572 = torch.constant.int 1
    %int1_3573 = torch.constant.int 1
    %int1_3574 = torch.constant.int 1
    %2575 = torch.prim.ListConstruct %int1_3572, %int1_3573, %int1_3574 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2576 = torch.aten.view %2574, %2575 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_3575 = torch.constant.int 22
    %2577 = torch.aten.mul.Scalar %2567, %int22_3575 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int6_3576 = torch.constant.int 6
    %int1_3577 = torch.constant.int 1
    %2578 = torch.aten.add.Scalar %2577, %int6_3576, %int1_3577 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_3578 = torch.constant.int 2
    %2579 = torch.aten.mul.Scalar %2578, %int2_3578 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_3579 = torch.constant.int 1
    %2580 = torch.aten.add.Tensor %2579, %2576, %int1_3579 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_3580 = torch.constant.int 4
    %2581 = torch.aten.mul.Scalar %2580, %int4_3580 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_3581 = torch.constant.int 1
    %2582 = torch.aten.add.Tensor %2581, %2573, %int1_3581 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_3582 = torch.constant.int 32
    %2583 = torch.aten.mul.Scalar %2582, %int32_3582 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_3583 = torch.constant.int 1
    %2584 = torch.aten.add.Tensor %2583, %2570, %int1_3583 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_3584 = torch.constant.none
    %none_3585 = torch.constant.none
    %int5_3586 = torch.constant.int 5
    %cpu_3587 = torch.constant.device "cpu"
    %int0_3588 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2562, %none_3584, %none_3585, %int5_3586, %cpu_3587, %int0_3588 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_3589 = torch.constant.int 22
    %int2_3590 = torch.constant.int 2
    %int4_3591 = torch.constant.int 4
    %int32_3592 = torch.constant.int 32
    %int64_3593 = torch.constant.int 64
    %2585 = torch.prim.ListConstruct %381, %int22_3589, %int2_3590, %int4_3591, %int32_3592, %int64_3593 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2586 = torch.aten.view %2280, %2585 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2586, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_3594 = torch.constant.int 64
    %2587 = torch.prim.ListConstruct %553, %int64_3594 : (!torch.int, !torch.int) -> !torch.list<int>
    %2588 = torch.aten.view %2586, %2587 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %2588, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %2589 = torch.prim.ListConstruct %2584 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_3595 = torch.constant.bool false
    %2590 = torch.aten.index_put %2588, %2589, %2562, %false_3595 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %2590, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_3596 = torch.constant.int 22
    %int2_3597 = torch.constant.int 2
    %int4_3598 = torch.constant.int 4
    %int32_3599 = torch.constant.int 32
    %int64_3600 = torch.constant.int 64
    %2591 = torch.prim.ListConstruct %381, %int22_3596, %int2_3597, %int4_3598, %int32_3599, %int64_3600 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2592 = torch.aten.view %2590, %2591 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2592, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_3601 = torch.constant.int 360448
    %2593 = torch.prim.ListConstruct %381, %int360448_3601 : (!torch.int, !torch.int) -> !torch.list<int>
    %2594 = torch.aten.view %2592, %2593 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %2594, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_3602 = torch.constant.int 22
    %int2_3603 = torch.constant.int 2
    %int4_3604 = torch.constant.int 4
    %int32_3605 = torch.constant.int 32
    %int64_3606 = torch.constant.int 64
    %2595 = torch.prim.ListConstruct %381, %int22_3602, %int2_3603, %int4_3604, %int32_3605, %int64_3606 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2596 = torch.aten.view %2594, %2595 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2596, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_3607 = torch.constant.int 64
    %2597 = torch.prim.ListConstruct %553, %int64_3607 : (!torch.int, !torch.int) -> !torch.list<int>
    %2598 = torch.aten.view %2596, %2597 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %2598, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_3608 = torch.constant.none
    %2599 = torch.aten.clone %106, %none_3608 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_3609 = torch.constant.int 1
    %int1_3610 = torch.constant.int 1
    %int1_3611 = torch.constant.int 1
    %2600 = torch.prim.ListConstruct %int1_3609, %int1_3610, %int1_3611 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2601 = torch.aten.view %2599, %2600 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_3612 = torch.constant.int 22
    %2602 = torch.aten.mul.Scalar %2567, %int22_3612 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int6_3613 = torch.constant.int 6
    %int1_3614 = torch.constant.int 1
    %2603 = torch.aten.add.Scalar %2602, %int6_3613, %int1_3614 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_3615 = torch.constant.int 2
    %2604 = torch.aten.mul.Scalar %2603, %int2_3615 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_3616 = torch.constant.int 1
    %2605 = torch.aten.add.Tensor %2604, %2601, %int1_3616 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_3617 = torch.constant.int 4
    %2606 = torch.aten.mul.Scalar %2605, %int4_3617 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_3618 = torch.constant.int 1
    %2607 = torch.aten.add.Tensor %2606, %2573, %int1_3618 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_3619 = torch.constant.int 32
    %2608 = torch.aten.mul.Scalar %2607, %int32_3619 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_3620 = torch.constant.int 1
    %2609 = torch.aten.add.Tensor %2608, %2570, %int1_3620 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_3621 = torch.constant.none
    %none_3622 = torch.constant.none
    %int5_3623 = torch.constant.int 5
    %cpu_3624 = torch.constant.device "cpu"
    %int0_3625 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2480, %none_3621, %none_3622, %int5_3623, %cpu_3624, %int0_3625 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %2610 = torch.prim.ListConstruct %2609 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_3626 = torch.constant.bool false
    %2611 = torch.aten.index_put %2598, %2610, %2480, %false_3626 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %2611, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_3627 = torch.constant.int 22
    %int2_3628 = torch.constant.int 2
    %int4_3629 = torch.constant.int 4
    %int32_3630 = torch.constant.int 32
    %int64_3631 = torch.constant.int 64
    %2612 = torch.prim.ListConstruct %381, %int22_3627, %int2_3628, %int4_3629, %int32_3630, %int64_3631 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2613 = torch.aten.view %2611, %2612 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2613, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_3632 = torch.constant.int 360448
    %2614 = torch.prim.ListConstruct %381, %int360448_3632 : (!torch.int, !torch.int) -> !torch.list<int>
    %2615 = torch.aten.view %2613, %2614 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %2615, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_3633 = torch.constant.none
    %2616 = torch.aten.clone %107, %none_3633 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_3634 = torch.constant.none
    %2617 = torch.aten.clone %108, %none_3634 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_3635 = torch.constant.none
    %2618 = torch.aten.clone %109, %none_3635 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_3636 = torch.constant.int 22
    %int2_3637 = torch.constant.int 2
    %int4_3638 = torch.constant.int 4
    %int32_3639 = torch.constant.int 32
    %int64_3640 = torch.constant.int 64
    %2619 = torch.prim.ListConstruct %381, %int22_3636, %int2_3637, %int4_3638, %int32_3639, %int64_3640 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2620 = torch.aten.view %2615, %2619 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2620, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %2621 = torch_c.to_builtin_tensor %2620 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %2622 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_3641 = tensor.cast %2622 : tensor<4x?xi64> to tensor<?x?xi64>
    %2623 = torch_c.to_builtin_tensor %2616 : !torch.vtensor<[],si64> -> tensor<i64>
    %2624 = torch_c.to_builtin_tensor %2617 : !torch.vtensor<[],si64> -> tensor<i64>
    %2625 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%2621, %cast_3641, %2623, %2624) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_3642 = tensor.cast %2625 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %2626 = torch_c.from_builtin_tensor %cast_3642 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %2626, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %2627 = torch_c.to_builtin_tensor %2620 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %2628 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_3643 = tensor.cast %2628 : tensor<4x?xi64> to tensor<?x?xi64>
    %2629 = torch_c.to_builtin_tensor %2616 : !torch.vtensor<[],si64> -> tensor<i64>
    %2630 = torch_c.to_builtin_tensor %2618 : !torch.vtensor<[],si64> -> tensor<i64>
    %2631 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%2627, %cast_3643, %2629, %2630) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_3644 = tensor.cast %2631 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %2632 = torch_c.from_builtin_tensor %cast_3644 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %2632, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_3645 = torch.constant.int 2
    %int3_3646 = torch.constant.int 3
    %2633 = torch.aten.transpose.int %2626, %int2_3645, %int3_3646 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %2633, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_3647 = torch.constant.int 0
    %2634 = torch.aten.clone %2633, %int0_3647 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %2634, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_3648 = torch.constant.int 4
    %int4_3649 = torch.constant.int 4
    %int64_3650 = torch.constant.int 64
    %2635 = torch.prim.ListConstruct %int4_3648, %623, %int4_3649, %int64_3650 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2636 = torch.aten._unsafe_view %2634, %2635 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %2636, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_3651 = torch.constant.int 2
    %int3_3652 = torch.constant.int 3
    %2637 = torch.aten.transpose.int %2632, %int2_3651, %int3_3652 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %2637, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_3653 = torch.constant.int 0
    %2638 = torch.aten.clone %2637, %int0_3653 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %2638, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_3654 = torch.constant.int 4
    %int4_3655 = torch.constant.int 4
    %int64_3656 = torch.constant.int 64
    %2639 = torch.prim.ListConstruct %int4_3654, %623, %int4_3655, %int64_3656 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2640 = torch.aten._unsafe_view %2638, %2639 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %2640, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_3657 = torch.constant.int 0
    %int1_3658 = torch.constant.int 1
    %none_3659 = torch.constant.none
    %none_3660 = torch.constant.none
    %cpu_3661 = torch.constant.device "cpu"
    %false_3662 = torch.constant.bool false
    %2641 = torch.aten.arange.start_step %int0_3657, %623, %int1_3658, %none_3659, %none_3660, %cpu_3661, %false_3662 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2641, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_3663 = torch.constant.int -1
    %2642 = torch.aten.unsqueeze %arg1, %int-1_3663 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %2643 = torch.aten.ge.Tensor %2641, %2642 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %2643, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_3664 = torch.constant.none
    %2644 = torch.aten.clone %110, %none_3664 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_3665 = torch.constant.int 0
    %2645 = torch.aten.where.ScalarOther %2643, %2644, %int0_3665 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %2645, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_3666 = torch.constant.none
    %none_3667 = torch.constant.none
    %int5_3668 = torch.constant.int 5
    %cpu_3669 = torch.constant.device "cpu"
    %int0_3670 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2645, %none_3666, %none_3667, %int5_3668, %cpu_3669, %int0_3670 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_3671 = torch.constant.int 1
    %2646 = torch.aten.unsqueeze %2645, %int1_3671 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %2646, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_3672 = torch.constant.int 1
    %2647 = torch.aten.unsqueeze %2646, %int1_3672 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %2647, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_3673 = torch.constant.int -2
    %2648 = torch.aten.unsqueeze %2636, %int-2_3673 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %2648, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_3674 = torch.constant.int 4
    %int4_3675 = torch.constant.int 4
    %int8_3676 = torch.constant.int 8
    %int64_3677 = torch.constant.int 64
    %2649 = torch.prim.ListConstruct %int4_3674, %623, %int4_3675, %int8_3676, %int64_3677 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3678 = torch.constant.bool false
    %2650 = torch.aten.expand %2648, %2649, %false_3678 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2650, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_3679 = torch.constant.int 0
    %2651 = torch.aten.clone %2650, %int0_3679 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2651, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_3680 = torch.constant.int 4
    %int32_3681 = torch.constant.int 32
    %int64_3682 = torch.constant.int 64
    %2652 = torch.prim.ListConstruct %int4_3680, %623, %int32_3681, %int64_3682 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2653 = torch.aten._unsafe_view %2651, %2652 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2653, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_3683 = torch.constant.int -2
    %2654 = torch.aten.unsqueeze %2640, %int-2_3683 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %2654, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_3684 = torch.constant.int 4
    %int4_3685 = torch.constant.int 4
    %int8_3686 = torch.constant.int 8
    %int64_3687 = torch.constant.int 64
    %2655 = torch.prim.ListConstruct %int4_3684, %623, %int4_3685, %int8_3686, %int64_3687 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3688 = torch.constant.bool false
    %2656 = torch.aten.expand %2654, %2655, %false_3688 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2656, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_3689 = torch.constant.int 0
    %2657 = torch.aten.clone %2656, %int0_3689 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2657, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_3690 = torch.constant.int 4
    %int32_3691 = torch.constant.int 32
    %int64_3692 = torch.constant.int 64
    %2658 = torch.prim.ListConstruct %int4_3690, %623, %int32_3691, %int64_3692 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2659 = torch.aten._unsafe_view %2657, %2658 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2659, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_3693 = torch.constant.int 1
    %int2_3694 = torch.constant.int 2
    %2660 = torch.aten.transpose.int %2521, %int1_3693, %int2_3694 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_3695 = torch.constant.int 1
    %int2_3696 = torch.constant.int 2
    %2661 = torch.aten.transpose.int %2653, %int1_3695, %int2_3696 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2661, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_3697 = torch.constant.int 1
    %int2_3698 = torch.constant.int 2
    %2662 = torch.aten.transpose.int %2659, %int1_3697, %int2_3698 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2662, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_3699 = torch.constant.float 0.000000e+00
    %false_3700 = torch.constant.bool false
    %none_3701 = torch.constant.none
    %false_3702 = torch.constant.bool false
    %2663 = torch.aten.scaled_dot_product_attention %2660, %2661, %2662, %2647, %float0.000000e00_3699, %false_3700, %none_3701, %false_3702 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_3703 = torch.constant.int 1
    %int2_3704 = torch.constant.int 2
    %2664 = torch.aten.transpose.int %2663, %int1_3703, %int2_3704 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_3705 = torch.constant.int 4
    %int1_3706 = torch.constant.int 1
    %int2048_3707 = torch.constant.int 2048
    %2665 = torch.prim.ListConstruct %int4_3705, %int1_3706, %int2048_3707 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2666 = torch.aten.view %2664, %2665 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_3708 = torch.constant.int 2
    %2667 = torch.aten.view.dtype %115, %int2_3708 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %2668 = torch.aten.detach %2667 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_3709 = torch.constant.int -1
    %int17_3710 = torch.constant.int 17
    %2669 = torch.prim.ListConstruct %int-1_3709, %int17_3710 : (!torch.int, !torch.int) -> !torch.list<int>
    %2670 = torch.aten.view %2668, %2669 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_3711 = torch.constant.int 2048
    %int-1_3712 = torch.constant.int -1
    %int17_3713 = torch.constant.int 17
    %2671 = torch.prim.ListConstruct %int2048_3711, %int-1_3712, %int17_3713 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2672 = torch.aten.view %2670, %2671 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_3714 = torch.constant.int 2
    %int0_3715 = torch.constant.int 0
    %int1_3716 = torch.constant.int 1
    %int1_3717 = torch.constant.int 1
    %2673 = torch.aten.slice.Tensor %2672, %int2_3714, %int0_3715, %int1_3716, %int1_3717 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_3718 = torch.constant.int 5
    %2674 = torch.aten.view.dtype %2673, %int5_3718 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %2675 = torch.aten.detach %2674 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_3719 = torch.constant.int 2
    %int1_3720 = torch.constant.int 1
    %int9223372036854775807_3721 = torch.constant.int 9223372036854775807
    %int1_3722 = torch.constant.int 1
    %2676 = torch.aten.slice.Tensor %2672, %int2_3719, %int1_3720, %int9223372036854775807_3721, %int1_3722 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_3723 = torch.constant.int 1
    %2677 = torch.aten.view.dtype %2676, %int1_3723 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %2678 = torch.aten.detach %2677 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %2679 = torch_c.to_builtin_tensor %2666 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_3724 = tensor.cast %2679 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %2680 = torch_c.to_builtin_tensor %2675 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %2681 = torch_c.to_builtin_tensor %2678 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %2682 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_3724, %2680, %2681) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_3725 = tensor.cast %2682 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %2683 = torch_c.from_builtin_tensor %cast_3725 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_3726 = torch.constant.none
    %none_3727 = torch.constant.none
    %int5_3728 = torch.constant.int 5
    %cpu_3729 = torch.constant.device "cpu"
    %int0_3730 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2683, %none_3726, %none_3727, %int5_3728, %cpu_3729, %int0_3730 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_3731 = torch.constant.int 1
    %2684 = torch.aten.add.Tensor %2413, %2683, %int1_3731 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_3732 = torch.constant.none
    %none_3733 = torch.constant.none
    %int5_3734 = torch.constant.int 5
    %cpu_3735 = torch.constant.device "cpu"
    %int0_3736 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2684, %none_3732, %none_3733, %int5_3734, %cpu_3735, %int0_3736 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3737 = torch.constant.int 6
    %2685 = torch.prims.convert_element_type %2684, %int6_3737 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_3738 = torch.constant.int 2
    %2686 = torch.aten.pow.Tensor_Scalar %2685, %int2_3738 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_3739 = torch.constant.int -1
    %2687 = torch.prim.ListConstruct %int-1_3739 : (!torch.int) -> !torch.list<int>
    %true_3740 = torch.constant.bool true
    %none_3741 = torch.constant.none
    %2688 = torch.aten.mean.dim %2686, %2687, %true_3740, %none_3741 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_3742 = torch.constant.float 9.9999997473787516E-6
    %int1_3743 = torch.constant.int 1
    %2689 = torch.aten.add.Scalar %2688, %float9.999990e-06_3742, %int1_3743 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2690 = torch.aten.rsqrt %2689 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %2691 = torch.aten.mul.Tensor %2685, %2690 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_3744 = torch.constant.none
    %none_3745 = torch.constant.none
    %int6_3746 = torch.constant.int 6
    %cpu_3747 = torch.constant.device "cpu"
    %int0_3748 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2691, %none_3744, %none_3745, %int6_3746, %cpu_3747, %int0_3748 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3749 = torch.constant.int 5
    %2692 = torch.prims.convert_element_type %2691, %int5_3749 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %2693 = torch.aten.mul.Tensor %116, %2692 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_3750 = torch.constant.none
    %none_3751 = torch.constant.none
    %int6_3752 = torch.constant.int 6
    %cpu_3753 = torch.constant.device "cpu"
    %int0_3754 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2693, %none_3750, %none_3751, %int6_3752, %cpu_3753, %int0_3754 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3755 = torch.constant.int 5
    %2694 = torch.prims.convert_element_type %2693, %int5_3755 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_3756 = torch.constant.int 2
    %2695 = torch.aten.view.dtype %117, %int2_3756 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %2696 = torch.aten.detach %2695 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_3757 = torch.constant.int -1
    %int17_3758 = torch.constant.int 17
    %2697 = torch.prim.ListConstruct %int-1_3757, %int17_3758 : (!torch.int, !torch.int) -> !torch.list<int>
    %2698 = torch.aten.view %2696, %2697 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_3759 = torch.constant.int 5632
    %int-1_3760 = torch.constant.int -1
    %int17_3761 = torch.constant.int 17
    %2699 = torch.prim.ListConstruct %int5632_3759, %int-1_3760, %int17_3761 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2700 = torch.aten.view %2698, %2699 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_3762 = torch.constant.int 2
    %int0_3763 = torch.constant.int 0
    %int1_3764 = torch.constant.int 1
    %int1_3765 = torch.constant.int 1
    %2701 = torch.aten.slice.Tensor %2700, %int2_3762, %int0_3763, %int1_3764, %int1_3765 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_3766 = torch.constant.int 5
    %2702 = torch.aten.view.dtype %2701, %int5_3766 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %2703 = torch.aten.detach %2702 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_3767 = torch.constant.int 2
    %int1_3768 = torch.constant.int 1
    %int9223372036854775807_3769 = torch.constant.int 9223372036854775807
    %int1_3770 = torch.constant.int 1
    %2704 = torch.aten.slice.Tensor %2700, %int2_3767, %int1_3768, %int9223372036854775807_3769, %int1_3770 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_3771 = torch.constant.int 1
    %2705 = torch.aten.view.dtype %2704, %int1_3771 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %2706 = torch.aten.detach %2705 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %2707 = torch_c.to_builtin_tensor %2694 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_3772 = tensor.cast %2707 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %2708 = torch_c.to_builtin_tensor %2703 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %2709 = torch_c.to_builtin_tensor %2706 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %2710 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_3772, %2708, %2709) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_3773 = tensor.cast %2710 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %2711 = torch_c.from_builtin_tensor %cast_3773 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %2712 = torch.aten.silu %2711 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_3774 = torch.constant.int 2
    %2713 = torch.aten.view.dtype %118, %int2_3774 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %2714 = torch.aten.detach %2713 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_3775 = torch.constant.int -1
    %int17_3776 = torch.constant.int 17
    %2715 = torch.prim.ListConstruct %int-1_3775, %int17_3776 : (!torch.int, !torch.int) -> !torch.list<int>
    %2716 = torch.aten.view %2714, %2715 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_3777 = torch.constant.int 5632
    %int-1_3778 = torch.constant.int -1
    %int17_3779 = torch.constant.int 17
    %2717 = torch.prim.ListConstruct %int5632_3777, %int-1_3778, %int17_3779 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2718 = torch.aten.view %2716, %2717 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_3780 = torch.constant.int 2
    %int0_3781 = torch.constant.int 0
    %int1_3782 = torch.constant.int 1
    %int1_3783 = torch.constant.int 1
    %2719 = torch.aten.slice.Tensor %2718, %int2_3780, %int0_3781, %int1_3782, %int1_3783 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_3784 = torch.constant.int 5
    %2720 = torch.aten.view.dtype %2719, %int5_3784 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %2721 = torch.aten.detach %2720 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_3785 = torch.constant.int 2
    %int1_3786 = torch.constant.int 1
    %int9223372036854775807_3787 = torch.constant.int 9223372036854775807
    %int1_3788 = torch.constant.int 1
    %2722 = torch.aten.slice.Tensor %2718, %int2_3785, %int1_3786, %int9223372036854775807_3787, %int1_3788 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_3789 = torch.constant.int 1
    %2723 = torch.aten.view.dtype %2722, %int1_3789 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %2724 = torch.aten.detach %2723 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %2725 = torch_c.to_builtin_tensor %2694 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_3790 = tensor.cast %2725 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %2726 = torch_c.to_builtin_tensor %2721 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %2727 = torch_c.to_builtin_tensor %2724 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %2728 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_3790, %2726, %2727) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_3791 = tensor.cast %2728 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %2729 = torch_c.from_builtin_tensor %cast_3791 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %2730 = torch.aten.mul.Tensor %2712, %2729 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_3792 = torch.constant.int 2
    %2731 = torch.aten.view.dtype %119, %int2_3792 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %2732 = torch.aten.detach %2731 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_3793 = torch.constant.int -1
    %int17_3794 = torch.constant.int 17
    %2733 = torch.prim.ListConstruct %int-1_3793, %int17_3794 : (!torch.int, !torch.int) -> !torch.list<int>
    %2734 = torch.aten.view %2732, %2733 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_3795 = torch.constant.int 2048
    %int-1_3796 = torch.constant.int -1
    %int17_3797 = torch.constant.int 17
    %2735 = torch.prim.ListConstruct %int2048_3795, %int-1_3796, %int17_3797 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2736 = torch.aten.view %2734, %2735 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_3798 = torch.constant.int 2
    %int0_3799 = torch.constant.int 0
    %int1_3800 = torch.constant.int 1
    %int1_3801 = torch.constant.int 1
    %2737 = torch.aten.slice.Tensor %2736, %int2_3798, %int0_3799, %int1_3800, %int1_3801 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_3802 = torch.constant.int 5
    %2738 = torch.aten.view.dtype %2737, %int5_3802 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %2739 = torch.aten.detach %2738 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_3803 = torch.constant.int 2
    %int1_3804 = torch.constant.int 1
    %int9223372036854775807_3805 = torch.constant.int 9223372036854775807
    %int1_3806 = torch.constant.int 1
    %2740 = torch.aten.slice.Tensor %2736, %int2_3803, %int1_3804, %int9223372036854775807_3805, %int1_3806 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_3807 = torch.constant.int 1
    %2741 = torch.aten.view.dtype %2740, %int1_3807 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %2742 = torch.aten.detach %2741 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %2743 = torch_c.to_builtin_tensor %2730 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_3808 = tensor.cast %2743 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %2744 = torch_c.to_builtin_tensor %2739 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %2745 = torch_c.to_builtin_tensor %2742 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %2746 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_3808, %2744, %2745) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_3809 = tensor.cast %2746 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %2747 = torch_c.from_builtin_tensor %cast_3809 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_3810 = torch.constant.int 1
    %2748 = torch.aten.add.Tensor %2684, %2747, %int1_3810 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_3811 = torch.constant.none
    %none_3812 = torch.constant.none
    %int5_3813 = torch.constant.int 5
    %cpu_3814 = torch.constant.device "cpu"
    %int0_3815 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2748, %none_3811, %none_3812, %int5_3813, %cpu_3814, %int0_3815 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3816 = torch.constant.int 6
    %2749 = torch.prims.convert_element_type %2748, %int6_3816 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_3817 = torch.constant.int 2
    %2750 = torch.aten.pow.Tensor_Scalar %2749, %int2_3817 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_3818 = torch.constant.int -1
    %2751 = torch.prim.ListConstruct %int-1_3818 : (!torch.int) -> !torch.list<int>
    %true_3819 = torch.constant.bool true
    %none_3820 = torch.constant.none
    %2752 = torch.aten.mean.dim %2750, %2751, %true_3819, %none_3820 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_3821 = torch.constant.float 9.9999997473787516E-6
    %int1_3822 = torch.constant.int 1
    %2753 = torch.aten.add.Scalar %2752, %float9.999990e-06_3821, %int1_3822 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2754 = torch.aten.rsqrt %2753 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %2755 = torch.aten.mul.Tensor %2749, %2754 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_3823 = torch.constant.none
    %none_3824 = torch.constant.none
    %int6_3825 = torch.constant.int 6
    %cpu_3826 = torch.constant.device "cpu"
    %int0_3827 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2755, %none_3823, %none_3824, %int6_3825, %cpu_3826, %int0_3827 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3828 = torch.constant.int 5
    %2756 = torch.prims.convert_element_type %2755, %int5_3828 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %2757 = torch.aten.mul.Tensor %128, %2756 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_3829 = torch.constant.none
    %none_3830 = torch.constant.none
    %int6_3831 = torch.constant.int 6
    %cpu_3832 = torch.constant.device "cpu"
    %int0_3833 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2757, %none_3829, %none_3830, %int6_3831, %cpu_3832, %int0_3833 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3834 = torch.constant.int 5
    %2758 = torch.prims.convert_element_type %2757, %int5_3834 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_3835 = torch.constant.int 2
    %2759 = torch.aten.view.dtype %129, %int2_3835 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %2760 = torch.aten.detach %2759 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_3836 = torch.constant.int -1
    %int17_3837 = torch.constant.int 17
    %2761 = torch.prim.ListConstruct %int-1_3836, %int17_3837 : (!torch.int, !torch.int) -> !torch.list<int>
    %2762 = torch.aten.view %2760, %2761 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_3838 = torch.constant.int 2048
    %int-1_3839 = torch.constant.int -1
    %int17_3840 = torch.constant.int 17
    %2763 = torch.prim.ListConstruct %int2048_3838, %int-1_3839, %int17_3840 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2764 = torch.aten.view %2762, %2763 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_3841 = torch.constant.int 2
    %int0_3842 = torch.constant.int 0
    %int1_3843 = torch.constant.int 1
    %int1_3844 = torch.constant.int 1
    %2765 = torch.aten.slice.Tensor %2764, %int2_3841, %int0_3842, %int1_3843, %int1_3844 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_3845 = torch.constant.int 5
    %2766 = torch.aten.view.dtype %2765, %int5_3845 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %2767 = torch.aten.detach %2766 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_3846 = torch.constant.int 2
    %int1_3847 = torch.constant.int 1
    %int9223372036854775807_3848 = torch.constant.int 9223372036854775807
    %int1_3849 = torch.constant.int 1
    %2768 = torch.aten.slice.Tensor %2764, %int2_3846, %int1_3847, %int9223372036854775807_3848, %int1_3849 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_3850 = torch.constant.int 1
    %2769 = torch.aten.view.dtype %2768, %int1_3850 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %2770 = torch.aten.detach %2769 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %2771 = torch_c.to_builtin_tensor %2758 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_3851 = tensor.cast %2771 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %2772 = torch_c.to_builtin_tensor %2767 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %2773 = torch_c.to_builtin_tensor %2770 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %2774 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_3851, %2772, %2773) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_3852 = tensor.cast %2774 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %2775 = torch_c.from_builtin_tensor %cast_3852 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_3853 = torch.constant.int 2
    %2776 = torch.aten.view.dtype %130, %int2_3853 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %2777 = torch.aten.detach %2776 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_3854 = torch.constant.int -1
    %int17_3855 = torch.constant.int 17
    %2778 = torch.prim.ListConstruct %int-1_3854, %int17_3855 : (!torch.int, !torch.int) -> !torch.list<int>
    %2779 = torch.aten.view %2777, %2778 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_3856 = torch.constant.int 256
    %int-1_3857 = torch.constant.int -1
    %int17_3858 = torch.constant.int 17
    %2780 = torch.prim.ListConstruct %int256_3856, %int-1_3857, %int17_3858 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2781 = torch.aten.view %2779, %2780 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_3859 = torch.constant.int 2
    %int0_3860 = torch.constant.int 0
    %int1_3861 = torch.constant.int 1
    %int1_3862 = torch.constant.int 1
    %2782 = torch.aten.slice.Tensor %2781, %int2_3859, %int0_3860, %int1_3861, %int1_3862 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_3863 = torch.constant.int 5
    %2783 = torch.aten.view.dtype %2782, %int5_3863 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %2784 = torch.aten.detach %2783 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_3864 = torch.constant.int 2
    %int1_3865 = torch.constant.int 1
    %int9223372036854775807_3866 = torch.constant.int 9223372036854775807
    %int1_3867 = torch.constant.int 1
    %2785 = torch.aten.slice.Tensor %2781, %int2_3864, %int1_3865, %int9223372036854775807_3866, %int1_3867 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_3868 = torch.constant.int 1
    %2786 = torch.aten.view.dtype %2785, %int1_3868 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %2787 = torch.aten.detach %2786 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %2788 = torch_c.to_builtin_tensor %2758 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_3869 = tensor.cast %2788 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %2789 = torch_c.to_builtin_tensor %2784 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %2790 = torch_c.to_builtin_tensor %2787 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %2791 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_3869, %2789, %2790) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_3870 = tensor.cast %2791 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %2792 = torch_c.from_builtin_tensor %cast_3870 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_3871 = torch.constant.int 2
    %2793 = torch.aten.view.dtype %131, %int2_3871 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %2794 = torch.aten.detach %2793 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_3872 = torch.constant.int -1
    %int17_3873 = torch.constant.int 17
    %2795 = torch.prim.ListConstruct %int-1_3872, %int17_3873 : (!torch.int, !torch.int) -> !torch.list<int>
    %2796 = torch.aten.view %2794, %2795 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_3874 = torch.constant.int 256
    %int-1_3875 = torch.constant.int -1
    %int17_3876 = torch.constant.int 17
    %2797 = torch.prim.ListConstruct %int256_3874, %int-1_3875, %int17_3876 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2798 = torch.aten.view %2796, %2797 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_3877 = torch.constant.int 2
    %int0_3878 = torch.constant.int 0
    %int1_3879 = torch.constant.int 1
    %int1_3880 = torch.constant.int 1
    %2799 = torch.aten.slice.Tensor %2798, %int2_3877, %int0_3878, %int1_3879, %int1_3880 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_3881 = torch.constant.int 5
    %2800 = torch.aten.view.dtype %2799, %int5_3881 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %2801 = torch.aten.detach %2800 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_3882 = torch.constant.int 2
    %int1_3883 = torch.constant.int 1
    %int9223372036854775807_3884 = torch.constant.int 9223372036854775807
    %int1_3885 = torch.constant.int 1
    %2802 = torch.aten.slice.Tensor %2798, %int2_3882, %int1_3883, %int9223372036854775807_3884, %int1_3885 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_3886 = torch.constant.int 1
    %2803 = torch.aten.view.dtype %2802, %int1_3886 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %2804 = torch.aten.detach %2803 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %2805 = torch_c.to_builtin_tensor %2758 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_3887 = tensor.cast %2805 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %2806 = torch_c.to_builtin_tensor %2801 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %2807 = torch_c.to_builtin_tensor %2804 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %2808 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_3887, %2806, %2807) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_3888 = tensor.cast %2808 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %2809 = torch_c.from_builtin_tensor %cast_3888 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_3889 = torch.constant.int 4
    %int1_3890 = torch.constant.int 1
    %int32_3891 = torch.constant.int 32
    %int64_3892 = torch.constant.int 64
    %2810 = torch.prim.ListConstruct %int4_3889, %int1_3890, %int32_3891, %int64_3892 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2811 = torch.aten.view %2775, %2810 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_3893 = torch.constant.int 4
    %int1_3894 = torch.constant.int 1
    %int4_3895 = torch.constant.int 4
    %int64_3896 = torch.constant.int 64
    %2812 = torch.prim.ListConstruct %int4_3893, %int1_3894, %int4_3895, %int64_3896 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2813 = torch.aten.view %2792, %2812 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_3897 = torch.constant.int 4
    %int1_3898 = torch.constant.int 1
    %int4_3899 = torch.constant.int 4
    %int64_3900 = torch.constant.int 64
    %2814 = torch.prim.ListConstruct %int4_3897, %int1_3898, %int4_3899, %int64_3900 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2815 = torch.aten.view %2809, %2814 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_3901 = torch.constant.int 0
    %int1_3902 = torch.constant.int 1
    %none_3903 = torch.constant.none
    %none_3904 = torch.constant.none
    %cpu_3905 = torch.constant.device "cpu"
    %false_3906 = torch.constant.bool false
    %2816 = torch.aten.arange.start %int0_3901, %int1_3902, %none_3903, %none_3904, %cpu_3905, %false_3906 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_3907 = torch.constant.int 0
    %2817 = torch.aten.unsqueeze %2816, %int0_3907 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_3908 = torch.constant.int 1
    %2818 = torch.aten.unsqueeze %arg2, %int1_3908 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3909 = torch.constant.int 1
    %2819 = torch.aten.add.Tensor %2817, %2818, %int1_3909 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_3910 = torch.constant.int 0
    %int64_3911 = torch.constant.int 64
    %int2_3912 = torch.constant.int 2
    %none_3913 = torch.constant.none
    %none_3914 = torch.constant.none
    %cpu_3915 = torch.constant.device "cpu"
    %false_3916 = torch.constant.bool false
    %2820 = torch.aten.arange.start_step %int0_3910, %int64_3911, %int2_3912, %none_3913, %none_3914, %cpu_3915, %false_3916 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_3917 = torch.constant.none
    %none_3918 = torch.constant.none
    %int4_3919 = torch.constant.int 4
    %cpu_3920 = torch.constant.device "cpu"
    %int0_3921 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2820, %none_3917, %none_3918, %int4_3919, %cpu_3920, %int0_3921 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3922 = torch.constant.int 6
    %2821 = torch.prims.convert_element_type %2820, %int6_3922 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_3923 = torch.constant.int 64
    %2822 = torch.aten.div.Scalar %2821, %int64_3923 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_3924 = torch.constant.float 1.000000e+04
    %2823 = torch.aten.pow.Scalar %float1.000000e04_3924, %2822 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %2824 = torch.aten.reciprocal %2823 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_3925 = torch.constant.float 1.000000e+00
    %2825 = torch.aten.mul.Scalar %2824, %float1.000000e00_3925 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_3926 = torch.constant.none
    %2826 = torch.aten.clone %120, %none_3926 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_3927 = torch.constant.int 0
    %2827 = torch.aten.unsqueeze %2825, %int0_3927 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_3928 = torch.constant.int 2
    %2828 = torch.aten.unsqueeze %2827, %int2_3928 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_3929 = torch.constant.none
    %none_3930 = torch.constant.none
    %int6_3931 = torch.constant.int 6
    %cpu_3932 = torch.constant.device "cpu"
    %int0_3933 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2828, %none_3929, %none_3930, %int6_3931, %cpu_3932, %int0_3933 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_3934 = torch.constant.int 4
    %int-1_3935 = torch.constant.int -1
    %int1_3936 = torch.constant.int 1
    %2829 = torch.prim.ListConstruct %int4_3934, %int-1_3935, %int1_3936 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3937 = torch.constant.bool false
    %2830 = torch.aten.expand %2828, %2829, %false_3937 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_3938 = torch.constant.int 1
    %2831 = torch.aten.unsqueeze %2819, %int1_3938 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_3939 = torch.constant.none
    %none_3940 = torch.constant.none
    %int4_3941 = torch.constant.int 4
    %cpu_3942 = torch.constant.device "cpu"
    %int0_3943 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2831, %none_3939, %none_3940, %int4_3941, %cpu_3942, %int0_3943 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_3944 = torch.constant.int 6
    %2832 = torch.prims.convert_element_type %2831, %int6_3944 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2833 = torch.aten.matmul %2830, %2832 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_3945 = torch.constant.int 1
    %int2_3946 = torch.constant.int 2
    %2834 = torch.aten.transpose.int %2833, %int1_3945, %int2_3946 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %2835 = torch.aten.cos %2834 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %2836 = torch.aten.mul.Tensor %2835, %2826 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_3947 = torch.constant.none
    %none_3948 = torch.constant.none
    %int6_3949 = torch.constant.int 6
    %cpu_3950 = torch.constant.device "cpu"
    %int0_3951 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2836, %none_3947, %none_3948, %int6_3949, %cpu_3950, %int0_3951 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3952 = torch.constant.int 5
    %2837 = torch.prims.convert_element_type %2836, %int5_3952 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %2838 = torch.aten.sin %2834 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %2839 = torch.aten.mul.Tensor %2838, %2826 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_3953 = torch.constant.none
    %none_3954 = torch.constant.none
    %int6_3955 = torch.constant.int 6
    %cpu_3956 = torch.constant.device "cpu"
    %int0_3957 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2839, %none_3953, %none_3954, %int6_3955, %cpu_3956, %int0_3957 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_3958 = torch.constant.int 5
    %2840 = torch.prims.convert_element_type %2839, %int5_3958 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_3959 = torch.constant.int 2
    %2841 = torch.aten.unsqueeze %2837, %int2_3959 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_3960 = torch.constant.int 2
    %2842 = torch.aten.unsqueeze %2840, %int2_3960 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_3961 = torch.constant.none
    %none_3962 = torch.constant.none
    %int5_3963 = torch.constant.int 5
    %cpu_3964 = torch.constant.device "cpu"
    %int0_3965 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2841, %none_3961, %none_3962, %int5_3963, %cpu_3964, %int0_3965 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_3966 = torch.constant.none
    %none_3967 = torch.constant.none
    %int5_3968 = torch.constant.int 5
    %cpu_3969 = torch.constant.device "cpu"
    %int0_3970 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2842, %none_3966, %none_3967, %int5_3968, %cpu_3969, %int0_3970 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_3971 = torch.constant.none
    %none_3972 = torch.constant.none
    %int5_3973 = torch.constant.int 5
    %cpu_3974 = torch.constant.device "cpu"
    %int0_3975 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2811, %none_3971, %none_3972, %int5_3973, %cpu_3974, %int0_3975 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_3976 = torch.constant.int 3
    %int0_3977 = torch.constant.int 0
    %int64_3978 = torch.constant.int 64
    %int2_3979 = torch.constant.int 2
    %2843 = torch.aten.slice.Tensor %2811, %int3_3976, %int0_3977, %int64_3978, %int2_3979 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_3980 = torch.constant.int 3
    %int1_3981 = torch.constant.int 1
    %int64_3982 = torch.constant.int 64
    %int2_3983 = torch.constant.int 2
    %2844 = torch.aten.slice.Tensor %2811, %int3_3980, %int1_3981, %int64_3982, %int2_3983 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %2845 = torch.aten.mul.Tensor %2843, %2841 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %2846 = torch.aten.mul.Tensor %2844, %2842 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_3984 = torch.constant.int 1
    %2847 = torch.aten.sub.Tensor %2845, %2846, %int1_3984 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %2848 = torch.aten.mul.Tensor %2844, %2841 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %2849 = torch.aten.mul.Tensor %2843, %2842 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_3985 = torch.constant.int 1
    %2850 = torch.aten.add.Tensor %2848, %2849, %int1_3985 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %2851 = torch_c.to_builtin_tensor %2847 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_3986 = tensor.cast %2851 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %2852 = torch_c.to_builtin_tensor %2850 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_3987 = tensor.cast %2852 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %2853 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_3986, %cast_3987) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_3988 = tensor.cast %2853 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %2854 = torch_c.from_builtin_tensor %cast_3988 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_3989 = torch.constant.int 4
    %int1_3990 = torch.constant.int 1
    %int32_3991 = torch.constant.int 32
    %int64_3992 = torch.constant.int 64
    %2855 = torch.prim.ListConstruct %int4_3989, %int1_3990, %int32_3991, %int64_3992 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2856 = torch.aten.view %2854, %2855 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_3993 = torch.constant.none
    %none_3994 = torch.constant.none
    %int5_3995 = torch.constant.int 5
    %cpu_3996 = torch.constant.device "cpu"
    %int0_3997 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2856, %none_3993, %none_3994, %int5_3995, %cpu_3996, %int0_3997 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_3998 = torch.constant.int 0
    %int1_3999 = torch.constant.int 1
    %none_4000 = torch.constant.none
    %none_4001 = torch.constant.none
    %cpu_4002 = torch.constant.device "cpu"
    %false_4003 = torch.constant.bool false
    %2857 = torch.aten.arange.start %int0_3998, %int1_3999, %none_4000, %none_4001, %cpu_4002, %false_4003 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_4004 = torch.constant.int 0
    %2858 = torch.aten.unsqueeze %2857, %int0_4004 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_4005 = torch.constant.int 1
    %2859 = torch.aten.unsqueeze %arg2, %int1_4005 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4006 = torch.constant.int 1
    %2860 = torch.aten.add.Tensor %2858, %2859, %int1_4006 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_4007 = torch.constant.int 0
    %int64_4008 = torch.constant.int 64
    %int2_4009 = torch.constant.int 2
    %none_4010 = torch.constant.none
    %none_4011 = torch.constant.none
    %cpu_4012 = torch.constant.device "cpu"
    %false_4013 = torch.constant.bool false
    %2861 = torch.aten.arange.start_step %int0_4007, %int64_4008, %int2_4009, %none_4010, %none_4011, %cpu_4012, %false_4013 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_4014 = torch.constant.none
    %none_4015 = torch.constant.none
    %int4_4016 = torch.constant.int 4
    %cpu_4017 = torch.constant.device "cpu"
    %int0_4018 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2861, %none_4014, %none_4015, %int4_4016, %cpu_4017, %int0_4018 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4019 = torch.constant.int 6
    %2862 = torch.prims.convert_element_type %2861, %int6_4019 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_4020 = torch.constant.int 64
    %2863 = torch.aten.div.Scalar %2862, %int64_4020 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_4021 = torch.constant.float 1.000000e+04
    %2864 = torch.aten.pow.Scalar %float1.000000e04_4021, %2863 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %2865 = torch.aten.reciprocal %2864 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_4022 = torch.constant.float 1.000000e+00
    %2866 = torch.aten.mul.Scalar %2865, %float1.000000e00_4022 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_4023 = torch.constant.none
    %2867 = torch.aten.clone %121, %none_4023 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_4024 = torch.constant.int 0
    %2868 = torch.aten.unsqueeze %2866, %int0_4024 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_4025 = torch.constant.int 2
    %2869 = torch.aten.unsqueeze %2868, %int2_4025 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_4026 = torch.constant.none
    %none_4027 = torch.constant.none
    %int6_4028 = torch.constant.int 6
    %cpu_4029 = torch.constant.device "cpu"
    %int0_4030 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2869, %none_4026, %none_4027, %int6_4028, %cpu_4029, %int0_4030 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_4031 = torch.constant.int 4
    %int-1_4032 = torch.constant.int -1
    %int1_4033 = torch.constant.int 1
    %2870 = torch.prim.ListConstruct %int4_4031, %int-1_4032, %int1_4033 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4034 = torch.constant.bool false
    %2871 = torch.aten.expand %2869, %2870, %false_4034 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_4035 = torch.constant.int 1
    %2872 = torch.aten.unsqueeze %2860, %int1_4035 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_4036 = torch.constant.none
    %none_4037 = torch.constant.none
    %int4_4038 = torch.constant.int 4
    %cpu_4039 = torch.constant.device "cpu"
    %int0_4040 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2872, %none_4036, %none_4037, %int4_4038, %cpu_4039, %int0_4040 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4041 = torch.constant.int 6
    %2873 = torch.prims.convert_element_type %2872, %int6_4041 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2874 = torch.aten.matmul %2871, %2873 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_4042 = torch.constant.int 1
    %int2_4043 = torch.constant.int 2
    %2875 = torch.aten.transpose.int %2874, %int1_4042, %int2_4043 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %2876 = torch.aten.cos %2875 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %2877 = torch.aten.mul.Tensor %2876, %2867 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_4044 = torch.constant.none
    %none_4045 = torch.constant.none
    %int6_4046 = torch.constant.int 6
    %cpu_4047 = torch.constant.device "cpu"
    %int0_4048 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2877, %none_4044, %none_4045, %int6_4046, %cpu_4047, %int0_4048 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4049 = torch.constant.int 5
    %2878 = torch.prims.convert_element_type %2877, %int5_4049 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %2879 = torch.aten.sin %2875 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %2880 = torch.aten.mul.Tensor %2879, %2867 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_4050 = torch.constant.none
    %none_4051 = torch.constant.none
    %int6_4052 = torch.constant.int 6
    %cpu_4053 = torch.constant.device "cpu"
    %int0_4054 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2880, %none_4050, %none_4051, %int6_4052, %cpu_4053, %int0_4054 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4055 = torch.constant.int 5
    %2881 = torch.prims.convert_element_type %2880, %int5_4055 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_4056 = torch.constant.int 2
    %2882 = torch.aten.unsqueeze %2878, %int2_4056 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_4057 = torch.constant.int 2
    %2883 = torch.aten.unsqueeze %2881, %int2_4057 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_4058 = torch.constant.none
    %none_4059 = torch.constant.none
    %int5_4060 = torch.constant.int 5
    %cpu_4061 = torch.constant.device "cpu"
    %int0_4062 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2882, %none_4058, %none_4059, %int5_4060, %cpu_4061, %int0_4062 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_4063 = torch.constant.none
    %none_4064 = torch.constant.none
    %int5_4065 = torch.constant.int 5
    %cpu_4066 = torch.constant.device "cpu"
    %int0_4067 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2883, %none_4063, %none_4064, %int5_4065, %cpu_4066, %int0_4067 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_4068 = torch.constant.none
    %none_4069 = torch.constant.none
    %int5_4070 = torch.constant.int 5
    %cpu_4071 = torch.constant.device "cpu"
    %int0_4072 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2813, %none_4068, %none_4069, %int5_4070, %cpu_4071, %int0_4072 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_4073 = torch.constant.int 3
    %int0_4074 = torch.constant.int 0
    %int64_4075 = torch.constant.int 64
    %int2_4076 = torch.constant.int 2
    %2884 = torch.aten.slice.Tensor %2813, %int3_4073, %int0_4074, %int64_4075, %int2_4076 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_4077 = torch.constant.int 3
    %int1_4078 = torch.constant.int 1
    %int64_4079 = torch.constant.int 64
    %int2_4080 = torch.constant.int 2
    %2885 = torch.aten.slice.Tensor %2813, %int3_4077, %int1_4078, %int64_4079, %int2_4080 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %2886 = torch.aten.mul.Tensor %2884, %2882 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %2887 = torch.aten.mul.Tensor %2885, %2883 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_4081 = torch.constant.int 1
    %2888 = torch.aten.sub.Tensor %2886, %2887, %int1_4081 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %2889 = torch.aten.mul.Tensor %2885, %2882 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %2890 = torch.aten.mul.Tensor %2884, %2883 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_4082 = torch.constant.int 1
    %2891 = torch.aten.add.Tensor %2889, %2890, %int1_4082 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %2892 = torch_c.to_builtin_tensor %2888 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_4083 = tensor.cast %2892 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %2893 = torch_c.to_builtin_tensor %2891 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_4084 = tensor.cast %2893 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %2894 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_4083, %cast_4084) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_4085 = tensor.cast %2894 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %2895 = torch_c.from_builtin_tensor %cast_4085 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_4086 = torch.constant.int 4
    %int1_4087 = torch.constant.int 1
    %int4_4088 = torch.constant.int 4
    %int64_4089 = torch.constant.int 64
    %2896 = torch.prim.ListConstruct %int4_4086, %int1_4087, %int4_4088, %int64_4089 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2897 = torch.aten.view %2895, %2896 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_4090 = torch.constant.none
    %none_4091 = torch.constant.none
    %int5_4092 = torch.constant.int 5
    %cpu_4093 = torch.constant.device "cpu"
    %int0_4094 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2897, %none_4090, %none_4091, %int5_4092, %cpu_4093, %int0_4094 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_4095 = torch.constant.int 32
    %2898 = torch.aten.floor_divide.Scalar %arg2, %int32_4095 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_4096 = torch.constant.int 1
    %2899 = torch.aten.unsqueeze %2898, %int1_4096 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4097 = torch.constant.int 1
    %false_4098 = torch.constant.bool false
    %2900 = torch.aten.gather %arg3, %int1_4097, %2899, %false_4098 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_4099 = torch.constant.int 4
    %int1_4100 = torch.constant.int 1
    %int1_4101 = torch.constant.int 1
    %2901 = torch.prim.ListConstruct %int4_4099, %int1_4100, %int1_4101 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2902 = torch.aten.view %2900, %2901 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_4102 = torch.constant.int 32
    %2903 = torch.aten.remainder.Scalar %arg2, %int32_4102 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_4103 = torch.constant.int 4
    %int1_4104 = torch.constant.int 1
    %int1_4105 = torch.constant.int 1
    %2904 = torch.prim.ListConstruct %int4_4103, %int1_4104, %int1_4105 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2905 = torch.aten.view %2903, %2904 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_4106 = torch.constant.int 4
    %none_4107 = torch.constant.none
    %none_4108 = torch.constant.none
    %cpu_4109 = torch.constant.device "cpu"
    %false_4110 = torch.constant.bool false
    %2906 = torch.aten.arange %int4_4106, %none_4107, %none_4108, %cpu_4109, %false_4110 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_4111 = torch.constant.int 1
    %int1_4112 = torch.constant.int 1
    %int4_4113 = torch.constant.int 4
    %2907 = torch.prim.ListConstruct %int1_4111, %int1_4112, %int4_4113 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2908 = torch.aten.view %2906, %2907 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_4114 = torch.constant.none
    %2909 = torch.aten.clone %122, %none_4114 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_4115 = torch.constant.int 1
    %int1_4116 = torch.constant.int 1
    %int1_4117 = torch.constant.int 1
    %2910 = torch.prim.ListConstruct %int1_4115, %int1_4116, %int1_4117 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2911 = torch.aten.view %2909, %2910 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_4118 = torch.constant.int 22
    %2912 = torch.aten.mul.Scalar %2902, %int22_4118 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int7 = torch.constant.int 7
    %int1_4119 = torch.constant.int 1
    %2913 = torch.aten.add.Scalar %2912, %int7, %int1_4119 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_4120 = torch.constant.int 2
    %2914 = torch.aten.mul.Scalar %2913, %int2_4120 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_4121 = torch.constant.int 1
    %2915 = torch.aten.add.Tensor %2914, %2911, %int1_4121 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_4122 = torch.constant.int 4
    %2916 = torch.aten.mul.Scalar %2915, %int4_4122 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_4123 = torch.constant.int 1
    %2917 = torch.aten.add.Tensor %2916, %2908, %int1_4123 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_4124 = torch.constant.int 32
    %2918 = torch.aten.mul.Scalar %2917, %int32_4124 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_4125 = torch.constant.int 1
    %2919 = torch.aten.add.Tensor %2918, %2905, %int1_4125 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_4126 = torch.constant.none
    %none_4127 = torch.constant.none
    %int5_4128 = torch.constant.int 5
    %cpu_4129 = torch.constant.device "cpu"
    %int0_4130 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2897, %none_4126, %none_4127, %int5_4128, %cpu_4129, %int0_4130 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_4131 = torch.constant.int 22
    %int2_4132 = torch.constant.int 2
    %int4_4133 = torch.constant.int 4
    %int32_4134 = torch.constant.int 32
    %int64_4135 = torch.constant.int 64
    %2920 = torch.prim.ListConstruct %381, %int22_4131, %int2_4132, %int4_4133, %int32_4134, %int64_4135 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2921 = torch.aten.view %2615, %2920 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2921, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_4136 = torch.constant.int 64
    %2922 = torch.prim.ListConstruct %553, %int64_4136 : (!torch.int, !torch.int) -> !torch.list<int>
    %2923 = torch.aten.view %2921, %2922 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %2923, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %2924 = torch.prim.ListConstruct %2919 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_4137 = torch.constant.bool false
    %2925 = torch.aten.index_put %2923, %2924, %2897, %false_4137 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %2925, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_4138 = torch.constant.int 22
    %int2_4139 = torch.constant.int 2
    %int4_4140 = torch.constant.int 4
    %int32_4141 = torch.constant.int 32
    %int64_4142 = torch.constant.int 64
    %2926 = torch.prim.ListConstruct %381, %int22_4138, %int2_4139, %int4_4140, %int32_4141, %int64_4142 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2927 = torch.aten.view %2925, %2926 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2927, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_4143 = torch.constant.int 360448
    %2928 = torch.prim.ListConstruct %381, %int360448_4143 : (!torch.int, !torch.int) -> !torch.list<int>
    %2929 = torch.aten.view %2927, %2928 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %2929, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_4144 = torch.constant.int 22
    %int2_4145 = torch.constant.int 2
    %int4_4146 = torch.constant.int 4
    %int32_4147 = torch.constant.int 32
    %int64_4148 = torch.constant.int 64
    %2930 = torch.prim.ListConstruct %381, %int22_4144, %int2_4145, %int4_4146, %int32_4147, %int64_4148 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2931 = torch.aten.view %2929, %2930 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2931, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_4149 = torch.constant.int 64
    %2932 = torch.prim.ListConstruct %553, %int64_4149 : (!torch.int, !torch.int) -> !torch.list<int>
    %2933 = torch.aten.view %2931, %2932 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %2933, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_4150 = torch.constant.none
    %2934 = torch.aten.clone %123, %none_4150 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_4151 = torch.constant.int 1
    %int1_4152 = torch.constant.int 1
    %int1_4153 = torch.constant.int 1
    %2935 = torch.prim.ListConstruct %int1_4151, %int1_4152, %int1_4153 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2936 = torch.aten.view %2934, %2935 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_4154 = torch.constant.int 22
    %2937 = torch.aten.mul.Scalar %2902, %int22_4154 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int7_4155 = torch.constant.int 7
    %int1_4156 = torch.constant.int 1
    %2938 = torch.aten.add.Scalar %2937, %int7_4155, %int1_4156 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_4157 = torch.constant.int 2
    %2939 = torch.aten.mul.Scalar %2938, %int2_4157 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_4158 = torch.constant.int 1
    %2940 = torch.aten.add.Tensor %2939, %2936, %int1_4158 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_4159 = torch.constant.int 4
    %2941 = torch.aten.mul.Scalar %2940, %int4_4159 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_4160 = torch.constant.int 1
    %2942 = torch.aten.add.Tensor %2941, %2908, %int1_4160 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_4161 = torch.constant.int 32
    %2943 = torch.aten.mul.Scalar %2942, %int32_4161 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_4162 = torch.constant.int 1
    %2944 = torch.aten.add.Tensor %2943, %2905, %int1_4162 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_4163 = torch.constant.none
    %none_4164 = torch.constant.none
    %int5_4165 = torch.constant.int 5
    %cpu_4166 = torch.constant.device "cpu"
    %int0_4167 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2815, %none_4163, %none_4164, %int5_4165, %cpu_4166, %int0_4167 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %2945 = torch.prim.ListConstruct %2944 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_4168 = torch.constant.bool false
    %2946 = torch.aten.index_put %2933, %2945, %2815, %false_4168 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %2946, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_4169 = torch.constant.int 22
    %int2_4170 = torch.constant.int 2
    %int4_4171 = torch.constant.int 4
    %int32_4172 = torch.constant.int 32
    %int64_4173 = torch.constant.int 64
    %2947 = torch.prim.ListConstruct %381, %int22_4169, %int2_4170, %int4_4171, %int32_4172, %int64_4173 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2948 = torch.aten.view %2946, %2947 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2948, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_4174 = torch.constant.int 360448
    %2949 = torch.prim.ListConstruct %381, %int360448_4174 : (!torch.int, !torch.int) -> !torch.list<int>
    %2950 = torch.aten.view %2948, %2949 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %2950, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_4175 = torch.constant.none
    %2951 = torch.aten.clone %124, %none_4175 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_4176 = torch.constant.none
    %2952 = torch.aten.clone %125, %none_4176 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_4177 = torch.constant.none
    %2953 = torch.aten.clone %126, %none_4177 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_4178 = torch.constant.int 22
    %int2_4179 = torch.constant.int 2
    %int4_4180 = torch.constant.int 4
    %int32_4181 = torch.constant.int 32
    %int64_4182 = torch.constant.int 64
    %2954 = torch.prim.ListConstruct %381, %int22_4178, %int2_4179, %int4_4180, %int32_4181, %int64_4182 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2955 = torch.aten.view %2950, %2954 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %2955, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %2956 = torch_c.to_builtin_tensor %2955 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %2957 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_4183 = tensor.cast %2957 : tensor<4x?xi64> to tensor<?x?xi64>
    %2958 = torch_c.to_builtin_tensor %2951 : !torch.vtensor<[],si64> -> tensor<i64>
    %2959 = torch_c.to_builtin_tensor %2952 : !torch.vtensor<[],si64> -> tensor<i64>
    %2960 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%2956, %cast_4183, %2958, %2959) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_4184 = tensor.cast %2960 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %2961 = torch_c.from_builtin_tensor %cast_4184 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %2961, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %2962 = torch_c.to_builtin_tensor %2955 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %2963 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_4185 = tensor.cast %2963 : tensor<4x?xi64> to tensor<?x?xi64>
    %2964 = torch_c.to_builtin_tensor %2951 : !torch.vtensor<[],si64> -> tensor<i64>
    %2965 = torch_c.to_builtin_tensor %2953 : !torch.vtensor<[],si64> -> tensor<i64>
    %2966 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%2962, %cast_4185, %2964, %2965) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_4186 = tensor.cast %2966 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %2967 = torch_c.from_builtin_tensor %cast_4186 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %2967, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_4187 = torch.constant.int 2
    %int3_4188 = torch.constant.int 3
    %2968 = torch.aten.transpose.int %2961, %int2_4187, %int3_4188 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %2968, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_4189 = torch.constant.int 0
    %2969 = torch.aten.clone %2968, %int0_4189 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %2969, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_4190 = torch.constant.int 4
    %int4_4191 = torch.constant.int 4
    %int64_4192 = torch.constant.int 64
    %2970 = torch.prim.ListConstruct %int4_4190, %623, %int4_4191, %int64_4192 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2971 = torch.aten._unsafe_view %2969, %2970 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %2971, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_4193 = torch.constant.int 2
    %int3_4194 = torch.constant.int 3
    %2972 = torch.aten.transpose.int %2967, %int2_4193, %int3_4194 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %2972, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_4195 = torch.constant.int 0
    %2973 = torch.aten.clone %2972, %int0_4195 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %2973, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_4196 = torch.constant.int 4
    %int4_4197 = torch.constant.int 4
    %int64_4198 = torch.constant.int 64
    %2974 = torch.prim.ListConstruct %int4_4196, %623, %int4_4197, %int64_4198 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2975 = torch.aten._unsafe_view %2973, %2974 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %2975, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_4199 = torch.constant.int 0
    %int1_4200 = torch.constant.int 1
    %none_4201 = torch.constant.none
    %none_4202 = torch.constant.none
    %cpu_4203 = torch.constant.device "cpu"
    %false_4204 = torch.constant.bool false
    %2976 = torch.aten.arange.start_step %int0_4199, %623, %int1_4200, %none_4201, %none_4202, %cpu_4203, %false_4204 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2976, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_4205 = torch.constant.int -1
    %2977 = torch.aten.unsqueeze %arg1, %int-1_4205 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %2978 = torch.aten.ge.Tensor %2976, %2977 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %2978, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_4206 = torch.constant.none
    %2979 = torch.aten.clone %127, %none_4206 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_4207 = torch.constant.int 0
    %2980 = torch.aten.where.ScalarOther %2978, %2979, %int0_4207 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %2980, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_4208 = torch.constant.none
    %none_4209 = torch.constant.none
    %int5_4210 = torch.constant.int 5
    %cpu_4211 = torch.constant.device "cpu"
    %int0_4212 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %2980, %none_4208, %none_4209, %int5_4210, %cpu_4211, %int0_4212 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_4213 = torch.constant.int 1
    %2981 = torch.aten.unsqueeze %2980, %int1_4213 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %2981, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_4214 = torch.constant.int 1
    %2982 = torch.aten.unsqueeze %2981, %int1_4214 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %2982, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_4215 = torch.constant.int -2
    %2983 = torch.aten.unsqueeze %2971, %int-2_4215 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %2983, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_4216 = torch.constant.int 4
    %int4_4217 = torch.constant.int 4
    %int8_4218 = torch.constant.int 8
    %int64_4219 = torch.constant.int 64
    %2984 = torch.prim.ListConstruct %int4_4216, %623, %int4_4217, %int8_4218, %int64_4219 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4220 = torch.constant.bool false
    %2985 = torch.aten.expand %2983, %2984, %false_4220 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2985, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_4221 = torch.constant.int 0
    %2986 = torch.aten.clone %2985, %int0_4221 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2986, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_4222 = torch.constant.int 4
    %int32_4223 = torch.constant.int 32
    %int64_4224 = torch.constant.int 64
    %2987 = torch.prim.ListConstruct %int4_4222, %623, %int32_4223, %int64_4224 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2988 = torch.aten._unsafe_view %2986, %2987 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2988, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_4225 = torch.constant.int -2
    %2989 = torch.aten.unsqueeze %2975, %int-2_4225 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %2989, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_4226 = torch.constant.int 4
    %int4_4227 = torch.constant.int 4
    %int8_4228 = torch.constant.int 8
    %int64_4229 = torch.constant.int 64
    %2990 = torch.prim.ListConstruct %int4_4226, %623, %int4_4227, %int8_4228, %int64_4229 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4230 = torch.constant.bool false
    %2991 = torch.aten.expand %2989, %2990, %false_4230 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2991, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_4231 = torch.constant.int 0
    %2992 = torch.aten.clone %2991, %int0_4231 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %2992, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_4232 = torch.constant.int 4
    %int32_4233 = torch.constant.int 32
    %int64_4234 = torch.constant.int 64
    %2993 = torch.prim.ListConstruct %int4_4232, %623, %int32_4233, %int64_4234 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2994 = torch.aten._unsafe_view %2992, %2993 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %2994, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_4235 = torch.constant.int 1
    %int2_4236 = torch.constant.int 2
    %2995 = torch.aten.transpose.int %2856, %int1_4235, %int2_4236 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_4237 = torch.constant.int 1
    %int2_4238 = torch.constant.int 2
    %2996 = torch.aten.transpose.int %2988, %int1_4237, %int2_4238 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2996, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_4239 = torch.constant.int 1
    %int2_4240 = torch.constant.int 2
    %2997 = torch.aten.transpose.int %2994, %int1_4239, %int2_4240 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %2997, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_4241 = torch.constant.float 0.000000e+00
    %false_4242 = torch.constant.bool false
    %none_4243 = torch.constant.none
    %false_4244 = torch.constant.bool false
    %2998 = torch.aten.scaled_dot_product_attention %2995, %2996, %2997, %2982, %float0.000000e00_4241, %false_4242, %none_4243, %false_4244 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_4245 = torch.constant.int 1
    %int2_4246 = torch.constant.int 2
    %2999 = torch.aten.transpose.int %2998, %int1_4245, %int2_4246 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_4247 = torch.constant.int 4
    %int1_4248 = torch.constant.int 1
    %int2048_4249 = torch.constant.int 2048
    %3000 = torch.prim.ListConstruct %int4_4247, %int1_4248, %int2048_4249 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3001 = torch.aten.view %2999, %3000 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_4250 = torch.constant.int 2
    %3002 = torch.aten.view.dtype %132, %int2_4250 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %3003 = torch.aten.detach %3002 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_4251 = torch.constant.int -1
    %int17_4252 = torch.constant.int 17
    %3004 = torch.prim.ListConstruct %int-1_4251, %int17_4252 : (!torch.int, !torch.int) -> !torch.list<int>
    %3005 = torch.aten.view %3003, %3004 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_4253 = torch.constant.int 2048
    %int-1_4254 = torch.constant.int -1
    %int17_4255 = torch.constant.int 17
    %3006 = torch.prim.ListConstruct %int2048_4253, %int-1_4254, %int17_4255 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3007 = torch.aten.view %3005, %3006 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_4256 = torch.constant.int 2
    %int0_4257 = torch.constant.int 0
    %int1_4258 = torch.constant.int 1
    %int1_4259 = torch.constant.int 1
    %3008 = torch.aten.slice.Tensor %3007, %int2_4256, %int0_4257, %int1_4258, %int1_4259 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_4260 = torch.constant.int 5
    %3009 = torch.aten.view.dtype %3008, %int5_4260 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %3010 = torch.aten.detach %3009 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_4261 = torch.constant.int 2
    %int1_4262 = torch.constant.int 1
    %int9223372036854775807_4263 = torch.constant.int 9223372036854775807
    %int1_4264 = torch.constant.int 1
    %3011 = torch.aten.slice.Tensor %3007, %int2_4261, %int1_4262, %int9223372036854775807_4263, %int1_4264 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_4265 = torch.constant.int 1
    %3012 = torch.aten.view.dtype %3011, %int1_4265 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %3013 = torch.aten.detach %3012 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %3014 = torch_c.to_builtin_tensor %3001 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_4266 = tensor.cast %3014 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %3015 = torch_c.to_builtin_tensor %3010 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %3016 = torch_c.to_builtin_tensor %3013 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %3017 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_4266, %3015, %3016) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_4267 = tensor.cast %3017 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %3018 = torch_c.from_builtin_tensor %cast_4267 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_4268 = torch.constant.none
    %none_4269 = torch.constant.none
    %int5_4270 = torch.constant.int 5
    %cpu_4271 = torch.constant.device "cpu"
    %int0_4272 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3018, %none_4268, %none_4269, %int5_4270, %cpu_4271, %int0_4272 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_4273 = torch.constant.int 1
    %3019 = torch.aten.add.Tensor %2748, %3018, %int1_4273 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_4274 = torch.constant.none
    %none_4275 = torch.constant.none
    %int5_4276 = torch.constant.int 5
    %cpu_4277 = torch.constant.device "cpu"
    %int0_4278 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3019, %none_4274, %none_4275, %int5_4276, %cpu_4277, %int0_4278 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4279 = torch.constant.int 6
    %3020 = torch.prims.convert_element_type %3019, %int6_4279 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_4280 = torch.constant.int 2
    %3021 = torch.aten.pow.Tensor_Scalar %3020, %int2_4280 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_4281 = torch.constant.int -1
    %3022 = torch.prim.ListConstruct %int-1_4281 : (!torch.int) -> !torch.list<int>
    %true_4282 = torch.constant.bool true
    %none_4283 = torch.constant.none
    %3023 = torch.aten.mean.dim %3021, %3022, %true_4282, %none_4283 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_4284 = torch.constant.float 9.9999997473787516E-6
    %int1_4285 = torch.constant.int 1
    %3024 = torch.aten.add.Scalar %3023, %float9.999990e-06_4284, %int1_4285 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3025 = torch.aten.rsqrt %3024 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %3026 = torch.aten.mul.Tensor %3020, %3025 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_4286 = torch.constant.none
    %none_4287 = torch.constant.none
    %int6_4288 = torch.constant.int 6
    %cpu_4289 = torch.constant.device "cpu"
    %int0_4290 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3026, %none_4286, %none_4287, %int6_4288, %cpu_4289, %int0_4290 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4291 = torch.constant.int 5
    %3027 = torch.prims.convert_element_type %3026, %int5_4291 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %3028 = torch.aten.mul.Tensor %133, %3027 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_4292 = torch.constant.none
    %none_4293 = torch.constant.none
    %int6_4294 = torch.constant.int 6
    %cpu_4295 = torch.constant.device "cpu"
    %int0_4296 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3028, %none_4292, %none_4293, %int6_4294, %cpu_4295, %int0_4296 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4297 = torch.constant.int 5
    %3029 = torch.prims.convert_element_type %3028, %int5_4297 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_4298 = torch.constant.int 2
    %3030 = torch.aten.view.dtype %134, %int2_4298 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %3031 = torch.aten.detach %3030 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_4299 = torch.constant.int -1
    %int17_4300 = torch.constant.int 17
    %3032 = torch.prim.ListConstruct %int-1_4299, %int17_4300 : (!torch.int, !torch.int) -> !torch.list<int>
    %3033 = torch.aten.view %3031, %3032 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_4301 = torch.constant.int 5632
    %int-1_4302 = torch.constant.int -1
    %int17_4303 = torch.constant.int 17
    %3034 = torch.prim.ListConstruct %int5632_4301, %int-1_4302, %int17_4303 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3035 = torch.aten.view %3033, %3034 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_4304 = torch.constant.int 2
    %int0_4305 = torch.constant.int 0
    %int1_4306 = torch.constant.int 1
    %int1_4307 = torch.constant.int 1
    %3036 = torch.aten.slice.Tensor %3035, %int2_4304, %int0_4305, %int1_4306, %int1_4307 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_4308 = torch.constant.int 5
    %3037 = torch.aten.view.dtype %3036, %int5_4308 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %3038 = torch.aten.detach %3037 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_4309 = torch.constant.int 2
    %int1_4310 = torch.constant.int 1
    %int9223372036854775807_4311 = torch.constant.int 9223372036854775807
    %int1_4312 = torch.constant.int 1
    %3039 = torch.aten.slice.Tensor %3035, %int2_4309, %int1_4310, %int9223372036854775807_4311, %int1_4312 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_4313 = torch.constant.int 1
    %3040 = torch.aten.view.dtype %3039, %int1_4313 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %3041 = torch.aten.detach %3040 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %3042 = torch_c.to_builtin_tensor %3029 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_4314 = tensor.cast %3042 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %3043 = torch_c.to_builtin_tensor %3038 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %3044 = torch_c.to_builtin_tensor %3041 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %3045 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_4314, %3043, %3044) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_4315 = tensor.cast %3045 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %3046 = torch_c.from_builtin_tensor %cast_4315 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %3047 = torch.aten.silu %3046 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_4316 = torch.constant.int 2
    %3048 = torch.aten.view.dtype %135, %int2_4316 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %3049 = torch.aten.detach %3048 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_4317 = torch.constant.int -1
    %int17_4318 = torch.constant.int 17
    %3050 = torch.prim.ListConstruct %int-1_4317, %int17_4318 : (!torch.int, !torch.int) -> !torch.list<int>
    %3051 = torch.aten.view %3049, %3050 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_4319 = torch.constant.int 5632
    %int-1_4320 = torch.constant.int -1
    %int17_4321 = torch.constant.int 17
    %3052 = torch.prim.ListConstruct %int5632_4319, %int-1_4320, %int17_4321 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3053 = torch.aten.view %3051, %3052 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_4322 = torch.constant.int 2
    %int0_4323 = torch.constant.int 0
    %int1_4324 = torch.constant.int 1
    %int1_4325 = torch.constant.int 1
    %3054 = torch.aten.slice.Tensor %3053, %int2_4322, %int0_4323, %int1_4324, %int1_4325 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_4326 = torch.constant.int 5
    %3055 = torch.aten.view.dtype %3054, %int5_4326 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %3056 = torch.aten.detach %3055 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_4327 = torch.constant.int 2
    %int1_4328 = torch.constant.int 1
    %int9223372036854775807_4329 = torch.constant.int 9223372036854775807
    %int1_4330 = torch.constant.int 1
    %3057 = torch.aten.slice.Tensor %3053, %int2_4327, %int1_4328, %int9223372036854775807_4329, %int1_4330 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_4331 = torch.constant.int 1
    %3058 = torch.aten.view.dtype %3057, %int1_4331 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %3059 = torch.aten.detach %3058 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %3060 = torch_c.to_builtin_tensor %3029 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_4332 = tensor.cast %3060 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %3061 = torch_c.to_builtin_tensor %3056 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %3062 = torch_c.to_builtin_tensor %3059 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %3063 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_4332, %3061, %3062) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_4333 = tensor.cast %3063 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %3064 = torch_c.from_builtin_tensor %cast_4333 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %3065 = torch.aten.mul.Tensor %3047, %3064 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_4334 = torch.constant.int 2
    %3066 = torch.aten.view.dtype %136, %int2_4334 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %3067 = torch.aten.detach %3066 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_4335 = torch.constant.int -1
    %int17_4336 = torch.constant.int 17
    %3068 = torch.prim.ListConstruct %int-1_4335, %int17_4336 : (!torch.int, !torch.int) -> !torch.list<int>
    %3069 = torch.aten.view %3067, %3068 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_4337 = torch.constant.int 2048
    %int-1_4338 = torch.constant.int -1
    %int17_4339 = torch.constant.int 17
    %3070 = torch.prim.ListConstruct %int2048_4337, %int-1_4338, %int17_4339 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3071 = torch.aten.view %3069, %3070 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_4340 = torch.constant.int 2
    %int0_4341 = torch.constant.int 0
    %int1_4342 = torch.constant.int 1
    %int1_4343 = torch.constant.int 1
    %3072 = torch.aten.slice.Tensor %3071, %int2_4340, %int0_4341, %int1_4342, %int1_4343 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_4344 = torch.constant.int 5
    %3073 = torch.aten.view.dtype %3072, %int5_4344 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %3074 = torch.aten.detach %3073 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_4345 = torch.constant.int 2
    %int1_4346 = torch.constant.int 1
    %int9223372036854775807_4347 = torch.constant.int 9223372036854775807
    %int1_4348 = torch.constant.int 1
    %3075 = torch.aten.slice.Tensor %3071, %int2_4345, %int1_4346, %int9223372036854775807_4347, %int1_4348 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_4349 = torch.constant.int 1
    %3076 = torch.aten.view.dtype %3075, %int1_4349 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %3077 = torch.aten.detach %3076 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %3078 = torch_c.to_builtin_tensor %3065 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_4350 = tensor.cast %3078 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %3079 = torch_c.to_builtin_tensor %3074 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %3080 = torch_c.to_builtin_tensor %3077 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %3081 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_4350, %3079, %3080) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_4351 = tensor.cast %3081 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %3082 = torch_c.from_builtin_tensor %cast_4351 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_4352 = torch.constant.int 1
    %3083 = torch.aten.add.Tensor %3019, %3082, %int1_4352 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_4353 = torch.constant.none
    %none_4354 = torch.constant.none
    %int5_4355 = torch.constant.int 5
    %cpu_4356 = torch.constant.device "cpu"
    %int0_4357 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3083, %none_4353, %none_4354, %int5_4355, %cpu_4356, %int0_4357 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4358 = torch.constant.int 6
    %3084 = torch.prims.convert_element_type %3083, %int6_4358 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_4359 = torch.constant.int 2
    %3085 = torch.aten.pow.Tensor_Scalar %3084, %int2_4359 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_4360 = torch.constant.int -1
    %3086 = torch.prim.ListConstruct %int-1_4360 : (!torch.int) -> !torch.list<int>
    %true_4361 = torch.constant.bool true
    %none_4362 = torch.constant.none
    %3087 = torch.aten.mean.dim %3085, %3086, %true_4361, %none_4362 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_4363 = torch.constant.float 9.9999997473787516E-6
    %int1_4364 = torch.constant.int 1
    %3088 = torch.aten.add.Scalar %3087, %float9.999990e-06_4363, %int1_4364 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3089 = torch.aten.rsqrt %3088 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %3090 = torch.aten.mul.Tensor %3084, %3089 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_4365 = torch.constant.none
    %none_4366 = torch.constant.none
    %int6_4367 = torch.constant.int 6
    %cpu_4368 = torch.constant.device "cpu"
    %int0_4369 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3090, %none_4365, %none_4366, %int6_4367, %cpu_4368, %int0_4369 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4370 = torch.constant.int 5
    %3091 = torch.prims.convert_element_type %3090, %int5_4370 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %3092 = torch.aten.mul.Tensor %145, %3091 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_4371 = torch.constant.none
    %none_4372 = torch.constant.none
    %int6_4373 = torch.constant.int 6
    %cpu_4374 = torch.constant.device "cpu"
    %int0_4375 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3092, %none_4371, %none_4372, %int6_4373, %cpu_4374, %int0_4375 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4376 = torch.constant.int 5
    %3093 = torch.prims.convert_element_type %3092, %int5_4376 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_4377 = torch.constant.int 2
    %3094 = torch.aten.view.dtype %146, %int2_4377 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %3095 = torch.aten.detach %3094 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_4378 = torch.constant.int -1
    %int17_4379 = torch.constant.int 17
    %3096 = torch.prim.ListConstruct %int-1_4378, %int17_4379 : (!torch.int, !torch.int) -> !torch.list<int>
    %3097 = torch.aten.view %3095, %3096 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_4380 = torch.constant.int 2048
    %int-1_4381 = torch.constant.int -1
    %int17_4382 = torch.constant.int 17
    %3098 = torch.prim.ListConstruct %int2048_4380, %int-1_4381, %int17_4382 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3099 = torch.aten.view %3097, %3098 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_4383 = torch.constant.int 2
    %int0_4384 = torch.constant.int 0
    %int1_4385 = torch.constant.int 1
    %int1_4386 = torch.constant.int 1
    %3100 = torch.aten.slice.Tensor %3099, %int2_4383, %int0_4384, %int1_4385, %int1_4386 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_4387 = torch.constant.int 5
    %3101 = torch.aten.view.dtype %3100, %int5_4387 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %3102 = torch.aten.detach %3101 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_4388 = torch.constant.int 2
    %int1_4389 = torch.constant.int 1
    %int9223372036854775807_4390 = torch.constant.int 9223372036854775807
    %int1_4391 = torch.constant.int 1
    %3103 = torch.aten.slice.Tensor %3099, %int2_4388, %int1_4389, %int9223372036854775807_4390, %int1_4391 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_4392 = torch.constant.int 1
    %3104 = torch.aten.view.dtype %3103, %int1_4392 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %3105 = torch.aten.detach %3104 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %3106 = torch_c.to_builtin_tensor %3093 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_4393 = tensor.cast %3106 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %3107 = torch_c.to_builtin_tensor %3102 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %3108 = torch_c.to_builtin_tensor %3105 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %3109 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_4393, %3107, %3108) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_4394 = tensor.cast %3109 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %3110 = torch_c.from_builtin_tensor %cast_4394 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_4395 = torch.constant.int 2
    %3111 = torch.aten.view.dtype %147, %int2_4395 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %3112 = torch.aten.detach %3111 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_4396 = torch.constant.int -1
    %int17_4397 = torch.constant.int 17
    %3113 = torch.prim.ListConstruct %int-1_4396, %int17_4397 : (!torch.int, !torch.int) -> !torch.list<int>
    %3114 = torch.aten.view %3112, %3113 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_4398 = torch.constant.int 256
    %int-1_4399 = torch.constant.int -1
    %int17_4400 = torch.constant.int 17
    %3115 = torch.prim.ListConstruct %int256_4398, %int-1_4399, %int17_4400 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3116 = torch.aten.view %3114, %3115 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_4401 = torch.constant.int 2
    %int0_4402 = torch.constant.int 0
    %int1_4403 = torch.constant.int 1
    %int1_4404 = torch.constant.int 1
    %3117 = torch.aten.slice.Tensor %3116, %int2_4401, %int0_4402, %int1_4403, %int1_4404 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_4405 = torch.constant.int 5
    %3118 = torch.aten.view.dtype %3117, %int5_4405 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %3119 = torch.aten.detach %3118 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_4406 = torch.constant.int 2
    %int1_4407 = torch.constant.int 1
    %int9223372036854775807_4408 = torch.constant.int 9223372036854775807
    %int1_4409 = torch.constant.int 1
    %3120 = torch.aten.slice.Tensor %3116, %int2_4406, %int1_4407, %int9223372036854775807_4408, %int1_4409 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_4410 = torch.constant.int 1
    %3121 = torch.aten.view.dtype %3120, %int1_4410 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %3122 = torch.aten.detach %3121 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %3123 = torch_c.to_builtin_tensor %3093 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_4411 = tensor.cast %3123 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %3124 = torch_c.to_builtin_tensor %3119 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %3125 = torch_c.to_builtin_tensor %3122 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %3126 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_4411, %3124, %3125) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_4412 = tensor.cast %3126 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %3127 = torch_c.from_builtin_tensor %cast_4412 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_4413 = torch.constant.int 2
    %3128 = torch.aten.view.dtype %148, %int2_4413 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %3129 = torch.aten.detach %3128 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_4414 = torch.constant.int -1
    %int17_4415 = torch.constant.int 17
    %3130 = torch.prim.ListConstruct %int-1_4414, %int17_4415 : (!torch.int, !torch.int) -> !torch.list<int>
    %3131 = torch.aten.view %3129, %3130 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_4416 = torch.constant.int 256
    %int-1_4417 = torch.constant.int -1
    %int17_4418 = torch.constant.int 17
    %3132 = torch.prim.ListConstruct %int256_4416, %int-1_4417, %int17_4418 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3133 = torch.aten.view %3131, %3132 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_4419 = torch.constant.int 2
    %int0_4420 = torch.constant.int 0
    %int1_4421 = torch.constant.int 1
    %int1_4422 = torch.constant.int 1
    %3134 = torch.aten.slice.Tensor %3133, %int2_4419, %int0_4420, %int1_4421, %int1_4422 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_4423 = torch.constant.int 5
    %3135 = torch.aten.view.dtype %3134, %int5_4423 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %3136 = torch.aten.detach %3135 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_4424 = torch.constant.int 2
    %int1_4425 = torch.constant.int 1
    %int9223372036854775807_4426 = torch.constant.int 9223372036854775807
    %int1_4427 = torch.constant.int 1
    %3137 = torch.aten.slice.Tensor %3133, %int2_4424, %int1_4425, %int9223372036854775807_4426, %int1_4427 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_4428 = torch.constant.int 1
    %3138 = torch.aten.view.dtype %3137, %int1_4428 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %3139 = torch.aten.detach %3138 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %3140 = torch_c.to_builtin_tensor %3093 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_4429 = tensor.cast %3140 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %3141 = torch_c.to_builtin_tensor %3136 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %3142 = torch_c.to_builtin_tensor %3139 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %3143 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_4429, %3141, %3142) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_4430 = tensor.cast %3143 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %3144 = torch_c.from_builtin_tensor %cast_4430 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_4431 = torch.constant.int 4
    %int1_4432 = torch.constant.int 1
    %int32_4433 = torch.constant.int 32
    %int64_4434 = torch.constant.int 64
    %3145 = torch.prim.ListConstruct %int4_4431, %int1_4432, %int32_4433, %int64_4434 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3146 = torch.aten.view %3110, %3145 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_4435 = torch.constant.int 4
    %int1_4436 = torch.constant.int 1
    %int4_4437 = torch.constant.int 4
    %int64_4438 = torch.constant.int 64
    %3147 = torch.prim.ListConstruct %int4_4435, %int1_4436, %int4_4437, %int64_4438 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3148 = torch.aten.view %3127, %3147 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_4439 = torch.constant.int 4
    %int1_4440 = torch.constant.int 1
    %int4_4441 = torch.constant.int 4
    %int64_4442 = torch.constant.int 64
    %3149 = torch.prim.ListConstruct %int4_4439, %int1_4440, %int4_4441, %int64_4442 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3150 = torch.aten.view %3144, %3149 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_4443 = torch.constant.int 0
    %int1_4444 = torch.constant.int 1
    %none_4445 = torch.constant.none
    %none_4446 = torch.constant.none
    %cpu_4447 = torch.constant.device "cpu"
    %false_4448 = torch.constant.bool false
    %3151 = torch.aten.arange.start %int0_4443, %int1_4444, %none_4445, %none_4446, %cpu_4447, %false_4448 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_4449 = torch.constant.int 0
    %3152 = torch.aten.unsqueeze %3151, %int0_4449 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_4450 = torch.constant.int 1
    %3153 = torch.aten.unsqueeze %arg2, %int1_4450 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4451 = torch.constant.int 1
    %3154 = torch.aten.add.Tensor %3152, %3153, %int1_4451 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_4452 = torch.constant.int 0
    %int64_4453 = torch.constant.int 64
    %int2_4454 = torch.constant.int 2
    %none_4455 = torch.constant.none
    %none_4456 = torch.constant.none
    %cpu_4457 = torch.constant.device "cpu"
    %false_4458 = torch.constant.bool false
    %3155 = torch.aten.arange.start_step %int0_4452, %int64_4453, %int2_4454, %none_4455, %none_4456, %cpu_4457, %false_4458 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_4459 = torch.constant.none
    %none_4460 = torch.constant.none
    %int4_4461 = torch.constant.int 4
    %cpu_4462 = torch.constant.device "cpu"
    %int0_4463 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3155, %none_4459, %none_4460, %int4_4461, %cpu_4462, %int0_4463 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4464 = torch.constant.int 6
    %3156 = torch.prims.convert_element_type %3155, %int6_4464 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_4465 = torch.constant.int 64
    %3157 = torch.aten.div.Scalar %3156, %int64_4465 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_4466 = torch.constant.float 1.000000e+04
    %3158 = torch.aten.pow.Scalar %float1.000000e04_4466, %3157 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %3159 = torch.aten.reciprocal %3158 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_4467 = torch.constant.float 1.000000e+00
    %3160 = torch.aten.mul.Scalar %3159, %float1.000000e00_4467 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_4468 = torch.constant.none
    %3161 = torch.aten.clone %137, %none_4468 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_4469 = torch.constant.int 0
    %3162 = torch.aten.unsqueeze %3160, %int0_4469 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_4470 = torch.constant.int 2
    %3163 = torch.aten.unsqueeze %3162, %int2_4470 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_4471 = torch.constant.none
    %none_4472 = torch.constant.none
    %int6_4473 = torch.constant.int 6
    %cpu_4474 = torch.constant.device "cpu"
    %int0_4475 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3163, %none_4471, %none_4472, %int6_4473, %cpu_4474, %int0_4475 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_4476 = torch.constant.int 4
    %int-1_4477 = torch.constant.int -1
    %int1_4478 = torch.constant.int 1
    %3164 = torch.prim.ListConstruct %int4_4476, %int-1_4477, %int1_4478 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4479 = torch.constant.bool false
    %3165 = torch.aten.expand %3163, %3164, %false_4479 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_4480 = torch.constant.int 1
    %3166 = torch.aten.unsqueeze %3154, %int1_4480 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_4481 = torch.constant.none
    %none_4482 = torch.constant.none
    %int4_4483 = torch.constant.int 4
    %cpu_4484 = torch.constant.device "cpu"
    %int0_4485 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3166, %none_4481, %none_4482, %int4_4483, %cpu_4484, %int0_4485 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4486 = torch.constant.int 6
    %3167 = torch.prims.convert_element_type %3166, %int6_4486 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3168 = torch.aten.matmul %3165, %3167 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_4487 = torch.constant.int 1
    %int2_4488 = torch.constant.int 2
    %3169 = torch.aten.transpose.int %3168, %int1_4487, %int2_4488 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %3170 = torch.aten.cos %3169 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %3171 = torch.aten.mul.Tensor %3170, %3161 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_4489 = torch.constant.none
    %none_4490 = torch.constant.none
    %int6_4491 = torch.constant.int 6
    %cpu_4492 = torch.constant.device "cpu"
    %int0_4493 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3171, %none_4489, %none_4490, %int6_4491, %cpu_4492, %int0_4493 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4494 = torch.constant.int 5
    %3172 = torch.prims.convert_element_type %3171, %int5_4494 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %3173 = torch.aten.sin %3169 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %3174 = torch.aten.mul.Tensor %3173, %3161 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_4495 = torch.constant.none
    %none_4496 = torch.constant.none
    %int6_4497 = torch.constant.int 6
    %cpu_4498 = torch.constant.device "cpu"
    %int0_4499 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3174, %none_4495, %none_4496, %int6_4497, %cpu_4498, %int0_4499 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4500 = torch.constant.int 5
    %3175 = torch.prims.convert_element_type %3174, %int5_4500 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_4501 = torch.constant.int 2
    %3176 = torch.aten.unsqueeze %3172, %int2_4501 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_4502 = torch.constant.int 2
    %3177 = torch.aten.unsqueeze %3175, %int2_4502 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_4503 = torch.constant.none
    %none_4504 = torch.constant.none
    %int5_4505 = torch.constant.int 5
    %cpu_4506 = torch.constant.device "cpu"
    %int0_4507 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3176, %none_4503, %none_4504, %int5_4505, %cpu_4506, %int0_4507 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_4508 = torch.constant.none
    %none_4509 = torch.constant.none
    %int5_4510 = torch.constant.int 5
    %cpu_4511 = torch.constant.device "cpu"
    %int0_4512 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3177, %none_4508, %none_4509, %int5_4510, %cpu_4511, %int0_4512 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_4513 = torch.constant.none
    %none_4514 = torch.constant.none
    %int5_4515 = torch.constant.int 5
    %cpu_4516 = torch.constant.device "cpu"
    %int0_4517 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3146, %none_4513, %none_4514, %int5_4515, %cpu_4516, %int0_4517 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_4518 = torch.constant.int 3
    %int0_4519 = torch.constant.int 0
    %int64_4520 = torch.constant.int 64
    %int2_4521 = torch.constant.int 2
    %3178 = torch.aten.slice.Tensor %3146, %int3_4518, %int0_4519, %int64_4520, %int2_4521 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_4522 = torch.constant.int 3
    %int1_4523 = torch.constant.int 1
    %int64_4524 = torch.constant.int 64
    %int2_4525 = torch.constant.int 2
    %3179 = torch.aten.slice.Tensor %3146, %int3_4522, %int1_4523, %int64_4524, %int2_4525 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %3180 = torch.aten.mul.Tensor %3178, %3176 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %3181 = torch.aten.mul.Tensor %3179, %3177 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_4526 = torch.constant.int 1
    %3182 = torch.aten.sub.Tensor %3180, %3181, %int1_4526 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %3183 = torch.aten.mul.Tensor %3179, %3176 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %3184 = torch.aten.mul.Tensor %3178, %3177 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_4527 = torch.constant.int 1
    %3185 = torch.aten.add.Tensor %3183, %3184, %int1_4527 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %3186 = torch_c.to_builtin_tensor %3182 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_4528 = tensor.cast %3186 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %3187 = torch_c.to_builtin_tensor %3185 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_4529 = tensor.cast %3187 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %3188 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_4528, %cast_4529) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_4530 = tensor.cast %3188 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %3189 = torch_c.from_builtin_tensor %cast_4530 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_4531 = torch.constant.int 4
    %int1_4532 = torch.constant.int 1
    %int32_4533 = torch.constant.int 32
    %int64_4534 = torch.constant.int 64
    %3190 = torch.prim.ListConstruct %int4_4531, %int1_4532, %int32_4533, %int64_4534 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3191 = torch.aten.view %3189, %3190 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_4535 = torch.constant.none
    %none_4536 = torch.constant.none
    %int5_4537 = torch.constant.int 5
    %cpu_4538 = torch.constant.device "cpu"
    %int0_4539 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3191, %none_4535, %none_4536, %int5_4537, %cpu_4538, %int0_4539 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_4540 = torch.constant.int 0
    %int1_4541 = torch.constant.int 1
    %none_4542 = torch.constant.none
    %none_4543 = torch.constant.none
    %cpu_4544 = torch.constant.device "cpu"
    %false_4545 = torch.constant.bool false
    %3192 = torch.aten.arange.start %int0_4540, %int1_4541, %none_4542, %none_4543, %cpu_4544, %false_4545 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_4546 = torch.constant.int 0
    %3193 = torch.aten.unsqueeze %3192, %int0_4546 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_4547 = torch.constant.int 1
    %3194 = torch.aten.unsqueeze %arg2, %int1_4547 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4548 = torch.constant.int 1
    %3195 = torch.aten.add.Tensor %3193, %3194, %int1_4548 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_4549 = torch.constant.int 0
    %int64_4550 = torch.constant.int 64
    %int2_4551 = torch.constant.int 2
    %none_4552 = torch.constant.none
    %none_4553 = torch.constant.none
    %cpu_4554 = torch.constant.device "cpu"
    %false_4555 = torch.constant.bool false
    %3196 = torch.aten.arange.start_step %int0_4549, %int64_4550, %int2_4551, %none_4552, %none_4553, %cpu_4554, %false_4555 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_4556 = torch.constant.none
    %none_4557 = torch.constant.none
    %int4_4558 = torch.constant.int 4
    %cpu_4559 = torch.constant.device "cpu"
    %int0_4560 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3196, %none_4556, %none_4557, %int4_4558, %cpu_4559, %int0_4560 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4561 = torch.constant.int 6
    %3197 = torch.prims.convert_element_type %3196, %int6_4561 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_4562 = torch.constant.int 64
    %3198 = torch.aten.div.Scalar %3197, %int64_4562 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_4563 = torch.constant.float 1.000000e+04
    %3199 = torch.aten.pow.Scalar %float1.000000e04_4563, %3198 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %3200 = torch.aten.reciprocal %3199 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_4564 = torch.constant.float 1.000000e+00
    %3201 = torch.aten.mul.Scalar %3200, %float1.000000e00_4564 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_4565 = torch.constant.none
    %3202 = torch.aten.clone %138, %none_4565 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_4566 = torch.constant.int 0
    %3203 = torch.aten.unsqueeze %3201, %int0_4566 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_4567 = torch.constant.int 2
    %3204 = torch.aten.unsqueeze %3203, %int2_4567 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_4568 = torch.constant.none
    %none_4569 = torch.constant.none
    %int6_4570 = torch.constant.int 6
    %cpu_4571 = torch.constant.device "cpu"
    %int0_4572 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3204, %none_4568, %none_4569, %int6_4570, %cpu_4571, %int0_4572 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_4573 = torch.constant.int 4
    %int-1_4574 = torch.constant.int -1
    %int1_4575 = torch.constant.int 1
    %3205 = torch.prim.ListConstruct %int4_4573, %int-1_4574, %int1_4575 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4576 = torch.constant.bool false
    %3206 = torch.aten.expand %3204, %3205, %false_4576 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_4577 = torch.constant.int 1
    %3207 = torch.aten.unsqueeze %3195, %int1_4577 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_4578 = torch.constant.none
    %none_4579 = torch.constant.none
    %int4_4580 = torch.constant.int 4
    %cpu_4581 = torch.constant.device "cpu"
    %int0_4582 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3207, %none_4578, %none_4579, %int4_4580, %cpu_4581, %int0_4582 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4583 = torch.constant.int 6
    %3208 = torch.prims.convert_element_type %3207, %int6_4583 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3209 = torch.aten.matmul %3206, %3208 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_4584 = torch.constant.int 1
    %int2_4585 = torch.constant.int 2
    %3210 = torch.aten.transpose.int %3209, %int1_4584, %int2_4585 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %3211 = torch.aten.cos %3210 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %3212 = torch.aten.mul.Tensor %3211, %3202 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_4586 = torch.constant.none
    %none_4587 = torch.constant.none
    %int6_4588 = torch.constant.int 6
    %cpu_4589 = torch.constant.device "cpu"
    %int0_4590 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3212, %none_4586, %none_4587, %int6_4588, %cpu_4589, %int0_4590 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4591 = torch.constant.int 5
    %3213 = torch.prims.convert_element_type %3212, %int5_4591 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %3214 = torch.aten.sin %3210 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %3215 = torch.aten.mul.Tensor %3214, %3202 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_4592 = torch.constant.none
    %none_4593 = torch.constant.none
    %int6_4594 = torch.constant.int 6
    %cpu_4595 = torch.constant.device "cpu"
    %int0_4596 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3215, %none_4592, %none_4593, %int6_4594, %cpu_4595, %int0_4596 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4597 = torch.constant.int 5
    %3216 = torch.prims.convert_element_type %3215, %int5_4597 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_4598 = torch.constant.int 2
    %3217 = torch.aten.unsqueeze %3213, %int2_4598 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_4599 = torch.constant.int 2
    %3218 = torch.aten.unsqueeze %3216, %int2_4599 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_4600 = torch.constant.none
    %none_4601 = torch.constant.none
    %int5_4602 = torch.constant.int 5
    %cpu_4603 = torch.constant.device "cpu"
    %int0_4604 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3217, %none_4600, %none_4601, %int5_4602, %cpu_4603, %int0_4604 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_4605 = torch.constant.none
    %none_4606 = torch.constant.none
    %int5_4607 = torch.constant.int 5
    %cpu_4608 = torch.constant.device "cpu"
    %int0_4609 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3218, %none_4605, %none_4606, %int5_4607, %cpu_4608, %int0_4609 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_4610 = torch.constant.none
    %none_4611 = torch.constant.none
    %int5_4612 = torch.constant.int 5
    %cpu_4613 = torch.constant.device "cpu"
    %int0_4614 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3148, %none_4610, %none_4611, %int5_4612, %cpu_4613, %int0_4614 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_4615 = torch.constant.int 3
    %int0_4616 = torch.constant.int 0
    %int64_4617 = torch.constant.int 64
    %int2_4618 = torch.constant.int 2
    %3219 = torch.aten.slice.Tensor %3148, %int3_4615, %int0_4616, %int64_4617, %int2_4618 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_4619 = torch.constant.int 3
    %int1_4620 = torch.constant.int 1
    %int64_4621 = torch.constant.int 64
    %int2_4622 = torch.constant.int 2
    %3220 = torch.aten.slice.Tensor %3148, %int3_4619, %int1_4620, %int64_4621, %int2_4622 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %3221 = torch.aten.mul.Tensor %3219, %3217 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %3222 = torch.aten.mul.Tensor %3220, %3218 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_4623 = torch.constant.int 1
    %3223 = torch.aten.sub.Tensor %3221, %3222, %int1_4623 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %3224 = torch.aten.mul.Tensor %3220, %3217 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %3225 = torch.aten.mul.Tensor %3219, %3218 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_4624 = torch.constant.int 1
    %3226 = torch.aten.add.Tensor %3224, %3225, %int1_4624 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %3227 = torch_c.to_builtin_tensor %3223 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_4625 = tensor.cast %3227 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %3228 = torch_c.to_builtin_tensor %3226 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_4626 = tensor.cast %3228 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %3229 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_4625, %cast_4626) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_4627 = tensor.cast %3229 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %3230 = torch_c.from_builtin_tensor %cast_4627 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_4628 = torch.constant.int 4
    %int1_4629 = torch.constant.int 1
    %int4_4630 = torch.constant.int 4
    %int64_4631 = torch.constant.int 64
    %3231 = torch.prim.ListConstruct %int4_4628, %int1_4629, %int4_4630, %int64_4631 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3232 = torch.aten.view %3230, %3231 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_4632 = torch.constant.none
    %none_4633 = torch.constant.none
    %int5_4634 = torch.constant.int 5
    %cpu_4635 = torch.constant.device "cpu"
    %int0_4636 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3232, %none_4632, %none_4633, %int5_4634, %cpu_4635, %int0_4636 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_4637 = torch.constant.int 32
    %3233 = torch.aten.floor_divide.Scalar %arg2, %int32_4637 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_4638 = torch.constant.int 1
    %3234 = torch.aten.unsqueeze %3233, %int1_4638 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4639 = torch.constant.int 1
    %false_4640 = torch.constant.bool false
    %3235 = torch.aten.gather %arg3, %int1_4639, %3234, %false_4640 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_4641 = torch.constant.int 4
    %int1_4642 = torch.constant.int 1
    %int1_4643 = torch.constant.int 1
    %3236 = torch.prim.ListConstruct %int4_4641, %int1_4642, %int1_4643 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3237 = torch.aten.view %3235, %3236 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_4644 = torch.constant.int 32
    %3238 = torch.aten.remainder.Scalar %arg2, %int32_4644 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_4645 = torch.constant.int 4
    %int1_4646 = torch.constant.int 1
    %int1_4647 = torch.constant.int 1
    %3239 = torch.prim.ListConstruct %int4_4645, %int1_4646, %int1_4647 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3240 = torch.aten.view %3238, %3239 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_4648 = torch.constant.int 4
    %none_4649 = torch.constant.none
    %none_4650 = torch.constant.none
    %cpu_4651 = torch.constant.device "cpu"
    %false_4652 = torch.constant.bool false
    %3241 = torch.aten.arange %int4_4648, %none_4649, %none_4650, %cpu_4651, %false_4652 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_4653 = torch.constant.int 1
    %int1_4654 = torch.constant.int 1
    %int4_4655 = torch.constant.int 4
    %3242 = torch.prim.ListConstruct %int1_4653, %int1_4654, %int4_4655 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3243 = torch.aten.view %3241, %3242 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_4656 = torch.constant.none
    %3244 = torch.aten.clone %139, %none_4656 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_4657 = torch.constant.int 1
    %int1_4658 = torch.constant.int 1
    %int1_4659 = torch.constant.int 1
    %3245 = torch.prim.ListConstruct %int1_4657, %int1_4658, %int1_4659 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3246 = torch.aten.view %3244, %3245 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_4660 = torch.constant.int 22
    %3247 = torch.aten.mul.Scalar %3237, %int22_4660 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int8_4661 = torch.constant.int 8
    %int1_4662 = torch.constant.int 1
    %3248 = torch.aten.add.Scalar %3247, %int8_4661, %int1_4662 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_4663 = torch.constant.int 2
    %3249 = torch.aten.mul.Scalar %3248, %int2_4663 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_4664 = torch.constant.int 1
    %3250 = torch.aten.add.Tensor %3249, %3246, %int1_4664 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_4665 = torch.constant.int 4
    %3251 = torch.aten.mul.Scalar %3250, %int4_4665 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_4666 = torch.constant.int 1
    %3252 = torch.aten.add.Tensor %3251, %3243, %int1_4666 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_4667 = torch.constant.int 32
    %3253 = torch.aten.mul.Scalar %3252, %int32_4667 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_4668 = torch.constant.int 1
    %3254 = torch.aten.add.Tensor %3253, %3240, %int1_4668 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_4669 = torch.constant.none
    %none_4670 = torch.constant.none
    %int5_4671 = torch.constant.int 5
    %cpu_4672 = torch.constant.device "cpu"
    %int0_4673 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3232, %none_4669, %none_4670, %int5_4671, %cpu_4672, %int0_4673 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_4674 = torch.constant.int 22
    %int2_4675 = torch.constant.int 2
    %int4_4676 = torch.constant.int 4
    %int32_4677 = torch.constant.int 32
    %int64_4678 = torch.constant.int 64
    %3255 = torch.prim.ListConstruct %381, %int22_4674, %int2_4675, %int4_4676, %int32_4677, %int64_4678 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3256 = torch.aten.view %2950, %3255 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3256, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_4679 = torch.constant.int 64
    %3257 = torch.prim.ListConstruct %553, %int64_4679 : (!torch.int, !torch.int) -> !torch.list<int>
    %3258 = torch.aten.view %3256, %3257 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %3258, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %3259 = torch.prim.ListConstruct %3254 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_4680 = torch.constant.bool false
    %3260 = torch.aten.index_put %3258, %3259, %3232, %false_4680 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %3260, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_4681 = torch.constant.int 22
    %int2_4682 = torch.constant.int 2
    %int4_4683 = torch.constant.int 4
    %int32_4684 = torch.constant.int 32
    %int64_4685 = torch.constant.int 64
    %3261 = torch.prim.ListConstruct %381, %int22_4681, %int2_4682, %int4_4683, %int32_4684, %int64_4685 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3262 = torch.aten.view %3260, %3261 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3262, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_4686 = torch.constant.int 360448
    %3263 = torch.prim.ListConstruct %381, %int360448_4686 : (!torch.int, !torch.int) -> !torch.list<int>
    %3264 = torch.aten.view %3262, %3263 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %3264, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_4687 = torch.constant.int 22
    %int2_4688 = torch.constant.int 2
    %int4_4689 = torch.constant.int 4
    %int32_4690 = torch.constant.int 32
    %int64_4691 = torch.constant.int 64
    %3265 = torch.prim.ListConstruct %381, %int22_4687, %int2_4688, %int4_4689, %int32_4690, %int64_4691 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3266 = torch.aten.view %3264, %3265 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3266, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_4692 = torch.constant.int 64
    %3267 = torch.prim.ListConstruct %553, %int64_4692 : (!torch.int, !torch.int) -> !torch.list<int>
    %3268 = torch.aten.view %3266, %3267 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %3268, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_4693 = torch.constant.none
    %3269 = torch.aten.clone %140, %none_4693 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_4694 = torch.constant.int 1
    %int1_4695 = torch.constant.int 1
    %int1_4696 = torch.constant.int 1
    %3270 = torch.prim.ListConstruct %int1_4694, %int1_4695, %int1_4696 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3271 = torch.aten.view %3269, %3270 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_4697 = torch.constant.int 22
    %3272 = torch.aten.mul.Scalar %3237, %int22_4697 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int8_4698 = torch.constant.int 8
    %int1_4699 = torch.constant.int 1
    %3273 = torch.aten.add.Scalar %3272, %int8_4698, %int1_4699 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_4700 = torch.constant.int 2
    %3274 = torch.aten.mul.Scalar %3273, %int2_4700 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_4701 = torch.constant.int 1
    %3275 = torch.aten.add.Tensor %3274, %3271, %int1_4701 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_4702 = torch.constant.int 4
    %3276 = torch.aten.mul.Scalar %3275, %int4_4702 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_4703 = torch.constant.int 1
    %3277 = torch.aten.add.Tensor %3276, %3243, %int1_4703 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_4704 = torch.constant.int 32
    %3278 = torch.aten.mul.Scalar %3277, %int32_4704 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_4705 = torch.constant.int 1
    %3279 = torch.aten.add.Tensor %3278, %3240, %int1_4705 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_4706 = torch.constant.none
    %none_4707 = torch.constant.none
    %int5_4708 = torch.constant.int 5
    %cpu_4709 = torch.constant.device "cpu"
    %int0_4710 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3150, %none_4706, %none_4707, %int5_4708, %cpu_4709, %int0_4710 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %3280 = torch.prim.ListConstruct %3279 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_4711 = torch.constant.bool false
    %3281 = torch.aten.index_put %3268, %3280, %3150, %false_4711 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %3281, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_4712 = torch.constant.int 22
    %int2_4713 = torch.constant.int 2
    %int4_4714 = torch.constant.int 4
    %int32_4715 = torch.constant.int 32
    %int64_4716 = torch.constant.int 64
    %3282 = torch.prim.ListConstruct %381, %int22_4712, %int2_4713, %int4_4714, %int32_4715, %int64_4716 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3283 = torch.aten.view %3281, %3282 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3283, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_4717 = torch.constant.int 360448
    %3284 = torch.prim.ListConstruct %381, %int360448_4717 : (!torch.int, !torch.int) -> !torch.list<int>
    %3285 = torch.aten.view %3283, %3284 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %3285, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_4718 = torch.constant.none
    %3286 = torch.aten.clone %141, %none_4718 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_4719 = torch.constant.none
    %3287 = torch.aten.clone %142, %none_4719 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_4720 = torch.constant.none
    %3288 = torch.aten.clone %143, %none_4720 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_4721 = torch.constant.int 22
    %int2_4722 = torch.constant.int 2
    %int4_4723 = torch.constant.int 4
    %int32_4724 = torch.constant.int 32
    %int64_4725 = torch.constant.int 64
    %3289 = torch.prim.ListConstruct %381, %int22_4721, %int2_4722, %int4_4723, %int32_4724, %int64_4725 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3290 = torch.aten.view %3285, %3289 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3290, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %3291 = torch_c.to_builtin_tensor %3290 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %3292 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_4726 = tensor.cast %3292 : tensor<4x?xi64> to tensor<?x?xi64>
    %3293 = torch_c.to_builtin_tensor %3286 : !torch.vtensor<[],si64> -> tensor<i64>
    %3294 = torch_c.to_builtin_tensor %3287 : !torch.vtensor<[],si64> -> tensor<i64>
    %3295 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%3291, %cast_4726, %3293, %3294) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_4727 = tensor.cast %3295 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %3296 = torch_c.from_builtin_tensor %cast_4727 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %3296, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %3297 = torch_c.to_builtin_tensor %3290 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %3298 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_4728 = tensor.cast %3298 : tensor<4x?xi64> to tensor<?x?xi64>
    %3299 = torch_c.to_builtin_tensor %3286 : !torch.vtensor<[],si64> -> tensor<i64>
    %3300 = torch_c.to_builtin_tensor %3288 : !torch.vtensor<[],si64> -> tensor<i64>
    %3301 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%3297, %cast_4728, %3299, %3300) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_4729 = tensor.cast %3301 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %3302 = torch_c.from_builtin_tensor %cast_4729 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %3302, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_4730 = torch.constant.int 2
    %int3_4731 = torch.constant.int 3
    %3303 = torch.aten.transpose.int %3296, %int2_4730, %int3_4731 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %3303, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_4732 = torch.constant.int 0
    %3304 = torch.aten.clone %3303, %int0_4732 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %3304, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_4733 = torch.constant.int 4
    %int4_4734 = torch.constant.int 4
    %int64_4735 = torch.constant.int 64
    %3305 = torch.prim.ListConstruct %int4_4733, %623, %int4_4734, %int64_4735 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3306 = torch.aten._unsafe_view %3304, %3305 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %3306, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_4736 = torch.constant.int 2
    %int3_4737 = torch.constant.int 3
    %3307 = torch.aten.transpose.int %3302, %int2_4736, %int3_4737 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %3307, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_4738 = torch.constant.int 0
    %3308 = torch.aten.clone %3307, %int0_4738 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %3308, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_4739 = torch.constant.int 4
    %int4_4740 = torch.constant.int 4
    %int64_4741 = torch.constant.int 64
    %3309 = torch.prim.ListConstruct %int4_4739, %623, %int4_4740, %int64_4741 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3310 = torch.aten._unsafe_view %3308, %3309 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %3310, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_4742 = torch.constant.int 0
    %int1_4743 = torch.constant.int 1
    %none_4744 = torch.constant.none
    %none_4745 = torch.constant.none
    %cpu_4746 = torch.constant.device "cpu"
    %false_4747 = torch.constant.bool false
    %3311 = torch.aten.arange.start_step %int0_4742, %623, %int1_4743, %none_4744, %none_4745, %cpu_4746, %false_4747 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3311, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_4748 = torch.constant.int -1
    %3312 = torch.aten.unsqueeze %arg1, %int-1_4748 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %3313 = torch.aten.ge.Tensor %3311, %3312 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %3313, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_4749 = torch.constant.none
    %3314 = torch.aten.clone %144, %none_4749 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_4750 = torch.constant.int 0
    %3315 = torch.aten.where.ScalarOther %3313, %3314, %int0_4750 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %3315, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_4751 = torch.constant.none
    %none_4752 = torch.constant.none
    %int5_4753 = torch.constant.int 5
    %cpu_4754 = torch.constant.device "cpu"
    %int0_4755 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3315, %none_4751, %none_4752, %int5_4753, %cpu_4754, %int0_4755 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_4756 = torch.constant.int 1
    %3316 = torch.aten.unsqueeze %3315, %int1_4756 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %3316, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_4757 = torch.constant.int 1
    %3317 = torch.aten.unsqueeze %3316, %int1_4757 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %3317, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_4758 = torch.constant.int -2
    %3318 = torch.aten.unsqueeze %3306, %int-2_4758 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %3318, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_4759 = torch.constant.int 4
    %int4_4760 = torch.constant.int 4
    %int8_4761 = torch.constant.int 8
    %int64_4762 = torch.constant.int 64
    %3319 = torch.prim.ListConstruct %int4_4759, %623, %int4_4760, %int8_4761, %int64_4762 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4763 = torch.constant.bool false
    %3320 = torch.aten.expand %3318, %3319, %false_4763 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3320, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_4764 = torch.constant.int 0
    %3321 = torch.aten.clone %3320, %int0_4764 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3321, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_4765 = torch.constant.int 4
    %int32_4766 = torch.constant.int 32
    %int64_4767 = torch.constant.int 64
    %3322 = torch.prim.ListConstruct %int4_4765, %623, %int32_4766, %int64_4767 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3323 = torch.aten._unsafe_view %3321, %3322 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3323, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_4768 = torch.constant.int -2
    %3324 = torch.aten.unsqueeze %3310, %int-2_4768 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %3324, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_4769 = torch.constant.int 4
    %int4_4770 = torch.constant.int 4
    %int8_4771 = torch.constant.int 8
    %int64_4772 = torch.constant.int 64
    %3325 = torch.prim.ListConstruct %int4_4769, %623, %int4_4770, %int8_4771, %int64_4772 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4773 = torch.constant.bool false
    %3326 = torch.aten.expand %3324, %3325, %false_4773 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3326, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_4774 = torch.constant.int 0
    %3327 = torch.aten.clone %3326, %int0_4774 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3327, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_4775 = torch.constant.int 4
    %int32_4776 = torch.constant.int 32
    %int64_4777 = torch.constant.int 64
    %3328 = torch.prim.ListConstruct %int4_4775, %623, %int32_4776, %int64_4777 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3329 = torch.aten._unsafe_view %3327, %3328 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3329, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_4778 = torch.constant.int 1
    %int2_4779 = torch.constant.int 2
    %3330 = torch.aten.transpose.int %3191, %int1_4778, %int2_4779 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_4780 = torch.constant.int 1
    %int2_4781 = torch.constant.int 2
    %3331 = torch.aten.transpose.int %3323, %int1_4780, %int2_4781 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %3331, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_4782 = torch.constant.int 1
    %int2_4783 = torch.constant.int 2
    %3332 = torch.aten.transpose.int %3329, %int1_4782, %int2_4783 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %3332, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_4784 = torch.constant.float 0.000000e+00
    %false_4785 = torch.constant.bool false
    %none_4786 = torch.constant.none
    %false_4787 = torch.constant.bool false
    %3333 = torch.aten.scaled_dot_product_attention %3330, %3331, %3332, %3317, %float0.000000e00_4784, %false_4785, %none_4786, %false_4787 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_4788 = torch.constant.int 1
    %int2_4789 = torch.constant.int 2
    %3334 = torch.aten.transpose.int %3333, %int1_4788, %int2_4789 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_4790 = torch.constant.int 4
    %int1_4791 = torch.constant.int 1
    %int2048_4792 = torch.constant.int 2048
    %3335 = torch.prim.ListConstruct %int4_4790, %int1_4791, %int2048_4792 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3336 = torch.aten.view %3334, %3335 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_4793 = torch.constant.int 2
    %3337 = torch.aten.view.dtype %149, %int2_4793 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %3338 = torch.aten.detach %3337 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_4794 = torch.constant.int -1
    %int17_4795 = torch.constant.int 17
    %3339 = torch.prim.ListConstruct %int-1_4794, %int17_4795 : (!torch.int, !torch.int) -> !torch.list<int>
    %3340 = torch.aten.view %3338, %3339 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_4796 = torch.constant.int 2048
    %int-1_4797 = torch.constant.int -1
    %int17_4798 = torch.constant.int 17
    %3341 = torch.prim.ListConstruct %int2048_4796, %int-1_4797, %int17_4798 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3342 = torch.aten.view %3340, %3341 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_4799 = torch.constant.int 2
    %int0_4800 = torch.constant.int 0
    %int1_4801 = torch.constant.int 1
    %int1_4802 = torch.constant.int 1
    %3343 = torch.aten.slice.Tensor %3342, %int2_4799, %int0_4800, %int1_4801, %int1_4802 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_4803 = torch.constant.int 5
    %3344 = torch.aten.view.dtype %3343, %int5_4803 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %3345 = torch.aten.detach %3344 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_4804 = torch.constant.int 2
    %int1_4805 = torch.constant.int 1
    %int9223372036854775807_4806 = torch.constant.int 9223372036854775807
    %int1_4807 = torch.constant.int 1
    %3346 = torch.aten.slice.Tensor %3342, %int2_4804, %int1_4805, %int9223372036854775807_4806, %int1_4807 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_4808 = torch.constant.int 1
    %3347 = torch.aten.view.dtype %3346, %int1_4808 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %3348 = torch.aten.detach %3347 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %3349 = torch_c.to_builtin_tensor %3336 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_4809 = tensor.cast %3349 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %3350 = torch_c.to_builtin_tensor %3345 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %3351 = torch_c.to_builtin_tensor %3348 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %3352 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_4809, %3350, %3351) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_4810 = tensor.cast %3352 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %3353 = torch_c.from_builtin_tensor %cast_4810 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_4811 = torch.constant.none
    %none_4812 = torch.constant.none
    %int5_4813 = torch.constant.int 5
    %cpu_4814 = torch.constant.device "cpu"
    %int0_4815 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3353, %none_4811, %none_4812, %int5_4813, %cpu_4814, %int0_4815 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_4816 = torch.constant.int 1
    %3354 = torch.aten.add.Tensor %3083, %3353, %int1_4816 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_4817 = torch.constant.none
    %none_4818 = torch.constant.none
    %int5_4819 = torch.constant.int 5
    %cpu_4820 = torch.constant.device "cpu"
    %int0_4821 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3354, %none_4817, %none_4818, %int5_4819, %cpu_4820, %int0_4821 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4822 = torch.constant.int 6
    %3355 = torch.prims.convert_element_type %3354, %int6_4822 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_4823 = torch.constant.int 2
    %3356 = torch.aten.pow.Tensor_Scalar %3355, %int2_4823 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_4824 = torch.constant.int -1
    %3357 = torch.prim.ListConstruct %int-1_4824 : (!torch.int) -> !torch.list<int>
    %true_4825 = torch.constant.bool true
    %none_4826 = torch.constant.none
    %3358 = torch.aten.mean.dim %3356, %3357, %true_4825, %none_4826 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_4827 = torch.constant.float 9.9999997473787516E-6
    %int1_4828 = torch.constant.int 1
    %3359 = torch.aten.add.Scalar %3358, %float9.999990e-06_4827, %int1_4828 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3360 = torch.aten.rsqrt %3359 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %3361 = torch.aten.mul.Tensor %3355, %3360 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_4829 = torch.constant.none
    %none_4830 = torch.constant.none
    %int6_4831 = torch.constant.int 6
    %cpu_4832 = torch.constant.device "cpu"
    %int0_4833 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3361, %none_4829, %none_4830, %int6_4831, %cpu_4832, %int0_4833 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4834 = torch.constant.int 5
    %3362 = torch.prims.convert_element_type %3361, %int5_4834 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %3363 = torch.aten.mul.Tensor %150, %3362 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_4835 = torch.constant.none
    %none_4836 = torch.constant.none
    %int6_4837 = torch.constant.int 6
    %cpu_4838 = torch.constant.device "cpu"
    %int0_4839 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3363, %none_4835, %none_4836, %int6_4837, %cpu_4838, %int0_4839 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4840 = torch.constant.int 5
    %3364 = torch.prims.convert_element_type %3363, %int5_4840 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_4841 = torch.constant.int 2
    %3365 = torch.aten.view.dtype %151, %int2_4841 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %3366 = torch.aten.detach %3365 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_4842 = torch.constant.int -1
    %int17_4843 = torch.constant.int 17
    %3367 = torch.prim.ListConstruct %int-1_4842, %int17_4843 : (!torch.int, !torch.int) -> !torch.list<int>
    %3368 = torch.aten.view %3366, %3367 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_4844 = torch.constant.int 5632
    %int-1_4845 = torch.constant.int -1
    %int17_4846 = torch.constant.int 17
    %3369 = torch.prim.ListConstruct %int5632_4844, %int-1_4845, %int17_4846 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3370 = torch.aten.view %3368, %3369 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_4847 = torch.constant.int 2
    %int0_4848 = torch.constant.int 0
    %int1_4849 = torch.constant.int 1
    %int1_4850 = torch.constant.int 1
    %3371 = torch.aten.slice.Tensor %3370, %int2_4847, %int0_4848, %int1_4849, %int1_4850 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_4851 = torch.constant.int 5
    %3372 = torch.aten.view.dtype %3371, %int5_4851 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %3373 = torch.aten.detach %3372 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_4852 = torch.constant.int 2
    %int1_4853 = torch.constant.int 1
    %int9223372036854775807_4854 = torch.constant.int 9223372036854775807
    %int1_4855 = torch.constant.int 1
    %3374 = torch.aten.slice.Tensor %3370, %int2_4852, %int1_4853, %int9223372036854775807_4854, %int1_4855 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_4856 = torch.constant.int 1
    %3375 = torch.aten.view.dtype %3374, %int1_4856 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %3376 = torch.aten.detach %3375 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %3377 = torch_c.to_builtin_tensor %3364 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_4857 = tensor.cast %3377 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %3378 = torch_c.to_builtin_tensor %3373 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %3379 = torch_c.to_builtin_tensor %3376 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %3380 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_4857, %3378, %3379) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_4858 = tensor.cast %3380 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %3381 = torch_c.from_builtin_tensor %cast_4858 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %3382 = torch.aten.silu %3381 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_4859 = torch.constant.int 2
    %3383 = torch.aten.view.dtype %152, %int2_4859 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %3384 = torch.aten.detach %3383 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_4860 = torch.constant.int -1
    %int17_4861 = torch.constant.int 17
    %3385 = torch.prim.ListConstruct %int-1_4860, %int17_4861 : (!torch.int, !torch.int) -> !torch.list<int>
    %3386 = torch.aten.view %3384, %3385 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_4862 = torch.constant.int 5632
    %int-1_4863 = torch.constant.int -1
    %int17_4864 = torch.constant.int 17
    %3387 = torch.prim.ListConstruct %int5632_4862, %int-1_4863, %int17_4864 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3388 = torch.aten.view %3386, %3387 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_4865 = torch.constant.int 2
    %int0_4866 = torch.constant.int 0
    %int1_4867 = torch.constant.int 1
    %int1_4868 = torch.constant.int 1
    %3389 = torch.aten.slice.Tensor %3388, %int2_4865, %int0_4866, %int1_4867, %int1_4868 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_4869 = torch.constant.int 5
    %3390 = torch.aten.view.dtype %3389, %int5_4869 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %3391 = torch.aten.detach %3390 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_4870 = torch.constant.int 2
    %int1_4871 = torch.constant.int 1
    %int9223372036854775807_4872 = torch.constant.int 9223372036854775807
    %int1_4873 = torch.constant.int 1
    %3392 = torch.aten.slice.Tensor %3388, %int2_4870, %int1_4871, %int9223372036854775807_4872, %int1_4873 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_4874 = torch.constant.int 1
    %3393 = torch.aten.view.dtype %3392, %int1_4874 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %3394 = torch.aten.detach %3393 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %3395 = torch_c.to_builtin_tensor %3364 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_4875 = tensor.cast %3395 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %3396 = torch_c.to_builtin_tensor %3391 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %3397 = torch_c.to_builtin_tensor %3394 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %3398 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_4875, %3396, %3397) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_4876 = tensor.cast %3398 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %3399 = torch_c.from_builtin_tensor %cast_4876 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %3400 = torch.aten.mul.Tensor %3382, %3399 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_4877 = torch.constant.int 2
    %3401 = torch.aten.view.dtype %153, %int2_4877 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %3402 = torch.aten.detach %3401 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_4878 = torch.constant.int -1
    %int17_4879 = torch.constant.int 17
    %3403 = torch.prim.ListConstruct %int-1_4878, %int17_4879 : (!torch.int, !torch.int) -> !torch.list<int>
    %3404 = torch.aten.view %3402, %3403 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_4880 = torch.constant.int 2048
    %int-1_4881 = torch.constant.int -1
    %int17_4882 = torch.constant.int 17
    %3405 = torch.prim.ListConstruct %int2048_4880, %int-1_4881, %int17_4882 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3406 = torch.aten.view %3404, %3405 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_4883 = torch.constant.int 2
    %int0_4884 = torch.constant.int 0
    %int1_4885 = torch.constant.int 1
    %int1_4886 = torch.constant.int 1
    %3407 = torch.aten.slice.Tensor %3406, %int2_4883, %int0_4884, %int1_4885, %int1_4886 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_4887 = torch.constant.int 5
    %3408 = torch.aten.view.dtype %3407, %int5_4887 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %3409 = torch.aten.detach %3408 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_4888 = torch.constant.int 2
    %int1_4889 = torch.constant.int 1
    %int9223372036854775807_4890 = torch.constant.int 9223372036854775807
    %int1_4891 = torch.constant.int 1
    %3410 = torch.aten.slice.Tensor %3406, %int2_4888, %int1_4889, %int9223372036854775807_4890, %int1_4891 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_4892 = torch.constant.int 1
    %3411 = torch.aten.view.dtype %3410, %int1_4892 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %3412 = torch.aten.detach %3411 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %3413 = torch_c.to_builtin_tensor %3400 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_4893 = tensor.cast %3413 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %3414 = torch_c.to_builtin_tensor %3409 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %3415 = torch_c.to_builtin_tensor %3412 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %3416 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_4893, %3414, %3415) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_4894 = tensor.cast %3416 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %3417 = torch_c.from_builtin_tensor %cast_4894 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_4895 = torch.constant.int 1
    %3418 = torch.aten.add.Tensor %3354, %3417, %int1_4895 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_4896 = torch.constant.none
    %none_4897 = torch.constant.none
    %int5_4898 = torch.constant.int 5
    %cpu_4899 = torch.constant.device "cpu"
    %int0_4900 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3418, %none_4896, %none_4897, %int5_4898, %cpu_4899, %int0_4900 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_4901 = torch.constant.int 6
    %3419 = torch.prims.convert_element_type %3418, %int6_4901 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_4902 = torch.constant.int 2
    %3420 = torch.aten.pow.Tensor_Scalar %3419, %int2_4902 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_4903 = torch.constant.int -1
    %3421 = torch.prim.ListConstruct %int-1_4903 : (!torch.int) -> !torch.list<int>
    %true_4904 = torch.constant.bool true
    %none_4905 = torch.constant.none
    %3422 = torch.aten.mean.dim %3420, %3421, %true_4904, %none_4905 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_4906 = torch.constant.float 9.9999997473787516E-6
    %int1_4907 = torch.constant.int 1
    %3423 = torch.aten.add.Scalar %3422, %float9.999990e-06_4906, %int1_4907 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3424 = torch.aten.rsqrt %3423 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %3425 = torch.aten.mul.Tensor %3419, %3424 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_4908 = torch.constant.none
    %none_4909 = torch.constant.none
    %int6_4910 = torch.constant.int 6
    %cpu_4911 = torch.constant.device "cpu"
    %int0_4912 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3425, %none_4908, %none_4909, %int6_4910, %cpu_4911, %int0_4912 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4913 = torch.constant.int 5
    %3426 = torch.prims.convert_element_type %3425, %int5_4913 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %3427 = torch.aten.mul.Tensor %162, %3426 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_4914 = torch.constant.none
    %none_4915 = torch.constant.none
    %int6_4916 = torch.constant.int 6
    %cpu_4917 = torch.constant.device "cpu"
    %int0_4918 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3427, %none_4914, %none_4915, %int6_4916, %cpu_4917, %int0_4918 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_4919 = torch.constant.int 5
    %3428 = torch.prims.convert_element_type %3427, %int5_4919 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_4920 = torch.constant.int 2
    %3429 = torch.aten.view.dtype %163, %int2_4920 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %3430 = torch.aten.detach %3429 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_4921 = torch.constant.int -1
    %int17_4922 = torch.constant.int 17
    %3431 = torch.prim.ListConstruct %int-1_4921, %int17_4922 : (!torch.int, !torch.int) -> !torch.list<int>
    %3432 = torch.aten.view %3430, %3431 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_4923 = torch.constant.int 2048
    %int-1_4924 = torch.constant.int -1
    %int17_4925 = torch.constant.int 17
    %3433 = torch.prim.ListConstruct %int2048_4923, %int-1_4924, %int17_4925 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3434 = torch.aten.view %3432, %3433 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_4926 = torch.constant.int 2
    %int0_4927 = torch.constant.int 0
    %int1_4928 = torch.constant.int 1
    %int1_4929 = torch.constant.int 1
    %3435 = torch.aten.slice.Tensor %3434, %int2_4926, %int0_4927, %int1_4928, %int1_4929 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_4930 = torch.constant.int 5
    %3436 = torch.aten.view.dtype %3435, %int5_4930 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %3437 = torch.aten.detach %3436 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_4931 = torch.constant.int 2
    %int1_4932 = torch.constant.int 1
    %int9223372036854775807_4933 = torch.constant.int 9223372036854775807
    %int1_4934 = torch.constant.int 1
    %3438 = torch.aten.slice.Tensor %3434, %int2_4931, %int1_4932, %int9223372036854775807_4933, %int1_4934 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_4935 = torch.constant.int 1
    %3439 = torch.aten.view.dtype %3438, %int1_4935 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %3440 = torch.aten.detach %3439 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %3441 = torch_c.to_builtin_tensor %3428 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_4936 = tensor.cast %3441 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %3442 = torch_c.to_builtin_tensor %3437 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %3443 = torch_c.to_builtin_tensor %3440 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %3444 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_4936, %3442, %3443) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_4937 = tensor.cast %3444 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %3445 = torch_c.from_builtin_tensor %cast_4937 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_4938 = torch.constant.int 2
    %3446 = torch.aten.view.dtype %164, %int2_4938 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %3447 = torch.aten.detach %3446 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_4939 = torch.constant.int -1
    %int17_4940 = torch.constant.int 17
    %3448 = torch.prim.ListConstruct %int-1_4939, %int17_4940 : (!torch.int, !torch.int) -> !torch.list<int>
    %3449 = torch.aten.view %3447, %3448 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_4941 = torch.constant.int 256
    %int-1_4942 = torch.constant.int -1
    %int17_4943 = torch.constant.int 17
    %3450 = torch.prim.ListConstruct %int256_4941, %int-1_4942, %int17_4943 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3451 = torch.aten.view %3449, %3450 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_4944 = torch.constant.int 2
    %int0_4945 = torch.constant.int 0
    %int1_4946 = torch.constant.int 1
    %int1_4947 = torch.constant.int 1
    %3452 = torch.aten.slice.Tensor %3451, %int2_4944, %int0_4945, %int1_4946, %int1_4947 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_4948 = torch.constant.int 5
    %3453 = torch.aten.view.dtype %3452, %int5_4948 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %3454 = torch.aten.detach %3453 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_4949 = torch.constant.int 2
    %int1_4950 = torch.constant.int 1
    %int9223372036854775807_4951 = torch.constant.int 9223372036854775807
    %int1_4952 = torch.constant.int 1
    %3455 = torch.aten.slice.Tensor %3451, %int2_4949, %int1_4950, %int9223372036854775807_4951, %int1_4952 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_4953 = torch.constant.int 1
    %3456 = torch.aten.view.dtype %3455, %int1_4953 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %3457 = torch.aten.detach %3456 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %3458 = torch_c.to_builtin_tensor %3428 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_4954 = tensor.cast %3458 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %3459 = torch_c.to_builtin_tensor %3454 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %3460 = torch_c.to_builtin_tensor %3457 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %3461 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_4954, %3459, %3460) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_4955 = tensor.cast %3461 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %3462 = torch_c.from_builtin_tensor %cast_4955 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_4956 = torch.constant.int 2
    %3463 = torch.aten.view.dtype %165, %int2_4956 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %3464 = torch.aten.detach %3463 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_4957 = torch.constant.int -1
    %int17_4958 = torch.constant.int 17
    %3465 = torch.prim.ListConstruct %int-1_4957, %int17_4958 : (!torch.int, !torch.int) -> !torch.list<int>
    %3466 = torch.aten.view %3464, %3465 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_4959 = torch.constant.int 256
    %int-1_4960 = torch.constant.int -1
    %int17_4961 = torch.constant.int 17
    %3467 = torch.prim.ListConstruct %int256_4959, %int-1_4960, %int17_4961 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3468 = torch.aten.view %3466, %3467 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_4962 = torch.constant.int 2
    %int0_4963 = torch.constant.int 0
    %int1_4964 = torch.constant.int 1
    %int1_4965 = torch.constant.int 1
    %3469 = torch.aten.slice.Tensor %3468, %int2_4962, %int0_4963, %int1_4964, %int1_4965 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_4966 = torch.constant.int 5
    %3470 = torch.aten.view.dtype %3469, %int5_4966 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %3471 = torch.aten.detach %3470 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_4967 = torch.constant.int 2
    %int1_4968 = torch.constant.int 1
    %int9223372036854775807_4969 = torch.constant.int 9223372036854775807
    %int1_4970 = torch.constant.int 1
    %3472 = torch.aten.slice.Tensor %3468, %int2_4967, %int1_4968, %int9223372036854775807_4969, %int1_4970 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_4971 = torch.constant.int 1
    %3473 = torch.aten.view.dtype %3472, %int1_4971 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %3474 = torch.aten.detach %3473 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %3475 = torch_c.to_builtin_tensor %3428 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_4972 = tensor.cast %3475 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %3476 = torch_c.to_builtin_tensor %3471 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %3477 = torch_c.to_builtin_tensor %3474 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %3478 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_4972, %3476, %3477) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_4973 = tensor.cast %3478 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %3479 = torch_c.from_builtin_tensor %cast_4973 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_4974 = torch.constant.int 4
    %int1_4975 = torch.constant.int 1
    %int32_4976 = torch.constant.int 32
    %int64_4977 = torch.constant.int 64
    %3480 = torch.prim.ListConstruct %int4_4974, %int1_4975, %int32_4976, %int64_4977 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3481 = torch.aten.view %3445, %3480 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_4978 = torch.constant.int 4
    %int1_4979 = torch.constant.int 1
    %int4_4980 = torch.constant.int 4
    %int64_4981 = torch.constant.int 64
    %3482 = torch.prim.ListConstruct %int4_4978, %int1_4979, %int4_4980, %int64_4981 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3483 = torch.aten.view %3462, %3482 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_4982 = torch.constant.int 4
    %int1_4983 = torch.constant.int 1
    %int4_4984 = torch.constant.int 4
    %int64_4985 = torch.constant.int 64
    %3484 = torch.prim.ListConstruct %int4_4982, %int1_4983, %int4_4984, %int64_4985 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3485 = torch.aten.view %3479, %3484 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_4986 = torch.constant.int 0
    %int1_4987 = torch.constant.int 1
    %none_4988 = torch.constant.none
    %none_4989 = torch.constant.none
    %cpu_4990 = torch.constant.device "cpu"
    %false_4991 = torch.constant.bool false
    %3486 = torch.aten.arange.start %int0_4986, %int1_4987, %none_4988, %none_4989, %cpu_4990, %false_4991 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_4992 = torch.constant.int 0
    %3487 = torch.aten.unsqueeze %3486, %int0_4992 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_4993 = torch.constant.int 1
    %3488 = torch.aten.unsqueeze %arg2, %int1_4993 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4994 = torch.constant.int 1
    %3489 = torch.aten.add.Tensor %3487, %3488, %int1_4994 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_4995 = torch.constant.int 0
    %int64_4996 = torch.constant.int 64
    %int2_4997 = torch.constant.int 2
    %none_4998 = torch.constant.none
    %none_4999 = torch.constant.none
    %cpu_5000 = torch.constant.device "cpu"
    %false_5001 = torch.constant.bool false
    %3490 = torch.aten.arange.start_step %int0_4995, %int64_4996, %int2_4997, %none_4998, %none_4999, %cpu_5000, %false_5001 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_5002 = torch.constant.none
    %none_5003 = torch.constant.none
    %int4_5004 = torch.constant.int 4
    %cpu_5005 = torch.constant.device "cpu"
    %int0_5006 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3490, %none_5002, %none_5003, %int4_5004, %cpu_5005, %int0_5006 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5007 = torch.constant.int 6
    %3491 = torch.prims.convert_element_type %3490, %int6_5007 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_5008 = torch.constant.int 64
    %3492 = torch.aten.div.Scalar %3491, %int64_5008 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_5009 = torch.constant.float 1.000000e+04
    %3493 = torch.aten.pow.Scalar %float1.000000e04_5009, %3492 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %3494 = torch.aten.reciprocal %3493 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_5010 = torch.constant.float 1.000000e+00
    %3495 = torch.aten.mul.Scalar %3494, %float1.000000e00_5010 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_5011 = torch.constant.none
    %3496 = torch.aten.clone %154, %none_5011 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_5012 = torch.constant.int 0
    %3497 = torch.aten.unsqueeze %3495, %int0_5012 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_5013 = torch.constant.int 2
    %3498 = torch.aten.unsqueeze %3497, %int2_5013 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_5014 = torch.constant.none
    %none_5015 = torch.constant.none
    %int6_5016 = torch.constant.int 6
    %cpu_5017 = torch.constant.device "cpu"
    %int0_5018 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3498, %none_5014, %none_5015, %int6_5016, %cpu_5017, %int0_5018 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_5019 = torch.constant.int 4
    %int-1_5020 = torch.constant.int -1
    %int1_5021 = torch.constant.int 1
    %3499 = torch.prim.ListConstruct %int4_5019, %int-1_5020, %int1_5021 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5022 = torch.constant.bool false
    %3500 = torch.aten.expand %3498, %3499, %false_5022 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_5023 = torch.constant.int 1
    %3501 = torch.aten.unsqueeze %3489, %int1_5023 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_5024 = torch.constant.none
    %none_5025 = torch.constant.none
    %int4_5026 = torch.constant.int 4
    %cpu_5027 = torch.constant.device "cpu"
    %int0_5028 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3501, %none_5024, %none_5025, %int4_5026, %cpu_5027, %int0_5028 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5029 = torch.constant.int 6
    %3502 = torch.prims.convert_element_type %3501, %int6_5029 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3503 = torch.aten.matmul %3500, %3502 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_5030 = torch.constant.int 1
    %int2_5031 = torch.constant.int 2
    %3504 = torch.aten.transpose.int %3503, %int1_5030, %int2_5031 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %3505 = torch.aten.cos %3504 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %3506 = torch.aten.mul.Tensor %3505, %3496 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_5032 = torch.constant.none
    %none_5033 = torch.constant.none
    %int6_5034 = torch.constant.int 6
    %cpu_5035 = torch.constant.device "cpu"
    %int0_5036 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3506, %none_5032, %none_5033, %int6_5034, %cpu_5035, %int0_5036 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5037 = torch.constant.int 5
    %3507 = torch.prims.convert_element_type %3506, %int5_5037 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %3508 = torch.aten.sin %3504 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %3509 = torch.aten.mul.Tensor %3508, %3496 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_5038 = torch.constant.none
    %none_5039 = torch.constant.none
    %int6_5040 = torch.constant.int 6
    %cpu_5041 = torch.constant.device "cpu"
    %int0_5042 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3509, %none_5038, %none_5039, %int6_5040, %cpu_5041, %int0_5042 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5043 = torch.constant.int 5
    %3510 = torch.prims.convert_element_type %3509, %int5_5043 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_5044 = torch.constant.int 2
    %3511 = torch.aten.unsqueeze %3507, %int2_5044 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_5045 = torch.constant.int 2
    %3512 = torch.aten.unsqueeze %3510, %int2_5045 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_5046 = torch.constant.none
    %none_5047 = torch.constant.none
    %int5_5048 = torch.constant.int 5
    %cpu_5049 = torch.constant.device "cpu"
    %int0_5050 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3511, %none_5046, %none_5047, %int5_5048, %cpu_5049, %int0_5050 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_5051 = torch.constant.none
    %none_5052 = torch.constant.none
    %int5_5053 = torch.constant.int 5
    %cpu_5054 = torch.constant.device "cpu"
    %int0_5055 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3512, %none_5051, %none_5052, %int5_5053, %cpu_5054, %int0_5055 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_5056 = torch.constant.none
    %none_5057 = torch.constant.none
    %int5_5058 = torch.constant.int 5
    %cpu_5059 = torch.constant.device "cpu"
    %int0_5060 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3481, %none_5056, %none_5057, %int5_5058, %cpu_5059, %int0_5060 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_5061 = torch.constant.int 3
    %int0_5062 = torch.constant.int 0
    %int64_5063 = torch.constant.int 64
    %int2_5064 = torch.constant.int 2
    %3513 = torch.aten.slice.Tensor %3481, %int3_5061, %int0_5062, %int64_5063, %int2_5064 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_5065 = torch.constant.int 3
    %int1_5066 = torch.constant.int 1
    %int64_5067 = torch.constant.int 64
    %int2_5068 = torch.constant.int 2
    %3514 = torch.aten.slice.Tensor %3481, %int3_5065, %int1_5066, %int64_5067, %int2_5068 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %3515 = torch.aten.mul.Tensor %3513, %3511 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %3516 = torch.aten.mul.Tensor %3514, %3512 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_5069 = torch.constant.int 1
    %3517 = torch.aten.sub.Tensor %3515, %3516, %int1_5069 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %3518 = torch.aten.mul.Tensor %3514, %3511 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %3519 = torch.aten.mul.Tensor %3513, %3512 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_5070 = torch.constant.int 1
    %3520 = torch.aten.add.Tensor %3518, %3519, %int1_5070 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %3521 = torch_c.to_builtin_tensor %3517 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_5071 = tensor.cast %3521 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %3522 = torch_c.to_builtin_tensor %3520 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_5072 = tensor.cast %3522 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %3523 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_5071, %cast_5072) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_5073 = tensor.cast %3523 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %3524 = torch_c.from_builtin_tensor %cast_5073 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_5074 = torch.constant.int 4
    %int1_5075 = torch.constant.int 1
    %int32_5076 = torch.constant.int 32
    %int64_5077 = torch.constant.int 64
    %3525 = torch.prim.ListConstruct %int4_5074, %int1_5075, %int32_5076, %int64_5077 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3526 = torch.aten.view %3524, %3525 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_5078 = torch.constant.none
    %none_5079 = torch.constant.none
    %int5_5080 = torch.constant.int 5
    %cpu_5081 = torch.constant.device "cpu"
    %int0_5082 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3526, %none_5078, %none_5079, %int5_5080, %cpu_5081, %int0_5082 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_5083 = torch.constant.int 0
    %int1_5084 = torch.constant.int 1
    %none_5085 = torch.constant.none
    %none_5086 = torch.constant.none
    %cpu_5087 = torch.constant.device "cpu"
    %false_5088 = torch.constant.bool false
    %3527 = torch.aten.arange.start %int0_5083, %int1_5084, %none_5085, %none_5086, %cpu_5087, %false_5088 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_5089 = torch.constant.int 0
    %3528 = torch.aten.unsqueeze %3527, %int0_5089 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_5090 = torch.constant.int 1
    %3529 = torch.aten.unsqueeze %arg2, %int1_5090 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5091 = torch.constant.int 1
    %3530 = torch.aten.add.Tensor %3528, %3529, %int1_5091 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_5092 = torch.constant.int 0
    %int64_5093 = torch.constant.int 64
    %int2_5094 = torch.constant.int 2
    %none_5095 = torch.constant.none
    %none_5096 = torch.constant.none
    %cpu_5097 = torch.constant.device "cpu"
    %false_5098 = torch.constant.bool false
    %3531 = torch.aten.arange.start_step %int0_5092, %int64_5093, %int2_5094, %none_5095, %none_5096, %cpu_5097, %false_5098 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_5099 = torch.constant.none
    %none_5100 = torch.constant.none
    %int4_5101 = torch.constant.int 4
    %cpu_5102 = torch.constant.device "cpu"
    %int0_5103 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3531, %none_5099, %none_5100, %int4_5101, %cpu_5102, %int0_5103 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5104 = torch.constant.int 6
    %3532 = torch.prims.convert_element_type %3531, %int6_5104 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_5105 = torch.constant.int 64
    %3533 = torch.aten.div.Scalar %3532, %int64_5105 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_5106 = torch.constant.float 1.000000e+04
    %3534 = torch.aten.pow.Scalar %float1.000000e04_5106, %3533 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %3535 = torch.aten.reciprocal %3534 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_5107 = torch.constant.float 1.000000e+00
    %3536 = torch.aten.mul.Scalar %3535, %float1.000000e00_5107 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_5108 = torch.constant.none
    %3537 = torch.aten.clone %155, %none_5108 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_5109 = torch.constant.int 0
    %3538 = torch.aten.unsqueeze %3536, %int0_5109 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_5110 = torch.constant.int 2
    %3539 = torch.aten.unsqueeze %3538, %int2_5110 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_5111 = torch.constant.none
    %none_5112 = torch.constant.none
    %int6_5113 = torch.constant.int 6
    %cpu_5114 = torch.constant.device "cpu"
    %int0_5115 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3539, %none_5111, %none_5112, %int6_5113, %cpu_5114, %int0_5115 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_5116 = torch.constant.int 4
    %int-1_5117 = torch.constant.int -1
    %int1_5118 = torch.constant.int 1
    %3540 = torch.prim.ListConstruct %int4_5116, %int-1_5117, %int1_5118 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5119 = torch.constant.bool false
    %3541 = torch.aten.expand %3539, %3540, %false_5119 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_5120 = torch.constant.int 1
    %3542 = torch.aten.unsqueeze %3530, %int1_5120 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_5121 = torch.constant.none
    %none_5122 = torch.constant.none
    %int4_5123 = torch.constant.int 4
    %cpu_5124 = torch.constant.device "cpu"
    %int0_5125 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3542, %none_5121, %none_5122, %int4_5123, %cpu_5124, %int0_5125 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5126 = torch.constant.int 6
    %3543 = torch.prims.convert_element_type %3542, %int6_5126 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3544 = torch.aten.matmul %3541, %3543 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_5127 = torch.constant.int 1
    %int2_5128 = torch.constant.int 2
    %3545 = torch.aten.transpose.int %3544, %int1_5127, %int2_5128 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %3546 = torch.aten.cos %3545 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %3547 = torch.aten.mul.Tensor %3546, %3537 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_5129 = torch.constant.none
    %none_5130 = torch.constant.none
    %int6_5131 = torch.constant.int 6
    %cpu_5132 = torch.constant.device "cpu"
    %int0_5133 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3547, %none_5129, %none_5130, %int6_5131, %cpu_5132, %int0_5133 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5134 = torch.constant.int 5
    %3548 = torch.prims.convert_element_type %3547, %int5_5134 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %3549 = torch.aten.sin %3545 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %3550 = torch.aten.mul.Tensor %3549, %3537 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_5135 = torch.constant.none
    %none_5136 = torch.constant.none
    %int6_5137 = torch.constant.int 6
    %cpu_5138 = torch.constant.device "cpu"
    %int0_5139 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3550, %none_5135, %none_5136, %int6_5137, %cpu_5138, %int0_5139 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5140 = torch.constant.int 5
    %3551 = torch.prims.convert_element_type %3550, %int5_5140 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_5141 = torch.constant.int 2
    %3552 = torch.aten.unsqueeze %3548, %int2_5141 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_5142 = torch.constant.int 2
    %3553 = torch.aten.unsqueeze %3551, %int2_5142 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_5143 = torch.constant.none
    %none_5144 = torch.constant.none
    %int5_5145 = torch.constant.int 5
    %cpu_5146 = torch.constant.device "cpu"
    %int0_5147 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3552, %none_5143, %none_5144, %int5_5145, %cpu_5146, %int0_5147 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_5148 = torch.constant.none
    %none_5149 = torch.constant.none
    %int5_5150 = torch.constant.int 5
    %cpu_5151 = torch.constant.device "cpu"
    %int0_5152 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3553, %none_5148, %none_5149, %int5_5150, %cpu_5151, %int0_5152 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_5153 = torch.constant.none
    %none_5154 = torch.constant.none
    %int5_5155 = torch.constant.int 5
    %cpu_5156 = torch.constant.device "cpu"
    %int0_5157 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3483, %none_5153, %none_5154, %int5_5155, %cpu_5156, %int0_5157 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_5158 = torch.constant.int 3
    %int0_5159 = torch.constant.int 0
    %int64_5160 = torch.constant.int 64
    %int2_5161 = torch.constant.int 2
    %3554 = torch.aten.slice.Tensor %3483, %int3_5158, %int0_5159, %int64_5160, %int2_5161 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_5162 = torch.constant.int 3
    %int1_5163 = torch.constant.int 1
    %int64_5164 = torch.constant.int 64
    %int2_5165 = torch.constant.int 2
    %3555 = torch.aten.slice.Tensor %3483, %int3_5162, %int1_5163, %int64_5164, %int2_5165 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %3556 = torch.aten.mul.Tensor %3554, %3552 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %3557 = torch.aten.mul.Tensor %3555, %3553 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_5166 = torch.constant.int 1
    %3558 = torch.aten.sub.Tensor %3556, %3557, %int1_5166 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %3559 = torch.aten.mul.Tensor %3555, %3552 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %3560 = torch.aten.mul.Tensor %3554, %3553 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_5167 = torch.constant.int 1
    %3561 = torch.aten.add.Tensor %3559, %3560, %int1_5167 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %3562 = torch_c.to_builtin_tensor %3558 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_5168 = tensor.cast %3562 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %3563 = torch_c.to_builtin_tensor %3561 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_5169 = tensor.cast %3563 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %3564 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_5168, %cast_5169) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_5170 = tensor.cast %3564 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %3565 = torch_c.from_builtin_tensor %cast_5170 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_5171 = torch.constant.int 4
    %int1_5172 = torch.constant.int 1
    %int4_5173 = torch.constant.int 4
    %int64_5174 = torch.constant.int 64
    %3566 = torch.prim.ListConstruct %int4_5171, %int1_5172, %int4_5173, %int64_5174 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3567 = torch.aten.view %3565, %3566 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_5175 = torch.constant.none
    %none_5176 = torch.constant.none
    %int5_5177 = torch.constant.int 5
    %cpu_5178 = torch.constant.device "cpu"
    %int0_5179 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3567, %none_5175, %none_5176, %int5_5177, %cpu_5178, %int0_5179 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_5180 = torch.constant.int 32
    %3568 = torch.aten.floor_divide.Scalar %arg2, %int32_5180 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_5181 = torch.constant.int 1
    %3569 = torch.aten.unsqueeze %3568, %int1_5181 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5182 = torch.constant.int 1
    %false_5183 = torch.constant.bool false
    %3570 = torch.aten.gather %arg3, %int1_5182, %3569, %false_5183 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_5184 = torch.constant.int 4
    %int1_5185 = torch.constant.int 1
    %int1_5186 = torch.constant.int 1
    %3571 = torch.prim.ListConstruct %int4_5184, %int1_5185, %int1_5186 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3572 = torch.aten.view %3570, %3571 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_5187 = torch.constant.int 32
    %3573 = torch.aten.remainder.Scalar %arg2, %int32_5187 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_5188 = torch.constant.int 4
    %int1_5189 = torch.constant.int 1
    %int1_5190 = torch.constant.int 1
    %3574 = torch.prim.ListConstruct %int4_5188, %int1_5189, %int1_5190 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3575 = torch.aten.view %3573, %3574 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_5191 = torch.constant.int 4
    %none_5192 = torch.constant.none
    %none_5193 = torch.constant.none
    %cpu_5194 = torch.constant.device "cpu"
    %false_5195 = torch.constant.bool false
    %3576 = torch.aten.arange %int4_5191, %none_5192, %none_5193, %cpu_5194, %false_5195 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_5196 = torch.constant.int 1
    %int1_5197 = torch.constant.int 1
    %int4_5198 = torch.constant.int 4
    %3577 = torch.prim.ListConstruct %int1_5196, %int1_5197, %int4_5198 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3578 = torch.aten.view %3576, %3577 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_5199 = torch.constant.none
    %3579 = torch.aten.clone %156, %none_5199 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_5200 = torch.constant.int 1
    %int1_5201 = torch.constant.int 1
    %int1_5202 = torch.constant.int 1
    %3580 = torch.prim.ListConstruct %int1_5200, %int1_5201, %int1_5202 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3581 = torch.aten.view %3579, %3580 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_5203 = torch.constant.int 22
    %3582 = torch.aten.mul.Scalar %3572, %int22_5203 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int9 = torch.constant.int 9
    %int1_5204 = torch.constant.int 1
    %3583 = torch.aten.add.Scalar %3582, %int9, %int1_5204 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_5205 = torch.constant.int 2
    %3584 = torch.aten.mul.Scalar %3583, %int2_5205 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_5206 = torch.constant.int 1
    %3585 = torch.aten.add.Tensor %3584, %3581, %int1_5206 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_5207 = torch.constant.int 4
    %3586 = torch.aten.mul.Scalar %3585, %int4_5207 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_5208 = torch.constant.int 1
    %3587 = torch.aten.add.Tensor %3586, %3578, %int1_5208 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_5209 = torch.constant.int 32
    %3588 = torch.aten.mul.Scalar %3587, %int32_5209 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_5210 = torch.constant.int 1
    %3589 = torch.aten.add.Tensor %3588, %3575, %int1_5210 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_5211 = torch.constant.none
    %none_5212 = torch.constant.none
    %int5_5213 = torch.constant.int 5
    %cpu_5214 = torch.constant.device "cpu"
    %int0_5215 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3567, %none_5211, %none_5212, %int5_5213, %cpu_5214, %int0_5215 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_5216 = torch.constant.int 22
    %int2_5217 = torch.constant.int 2
    %int4_5218 = torch.constant.int 4
    %int32_5219 = torch.constant.int 32
    %int64_5220 = torch.constant.int 64
    %3590 = torch.prim.ListConstruct %381, %int22_5216, %int2_5217, %int4_5218, %int32_5219, %int64_5220 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3591 = torch.aten.view %3285, %3590 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3591, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_5221 = torch.constant.int 64
    %3592 = torch.prim.ListConstruct %553, %int64_5221 : (!torch.int, !torch.int) -> !torch.list<int>
    %3593 = torch.aten.view %3591, %3592 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %3593, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %3594 = torch.prim.ListConstruct %3589 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_5222 = torch.constant.bool false
    %3595 = torch.aten.index_put %3593, %3594, %3567, %false_5222 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %3595, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_5223 = torch.constant.int 22
    %int2_5224 = torch.constant.int 2
    %int4_5225 = torch.constant.int 4
    %int32_5226 = torch.constant.int 32
    %int64_5227 = torch.constant.int 64
    %3596 = torch.prim.ListConstruct %381, %int22_5223, %int2_5224, %int4_5225, %int32_5226, %int64_5227 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3597 = torch.aten.view %3595, %3596 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3597, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_5228 = torch.constant.int 360448
    %3598 = torch.prim.ListConstruct %381, %int360448_5228 : (!torch.int, !torch.int) -> !torch.list<int>
    %3599 = torch.aten.view %3597, %3598 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %3599, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_5229 = torch.constant.int 22
    %int2_5230 = torch.constant.int 2
    %int4_5231 = torch.constant.int 4
    %int32_5232 = torch.constant.int 32
    %int64_5233 = torch.constant.int 64
    %3600 = torch.prim.ListConstruct %381, %int22_5229, %int2_5230, %int4_5231, %int32_5232, %int64_5233 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3601 = torch.aten.view %3599, %3600 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3601, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_5234 = torch.constant.int 64
    %3602 = torch.prim.ListConstruct %553, %int64_5234 : (!torch.int, !torch.int) -> !torch.list<int>
    %3603 = torch.aten.view %3601, %3602 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %3603, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_5235 = torch.constant.none
    %3604 = torch.aten.clone %157, %none_5235 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_5236 = torch.constant.int 1
    %int1_5237 = torch.constant.int 1
    %int1_5238 = torch.constant.int 1
    %3605 = torch.prim.ListConstruct %int1_5236, %int1_5237, %int1_5238 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3606 = torch.aten.view %3604, %3605 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_5239 = torch.constant.int 22
    %3607 = torch.aten.mul.Scalar %3572, %int22_5239 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int9_5240 = torch.constant.int 9
    %int1_5241 = torch.constant.int 1
    %3608 = torch.aten.add.Scalar %3607, %int9_5240, %int1_5241 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_5242 = torch.constant.int 2
    %3609 = torch.aten.mul.Scalar %3608, %int2_5242 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_5243 = torch.constant.int 1
    %3610 = torch.aten.add.Tensor %3609, %3606, %int1_5243 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_5244 = torch.constant.int 4
    %3611 = torch.aten.mul.Scalar %3610, %int4_5244 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_5245 = torch.constant.int 1
    %3612 = torch.aten.add.Tensor %3611, %3578, %int1_5245 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_5246 = torch.constant.int 32
    %3613 = torch.aten.mul.Scalar %3612, %int32_5246 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_5247 = torch.constant.int 1
    %3614 = torch.aten.add.Tensor %3613, %3575, %int1_5247 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_5248 = torch.constant.none
    %none_5249 = torch.constant.none
    %int5_5250 = torch.constant.int 5
    %cpu_5251 = torch.constant.device "cpu"
    %int0_5252 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3485, %none_5248, %none_5249, %int5_5250, %cpu_5251, %int0_5252 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %3615 = torch.prim.ListConstruct %3614 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_5253 = torch.constant.bool false
    %3616 = torch.aten.index_put %3603, %3615, %3485, %false_5253 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %3616, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_5254 = torch.constant.int 22
    %int2_5255 = torch.constant.int 2
    %int4_5256 = torch.constant.int 4
    %int32_5257 = torch.constant.int 32
    %int64_5258 = torch.constant.int 64
    %3617 = torch.prim.ListConstruct %381, %int22_5254, %int2_5255, %int4_5256, %int32_5257, %int64_5258 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3618 = torch.aten.view %3616, %3617 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3618, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_5259 = torch.constant.int 360448
    %3619 = torch.prim.ListConstruct %381, %int360448_5259 : (!torch.int, !torch.int) -> !torch.list<int>
    %3620 = torch.aten.view %3618, %3619 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %3620, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_5260 = torch.constant.none
    %3621 = torch.aten.clone %158, %none_5260 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_5261 = torch.constant.none
    %3622 = torch.aten.clone %159, %none_5261 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_5262 = torch.constant.none
    %3623 = torch.aten.clone %160, %none_5262 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_5263 = torch.constant.int 22
    %int2_5264 = torch.constant.int 2
    %int4_5265 = torch.constant.int 4
    %int32_5266 = torch.constant.int 32
    %int64_5267 = torch.constant.int 64
    %3624 = torch.prim.ListConstruct %381, %int22_5263, %int2_5264, %int4_5265, %int32_5266, %int64_5267 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3625 = torch.aten.view %3620, %3624 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3625, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %3626 = torch_c.to_builtin_tensor %3625 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %3627 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_5268 = tensor.cast %3627 : tensor<4x?xi64> to tensor<?x?xi64>
    %3628 = torch_c.to_builtin_tensor %3621 : !torch.vtensor<[],si64> -> tensor<i64>
    %3629 = torch_c.to_builtin_tensor %3622 : !torch.vtensor<[],si64> -> tensor<i64>
    %3630 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%3626, %cast_5268, %3628, %3629) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_5269 = tensor.cast %3630 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %3631 = torch_c.from_builtin_tensor %cast_5269 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %3631, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %3632 = torch_c.to_builtin_tensor %3625 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %3633 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_5270 = tensor.cast %3633 : tensor<4x?xi64> to tensor<?x?xi64>
    %3634 = torch_c.to_builtin_tensor %3621 : !torch.vtensor<[],si64> -> tensor<i64>
    %3635 = torch_c.to_builtin_tensor %3623 : !torch.vtensor<[],si64> -> tensor<i64>
    %3636 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%3632, %cast_5270, %3634, %3635) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_5271 = tensor.cast %3636 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %3637 = torch_c.from_builtin_tensor %cast_5271 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %3637, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_5272 = torch.constant.int 2
    %int3_5273 = torch.constant.int 3
    %3638 = torch.aten.transpose.int %3631, %int2_5272, %int3_5273 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %3638, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_5274 = torch.constant.int 0
    %3639 = torch.aten.clone %3638, %int0_5274 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %3639, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_5275 = torch.constant.int 4
    %int4_5276 = torch.constant.int 4
    %int64_5277 = torch.constant.int 64
    %3640 = torch.prim.ListConstruct %int4_5275, %623, %int4_5276, %int64_5277 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3641 = torch.aten._unsafe_view %3639, %3640 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %3641, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_5278 = torch.constant.int 2
    %int3_5279 = torch.constant.int 3
    %3642 = torch.aten.transpose.int %3637, %int2_5278, %int3_5279 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %3642, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_5280 = torch.constant.int 0
    %3643 = torch.aten.clone %3642, %int0_5280 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %3643, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_5281 = torch.constant.int 4
    %int4_5282 = torch.constant.int 4
    %int64_5283 = torch.constant.int 64
    %3644 = torch.prim.ListConstruct %int4_5281, %623, %int4_5282, %int64_5283 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3645 = torch.aten._unsafe_view %3643, %3644 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %3645, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_5284 = torch.constant.int 0
    %int1_5285 = torch.constant.int 1
    %none_5286 = torch.constant.none
    %none_5287 = torch.constant.none
    %cpu_5288 = torch.constant.device "cpu"
    %false_5289 = torch.constant.bool false
    %3646 = torch.aten.arange.start_step %int0_5284, %623, %int1_5285, %none_5286, %none_5287, %cpu_5288, %false_5289 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3646, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_5290 = torch.constant.int -1
    %3647 = torch.aten.unsqueeze %arg1, %int-1_5290 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %3648 = torch.aten.ge.Tensor %3646, %3647 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %3648, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_5291 = torch.constant.none
    %3649 = torch.aten.clone %161, %none_5291 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_5292 = torch.constant.int 0
    %3650 = torch.aten.where.ScalarOther %3648, %3649, %int0_5292 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %3650, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_5293 = torch.constant.none
    %none_5294 = torch.constant.none
    %int5_5295 = torch.constant.int 5
    %cpu_5296 = torch.constant.device "cpu"
    %int0_5297 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3650, %none_5293, %none_5294, %int5_5295, %cpu_5296, %int0_5297 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_5298 = torch.constant.int 1
    %3651 = torch.aten.unsqueeze %3650, %int1_5298 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %3651, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_5299 = torch.constant.int 1
    %3652 = torch.aten.unsqueeze %3651, %int1_5299 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %3652, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_5300 = torch.constant.int -2
    %3653 = torch.aten.unsqueeze %3641, %int-2_5300 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %3653, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_5301 = torch.constant.int 4
    %int4_5302 = torch.constant.int 4
    %int8_5303 = torch.constant.int 8
    %int64_5304 = torch.constant.int 64
    %3654 = torch.prim.ListConstruct %int4_5301, %623, %int4_5302, %int8_5303, %int64_5304 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5305 = torch.constant.bool false
    %3655 = torch.aten.expand %3653, %3654, %false_5305 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3655, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_5306 = torch.constant.int 0
    %3656 = torch.aten.clone %3655, %int0_5306 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3656, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_5307 = torch.constant.int 4
    %int32_5308 = torch.constant.int 32
    %int64_5309 = torch.constant.int 64
    %3657 = torch.prim.ListConstruct %int4_5307, %623, %int32_5308, %int64_5309 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3658 = torch.aten._unsafe_view %3656, %3657 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3658, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_5310 = torch.constant.int -2
    %3659 = torch.aten.unsqueeze %3645, %int-2_5310 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %3659, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_5311 = torch.constant.int 4
    %int4_5312 = torch.constant.int 4
    %int8_5313 = torch.constant.int 8
    %int64_5314 = torch.constant.int 64
    %3660 = torch.prim.ListConstruct %int4_5311, %623, %int4_5312, %int8_5313, %int64_5314 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5315 = torch.constant.bool false
    %3661 = torch.aten.expand %3659, %3660, %false_5315 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3661, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_5316 = torch.constant.int 0
    %3662 = torch.aten.clone %3661, %int0_5316 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3662, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_5317 = torch.constant.int 4
    %int32_5318 = torch.constant.int 32
    %int64_5319 = torch.constant.int 64
    %3663 = torch.prim.ListConstruct %int4_5317, %623, %int32_5318, %int64_5319 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3664 = torch.aten._unsafe_view %3662, %3663 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3664, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_5320 = torch.constant.int 1
    %int2_5321 = torch.constant.int 2
    %3665 = torch.aten.transpose.int %3526, %int1_5320, %int2_5321 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_5322 = torch.constant.int 1
    %int2_5323 = torch.constant.int 2
    %3666 = torch.aten.transpose.int %3658, %int1_5322, %int2_5323 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %3666, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_5324 = torch.constant.int 1
    %int2_5325 = torch.constant.int 2
    %3667 = torch.aten.transpose.int %3664, %int1_5324, %int2_5325 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %3667, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_5326 = torch.constant.float 0.000000e+00
    %false_5327 = torch.constant.bool false
    %none_5328 = torch.constant.none
    %false_5329 = torch.constant.bool false
    %3668 = torch.aten.scaled_dot_product_attention %3665, %3666, %3667, %3652, %float0.000000e00_5326, %false_5327, %none_5328, %false_5329 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_5330 = torch.constant.int 1
    %int2_5331 = torch.constant.int 2
    %3669 = torch.aten.transpose.int %3668, %int1_5330, %int2_5331 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_5332 = torch.constant.int 4
    %int1_5333 = torch.constant.int 1
    %int2048_5334 = torch.constant.int 2048
    %3670 = torch.prim.ListConstruct %int4_5332, %int1_5333, %int2048_5334 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3671 = torch.aten.view %3669, %3670 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_5335 = torch.constant.int 2
    %3672 = torch.aten.view.dtype %166, %int2_5335 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %3673 = torch.aten.detach %3672 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_5336 = torch.constant.int -1
    %int17_5337 = torch.constant.int 17
    %3674 = torch.prim.ListConstruct %int-1_5336, %int17_5337 : (!torch.int, !torch.int) -> !torch.list<int>
    %3675 = torch.aten.view %3673, %3674 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_5338 = torch.constant.int 2048
    %int-1_5339 = torch.constant.int -1
    %int17_5340 = torch.constant.int 17
    %3676 = torch.prim.ListConstruct %int2048_5338, %int-1_5339, %int17_5340 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3677 = torch.aten.view %3675, %3676 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_5341 = torch.constant.int 2
    %int0_5342 = torch.constant.int 0
    %int1_5343 = torch.constant.int 1
    %int1_5344 = torch.constant.int 1
    %3678 = torch.aten.slice.Tensor %3677, %int2_5341, %int0_5342, %int1_5343, %int1_5344 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_5345 = torch.constant.int 5
    %3679 = torch.aten.view.dtype %3678, %int5_5345 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %3680 = torch.aten.detach %3679 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_5346 = torch.constant.int 2
    %int1_5347 = torch.constant.int 1
    %int9223372036854775807_5348 = torch.constant.int 9223372036854775807
    %int1_5349 = torch.constant.int 1
    %3681 = torch.aten.slice.Tensor %3677, %int2_5346, %int1_5347, %int9223372036854775807_5348, %int1_5349 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_5350 = torch.constant.int 1
    %3682 = torch.aten.view.dtype %3681, %int1_5350 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %3683 = torch.aten.detach %3682 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %3684 = torch_c.to_builtin_tensor %3671 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_5351 = tensor.cast %3684 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %3685 = torch_c.to_builtin_tensor %3680 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %3686 = torch_c.to_builtin_tensor %3683 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %3687 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_5351, %3685, %3686) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_5352 = tensor.cast %3687 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %3688 = torch_c.from_builtin_tensor %cast_5352 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_5353 = torch.constant.none
    %none_5354 = torch.constant.none
    %int5_5355 = torch.constant.int 5
    %cpu_5356 = torch.constant.device "cpu"
    %int0_5357 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3688, %none_5353, %none_5354, %int5_5355, %cpu_5356, %int0_5357 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_5358 = torch.constant.int 1
    %3689 = torch.aten.add.Tensor %3418, %3688, %int1_5358 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_5359 = torch.constant.none
    %none_5360 = torch.constant.none
    %int5_5361 = torch.constant.int 5
    %cpu_5362 = torch.constant.device "cpu"
    %int0_5363 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3689, %none_5359, %none_5360, %int5_5361, %cpu_5362, %int0_5363 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5364 = torch.constant.int 6
    %3690 = torch.prims.convert_element_type %3689, %int6_5364 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_5365 = torch.constant.int 2
    %3691 = torch.aten.pow.Tensor_Scalar %3690, %int2_5365 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_5366 = torch.constant.int -1
    %3692 = torch.prim.ListConstruct %int-1_5366 : (!torch.int) -> !torch.list<int>
    %true_5367 = torch.constant.bool true
    %none_5368 = torch.constant.none
    %3693 = torch.aten.mean.dim %3691, %3692, %true_5367, %none_5368 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_5369 = torch.constant.float 9.9999997473787516E-6
    %int1_5370 = torch.constant.int 1
    %3694 = torch.aten.add.Scalar %3693, %float9.999990e-06_5369, %int1_5370 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3695 = torch.aten.rsqrt %3694 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %3696 = torch.aten.mul.Tensor %3690, %3695 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_5371 = torch.constant.none
    %none_5372 = torch.constant.none
    %int6_5373 = torch.constant.int 6
    %cpu_5374 = torch.constant.device "cpu"
    %int0_5375 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3696, %none_5371, %none_5372, %int6_5373, %cpu_5374, %int0_5375 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5376 = torch.constant.int 5
    %3697 = torch.prims.convert_element_type %3696, %int5_5376 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %3698 = torch.aten.mul.Tensor %167, %3697 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_5377 = torch.constant.none
    %none_5378 = torch.constant.none
    %int6_5379 = torch.constant.int 6
    %cpu_5380 = torch.constant.device "cpu"
    %int0_5381 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3698, %none_5377, %none_5378, %int6_5379, %cpu_5380, %int0_5381 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5382 = torch.constant.int 5
    %3699 = torch.prims.convert_element_type %3698, %int5_5382 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_5383 = torch.constant.int 2
    %3700 = torch.aten.view.dtype %168, %int2_5383 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %3701 = torch.aten.detach %3700 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_5384 = torch.constant.int -1
    %int17_5385 = torch.constant.int 17
    %3702 = torch.prim.ListConstruct %int-1_5384, %int17_5385 : (!torch.int, !torch.int) -> !torch.list<int>
    %3703 = torch.aten.view %3701, %3702 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_5386 = torch.constant.int 5632
    %int-1_5387 = torch.constant.int -1
    %int17_5388 = torch.constant.int 17
    %3704 = torch.prim.ListConstruct %int5632_5386, %int-1_5387, %int17_5388 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3705 = torch.aten.view %3703, %3704 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_5389 = torch.constant.int 2
    %int0_5390 = torch.constant.int 0
    %int1_5391 = torch.constant.int 1
    %int1_5392 = torch.constant.int 1
    %3706 = torch.aten.slice.Tensor %3705, %int2_5389, %int0_5390, %int1_5391, %int1_5392 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_5393 = torch.constant.int 5
    %3707 = torch.aten.view.dtype %3706, %int5_5393 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %3708 = torch.aten.detach %3707 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_5394 = torch.constant.int 2
    %int1_5395 = torch.constant.int 1
    %int9223372036854775807_5396 = torch.constant.int 9223372036854775807
    %int1_5397 = torch.constant.int 1
    %3709 = torch.aten.slice.Tensor %3705, %int2_5394, %int1_5395, %int9223372036854775807_5396, %int1_5397 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_5398 = torch.constant.int 1
    %3710 = torch.aten.view.dtype %3709, %int1_5398 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %3711 = torch.aten.detach %3710 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %3712 = torch_c.to_builtin_tensor %3699 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_5399 = tensor.cast %3712 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %3713 = torch_c.to_builtin_tensor %3708 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %3714 = torch_c.to_builtin_tensor %3711 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %3715 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_5399, %3713, %3714) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_5400 = tensor.cast %3715 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %3716 = torch_c.from_builtin_tensor %cast_5400 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %3717 = torch.aten.silu %3716 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_5401 = torch.constant.int 2
    %3718 = torch.aten.view.dtype %169, %int2_5401 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %3719 = torch.aten.detach %3718 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_5402 = torch.constant.int -1
    %int17_5403 = torch.constant.int 17
    %3720 = torch.prim.ListConstruct %int-1_5402, %int17_5403 : (!torch.int, !torch.int) -> !torch.list<int>
    %3721 = torch.aten.view %3719, %3720 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_5404 = torch.constant.int 5632
    %int-1_5405 = torch.constant.int -1
    %int17_5406 = torch.constant.int 17
    %3722 = torch.prim.ListConstruct %int5632_5404, %int-1_5405, %int17_5406 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3723 = torch.aten.view %3721, %3722 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_5407 = torch.constant.int 2
    %int0_5408 = torch.constant.int 0
    %int1_5409 = torch.constant.int 1
    %int1_5410 = torch.constant.int 1
    %3724 = torch.aten.slice.Tensor %3723, %int2_5407, %int0_5408, %int1_5409, %int1_5410 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_5411 = torch.constant.int 5
    %3725 = torch.aten.view.dtype %3724, %int5_5411 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %3726 = torch.aten.detach %3725 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_5412 = torch.constant.int 2
    %int1_5413 = torch.constant.int 1
    %int9223372036854775807_5414 = torch.constant.int 9223372036854775807
    %int1_5415 = torch.constant.int 1
    %3727 = torch.aten.slice.Tensor %3723, %int2_5412, %int1_5413, %int9223372036854775807_5414, %int1_5415 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_5416 = torch.constant.int 1
    %3728 = torch.aten.view.dtype %3727, %int1_5416 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %3729 = torch.aten.detach %3728 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %3730 = torch_c.to_builtin_tensor %3699 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_5417 = tensor.cast %3730 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %3731 = torch_c.to_builtin_tensor %3726 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %3732 = torch_c.to_builtin_tensor %3729 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %3733 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_5417, %3731, %3732) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_5418 = tensor.cast %3733 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %3734 = torch_c.from_builtin_tensor %cast_5418 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %3735 = torch.aten.mul.Tensor %3717, %3734 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_5419 = torch.constant.int 2
    %3736 = torch.aten.view.dtype %170, %int2_5419 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %3737 = torch.aten.detach %3736 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_5420 = torch.constant.int -1
    %int17_5421 = torch.constant.int 17
    %3738 = torch.prim.ListConstruct %int-1_5420, %int17_5421 : (!torch.int, !torch.int) -> !torch.list<int>
    %3739 = torch.aten.view %3737, %3738 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_5422 = torch.constant.int 2048
    %int-1_5423 = torch.constant.int -1
    %int17_5424 = torch.constant.int 17
    %3740 = torch.prim.ListConstruct %int2048_5422, %int-1_5423, %int17_5424 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3741 = torch.aten.view %3739, %3740 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_5425 = torch.constant.int 2
    %int0_5426 = torch.constant.int 0
    %int1_5427 = torch.constant.int 1
    %int1_5428 = torch.constant.int 1
    %3742 = torch.aten.slice.Tensor %3741, %int2_5425, %int0_5426, %int1_5427, %int1_5428 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_5429 = torch.constant.int 5
    %3743 = torch.aten.view.dtype %3742, %int5_5429 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %3744 = torch.aten.detach %3743 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_5430 = torch.constant.int 2
    %int1_5431 = torch.constant.int 1
    %int9223372036854775807_5432 = torch.constant.int 9223372036854775807
    %int1_5433 = torch.constant.int 1
    %3745 = torch.aten.slice.Tensor %3741, %int2_5430, %int1_5431, %int9223372036854775807_5432, %int1_5433 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_5434 = torch.constant.int 1
    %3746 = torch.aten.view.dtype %3745, %int1_5434 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %3747 = torch.aten.detach %3746 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %3748 = torch_c.to_builtin_tensor %3735 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_5435 = tensor.cast %3748 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %3749 = torch_c.to_builtin_tensor %3744 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %3750 = torch_c.to_builtin_tensor %3747 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %3751 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_5435, %3749, %3750) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_5436 = tensor.cast %3751 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %3752 = torch_c.from_builtin_tensor %cast_5436 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_5437 = torch.constant.int 1
    %3753 = torch.aten.add.Tensor %3689, %3752, %int1_5437 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_5438 = torch.constant.none
    %none_5439 = torch.constant.none
    %int5_5440 = torch.constant.int 5
    %cpu_5441 = torch.constant.device "cpu"
    %int0_5442 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3753, %none_5438, %none_5439, %int5_5440, %cpu_5441, %int0_5442 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5443 = torch.constant.int 6
    %3754 = torch.prims.convert_element_type %3753, %int6_5443 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_5444 = torch.constant.int 2
    %3755 = torch.aten.pow.Tensor_Scalar %3754, %int2_5444 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_5445 = torch.constant.int -1
    %3756 = torch.prim.ListConstruct %int-1_5445 : (!torch.int) -> !torch.list<int>
    %true_5446 = torch.constant.bool true
    %none_5447 = torch.constant.none
    %3757 = torch.aten.mean.dim %3755, %3756, %true_5446, %none_5447 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_5448 = torch.constant.float 9.9999997473787516E-6
    %int1_5449 = torch.constant.int 1
    %3758 = torch.aten.add.Scalar %3757, %float9.999990e-06_5448, %int1_5449 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3759 = torch.aten.rsqrt %3758 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %3760 = torch.aten.mul.Tensor %3754, %3759 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_5450 = torch.constant.none
    %none_5451 = torch.constant.none
    %int6_5452 = torch.constant.int 6
    %cpu_5453 = torch.constant.device "cpu"
    %int0_5454 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3760, %none_5450, %none_5451, %int6_5452, %cpu_5453, %int0_5454 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5455 = torch.constant.int 5
    %3761 = torch.prims.convert_element_type %3760, %int5_5455 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %3762 = torch.aten.mul.Tensor %179, %3761 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_5456 = torch.constant.none
    %none_5457 = torch.constant.none
    %int6_5458 = torch.constant.int 6
    %cpu_5459 = torch.constant.device "cpu"
    %int0_5460 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3762, %none_5456, %none_5457, %int6_5458, %cpu_5459, %int0_5460 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5461 = torch.constant.int 5
    %3763 = torch.prims.convert_element_type %3762, %int5_5461 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_5462 = torch.constant.int 2
    %3764 = torch.aten.view.dtype %180, %int2_5462 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %3765 = torch.aten.detach %3764 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_5463 = torch.constant.int -1
    %int17_5464 = torch.constant.int 17
    %3766 = torch.prim.ListConstruct %int-1_5463, %int17_5464 : (!torch.int, !torch.int) -> !torch.list<int>
    %3767 = torch.aten.view %3765, %3766 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_5465 = torch.constant.int 2048
    %int-1_5466 = torch.constant.int -1
    %int17_5467 = torch.constant.int 17
    %3768 = torch.prim.ListConstruct %int2048_5465, %int-1_5466, %int17_5467 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3769 = torch.aten.view %3767, %3768 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_5468 = torch.constant.int 2
    %int0_5469 = torch.constant.int 0
    %int1_5470 = torch.constant.int 1
    %int1_5471 = torch.constant.int 1
    %3770 = torch.aten.slice.Tensor %3769, %int2_5468, %int0_5469, %int1_5470, %int1_5471 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_5472 = torch.constant.int 5
    %3771 = torch.aten.view.dtype %3770, %int5_5472 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %3772 = torch.aten.detach %3771 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_5473 = torch.constant.int 2
    %int1_5474 = torch.constant.int 1
    %int9223372036854775807_5475 = torch.constant.int 9223372036854775807
    %int1_5476 = torch.constant.int 1
    %3773 = torch.aten.slice.Tensor %3769, %int2_5473, %int1_5474, %int9223372036854775807_5475, %int1_5476 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_5477 = torch.constant.int 1
    %3774 = torch.aten.view.dtype %3773, %int1_5477 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %3775 = torch.aten.detach %3774 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %3776 = torch_c.to_builtin_tensor %3763 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_5478 = tensor.cast %3776 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %3777 = torch_c.to_builtin_tensor %3772 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %3778 = torch_c.to_builtin_tensor %3775 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %3779 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_5478, %3777, %3778) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_5479 = tensor.cast %3779 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %3780 = torch_c.from_builtin_tensor %cast_5479 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_5480 = torch.constant.int 2
    %3781 = torch.aten.view.dtype %181, %int2_5480 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %3782 = torch.aten.detach %3781 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_5481 = torch.constant.int -1
    %int17_5482 = torch.constant.int 17
    %3783 = torch.prim.ListConstruct %int-1_5481, %int17_5482 : (!torch.int, !torch.int) -> !torch.list<int>
    %3784 = torch.aten.view %3782, %3783 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_5483 = torch.constant.int 256
    %int-1_5484 = torch.constant.int -1
    %int17_5485 = torch.constant.int 17
    %3785 = torch.prim.ListConstruct %int256_5483, %int-1_5484, %int17_5485 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3786 = torch.aten.view %3784, %3785 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_5486 = torch.constant.int 2
    %int0_5487 = torch.constant.int 0
    %int1_5488 = torch.constant.int 1
    %int1_5489 = torch.constant.int 1
    %3787 = torch.aten.slice.Tensor %3786, %int2_5486, %int0_5487, %int1_5488, %int1_5489 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_5490 = torch.constant.int 5
    %3788 = torch.aten.view.dtype %3787, %int5_5490 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %3789 = torch.aten.detach %3788 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_5491 = torch.constant.int 2
    %int1_5492 = torch.constant.int 1
    %int9223372036854775807_5493 = torch.constant.int 9223372036854775807
    %int1_5494 = torch.constant.int 1
    %3790 = torch.aten.slice.Tensor %3786, %int2_5491, %int1_5492, %int9223372036854775807_5493, %int1_5494 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_5495 = torch.constant.int 1
    %3791 = torch.aten.view.dtype %3790, %int1_5495 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %3792 = torch.aten.detach %3791 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %3793 = torch_c.to_builtin_tensor %3763 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_5496 = tensor.cast %3793 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %3794 = torch_c.to_builtin_tensor %3789 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %3795 = torch_c.to_builtin_tensor %3792 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %3796 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_5496, %3794, %3795) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_5497 = tensor.cast %3796 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %3797 = torch_c.from_builtin_tensor %cast_5497 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_5498 = torch.constant.int 2
    %3798 = torch.aten.view.dtype %182, %int2_5498 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %3799 = torch.aten.detach %3798 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_5499 = torch.constant.int -1
    %int17_5500 = torch.constant.int 17
    %3800 = torch.prim.ListConstruct %int-1_5499, %int17_5500 : (!torch.int, !torch.int) -> !torch.list<int>
    %3801 = torch.aten.view %3799, %3800 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_5501 = torch.constant.int 256
    %int-1_5502 = torch.constant.int -1
    %int17_5503 = torch.constant.int 17
    %3802 = torch.prim.ListConstruct %int256_5501, %int-1_5502, %int17_5503 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3803 = torch.aten.view %3801, %3802 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_5504 = torch.constant.int 2
    %int0_5505 = torch.constant.int 0
    %int1_5506 = torch.constant.int 1
    %int1_5507 = torch.constant.int 1
    %3804 = torch.aten.slice.Tensor %3803, %int2_5504, %int0_5505, %int1_5506, %int1_5507 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_5508 = torch.constant.int 5
    %3805 = torch.aten.view.dtype %3804, %int5_5508 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %3806 = torch.aten.detach %3805 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_5509 = torch.constant.int 2
    %int1_5510 = torch.constant.int 1
    %int9223372036854775807_5511 = torch.constant.int 9223372036854775807
    %int1_5512 = torch.constant.int 1
    %3807 = torch.aten.slice.Tensor %3803, %int2_5509, %int1_5510, %int9223372036854775807_5511, %int1_5512 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_5513 = torch.constant.int 1
    %3808 = torch.aten.view.dtype %3807, %int1_5513 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %3809 = torch.aten.detach %3808 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %3810 = torch_c.to_builtin_tensor %3763 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_5514 = tensor.cast %3810 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %3811 = torch_c.to_builtin_tensor %3806 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %3812 = torch_c.to_builtin_tensor %3809 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %3813 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_5514, %3811, %3812) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_5515 = tensor.cast %3813 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %3814 = torch_c.from_builtin_tensor %cast_5515 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_5516 = torch.constant.int 4
    %int1_5517 = torch.constant.int 1
    %int32_5518 = torch.constant.int 32
    %int64_5519 = torch.constant.int 64
    %3815 = torch.prim.ListConstruct %int4_5516, %int1_5517, %int32_5518, %int64_5519 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3816 = torch.aten.view %3780, %3815 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_5520 = torch.constant.int 4
    %int1_5521 = torch.constant.int 1
    %int4_5522 = torch.constant.int 4
    %int64_5523 = torch.constant.int 64
    %3817 = torch.prim.ListConstruct %int4_5520, %int1_5521, %int4_5522, %int64_5523 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3818 = torch.aten.view %3797, %3817 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_5524 = torch.constant.int 4
    %int1_5525 = torch.constant.int 1
    %int4_5526 = torch.constant.int 4
    %int64_5527 = torch.constant.int 64
    %3819 = torch.prim.ListConstruct %int4_5524, %int1_5525, %int4_5526, %int64_5527 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3820 = torch.aten.view %3814, %3819 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_5528 = torch.constant.int 0
    %int1_5529 = torch.constant.int 1
    %none_5530 = torch.constant.none
    %none_5531 = torch.constant.none
    %cpu_5532 = torch.constant.device "cpu"
    %false_5533 = torch.constant.bool false
    %3821 = torch.aten.arange.start %int0_5528, %int1_5529, %none_5530, %none_5531, %cpu_5532, %false_5533 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_5534 = torch.constant.int 0
    %3822 = torch.aten.unsqueeze %3821, %int0_5534 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_5535 = torch.constant.int 1
    %3823 = torch.aten.unsqueeze %arg2, %int1_5535 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5536 = torch.constant.int 1
    %3824 = torch.aten.add.Tensor %3822, %3823, %int1_5536 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_5537 = torch.constant.int 0
    %int64_5538 = torch.constant.int 64
    %int2_5539 = torch.constant.int 2
    %none_5540 = torch.constant.none
    %none_5541 = torch.constant.none
    %cpu_5542 = torch.constant.device "cpu"
    %false_5543 = torch.constant.bool false
    %3825 = torch.aten.arange.start_step %int0_5537, %int64_5538, %int2_5539, %none_5540, %none_5541, %cpu_5542, %false_5543 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_5544 = torch.constant.none
    %none_5545 = torch.constant.none
    %int4_5546 = torch.constant.int 4
    %cpu_5547 = torch.constant.device "cpu"
    %int0_5548 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3825, %none_5544, %none_5545, %int4_5546, %cpu_5547, %int0_5548 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5549 = torch.constant.int 6
    %3826 = torch.prims.convert_element_type %3825, %int6_5549 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_5550 = torch.constant.int 64
    %3827 = torch.aten.div.Scalar %3826, %int64_5550 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_5551 = torch.constant.float 1.000000e+04
    %3828 = torch.aten.pow.Scalar %float1.000000e04_5551, %3827 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %3829 = torch.aten.reciprocal %3828 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_5552 = torch.constant.float 1.000000e+00
    %3830 = torch.aten.mul.Scalar %3829, %float1.000000e00_5552 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_5553 = torch.constant.none
    %3831 = torch.aten.clone %171, %none_5553 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_5554 = torch.constant.int 0
    %3832 = torch.aten.unsqueeze %3830, %int0_5554 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_5555 = torch.constant.int 2
    %3833 = torch.aten.unsqueeze %3832, %int2_5555 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_5556 = torch.constant.none
    %none_5557 = torch.constant.none
    %int6_5558 = torch.constant.int 6
    %cpu_5559 = torch.constant.device "cpu"
    %int0_5560 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3833, %none_5556, %none_5557, %int6_5558, %cpu_5559, %int0_5560 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_5561 = torch.constant.int 4
    %int-1_5562 = torch.constant.int -1
    %int1_5563 = torch.constant.int 1
    %3834 = torch.prim.ListConstruct %int4_5561, %int-1_5562, %int1_5563 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5564 = torch.constant.bool false
    %3835 = torch.aten.expand %3833, %3834, %false_5564 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_5565 = torch.constant.int 1
    %3836 = torch.aten.unsqueeze %3824, %int1_5565 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_5566 = torch.constant.none
    %none_5567 = torch.constant.none
    %int4_5568 = torch.constant.int 4
    %cpu_5569 = torch.constant.device "cpu"
    %int0_5570 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3836, %none_5566, %none_5567, %int4_5568, %cpu_5569, %int0_5570 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5571 = torch.constant.int 6
    %3837 = torch.prims.convert_element_type %3836, %int6_5571 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3838 = torch.aten.matmul %3835, %3837 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_5572 = torch.constant.int 1
    %int2_5573 = torch.constant.int 2
    %3839 = torch.aten.transpose.int %3838, %int1_5572, %int2_5573 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %3840 = torch.aten.cos %3839 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %3841 = torch.aten.mul.Tensor %3840, %3831 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_5574 = torch.constant.none
    %none_5575 = torch.constant.none
    %int6_5576 = torch.constant.int 6
    %cpu_5577 = torch.constant.device "cpu"
    %int0_5578 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3841, %none_5574, %none_5575, %int6_5576, %cpu_5577, %int0_5578 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5579 = torch.constant.int 5
    %3842 = torch.prims.convert_element_type %3841, %int5_5579 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %3843 = torch.aten.sin %3839 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %3844 = torch.aten.mul.Tensor %3843, %3831 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_5580 = torch.constant.none
    %none_5581 = torch.constant.none
    %int6_5582 = torch.constant.int 6
    %cpu_5583 = torch.constant.device "cpu"
    %int0_5584 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3844, %none_5580, %none_5581, %int6_5582, %cpu_5583, %int0_5584 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5585 = torch.constant.int 5
    %3845 = torch.prims.convert_element_type %3844, %int5_5585 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_5586 = torch.constant.int 2
    %3846 = torch.aten.unsqueeze %3842, %int2_5586 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_5587 = torch.constant.int 2
    %3847 = torch.aten.unsqueeze %3845, %int2_5587 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_5588 = torch.constant.none
    %none_5589 = torch.constant.none
    %int5_5590 = torch.constant.int 5
    %cpu_5591 = torch.constant.device "cpu"
    %int0_5592 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3846, %none_5588, %none_5589, %int5_5590, %cpu_5591, %int0_5592 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_5593 = torch.constant.none
    %none_5594 = torch.constant.none
    %int5_5595 = torch.constant.int 5
    %cpu_5596 = torch.constant.device "cpu"
    %int0_5597 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3847, %none_5593, %none_5594, %int5_5595, %cpu_5596, %int0_5597 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_5598 = torch.constant.none
    %none_5599 = torch.constant.none
    %int5_5600 = torch.constant.int 5
    %cpu_5601 = torch.constant.device "cpu"
    %int0_5602 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3816, %none_5598, %none_5599, %int5_5600, %cpu_5601, %int0_5602 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_5603 = torch.constant.int 3
    %int0_5604 = torch.constant.int 0
    %int64_5605 = torch.constant.int 64
    %int2_5606 = torch.constant.int 2
    %3848 = torch.aten.slice.Tensor %3816, %int3_5603, %int0_5604, %int64_5605, %int2_5606 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_5607 = torch.constant.int 3
    %int1_5608 = torch.constant.int 1
    %int64_5609 = torch.constant.int 64
    %int2_5610 = torch.constant.int 2
    %3849 = torch.aten.slice.Tensor %3816, %int3_5607, %int1_5608, %int64_5609, %int2_5610 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %3850 = torch.aten.mul.Tensor %3848, %3846 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %3851 = torch.aten.mul.Tensor %3849, %3847 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_5611 = torch.constant.int 1
    %3852 = torch.aten.sub.Tensor %3850, %3851, %int1_5611 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %3853 = torch.aten.mul.Tensor %3849, %3846 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %3854 = torch.aten.mul.Tensor %3848, %3847 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_5612 = torch.constant.int 1
    %3855 = torch.aten.add.Tensor %3853, %3854, %int1_5612 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %3856 = torch_c.to_builtin_tensor %3852 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_5613 = tensor.cast %3856 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %3857 = torch_c.to_builtin_tensor %3855 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_5614 = tensor.cast %3857 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %3858 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_5613, %cast_5614) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_5615 = tensor.cast %3858 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %3859 = torch_c.from_builtin_tensor %cast_5615 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_5616 = torch.constant.int 4
    %int1_5617 = torch.constant.int 1
    %int32_5618 = torch.constant.int 32
    %int64_5619 = torch.constant.int 64
    %3860 = torch.prim.ListConstruct %int4_5616, %int1_5617, %int32_5618, %int64_5619 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3861 = torch.aten.view %3859, %3860 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_5620 = torch.constant.none
    %none_5621 = torch.constant.none
    %int5_5622 = torch.constant.int 5
    %cpu_5623 = torch.constant.device "cpu"
    %int0_5624 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3861, %none_5620, %none_5621, %int5_5622, %cpu_5623, %int0_5624 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_5625 = torch.constant.int 0
    %int1_5626 = torch.constant.int 1
    %none_5627 = torch.constant.none
    %none_5628 = torch.constant.none
    %cpu_5629 = torch.constant.device "cpu"
    %false_5630 = torch.constant.bool false
    %3862 = torch.aten.arange.start %int0_5625, %int1_5626, %none_5627, %none_5628, %cpu_5629, %false_5630 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_5631 = torch.constant.int 0
    %3863 = torch.aten.unsqueeze %3862, %int0_5631 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_5632 = torch.constant.int 1
    %3864 = torch.aten.unsqueeze %arg2, %int1_5632 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5633 = torch.constant.int 1
    %3865 = torch.aten.add.Tensor %3863, %3864, %int1_5633 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_5634 = torch.constant.int 0
    %int64_5635 = torch.constant.int 64
    %int2_5636 = torch.constant.int 2
    %none_5637 = torch.constant.none
    %none_5638 = torch.constant.none
    %cpu_5639 = torch.constant.device "cpu"
    %false_5640 = torch.constant.bool false
    %3866 = torch.aten.arange.start_step %int0_5634, %int64_5635, %int2_5636, %none_5637, %none_5638, %cpu_5639, %false_5640 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_5641 = torch.constant.none
    %none_5642 = torch.constant.none
    %int4_5643 = torch.constant.int 4
    %cpu_5644 = torch.constant.device "cpu"
    %int0_5645 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3866, %none_5641, %none_5642, %int4_5643, %cpu_5644, %int0_5645 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5646 = torch.constant.int 6
    %3867 = torch.prims.convert_element_type %3866, %int6_5646 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_5647 = torch.constant.int 64
    %3868 = torch.aten.div.Scalar %3867, %int64_5647 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_5648 = torch.constant.float 1.000000e+04
    %3869 = torch.aten.pow.Scalar %float1.000000e04_5648, %3868 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %3870 = torch.aten.reciprocal %3869 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_5649 = torch.constant.float 1.000000e+00
    %3871 = torch.aten.mul.Scalar %3870, %float1.000000e00_5649 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_5650 = torch.constant.none
    %3872 = torch.aten.clone %172, %none_5650 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_5651 = torch.constant.int 0
    %3873 = torch.aten.unsqueeze %3871, %int0_5651 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_5652 = torch.constant.int 2
    %3874 = torch.aten.unsqueeze %3873, %int2_5652 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_5653 = torch.constant.none
    %none_5654 = torch.constant.none
    %int6_5655 = torch.constant.int 6
    %cpu_5656 = torch.constant.device "cpu"
    %int0_5657 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3874, %none_5653, %none_5654, %int6_5655, %cpu_5656, %int0_5657 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_5658 = torch.constant.int 4
    %int-1_5659 = torch.constant.int -1
    %int1_5660 = torch.constant.int 1
    %3875 = torch.prim.ListConstruct %int4_5658, %int-1_5659, %int1_5660 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5661 = torch.constant.bool false
    %3876 = torch.aten.expand %3874, %3875, %false_5661 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_5662 = torch.constant.int 1
    %3877 = torch.aten.unsqueeze %3865, %int1_5662 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_5663 = torch.constant.none
    %none_5664 = torch.constant.none
    %int4_5665 = torch.constant.int 4
    %cpu_5666 = torch.constant.device "cpu"
    %int0_5667 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3877, %none_5663, %none_5664, %int4_5665, %cpu_5666, %int0_5667 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5668 = torch.constant.int 6
    %3878 = torch.prims.convert_element_type %3877, %int6_5668 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3879 = torch.aten.matmul %3876, %3878 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_5669 = torch.constant.int 1
    %int2_5670 = torch.constant.int 2
    %3880 = torch.aten.transpose.int %3879, %int1_5669, %int2_5670 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %3881 = torch.aten.cos %3880 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %3882 = torch.aten.mul.Tensor %3881, %3872 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_5671 = torch.constant.none
    %none_5672 = torch.constant.none
    %int6_5673 = torch.constant.int 6
    %cpu_5674 = torch.constant.device "cpu"
    %int0_5675 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3882, %none_5671, %none_5672, %int6_5673, %cpu_5674, %int0_5675 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5676 = torch.constant.int 5
    %3883 = torch.prims.convert_element_type %3882, %int5_5676 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %3884 = torch.aten.sin %3880 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %3885 = torch.aten.mul.Tensor %3884, %3872 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_5677 = torch.constant.none
    %none_5678 = torch.constant.none
    %int6_5679 = torch.constant.int 6
    %cpu_5680 = torch.constant.device "cpu"
    %int0_5681 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3885, %none_5677, %none_5678, %int6_5679, %cpu_5680, %int0_5681 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5682 = torch.constant.int 5
    %3886 = torch.prims.convert_element_type %3885, %int5_5682 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_5683 = torch.constant.int 2
    %3887 = torch.aten.unsqueeze %3883, %int2_5683 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_5684 = torch.constant.int 2
    %3888 = torch.aten.unsqueeze %3886, %int2_5684 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_5685 = torch.constant.none
    %none_5686 = torch.constant.none
    %int5_5687 = torch.constant.int 5
    %cpu_5688 = torch.constant.device "cpu"
    %int0_5689 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3887, %none_5685, %none_5686, %int5_5687, %cpu_5688, %int0_5689 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_5690 = torch.constant.none
    %none_5691 = torch.constant.none
    %int5_5692 = torch.constant.int 5
    %cpu_5693 = torch.constant.device "cpu"
    %int0_5694 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3888, %none_5690, %none_5691, %int5_5692, %cpu_5693, %int0_5694 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_5695 = torch.constant.none
    %none_5696 = torch.constant.none
    %int5_5697 = torch.constant.int 5
    %cpu_5698 = torch.constant.device "cpu"
    %int0_5699 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3818, %none_5695, %none_5696, %int5_5697, %cpu_5698, %int0_5699 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_5700 = torch.constant.int 3
    %int0_5701 = torch.constant.int 0
    %int64_5702 = torch.constant.int 64
    %int2_5703 = torch.constant.int 2
    %3889 = torch.aten.slice.Tensor %3818, %int3_5700, %int0_5701, %int64_5702, %int2_5703 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_5704 = torch.constant.int 3
    %int1_5705 = torch.constant.int 1
    %int64_5706 = torch.constant.int 64
    %int2_5707 = torch.constant.int 2
    %3890 = torch.aten.slice.Tensor %3818, %int3_5704, %int1_5705, %int64_5706, %int2_5707 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %3891 = torch.aten.mul.Tensor %3889, %3887 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %3892 = torch.aten.mul.Tensor %3890, %3888 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_5708 = torch.constant.int 1
    %3893 = torch.aten.sub.Tensor %3891, %3892, %int1_5708 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %3894 = torch.aten.mul.Tensor %3890, %3887 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %3895 = torch.aten.mul.Tensor %3889, %3888 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_5709 = torch.constant.int 1
    %3896 = torch.aten.add.Tensor %3894, %3895, %int1_5709 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %3897 = torch_c.to_builtin_tensor %3893 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_5710 = tensor.cast %3897 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %3898 = torch_c.to_builtin_tensor %3896 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_5711 = tensor.cast %3898 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %3899 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_5710, %cast_5711) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_5712 = tensor.cast %3899 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %3900 = torch_c.from_builtin_tensor %cast_5712 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_5713 = torch.constant.int 4
    %int1_5714 = torch.constant.int 1
    %int4_5715 = torch.constant.int 4
    %int64_5716 = torch.constant.int 64
    %3901 = torch.prim.ListConstruct %int4_5713, %int1_5714, %int4_5715, %int64_5716 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3902 = torch.aten.view %3900, %3901 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_5717 = torch.constant.none
    %none_5718 = torch.constant.none
    %int5_5719 = torch.constant.int 5
    %cpu_5720 = torch.constant.device "cpu"
    %int0_5721 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3902, %none_5717, %none_5718, %int5_5719, %cpu_5720, %int0_5721 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_5722 = torch.constant.int 32
    %3903 = torch.aten.floor_divide.Scalar %arg2, %int32_5722 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_5723 = torch.constant.int 1
    %3904 = torch.aten.unsqueeze %3903, %int1_5723 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5724 = torch.constant.int 1
    %false_5725 = torch.constant.bool false
    %3905 = torch.aten.gather %arg3, %int1_5724, %3904, %false_5725 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_5726 = torch.constant.int 4
    %int1_5727 = torch.constant.int 1
    %int1_5728 = torch.constant.int 1
    %3906 = torch.prim.ListConstruct %int4_5726, %int1_5727, %int1_5728 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3907 = torch.aten.view %3905, %3906 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_5729 = torch.constant.int 32
    %3908 = torch.aten.remainder.Scalar %arg2, %int32_5729 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_5730 = torch.constant.int 4
    %int1_5731 = torch.constant.int 1
    %int1_5732 = torch.constant.int 1
    %3909 = torch.prim.ListConstruct %int4_5730, %int1_5731, %int1_5732 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3910 = torch.aten.view %3908, %3909 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_5733 = torch.constant.int 4
    %none_5734 = torch.constant.none
    %none_5735 = torch.constant.none
    %cpu_5736 = torch.constant.device "cpu"
    %false_5737 = torch.constant.bool false
    %3911 = torch.aten.arange %int4_5733, %none_5734, %none_5735, %cpu_5736, %false_5737 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_5738 = torch.constant.int 1
    %int1_5739 = torch.constant.int 1
    %int4_5740 = torch.constant.int 4
    %3912 = torch.prim.ListConstruct %int1_5738, %int1_5739, %int4_5740 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3913 = torch.aten.view %3911, %3912 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_5741 = torch.constant.none
    %3914 = torch.aten.clone %173, %none_5741 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_5742 = torch.constant.int 1
    %int1_5743 = torch.constant.int 1
    %int1_5744 = torch.constant.int 1
    %3915 = torch.prim.ListConstruct %int1_5742, %int1_5743, %int1_5744 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3916 = torch.aten.view %3914, %3915 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_5745 = torch.constant.int 22
    %3917 = torch.aten.mul.Scalar %3907, %int22_5745 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int10 = torch.constant.int 10
    %int1_5746 = torch.constant.int 1
    %3918 = torch.aten.add.Scalar %3917, %int10, %int1_5746 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_5747 = torch.constant.int 2
    %3919 = torch.aten.mul.Scalar %3918, %int2_5747 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_5748 = torch.constant.int 1
    %3920 = torch.aten.add.Tensor %3919, %3916, %int1_5748 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_5749 = torch.constant.int 4
    %3921 = torch.aten.mul.Scalar %3920, %int4_5749 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_5750 = torch.constant.int 1
    %3922 = torch.aten.add.Tensor %3921, %3913, %int1_5750 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_5751 = torch.constant.int 32
    %3923 = torch.aten.mul.Scalar %3922, %int32_5751 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_5752 = torch.constant.int 1
    %3924 = torch.aten.add.Tensor %3923, %3910, %int1_5752 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_5753 = torch.constant.none
    %none_5754 = torch.constant.none
    %int5_5755 = torch.constant.int 5
    %cpu_5756 = torch.constant.device "cpu"
    %int0_5757 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3902, %none_5753, %none_5754, %int5_5755, %cpu_5756, %int0_5757 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_5758 = torch.constant.int 22
    %int2_5759 = torch.constant.int 2
    %int4_5760 = torch.constant.int 4
    %int32_5761 = torch.constant.int 32
    %int64_5762 = torch.constant.int 64
    %3925 = torch.prim.ListConstruct %381, %int22_5758, %int2_5759, %int4_5760, %int32_5761, %int64_5762 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3926 = torch.aten.view %3620, %3925 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3926, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_5763 = torch.constant.int 64
    %3927 = torch.prim.ListConstruct %553, %int64_5763 : (!torch.int, !torch.int) -> !torch.list<int>
    %3928 = torch.aten.view %3926, %3927 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %3928, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %3929 = torch.prim.ListConstruct %3924 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_5764 = torch.constant.bool false
    %3930 = torch.aten.index_put %3928, %3929, %3902, %false_5764 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %3930, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_5765 = torch.constant.int 22
    %int2_5766 = torch.constant.int 2
    %int4_5767 = torch.constant.int 4
    %int32_5768 = torch.constant.int 32
    %int64_5769 = torch.constant.int 64
    %3931 = torch.prim.ListConstruct %381, %int22_5765, %int2_5766, %int4_5767, %int32_5768, %int64_5769 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3932 = torch.aten.view %3930, %3931 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3932, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_5770 = torch.constant.int 360448
    %3933 = torch.prim.ListConstruct %381, %int360448_5770 : (!torch.int, !torch.int) -> !torch.list<int>
    %3934 = torch.aten.view %3932, %3933 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %3934, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_5771 = torch.constant.int 22
    %int2_5772 = torch.constant.int 2
    %int4_5773 = torch.constant.int 4
    %int32_5774 = torch.constant.int 32
    %int64_5775 = torch.constant.int 64
    %3935 = torch.prim.ListConstruct %381, %int22_5771, %int2_5772, %int4_5773, %int32_5774, %int64_5775 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3936 = torch.aten.view %3934, %3935 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3936, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_5776 = torch.constant.int 64
    %3937 = torch.prim.ListConstruct %553, %int64_5776 : (!torch.int, !torch.int) -> !torch.list<int>
    %3938 = torch.aten.view %3936, %3937 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %3938, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_5777 = torch.constant.none
    %3939 = torch.aten.clone %174, %none_5777 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_5778 = torch.constant.int 1
    %int1_5779 = torch.constant.int 1
    %int1_5780 = torch.constant.int 1
    %3940 = torch.prim.ListConstruct %int1_5778, %int1_5779, %int1_5780 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3941 = torch.aten.view %3939, %3940 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_5781 = torch.constant.int 22
    %3942 = torch.aten.mul.Scalar %3907, %int22_5781 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int10_5782 = torch.constant.int 10
    %int1_5783 = torch.constant.int 1
    %3943 = torch.aten.add.Scalar %3942, %int10_5782, %int1_5783 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_5784 = torch.constant.int 2
    %3944 = torch.aten.mul.Scalar %3943, %int2_5784 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_5785 = torch.constant.int 1
    %3945 = torch.aten.add.Tensor %3944, %3941, %int1_5785 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_5786 = torch.constant.int 4
    %3946 = torch.aten.mul.Scalar %3945, %int4_5786 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_5787 = torch.constant.int 1
    %3947 = torch.aten.add.Tensor %3946, %3913, %int1_5787 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_5788 = torch.constant.int 32
    %3948 = torch.aten.mul.Scalar %3947, %int32_5788 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_5789 = torch.constant.int 1
    %3949 = torch.aten.add.Tensor %3948, %3910, %int1_5789 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_5790 = torch.constant.none
    %none_5791 = torch.constant.none
    %int5_5792 = torch.constant.int 5
    %cpu_5793 = torch.constant.device "cpu"
    %int0_5794 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3820, %none_5790, %none_5791, %int5_5792, %cpu_5793, %int0_5794 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %3950 = torch.prim.ListConstruct %3949 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_5795 = torch.constant.bool false
    %3951 = torch.aten.index_put %3938, %3950, %3820, %false_5795 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %3951, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_5796 = torch.constant.int 22
    %int2_5797 = torch.constant.int 2
    %int4_5798 = torch.constant.int 4
    %int32_5799 = torch.constant.int 32
    %int64_5800 = torch.constant.int 64
    %3952 = torch.prim.ListConstruct %381, %int22_5796, %int2_5797, %int4_5798, %int32_5799, %int64_5800 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3953 = torch.aten.view %3951, %3952 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3953, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_5801 = torch.constant.int 360448
    %3954 = torch.prim.ListConstruct %381, %int360448_5801 : (!torch.int, !torch.int) -> !torch.list<int>
    %3955 = torch.aten.view %3953, %3954 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %3955, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_5802 = torch.constant.none
    %3956 = torch.aten.clone %175, %none_5802 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_5803 = torch.constant.none
    %3957 = torch.aten.clone %176, %none_5803 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_5804 = torch.constant.none
    %3958 = torch.aten.clone %177, %none_5804 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_5805 = torch.constant.int 22
    %int2_5806 = torch.constant.int 2
    %int4_5807 = torch.constant.int 4
    %int32_5808 = torch.constant.int 32
    %int64_5809 = torch.constant.int 64
    %3959 = torch.prim.ListConstruct %381, %int22_5805, %int2_5806, %int4_5807, %int32_5808, %int64_5809 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3960 = torch.aten.view %3955, %3959 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %3960, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %3961 = torch_c.to_builtin_tensor %3960 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %3962 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_5810 = tensor.cast %3962 : tensor<4x?xi64> to tensor<?x?xi64>
    %3963 = torch_c.to_builtin_tensor %3956 : !torch.vtensor<[],si64> -> tensor<i64>
    %3964 = torch_c.to_builtin_tensor %3957 : !torch.vtensor<[],si64> -> tensor<i64>
    %3965 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%3961, %cast_5810, %3963, %3964) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_5811 = tensor.cast %3965 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %3966 = torch_c.from_builtin_tensor %cast_5811 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %3966, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %3967 = torch_c.to_builtin_tensor %3960 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %3968 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_5812 = tensor.cast %3968 : tensor<4x?xi64> to tensor<?x?xi64>
    %3969 = torch_c.to_builtin_tensor %3956 : !torch.vtensor<[],si64> -> tensor<i64>
    %3970 = torch_c.to_builtin_tensor %3958 : !torch.vtensor<[],si64> -> tensor<i64>
    %3971 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%3967, %cast_5812, %3969, %3970) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_5813 = tensor.cast %3971 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %3972 = torch_c.from_builtin_tensor %cast_5813 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %3972, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_5814 = torch.constant.int 2
    %int3_5815 = torch.constant.int 3
    %3973 = torch.aten.transpose.int %3966, %int2_5814, %int3_5815 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %3973, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_5816 = torch.constant.int 0
    %3974 = torch.aten.clone %3973, %int0_5816 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %3974, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_5817 = torch.constant.int 4
    %int4_5818 = torch.constant.int 4
    %int64_5819 = torch.constant.int 64
    %3975 = torch.prim.ListConstruct %int4_5817, %623, %int4_5818, %int64_5819 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3976 = torch.aten._unsafe_view %3974, %3975 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %3976, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_5820 = torch.constant.int 2
    %int3_5821 = torch.constant.int 3
    %3977 = torch.aten.transpose.int %3972, %int2_5820, %int3_5821 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %3977, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_5822 = torch.constant.int 0
    %3978 = torch.aten.clone %3977, %int0_5822 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %3978, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_5823 = torch.constant.int 4
    %int4_5824 = torch.constant.int 4
    %int64_5825 = torch.constant.int 64
    %3979 = torch.prim.ListConstruct %int4_5823, %623, %int4_5824, %int64_5825 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3980 = torch.aten._unsafe_view %3978, %3979 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %3980, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_5826 = torch.constant.int 0
    %int1_5827 = torch.constant.int 1
    %none_5828 = torch.constant.none
    %none_5829 = torch.constant.none
    %cpu_5830 = torch.constant.device "cpu"
    %false_5831 = torch.constant.bool false
    %3981 = torch.aten.arange.start_step %int0_5826, %623, %int1_5827, %none_5828, %none_5829, %cpu_5830, %false_5831 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3981, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_5832 = torch.constant.int -1
    %3982 = torch.aten.unsqueeze %arg1, %int-1_5832 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %3983 = torch.aten.ge.Tensor %3981, %3982 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %3983, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_5833 = torch.constant.none
    %3984 = torch.aten.clone %178, %none_5833 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_5834 = torch.constant.int 0
    %3985 = torch.aten.where.ScalarOther %3983, %3984, %int0_5834 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %3985, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_5835 = torch.constant.none
    %none_5836 = torch.constant.none
    %int5_5837 = torch.constant.int 5
    %cpu_5838 = torch.constant.device "cpu"
    %int0_5839 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %3985, %none_5835, %none_5836, %int5_5837, %cpu_5838, %int0_5839 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_5840 = torch.constant.int 1
    %3986 = torch.aten.unsqueeze %3985, %int1_5840 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %3986, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_5841 = torch.constant.int 1
    %3987 = torch.aten.unsqueeze %3986, %int1_5841 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %3987, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_5842 = torch.constant.int -2
    %3988 = torch.aten.unsqueeze %3976, %int-2_5842 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %3988, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_5843 = torch.constant.int 4
    %int4_5844 = torch.constant.int 4
    %int8_5845 = torch.constant.int 8
    %int64_5846 = torch.constant.int 64
    %3989 = torch.prim.ListConstruct %int4_5843, %623, %int4_5844, %int8_5845, %int64_5846 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5847 = torch.constant.bool false
    %3990 = torch.aten.expand %3988, %3989, %false_5847 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3990, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_5848 = torch.constant.int 0
    %3991 = torch.aten.clone %3990, %int0_5848 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3991, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_5849 = torch.constant.int 4
    %int32_5850 = torch.constant.int 32
    %int64_5851 = torch.constant.int 64
    %3992 = torch.prim.ListConstruct %int4_5849, %623, %int32_5850, %int64_5851 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3993 = torch.aten._unsafe_view %3991, %3992 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3993, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_5852 = torch.constant.int -2
    %3994 = torch.aten.unsqueeze %3980, %int-2_5852 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %3994, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_5853 = torch.constant.int 4
    %int4_5854 = torch.constant.int 4
    %int8_5855 = torch.constant.int 8
    %int64_5856 = torch.constant.int 64
    %3995 = torch.prim.ListConstruct %int4_5853, %623, %int4_5854, %int8_5855, %int64_5856 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5857 = torch.constant.bool false
    %3996 = torch.aten.expand %3994, %3995, %false_5857 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3996, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_5858 = torch.constant.int 0
    %3997 = torch.aten.clone %3996, %int0_5858 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %3997, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_5859 = torch.constant.int 4
    %int32_5860 = torch.constant.int 32
    %int64_5861 = torch.constant.int 64
    %3998 = torch.prim.ListConstruct %int4_5859, %623, %int32_5860, %int64_5861 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3999 = torch.aten._unsafe_view %3997, %3998 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %3999, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_5862 = torch.constant.int 1
    %int2_5863 = torch.constant.int 2
    %4000 = torch.aten.transpose.int %3861, %int1_5862, %int2_5863 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_5864 = torch.constant.int 1
    %int2_5865 = torch.constant.int 2
    %4001 = torch.aten.transpose.int %3993, %int1_5864, %int2_5865 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %4001, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_5866 = torch.constant.int 1
    %int2_5867 = torch.constant.int 2
    %4002 = torch.aten.transpose.int %3999, %int1_5866, %int2_5867 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %4002, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_5868 = torch.constant.float 0.000000e+00
    %false_5869 = torch.constant.bool false
    %none_5870 = torch.constant.none
    %false_5871 = torch.constant.bool false
    %4003 = torch.aten.scaled_dot_product_attention %4000, %4001, %4002, %3987, %float0.000000e00_5868, %false_5869, %none_5870, %false_5871 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_5872 = torch.constant.int 1
    %int2_5873 = torch.constant.int 2
    %4004 = torch.aten.transpose.int %4003, %int1_5872, %int2_5873 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_5874 = torch.constant.int 4
    %int1_5875 = torch.constant.int 1
    %int2048_5876 = torch.constant.int 2048
    %4005 = torch.prim.ListConstruct %int4_5874, %int1_5875, %int2048_5876 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4006 = torch.aten.view %4004, %4005 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_5877 = torch.constant.int 2
    %4007 = torch.aten.view.dtype %183, %int2_5877 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %4008 = torch.aten.detach %4007 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_5878 = torch.constant.int -1
    %int17_5879 = torch.constant.int 17
    %4009 = torch.prim.ListConstruct %int-1_5878, %int17_5879 : (!torch.int, !torch.int) -> !torch.list<int>
    %4010 = torch.aten.view %4008, %4009 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_5880 = torch.constant.int 2048
    %int-1_5881 = torch.constant.int -1
    %int17_5882 = torch.constant.int 17
    %4011 = torch.prim.ListConstruct %int2048_5880, %int-1_5881, %int17_5882 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4012 = torch.aten.view %4010, %4011 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_5883 = torch.constant.int 2
    %int0_5884 = torch.constant.int 0
    %int1_5885 = torch.constant.int 1
    %int1_5886 = torch.constant.int 1
    %4013 = torch.aten.slice.Tensor %4012, %int2_5883, %int0_5884, %int1_5885, %int1_5886 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_5887 = torch.constant.int 5
    %4014 = torch.aten.view.dtype %4013, %int5_5887 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %4015 = torch.aten.detach %4014 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_5888 = torch.constant.int 2
    %int1_5889 = torch.constant.int 1
    %int9223372036854775807_5890 = torch.constant.int 9223372036854775807
    %int1_5891 = torch.constant.int 1
    %4016 = torch.aten.slice.Tensor %4012, %int2_5888, %int1_5889, %int9223372036854775807_5890, %int1_5891 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_5892 = torch.constant.int 1
    %4017 = torch.aten.view.dtype %4016, %int1_5892 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %4018 = torch.aten.detach %4017 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %4019 = torch_c.to_builtin_tensor %4006 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_5893 = tensor.cast %4019 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %4020 = torch_c.to_builtin_tensor %4015 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %4021 = torch_c.to_builtin_tensor %4018 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %4022 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_5893, %4020, %4021) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_5894 = tensor.cast %4022 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %4023 = torch_c.from_builtin_tensor %cast_5894 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_5895 = torch.constant.none
    %none_5896 = torch.constant.none
    %int5_5897 = torch.constant.int 5
    %cpu_5898 = torch.constant.device "cpu"
    %int0_5899 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4023, %none_5895, %none_5896, %int5_5897, %cpu_5898, %int0_5899 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_5900 = torch.constant.int 1
    %4024 = torch.aten.add.Tensor %3753, %4023, %int1_5900 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_5901 = torch.constant.none
    %none_5902 = torch.constant.none
    %int5_5903 = torch.constant.int 5
    %cpu_5904 = torch.constant.device "cpu"
    %int0_5905 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4024, %none_5901, %none_5902, %int5_5903, %cpu_5904, %int0_5905 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5906 = torch.constant.int 6
    %4025 = torch.prims.convert_element_type %4024, %int6_5906 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_5907 = torch.constant.int 2
    %4026 = torch.aten.pow.Tensor_Scalar %4025, %int2_5907 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_5908 = torch.constant.int -1
    %4027 = torch.prim.ListConstruct %int-1_5908 : (!torch.int) -> !torch.list<int>
    %true_5909 = torch.constant.bool true
    %none_5910 = torch.constant.none
    %4028 = torch.aten.mean.dim %4026, %4027, %true_5909, %none_5910 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_5911 = torch.constant.float 9.9999997473787516E-6
    %int1_5912 = torch.constant.int 1
    %4029 = torch.aten.add.Scalar %4028, %float9.999990e-06_5911, %int1_5912 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4030 = torch.aten.rsqrt %4029 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %4031 = torch.aten.mul.Tensor %4025, %4030 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_5913 = torch.constant.none
    %none_5914 = torch.constant.none
    %int6_5915 = torch.constant.int 6
    %cpu_5916 = torch.constant.device "cpu"
    %int0_5917 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4031, %none_5913, %none_5914, %int6_5915, %cpu_5916, %int0_5917 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5918 = torch.constant.int 5
    %4032 = torch.prims.convert_element_type %4031, %int5_5918 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %4033 = torch.aten.mul.Tensor %184, %4032 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_5919 = torch.constant.none
    %none_5920 = torch.constant.none
    %int6_5921 = torch.constant.int 6
    %cpu_5922 = torch.constant.device "cpu"
    %int0_5923 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4033, %none_5919, %none_5920, %int6_5921, %cpu_5922, %int0_5923 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5924 = torch.constant.int 5
    %4034 = torch.prims.convert_element_type %4033, %int5_5924 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_5925 = torch.constant.int 2
    %4035 = torch.aten.view.dtype %185, %int2_5925 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %4036 = torch.aten.detach %4035 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_5926 = torch.constant.int -1
    %int17_5927 = torch.constant.int 17
    %4037 = torch.prim.ListConstruct %int-1_5926, %int17_5927 : (!torch.int, !torch.int) -> !torch.list<int>
    %4038 = torch.aten.view %4036, %4037 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_5928 = torch.constant.int 5632
    %int-1_5929 = torch.constant.int -1
    %int17_5930 = torch.constant.int 17
    %4039 = torch.prim.ListConstruct %int5632_5928, %int-1_5929, %int17_5930 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4040 = torch.aten.view %4038, %4039 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_5931 = torch.constant.int 2
    %int0_5932 = torch.constant.int 0
    %int1_5933 = torch.constant.int 1
    %int1_5934 = torch.constant.int 1
    %4041 = torch.aten.slice.Tensor %4040, %int2_5931, %int0_5932, %int1_5933, %int1_5934 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_5935 = torch.constant.int 5
    %4042 = torch.aten.view.dtype %4041, %int5_5935 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %4043 = torch.aten.detach %4042 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_5936 = torch.constant.int 2
    %int1_5937 = torch.constant.int 1
    %int9223372036854775807_5938 = torch.constant.int 9223372036854775807
    %int1_5939 = torch.constant.int 1
    %4044 = torch.aten.slice.Tensor %4040, %int2_5936, %int1_5937, %int9223372036854775807_5938, %int1_5939 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_5940 = torch.constant.int 1
    %4045 = torch.aten.view.dtype %4044, %int1_5940 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %4046 = torch.aten.detach %4045 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %4047 = torch_c.to_builtin_tensor %4034 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_5941 = tensor.cast %4047 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %4048 = torch_c.to_builtin_tensor %4043 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %4049 = torch_c.to_builtin_tensor %4046 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %4050 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_5941, %4048, %4049) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_5942 = tensor.cast %4050 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %4051 = torch_c.from_builtin_tensor %cast_5942 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %4052 = torch.aten.silu %4051 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_5943 = torch.constant.int 2
    %4053 = torch.aten.view.dtype %186, %int2_5943 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %4054 = torch.aten.detach %4053 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_5944 = torch.constant.int -1
    %int17_5945 = torch.constant.int 17
    %4055 = torch.prim.ListConstruct %int-1_5944, %int17_5945 : (!torch.int, !torch.int) -> !torch.list<int>
    %4056 = torch.aten.view %4054, %4055 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_5946 = torch.constant.int 5632
    %int-1_5947 = torch.constant.int -1
    %int17_5948 = torch.constant.int 17
    %4057 = torch.prim.ListConstruct %int5632_5946, %int-1_5947, %int17_5948 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4058 = torch.aten.view %4056, %4057 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_5949 = torch.constant.int 2
    %int0_5950 = torch.constant.int 0
    %int1_5951 = torch.constant.int 1
    %int1_5952 = torch.constant.int 1
    %4059 = torch.aten.slice.Tensor %4058, %int2_5949, %int0_5950, %int1_5951, %int1_5952 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_5953 = torch.constant.int 5
    %4060 = torch.aten.view.dtype %4059, %int5_5953 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %4061 = torch.aten.detach %4060 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_5954 = torch.constant.int 2
    %int1_5955 = torch.constant.int 1
    %int9223372036854775807_5956 = torch.constant.int 9223372036854775807
    %int1_5957 = torch.constant.int 1
    %4062 = torch.aten.slice.Tensor %4058, %int2_5954, %int1_5955, %int9223372036854775807_5956, %int1_5957 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_5958 = torch.constant.int 1
    %4063 = torch.aten.view.dtype %4062, %int1_5958 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %4064 = torch.aten.detach %4063 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %4065 = torch_c.to_builtin_tensor %4034 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_5959 = tensor.cast %4065 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %4066 = torch_c.to_builtin_tensor %4061 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %4067 = torch_c.to_builtin_tensor %4064 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %4068 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_5959, %4066, %4067) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_5960 = tensor.cast %4068 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %4069 = torch_c.from_builtin_tensor %cast_5960 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %4070 = torch.aten.mul.Tensor %4052, %4069 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_5961 = torch.constant.int 2
    %4071 = torch.aten.view.dtype %187, %int2_5961 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %4072 = torch.aten.detach %4071 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_5962 = torch.constant.int -1
    %int17_5963 = torch.constant.int 17
    %4073 = torch.prim.ListConstruct %int-1_5962, %int17_5963 : (!torch.int, !torch.int) -> !torch.list<int>
    %4074 = torch.aten.view %4072, %4073 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_5964 = torch.constant.int 2048
    %int-1_5965 = torch.constant.int -1
    %int17_5966 = torch.constant.int 17
    %4075 = torch.prim.ListConstruct %int2048_5964, %int-1_5965, %int17_5966 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4076 = torch.aten.view %4074, %4075 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_5967 = torch.constant.int 2
    %int0_5968 = torch.constant.int 0
    %int1_5969 = torch.constant.int 1
    %int1_5970 = torch.constant.int 1
    %4077 = torch.aten.slice.Tensor %4076, %int2_5967, %int0_5968, %int1_5969, %int1_5970 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_5971 = torch.constant.int 5
    %4078 = torch.aten.view.dtype %4077, %int5_5971 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %4079 = torch.aten.detach %4078 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_5972 = torch.constant.int 2
    %int1_5973 = torch.constant.int 1
    %int9223372036854775807_5974 = torch.constant.int 9223372036854775807
    %int1_5975 = torch.constant.int 1
    %4080 = torch.aten.slice.Tensor %4076, %int2_5972, %int1_5973, %int9223372036854775807_5974, %int1_5975 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_5976 = torch.constant.int 1
    %4081 = torch.aten.view.dtype %4080, %int1_5976 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %4082 = torch.aten.detach %4081 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %4083 = torch_c.to_builtin_tensor %4070 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_5977 = tensor.cast %4083 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %4084 = torch_c.to_builtin_tensor %4079 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %4085 = torch_c.to_builtin_tensor %4082 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %4086 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_5977, %4084, %4085) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_5978 = tensor.cast %4086 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %4087 = torch_c.from_builtin_tensor %cast_5978 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_5979 = torch.constant.int 1
    %4088 = torch.aten.add.Tensor %4024, %4087, %int1_5979 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_5980 = torch.constant.none
    %none_5981 = torch.constant.none
    %int5_5982 = torch.constant.int 5
    %cpu_5983 = torch.constant.device "cpu"
    %int0_5984 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4088, %none_5980, %none_5981, %int5_5982, %cpu_5983, %int0_5984 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_5985 = torch.constant.int 6
    %4089 = torch.prims.convert_element_type %4088, %int6_5985 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_5986 = torch.constant.int 2
    %4090 = torch.aten.pow.Tensor_Scalar %4089, %int2_5986 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_5987 = torch.constant.int -1
    %4091 = torch.prim.ListConstruct %int-1_5987 : (!torch.int) -> !torch.list<int>
    %true_5988 = torch.constant.bool true
    %none_5989 = torch.constant.none
    %4092 = torch.aten.mean.dim %4090, %4091, %true_5988, %none_5989 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_5990 = torch.constant.float 9.9999997473787516E-6
    %int1_5991 = torch.constant.int 1
    %4093 = torch.aten.add.Scalar %4092, %float9.999990e-06_5990, %int1_5991 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4094 = torch.aten.rsqrt %4093 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %4095 = torch.aten.mul.Tensor %4089, %4094 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_5992 = torch.constant.none
    %none_5993 = torch.constant.none
    %int6_5994 = torch.constant.int 6
    %cpu_5995 = torch.constant.device "cpu"
    %int0_5996 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4095, %none_5992, %none_5993, %int6_5994, %cpu_5995, %int0_5996 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_5997 = torch.constant.int 5
    %4096 = torch.prims.convert_element_type %4095, %int5_5997 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %4097 = torch.aten.mul.Tensor %196, %4096 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_5998 = torch.constant.none
    %none_5999 = torch.constant.none
    %int6_6000 = torch.constant.int 6
    %cpu_6001 = torch.constant.device "cpu"
    %int0_6002 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4097, %none_5998, %none_5999, %int6_6000, %cpu_6001, %int0_6002 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6003 = torch.constant.int 5
    %4098 = torch.prims.convert_element_type %4097, %int5_6003 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_6004 = torch.constant.int 2
    %4099 = torch.aten.view.dtype %197, %int2_6004 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %4100 = torch.aten.detach %4099 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_6005 = torch.constant.int -1
    %int17_6006 = torch.constant.int 17
    %4101 = torch.prim.ListConstruct %int-1_6005, %int17_6006 : (!torch.int, !torch.int) -> !torch.list<int>
    %4102 = torch.aten.view %4100, %4101 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_6007 = torch.constant.int 2048
    %int-1_6008 = torch.constant.int -1
    %int17_6009 = torch.constant.int 17
    %4103 = torch.prim.ListConstruct %int2048_6007, %int-1_6008, %int17_6009 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4104 = torch.aten.view %4102, %4103 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_6010 = torch.constant.int 2
    %int0_6011 = torch.constant.int 0
    %int1_6012 = torch.constant.int 1
    %int1_6013 = torch.constant.int 1
    %4105 = torch.aten.slice.Tensor %4104, %int2_6010, %int0_6011, %int1_6012, %int1_6013 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_6014 = torch.constant.int 5
    %4106 = torch.aten.view.dtype %4105, %int5_6014 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %4107 = torch.aten.detach %4106 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_6015 = torch.constant.int 2
    %int1_6016 = torch.constant.int 1
    %int9223372036854775807_6017 = torch.constant.int 9223372036854775807
    %int1_6018 = torch.constant.int 1
    %4108 = torch.aten.slice.Tensor %4104, %int2_6015, %int1_6016, %int9223372036854775807_6017, %int1_6018 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_6019 = torch.constant.int 1
    %4109 = torch.aten.view.dtype %4108, %int1_6019 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %4110 = torch.aten.detach %4109 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %4111 = torch_c.to_builtin_tensor %4098 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_6020 = tensor.cast %4111 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %4112 = torch_c.to_builtin_tensor %4107 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %4113 = torch_c.to_builtin_tensor %4110 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %4114 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_6020, %4112, %4113) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_6021 = tensor.cast %4114 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %4115 = torch_c.from_builtin_tensor %cast_6021 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_6022 = torch.constant.int 2
    %4116 = torch.aten.view.dtype %198, %int2_6022 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %4117 = torch.aten.detach %4116 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_6023 = torch.constant.int -1
    %int17_6024 = torch.constant.int 17
    %4118 = torch.prim.ListConstruct %int-1_6023, %int17_6024 : (!torch.int, !torch.int) -> !torch.list<int>
    %4119 = torch.aten.view %4117, %4118 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_6025 = torch.constant.int 256
    %int-1_6026 = torch.constant.int -1
    %int17_6027 = torch.constant.int 17
    %4120 = torch.prim.ListConstruct %int256_6025, %int-1_6026, %int17_6027 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4121 = torch.aten.view %4119, %4120 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_6028 = torch.constant.int 2
    %int0_6029 = torch.constant.int 0
    %int1_6030 = torch.constant.int 1
    %int1_6031 = torch.constant.int 1
    %4122 = torch.aten.slice.Tensor %4121, %int2_6028, %int0_6029, %int1_6030, %int1_6031 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_6032 = torch.constant.int 5
    %4123 = torch.aten.view.dtype %4122, %int5_6032 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %4124 = torch.aten.detach %4123 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_6033 = torch.constant.int 2
    %int1_6034 = torch.constant.int 1
    %int9223372036854775807_6035 = torch.constant.int 9223372036854775807
    %int1_6036 = torch.constant.int 1
    %4125 = torch.aten.slice.Tensor %4121, %int2_6033, %int1_6034, %int9223372036854775807_6035, %int1_6036 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_6037 = torch.constant.int 1
    %4126 = torch.aten.view.dtype %4125, %int1_6037 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %4127 = torch.aten.detach %4126 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %4128 = torch_c.to_builtin_tensor %4098 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_6038 = tensor.cast %4128 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %4129 = torch_c.to_builtin_tensor %4124 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %4130 = torch_c.to_builtin_tensor %4127 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %4131 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_6038, %4129, %4130) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_6039 = tensor.cast %4131 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %4132 = torch_c.from_builtin_tensor %cast_6039 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_6040 = torch.constant.int 2
    %4133 = torch.aten.view.dtype %199, %int2_6040 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %4134 = torch.aten.detach %4133 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_6041 = torch.constant.int -1
    %int17_6042 = torch.constant.int 17
    %4135 = torch.prim.ListConstruct %int-1_6041, %int17_6042 : (!torch.int, !torch.int) -> !torch.list<int>
    %4136 = torch.aten.view %4134, %4135 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_6043 = torch.constant.int 256
    %int-1_6044 = torch.constant.int -1
    %int17_6045 = torch.constant.int 17
    %4137 = torch.prim.ListConstruct %int256_6043, %int-1_6044, %int17_6045 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4138 = torch.aten.view %4136, %4137 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_6046 = torch.constant.int 2
    %int0_6047 = torch.constant.int 0
    %int1_6048 = torch.constant.int 1
    %int1_6049 = torch.constant.int 1
    %4139 = torch.aten.slice.Tensor %4138, %int2_6046, %int0_6047, %int1_6048, %int1_6049 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_6050 = torch.constant.int 5
    %4140 = torch.aten.view.dtype %4139, %int5_6050 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %4141 = torch.aten.detach %4140 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_6051 = torch.constant.int 2
    %int1_6052 = torch.constant.int 1
    %int9223372036854775807_6053 = torch.constant.int 9223372036854775807
    %int1_6054 = torch.constant.int 1
    %4142 = torch.aten.slice.Tensor %4138, %int2_6051, %int1_6052, %int9223372036854775807_6053, %int1_6054 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_6055 = torch.constant.int 1
    %4143 = torch.aten.view.dtype %4142, %int1_6055 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %4144 = torch.aten.detach %4143 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %4145 = torch_c.to_builtin_tensor %4098 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_6056 = tensor.cast %4145 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %4146 = torch_c.to_builtin_tensor %4141 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %4147 = torch_c.to_builtin_tensor %4144 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %4148 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_6056, %4146, %4147) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_6057 = tensor.cast %4148 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %4149 = torch_c.from_builtin_tensor %cast_6057 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_6058 = torch.constant.int 4
    %int1_6059 = torch.constant.int 1
    %int32_6060 = torch.constant.int 32
    %int64_6061 = torch.constant.int 64
    %4150 = torch.prim.ListConstruct %int4_6058, %int1_6059, %int32_6060, %int64_6061 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4151 = torch.aten.view %4115, %4150 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_6062 = torch.constant.int 4
    %int1_6063 = torch.constant.int 1
    %int4_6064 = torch.constant.int 4
    %int64_6065 = torch.constant.int 64
    %4152 = torch.prim.ListConstruct %int4_6062, %int1_6063, %int4_6064, %int64_6065 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4153 = torch.aten.view %4132, %4152 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_6066 = torch.constant.int 4
    %int1_6067 = torch.constant.int 1
    %int4_6068 = torch.constant.int 4
    %int64_6069 = torch.constant.int 64
    %4154 = torch.prim.ListConstruct %int4_6066, %int1_6067, %int4_6068, %int64_6069 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4155 = torch.aten.view %4149, %4154 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_6070 = torch.constant.int 0
    %int1_6071 = torch.constant.int 1
    %none_6072 = torch.constant.none
    %none_6073 = torch.constant.none
    %cpu_6074 = torch.constant.device "cpu"
    %false_6075 = torch.constant.bool false
    %4156 = torch.aten.arange.start %int0_6070, %int1_6071, %none_6072, %none_6073, %cpu_6074, %false_6075 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_6076 = torch.constant.int 0
    %4157 = torch.aten.unsqueeze %4156, %int0_6076 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_6077 = torch.constant.int 1
    %4158 = torch.aten.unsqueeze %arg2, %int1_6077 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6078 = torch.constant.int 1
    %4159 = torch.aten.add.Tensor %4157, %4158, %int1_6078 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_6079 = torch.constant.int 0
    %int64_6080 = torch.constant.int 64
    %int2_6081 = torch.constant.int 2
    %none_6082 = torch.constant.none
    %none_6083 = torch.constant.none
    %cpu_6084 = torch.constant.device "cpu"
    %false_6085 = torch.constant.bool false
    %4160 = torch.aten.arange.start_step %int0_6079, %int64_6080, %int2_6081, %none_6082, %none_6083, %cpu_6084, %false_6085 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_6086 = torch.constant.none
    %none_6087 = torch.constant.none
    %int4_6088 = torch.constant.int 4
    %cpu_6089 = torch.constant.device "cpu"
    %int0_6090 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4160, %none_6086, %none_6087, %int4_6088, %cpu_6089, %int0_6090 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6091 = torch.constant.int 6
    %4161 = torch.prims.convert_element_type %4160, %int6_6091 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_6092 = torch.constant.int 64
    %4162 = torch.aten.div.Scalar %4161, %int64_6092 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_6093 = torch.constant.float 1.000000e+04
    %4163 = torch.aten.pow.Scalar %float1.000000e04_6093, %4162 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %4164 = torch.aten.reciprocal %4163 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_6094 = torch.constant.float 1.000000e+00
    %4165 = torch.aten.mul.Scalar %4164, %float1.000000e00_6094 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_6095 = torch.constant.none
    %4166 = torch.aten.clone %188, %none_6095 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_6096 = torch.constant.int 0
    %4167 = torch.aten.unsqueeze %4165, %int0_6096 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_6097 = torch.constant.int 2
    %4168 = torch.aten.unsqueeze %4167, %int2_6097 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_6098 = torch.constant.none
    %none_6099 = torch.constant.none
    %int6_6100 = torch.constant.int 6
    %cpu_6101 = torch.constant.device "cpu"
    %int0_6102 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4168, %none_6098, %none_6099, %int6_6100, %cpu_6101, %int0_6102 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_6103 = torch.constant.int 4
    %int-1_6104 = torch.constant.int -1
    %int1_6105 = torch.constant.int 1
    %4169 = torch.prim.ListConstruct %int4_6103, %int-1_6104, %int1_6105 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6106 = torch.constant.bool false
    %4170 = torch.aten.expand %4168, %4169, %false_6106 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_6107 = torch.constant.int 1
    %4171 = torch.aten.unsqueeze %4159, %int1_6107 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_6108 = torch.constant.none
    %none_6109 = torch.constant.none
    %int4_6110 = torch.constant.int 4
    %cpu_6111 = torch.constant.device "cpu"
    %int0_6112 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4171, %none_6108, %none_6109, %int4_6110, %cpu_6111, %int0_6112 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6113 = torch.constant.int 6
    %4172 = torch.prims.convert_element_type %4171, %int6_6113 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4173 = torch.aten.matmul %4170, %4172 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_6114 = torch.constant.int 1
    %int2_6115 = torch.constant.int 2
    %4174 = torch.aten.transpose.int %4173, %int1_6114, %int2_6115 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %4175 = torch.aten.cos %4174 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %4176 = torch.aten.mul.Tensor %4175, %4166 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_6116 = torch.constant.none
    %none_6117 = torch.constant.none
    %int6_6118 = torch.constant.int 6
    %cpu_6119 = torch.constant.device "cpu"
    %int0_6120 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4176, %none_6116, %none_6117, %int6_6118, %cpu_6119, %int0_6120 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6121 = torch.constant.int 5
    %4177 = torch.prims.convert_element_type %4176, %int5_6121 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %4178 = torch.aten.sin %4174 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %4179 = torch.aten.mul.Tensor %4178, %4166 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_6122 = torch.constant.none
    %none_6123 = torch.constant.none
    %int6_6124 = torch.constant.int 6
    %cpu_6125 = torch.constant.device "cpu"
    %int0_6126 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4179, %none_6122, %none_6123, %int6_6124, %cpu_6125, %int0_6126 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6127 = torch.constant.int 5
    %4180 = torch.prims.convert_element_type %4179, %int5_6127 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_6128 = torch.constant.int 2
    %4181 = torch.aten.unsqueeze %4177, %int2_6128 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_6129 = torch.constant.int 2
    %4182 = torch.aten.unsqueeze %4180, %int2_6129 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_6130 = torch.constant.none
    %none_6131 = torch.constant.none
    %int5_6132 = torch.constant.int 5
    %cpu_6133 = torch.constant.device "cpu"
    %int0_6134 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4181, %none_6130, %none_6131, %int5_6132, %cpu_6133, %int0_6134 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_6135 = torch.constant.none
    %none_6136 = torch.constant.none
    %int5_6137 = torch.constant.int 5
    %cpu_6138 = torch.constant.device "cpu"
    %int0_6139 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4182, %none_6135, %none_6136, %int5_6137, %cpu_6138, %int0_6139 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_6140 = torch.constant.none
    %none_6141 = torch.constant.none
    %int5_6142 = torch.constant.int 5
    %cpu_6143 = torch.constant.device "cpu"
    %int0_6144 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4151, %none_6140, %none_6141, %int5_6142, %cpu_6143, %int0_6144 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_6145 = torch.constant.int 3
    %int0_6146 = torch.constant.int 0
    %int64_6147 = torch.constant.int 64
    %int2_6148 = torch.constant.int 2
    %4183 = torch.aten.slice.Tensor %4151, %int3_6145, %int0_6146, %int64_6147, %int2_6148 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_6149 = torch.constant.int 3
    %int1_6150 = torch.constant.int 1
    %int64_6151 = torch.constant.int 64
    %int2_6152 = torch.constant.int 2
    %4184 = torch.aten.slice.Tensor %4151, %int3_6149, %int1_6150, %int64_6151, %int2_6152 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %4185 = torch.aten.mul.Tensor %4183, %4181 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %4186 = torch.aten.mul.Tensor %4184, %4182 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_6153 = torch.constant.int 1
    %4187 = torch.aten.sub.Tensor %4185, %4186, %int1_6153 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %4188 = torch.aten.mul.Tensor %4184, %4181 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %4189 = torch.aten.mul.Tensor %4183, %4182 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_6154 = torch.constant.int 1
    %4190 = torch.aten.add.Tensor %4188, %4189, %int1_6154 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %4191 = torch_c.to_builtin_tensor %4187 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_6155 = tensor.cast %4191 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %4192 = torch_c.to_builtin_tensor %4190 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_6156 = tensor.cast %4192 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %4193 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_6155, %cast_6156) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_6157 = tensor.cast %4193 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %4194 = torch_c.from_builtin_tensor %cast_6157 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_6158 = torch.constant.int 4
    %int1_6159 = torch.constant.int 1
    %int32_6160 = torch.constant.int 32
    %int64_6161 = torch.constant.int 64
    %4195 = torch.prim.ListConstruct %int4_6158, %int1_6159, %int32_6160, %int64_6161 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4196 = torch.aten.view %4194, %4195 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_6162 = torch.constant.none
    %none_6163 = torch.constant.none
    %int5_6164 = torch.constant.int 5
    %cpu_6165 = torch.constant.device "cpu"
    %int0_6166 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4196, %none_6162, %none_6163, %int5_6164, %cpu_6165, %int0_6166 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_6167 = torch.constant.int 0
    %int1_6168 = torch.constant.int 1
    %none_6169 = torch.constant.none
    %none_6170 = torch.constant.none
    %cpu_6171 = torch.constant.device "cpu"
    %false_6172 = torch.constant.bool false
    %4197 = torch.aten.arange.start %int0_6167, %int1_6168, %none_6169, %none_6170, %cpu_6171, %false_6172 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_6173 = torch.constant.int 0
    %4198 = torch.aten.unsqueeze %4197, %int0_6173 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_6174 = torch.constant.int 1
    %4199 = torch.aten.unsqueeze %arg2, %int1_6174 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6175 = torch.constant.int 1
    %4200 = torch.aten.add.Tensor %4198, %4199, %int1_6175 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_6176 = torch.constant.int 0
    %int64_6177 = torch.constant.int 64
    %int2_6178 = torch.constant.int 2
    %none_6179 = torch.constant.none
    %none_6180 = torch.constant.none
    %cpu_6181 = torch.constant.device "cpu"
    %false_6182 = torch.constant.bool false
    %4201 = torch.aten.arange.start_step %int0_6176, %int64_6177, %int2_6178, %none_6179, %none_6180, %cpu_6181, %false_6182 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_6183 = torch.constant.none
    %none_6184 = torch.constant.none
    %int4_6185 = torch.constant.int 4
    %cpu_6186 = torch.constant.device "cpu"
    %int0_6187 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4201, %none_6183, %none_6184, %int4_6185, %cpu_6186, %int0_6187 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6188 = torch.constant.int 6
    %4202 = torch.prims.convert_element_type %4201, %int6_6188 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_6189 = torch.constant.int 64
    %4203 = torch.aten.div.Scalar %4202, %int64_6189 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_6190 = torch.constant.float 1.000000e+04
    %4204 = torch.aten.pow.Scalar %float1.000000e04_6190, %4203 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %4205 = torch.aten.reciprocal %4204 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_6191 = torch.constant.float 1.000000e+00
    %4206 = torch.aten.mul.Scalar %4205, %float1.000000e00_6191 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_6192 = torch.constant.none
    %4207 = torch.aten.clone %189, %none_6192 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_6193 = torch.constant.int 0
    %4208 = torch.aten.unsqueeze %4206, %int0_6193 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_6194 = torch.constant.int 2
    %4209 = torch.aten.unsqueeze %4208, %int2_6194 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_6195 = torch.constant.none
    %none_6196 = torch.constant.none
    %int6_6197 = torch.constant.int 6
    %cpu_6198 = torch.constant.device "cpu"
    %int0_6199 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4209, %none_6195, %none_6196, %int6_6197, %cpu_6198, %int0_6199 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_6200 = torch.constant.int 4
    %int-1_6201 = torch.constant.int -1
    %int1_6202 = torch.constant.int 1
    %4210 = torch.prim.ListConstruct %int4_6200, %int-1_6201, %int1_6202 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6203 = torch.constant.bool false
    %4211 = torch.aten.expand %4209, %4210, %false_6203 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_6204 = torch.constant.int 1
    %4212 = torch.aten.unsqueeze %4200, %int1_6204 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_6205 = torch.constant.none
    %none_6206 = torch.constant.none
    %int4_6207 = torch.constant.int 4
    %cpu_6208 = torch.constant.device "cpu"
    %int0_6209 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4212, %none_6205, %none_6206, %int4_6207, %cpu_6208, %int0_6209 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6210 = torch.constant.int 6
    %4213 = torch.prims.convert_element_type %4212, %int6_6210 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4214 = torch.aten.matmul %4211, %4213 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_6211 = torch.constant.int 1
    %int2_6212 = torch.constant.int 2
    %4215 = torch.aten.transpose.int %4214, %int1_6211, %int2_6212 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %4216 = torch.aten.cos %4215 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %4217 = torch.aten.mul.Tensor %4216, %4207 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_6213 = torch.constant.none
    %none_6214 = torch.constant.none
    %int6_6215 = torch.constant.int 6
    %cpu_6216 = torch.constant.device "cpu"
    %int0_6217 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4217, %none_6213, %none_6214, %int6_6215, %cpu_6216, %int0_6217 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6218 = torch.constant.int 5
    %4218 = torch.prims.convert_element_type %4217, %int5_6218 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %4219 = torch.aten.sin %4215 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %4220 = torch.aten.mul.Tensor %4219, %4207 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_6219 = torch.constant.none
    %none_6220 = torch.constant.none
    %int6_6221 = torch.constant.int 6
    %cpu_6222 = torch.constant.device "cpu"
    %int0_6223 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4220, %none_6219, %none_6220, %int6_6221, %cpu_6222, %int0_6223 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6224 = torch.constant.int 5
    %4221 = torch.prims.convert_element_type %4220, %int5_6224 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_6225 = torch.constant.int 2
    %4222 = torch.aten.unsqueeze %4218, %int2_6225 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_6226 = torch.constant.int 2
    %4223 = torch.aten.unsqueeze %4221, %int2_6226 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_6227 = torch.constant.none
    %none_6228 = torch.constant.none
    %int5_6229 = torch.constant.int 5
    %cpu_6230 = torch.constant.device "cpu"
    %int0_6231 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4222, %none_6227, %none_6228, %int5_6229, %cpu_6230, %int0_6231 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_6232 = torch.constant.none
    %none_6233 = torch.constant.none
    %int5_6234 = torch.constant.int 5
    %cpu_6235 = torch.constant.device "cpu"
    %int0_6236 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4223, %none_6232, %none_6233, %int5_6234, %cpu_6235, %int0_6236 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_6237 = torch.constant.none
    %none_6238 = torch.constant.none
    %int5_6239 = torch.constant.int 5
    %cpu_6240 = torch.constant.device "cpu"
    %int0_6241 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4153, %none_6237, %none_6238, %int5_6239, %cpu_6240, %int0_6241 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_6242 = torch.constant.int 3
    %int0_6243 = torch.constant.int 0
    %int64_6244 = torch.constant.int 64
    %int2_6245 = torch.constant.int 2
    %4224 = torch.aten.slice.Tensor %4153, %int3_6242, %int0_6243, %int64_6244, %int2_6245 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_6246 = torch.constant.int 3
    %int1_6247 = torch.constant.int 1
    %int64_6248 = torch.constant.int 64
    %int2_6249 = torch.constant.int 2
    %4225 = torch.aten.slice.Tensor %4153, %int3_6246, %int1_6247, %int64_6248, %int2_6249 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %4226 = torch.aten.mul.Tensor %4224, %4222 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %4227 = torch.aten.mul.Tensor %4225, %4223 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_6250 = torch.constant.int 1
    %4228 = torch.aten.sub.Tensor %4226, %4227, %int1_6250 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %4229 = torch.aten.mul.Tensor %4225, %4222 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %4230 = torch.aten.mul.Tensor %4224, %4223 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_6251 = torch.constant.int 1
    %4231 = torch.aten.add.Tensor %4229, %4230, %int1_6251 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %4232 = torch_c.to_builtin_tensor %4228 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_6252 = tensor.cast %4232 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %4233 = torch_c.to_builtin_tensor %4231 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_6253 = tensor.cast %4233 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %4234 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_6252, %cast_6253) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_6254 = tensor.cast %4234 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %4235 = torch_c.from_builtin_tensor %cast_6254 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_6255 = torch.constant.int 4
    %int1_6256 = torch.constant.int 1
    %int4_6257 = torch.constant.int 4
    %int64_6258 = torch.constant.int 64
    %4236 = torch.prim.ListConstruct %int4_6255, %int1_6256, %int4_6257, %int64_6258 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4237 = torch.aten.view %4235, %4236 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_6259 = torch.constant.none
    %none_6260 = torch.constant.none
    %int5_6261 = torch.constant.int 5
    %cpu_6262 = torch.constant.device "cpu"
    %int0_6263 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4237, %none_6259, %none_6260, %int5_6261, %cpu_6262, %int0_6263 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_6264 = torch.constant.int 32
    %4238 = torch.aten.floor_divide.Scalar %arg2, %int32_6264 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_6265 = torch.constant.int 1
    %4239 = torch.aten.unsqueeze %4238, %int1_6265 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6266 = torch.constant.int 1
    %false_6267 = torch.constant.bool false
    %4240 = torch.aten.gather %arg3, %int1_6266, %4239, %false_6267 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_6268 = torch.constant.int 4
    %int1_6269 = torch.constant.int 1
    %int1_6270 = torch.constant.int 1
    %4241 = torch.prim.ListConstruct %int4_6268, %int1_6269, %int1_6270 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4242 = torch.aten.view %4240, %4241 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_6271 = torch.constant.int 32
    %4243 = torch.aten.remainder.Scalar %arg2, %int32_6271 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_6272 = torch.constant.int 4
    %int1_6273 = torch.constant.int 1
    %int1_6274 = torch.constant.int 1
    %4244 = torch.prim.ListConstruct %int4_6272, %int1_6273, %int1_6274 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4245 = torch.aten.view %4243, %4244 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_6275 = torch.constant.int 4
    %none_6276 = torch.constant.none
    %none_6277 = torch.constant.none
    %cpu_6278 = torch.constant.device "cpu"
    %false_6279 = torch.constant.bool false
    %4246 = torch.aten.arange %int4_6275, %none_6276, %none_6277, %cpu_6278, %false_6279 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_6280 = torch.constant.int 1
    %int1_6281 = torch.constant.int 1
    %int4_6282 = torch.constant.int 4
    %4247 = torch.prim.ListConstruct %int1_6280, %int1_6281, %int4_6282 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4248 = torch.aten.view %4246, %4247 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_6283 = torch.constant.none
    %4249 = torch.aten.clone %190, %none_6283 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_6284 = torch.constant.int 1
    %int1_6285 = torch.constant.int 1
    %int1_6286 = torch.constant.int 1
    %4250 = torch.prim.ListConstruct %int1_6284, %int1_6285, %int1_6286 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4251 = torch.aten.view %4249, %4250 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_6287 = torch.constant.int 22
    %4252 = torch.aten.mul.Scalar %4242, %int22_6287 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int11 = torch.constant.int 11
    %int1_6288 = torch.constant.int 1
    %4253 = torch.aten.add.Scalar %4252, %int11, %int1_6288 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_6289 = torch.constant.int 2
    %4254 = torch.aten.mul.Scalar %4253, %int2_6289 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_6290 = torch.constant.int 1
    %4255 = torch.aten.add.Tensor %4254, %4251, %int1_6290 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_6291 = torch.constant.int 4
    %4256 = torch.aten.mul.Scalar %4255, %int4_6291 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_6292 = torch.constant.int 1
    %4257 = torch.aten.add.Tensor %4256, %4248, %int1_6292 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_6293 = torch.constant.int 32
    %4258 = torch.aten.mul.Scalar %4257, %int32_6293 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_6294 = torch.constant.int 1
    %4259 = torch.aten.add.Tensor %4258, %4245, %int1_6294 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_6295 = torch.constant.none
    %none_6296 = torch.constant.none
    %int5_6297 = torch.constant.int 5
    %cpu_6298 = torch.constant.device "cpu"
    %int0_6299 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4237, %none_6295, %none_6296, %int5_6297, %cpu_6298, %int0_6299 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_6300 = torch.constant.int 22
    %int2_6301 = torch.constant.int 2
    %int4_6302 = torch.constant.int 4
    %int32_6303 = torch.constant.int 32
    %int64_6304 = torch.constant.int 64
    %4260 = torch.prim.ListConstruct %381, %int22_6300, %int2_6301, %int4_6302, %int32_6303, %int64_6304 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4261 = torch.aten.view %3955, %4260 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4261, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_6305 = torch.constant.int 64
    %4262 = torch.prim.ListConstruct %553, %int64_6305 : (!torch.int, !torch.int) -> !torch.list<int>
    %4263 = torch.aten.view %4261, %4262 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %4263, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %4264 = torch.prim.ListConstruct %4259 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_6306 = torch.constant.bool false
    %4265 = torch.aten.index_put %4263, %4264, %4237, %false_6306 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %4265, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_6307 = torch.constant.int 22
    %int2_6308 = torch.constant.int 2
    %int4_6309 = torch.constant.int 4
    %int32_6310 = torch.constant.int 32
    %int64_6311 = torch.constant.int 64
    %4266 = torch.prim.ListConstruct %381, %int22_6307, %int2_6308, %int4_6309, %int32_6310, %int64_6311 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4267 = torch.aten.view %4265, %4266 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4267, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_6312 = torch.constant.int 360448
    %4268 = torch.prim.ListConstruct %381, %int360448_6312 : (!torch.int, !torch.int) -> !torch.list<int>
    %4269 = torch.aten.view %4267, %4268 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %4269, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_6313 = torch.constant.int 22
    %int2_6314 = torch.constant.int 2
    %int4_6315 = torch.constant.int 4
    %int32_6316 = torch.constant.int 32
    %int64_6317 = torch.constant.int 64
    %4270 = torch.prim.ListConstruct %381, %int22_6313, %int2_6314, %int4_6315, %int32_6316, %int64_6317 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4271 = torch.aten.view %4269, %4270 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4271, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_6318 = torch.constant.int 64
    %4272 = torch.prim.ListConstruct %553, %int64_6318 : (!torch.int, !torch.int) -> !torch.list<int>
    %4273 = torch.aten.view %4271, %4272 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %4273, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_6319 = torch.constant.none
    %4274 = torch.aten.clone %191, %none_6319 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_6320 = torch.constant.int 1
    %int1_6321 = torch.constant.int 1
    %int1_6322 = torch.constant.int 1
    %4275 = torch.prim.ListConstruct %int1_6320, %int1_6321, %int1_6322 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4276 = torch.aten.view %4274, %4275 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_6323 = torch.constant.int 22
    %4277 = torch.aten.mul.Scalar %4242, %int22_6323 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int11_6324 = torch.constant.int 11
    %int1_6325 = torch.constant.int 1
    %4278 = torch.aten.add.Scalar %4277, %int11_6324, %int1_6325 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_6326 = torch.constant.int 2
    %4279 = torch.aten.mul.Scalar %4278, %int2_6326 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_6327 = torch.constant.int 1
    %4280 = torch.aten.add.Tensor %4279, %4276, %int1_6327 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_6328 = torch.constant.int 4
    %4281 = torch.aten.mul.Scalar %4280, %int4_6328 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_6329 = torch.constant.int 1
    %4282 = torch.aten.add.Tensor %4281, %4248, %int1_6329 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_6330 = torch.constant.int 32
    %4283 = torch.aten.mul.Scalar %4282, %int32_6330 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_6331 = torch.constant.int 1
    %4284 = torch.aten.add.Tensor %4283, %4245, %int1_6331 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_6332 = torch.constant.none
    %none_6333 = torch.constant.none
    %int5_6334 = torch.constant.int 5
    %cpu_6335 = torch.constant.device "cpu"
    %int0_6336 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4155, %none_6332, %none_6333, %int5_6334, %cpu_6335, %int0_6336 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %4285 = torch.prim.ListConstruct %4284 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_6337 = torch.constant.bool false
    %4286 = torch.aten.index_put %4273, %4285, %4155, %false_6337 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %4286, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_6338 = torch.constant.int 22
    %int2_6339 = torch.constant.int 2
    %int4_6340 = torch.constant.int 4
    %int32_6341 = torch.constant.int 32
    %int64_6342 = torch.constant.int 64
    %4287 = torch.prim.ListConstruct %381, %int22_6338, %int2_6339, %int4_6340, %int32_6341, %int64_6342 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4288 = torch.aten.view %4286, %4287 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4288, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_6343 = torch.constant.int 360448
    %4289 = torch.prim.ListConstruct %381, %int360448_6343 : (!torch.int, !torch.int) -> !torch.list<int>
    %4290 = torch.aten.view %4288, %4289 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %4290, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_6344 = torch.constant.none
    %4291 = torch.aten.clone %192, %none_6344 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_6345 = torch.constant.none
    %4292 = torch.aten.clone %193, %none_6345 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_6346 = torch.constant.none
    %4293 = torch.aten.clone %194, %none_6346 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_6347 = torch.constant.int 22
    %int2_6348 = torch.constant.int 2
    %int4_6349 = torch.constant.int 4
    %int32_6350 = torch.constant.int 32
    %int64_6351 = torch.constant.int 64
    %4294 = torch.prim.ListConstruct %381, %int22_6347, %int2_6348, %int4_6349, %int32_6350, %int64_6351 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4295 = torch.aten.view %4290, %4294 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4295, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %4296 = torch_c.to_builtin_tensor %4295 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %4297 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_6352 = tensor.cast %4297 : tensor<4x?xi64> to tensor<?x?xi64>
    %4298 = torch_c.to_builtin_tensor %4291 : !torch.vtensor<[],si64> -> tensor<i64>
    %4299 = torch_c.to_builtin_tensor %4292 : !torch.vtensor<[],si64> -> tensor<i64>
    %4300 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%4296, %cast_6352, %4298, %4299) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_6353 = tensor.cast %4300 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %4301 = torch_c.from_builtin_tensor %cast_6353 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %4301, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %4302 = torch_c.to_builtin_tensor %4295 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %4303 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_6354 = tensor.cast %4303 : tensor<4x?xi64> to tensor<?x?xi64>
    %4304 = torch_c.to_builtin_tensor %4291 : !torch.vtensor<[],si64> -> tensor<i64>
    %4305 = torch_c.to_builtin_tensor %4293 : !torch.vtensor<[],si64> -> tensor<i64>
    %4306 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%4302, %cast_6354, %4304, %4305) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_6355 = tensor.cast %4306 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %4307 = torch_c.from_builtin_tensor %cast_6355 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %4307, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_6356 = torch.constant.int 2
    %int3_6357 = torch.constant.int 3
    %4308 = torch.aten.transpose.int %4301, %int2_6356, %int3_6357 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %4308, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_6358 = torch.constant.int 0
    %4309 = torch.aten.clone %4308, %int0_6358 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %4309, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_6359 = torch.constant.int 4
    %int4_6360 = torch.constant.int 4
    %int64_6361 = torch.constant.int 64
    %4310 = torch.prim.ListConstruct %int4_6359, %623, %int4_6360, %int64_6361 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4311 = torch.aten._unsafe_view %4309, %4310 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %4311, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_6362 = torch.constant.int 2
    %int3_6363 = torch.constant.int 3
    %4312 = torch.aten.transpose.int %4307, %int2_6362, %int3_6363 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %4312, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_6364 = torch.constant.int 0
    %4313 = torch.aten.clone %4312, %int0_6364 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %4313, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_6365 = torch.constant.int 4
    %int4_6366 = torch.constant.int 4
    %int64_6367 = torch.constant.int 64
    %4314 = torch.prim.ListConstruct %int4_6365, %623, %int4_6366, %int64_6367 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4315 = torch.aten._unsafe_view %4313, %4314 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %4315, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_6368 = torch.constant.int 0
    %int1_6369 = torch.constant.int 1
    %none_6370 = torch.constant.none
    %none_6371 = torch.constant.none
    %cpu_6372 = torch.constant.device "cpu"
    %false_6373 = torch.constant.bool false
    %4316 = torch.aten.arange.start_step %int0_6368, %623, %int1_6369, %none_6370, %none_6371, %cpu_6372, %false_6373 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4316, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_6374 = torch.constant.int -1
    %4317 = torch.aten.unsqueeze %arg1, %int-1_6374 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %4318 = torch.aten.ge.Tensor %4316, %4317 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %4318, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_6375 = torch.constant.none
    %4319 = torch.aten.clone %195, %none_6375 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_6376 = torch.constant.int 0
    %4320 = torch.aten.where.ScalarOther %4318, %4319, %int0_6376 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %4320, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_6377 = torch.constant.none
    %none_6378 = torch.constant.none
    %int5_6379 = torch.constant.int 5
    %cpu_6380 = torch.constant.device "cpu"
    %int0_6381 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4320, %none_6377, %none_6378, %int5_6379, %cpu_6380, %int0_6381 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_6382 = torch.constant.int 1
    %4321 = torch.aten.unsqueeze %4320, %int1_6382 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %4321, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_6383 = torch.constant.int 1
    %4322 = torch.aten.unsqueeze %4321, %int1_6383 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %4322, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_6384 = torch.constant.int -2
    %4323 = torch.aten.unsqueeze %4311, %int-2_6384 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %4323, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_6385 = torch.constant.int 4
    %int4_6386 = torch.constant.int 4
    %int8_6387 = torch.constant.int 8
    %int64_6388 = torch.constant.int 64
    %4324 = torch.prim.ListConstruct %int4_6385, %623, %int4_6386, %int8_6387, %int64_6388 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6389 = torch.constant.bool false
    %4325 = torch.aten.expand %4323, %4324, %false_6389 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4325, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_6390 = torch.constant.int 0
    %4326 = torch.aten.clone %4325, %int0_6390 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4326, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_6391 = torch.constant.int 4
    %int32_6392 = torch.constant.int 32
    %int64_6393 = torch.constant.int 64
    %4327 = torch.prim.ListConstruct %int4_6391, %623, %int32_6392, %int64_6393 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4328 = torch.aten._unsafe_view %4326, %4327 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4328, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_6394 = torch.constant.int -2
    %4329 = torch.aten.unsqueeze %4315, %int-2_6394 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %4329, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_6395 = torch.constant.int 4
    %int4_6396 = torch.constant.int 4
    %int8_6397 = torch.constant.int 8
    %int64_6398 = torch.constant.int 64
    %4330 = torch.prim.ListConstruct %int4_6395, %623, %int4_6396, %int8_6397, %int64_6398 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6399 = torch.constant.bool false
    %4331 = torch.aten.expand %4329, %4330, %false_6399 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4331, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_6400 = torch.constant.int 0
    %4332 = torch.aten.clone %4331, %int0_6400 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4332, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_6401 = torch.constant.int 4
    %int32_6402 = torch.constant.int 32
    %int64_6403 = torch.constant.int 64
    %4333 = torch.prim.ListConstruct %int4_6401, %623, %int32_6402, %int64_6403 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4334 = torch.aten._unsafe_view %4332, %4333 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4334, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_6404 = torch.constant.int 1
    %int2_6405 = torch.constant.int 2
    %4335 = torch.aten.transpose.int %4196, %int1_6404, %int2_6405 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_6406 = torch.constant.int 1
    %int2_6407 = torch.constant.int 2
    %4336 = torch.aten.transpose.int %4328, %int1_6406, %int2_6407 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %4336, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_6408 = torch.constant.int 1
    %int2_6409 = torch.constant.int 2
    %4337 = torch.aten.transpose.int %4334, %int1_6408, %int2_6409 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %4337, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_6410 = torch.constant.float 0.000000e+00
    %false_6411 = torch.constant.bool false
    %none_6412 = torch.constant.none
    %false_6413 = torch.constant.bool false
    %4338 = torch.aten.scaled_dot_product_attention %4335, %4336, %4337, %4322, %float0.000000e00_6410, %false_6411, %none_6412, %false_6413 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_6414 = torch.constant.int 1
    %int2_6415 = torch.constant.int 2
    %4339 = torch.aten.transpose.int %4338, %int1_6414, %int2_6415 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_6416 = torch.constant.int 4
    %int1_6417 = torch.constant.int 1
    %int2048_6418 = torch.constant.int 2048
    %4340 = torch.prim.ListConstruct %int4_6416, %int1_6417, %int2048_6418 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4341 = torch.aten.view %4339, %4340 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_6419 = torch.constant.int 2
    %4342 = torch.aten.view.dtype %200, %int2_6419 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %4343 = torch.aten.detach %4342 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_6420 = torch.constant.int -1
    %int17_6421 = torch.constant.int 17
    %4344 = torch.prim.ListConstruct %int-1_6420, %int17_6421 : (!torch.int, !torch.int) -> !torch.list<int>
    %4345 = torch.aten.view %4343, %4344 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_6422 = torch.constant.int 2048
    %int-1_6423 = torch.constant.int -1
    %int17_6424 = torch.constant.int 17
    %4346 = torch.prim.ListConstruct %int2048_6422, %int-1_6423, %int17_6424 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4347 = torch.aten.view %4345, %4346 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_6425 = torch.constant.int 2
    %int0_6426 = torch.constant.int 0
    %int1_6427 = torch.constant.int 1
    %int1_6428 = torch.constant.int 1
    %4348 = torch.aten.slice.Tensor %4347, %int2_6425, %int0_6426, %int1_6427, %int1_6428 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_6429 = torch.constant.int 5
    %4349 = torch.aten.view.dtype %4348, %int5_6429 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %4350 = torch.aten.detach %4349 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_6430 = torch.constant.int 2
    %int1_6431 = torch.constant.int 1
    %int9223372036854775807_6432 = torch.constant.int 9223372036854775807
    %int1_6433 = torch.constant.int 1
    %4351 = torch.aten.slice.Tensor %4347, %int2_6430, %int1_6431, %int9223372036854775807_6432, %int1_6433 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_6434 = torch.constant.int 1
    %4352 = torch.aten.view.dtype %4351, %int1_6434 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %4353 = torch.aten.detach %4352 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %4354 = torch_c.to_builtin_tensor %4341 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_6435 = tensor.cast %4354 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %4355 = torch_c.to_builtin_tensor %4350 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %4356 = torch_c.to_builtin_tensor %4353 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %4357 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_6435, %4355, %4356) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_6436 = tensor.cast %4357 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %4358 = torch_c.from_builtin_tensor %cast_6436 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_6437 = torch.constant.none
    %none_6438 = torch.constant.none
    %int5_6439 = torch.constant.int 5
    %cpu_6440 = torch.constant.device "cpu"
    %int0_6441 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4358, %none_6437, %none_6438, %int5_6439, %cpu_6440, %int0_6441 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_6442 = torch.constant.int 1
    %4359 = torch.aten.add.Tensor %4088, %4358, %int1_6442 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_6443 = torch.constant.none
    %none_6444 = torch.constant.none
    %int5_6445 = torch.constant.int 5
    %cpu_6446 = torch.constant.device "cpu"
    %int0_6447 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4359, %none_6443, %none_6444, %int5_6445, %cpu_6446, %int0_6447 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6448 = torch.constant.int 6
    %4360 = torch.prims.convert_element_type %4359, %int6_6448 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_6449 = torch.constant.int 2
    %4361 = torch.aten.pow.Tensor_Scalar %4360, %int2_6449 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_6450 = torch.constant.int -1
    %4362 = torch.prim.ListConstruct %int-1_6450 : (!torch.int) -> !torch.list<int>
    %true_6451 = torch.constant.bool true
    %none_6452 = torch.constant.none
    %4363 = torch.aten.mean.dim %4361, %4362, %true_6451, %none_6452 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_6453 = torch.constant.float 9.9999997473787516E-6
    %int1_6454 = torch.constant.int 1
    %4364 = torch.aten.add.Scalar %4363, %float9.999990e-06_6453, %int1_6454 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4365 = torch.aten.rsqrt %4364 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %4366 = torch.aten.mul.Tensor %4360, %4365 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_6455 = torch.constant.none
    %none_6456 = torch.constant.none
    %int6_6457 = torch.constant.int 6
    %cpu_6458 = torch.constant.device "cpu"
    %int0_6459 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4366, %none_6455, %none_6456, %int6_6457, %cpu_6458, %int0_6459 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6460 = torch.constant.int 5
    %4367 = torch.prims.convert_element_type %4366, %int5_6460 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %4368 = torch.aten.mul.Tensor %201, %4367 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_6461 = torch.constant.none
    %none_6462 = torch.constant.none
    %int6_6463 = torch.constant.int 6
    %cpu_6464 = torch.constant.device "cpu"
    %int0_6465 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4368, %none_6461, %none_6462, %int6_6463, %cpu_6464, %int0_6465 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6466 = torch.constant.int 5
    %4369 = torch.prims.convert_element_type %4368, %int5_6466 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_6467 = torch.constant.int 2
    %4370 = torch.aten.view.dtype %202, %int2_6467 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %4371 = torch.aten.detach %4370 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_6468 = torch.constant.int -1
    %int17_6469 = torch.constant.int 17
    %4372 = torch.prim.ListConstruct %int-1_6468, %int17_6469 : (!torch.int, !torch.int) -> !torch.list<int>
    %4373 = torch.aten.view %4371, %4372 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_6470 = torch.constant.int 5632
    %int-1_6471 = torch.constant.int -1
    %int17_6472 = torch.constant.int 17
    %4374 = torch.prim.ListConstruct %int5632_6470, %int-1_6471, %int17_6472 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4375 = torch.aten.view %4373, %4374 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_6473 = torch.constant.int 2
    %int0_6474 = torch.constant.int 0
    %int1_6475 = torch.constant.int 1
    %int1_6476 = torch.constant.int 1
    %4376 = torch.aten.slice.Tensor %4375, %int2_6473, %int0_6474, %int1_6475, %int1_6476 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_6477 = torch.constant.int 5
    %4377 = torch.aten.view.dtype %4376, %int5_6477 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %4378 = torch.aten.detach %4377 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_6478 = torch.constant.int 2
    %int1_6479 = torch.constant.int 1
    %int9223372036854775807_6480 = torch.constant.int 9223372036854775807
    %int1_6481 = torch.constant.int 1
    %4379 = torch.aten.slice.Tensor %4375, %int2_6478, %int1_6479, %int9223372036854775807_6480, %int1_6481 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_6482 = torch.constant.int 1
    %4380 = torch.aten.view.dtype %4379, %int1_6482 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %4381 = torch.aten.detach %4380 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %4382 = torch_c.to_builtin_tensor %4369 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_6483 = tensor.cast %4382 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %4383 = torch_c.to_builtin_tensor %4378 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %4384 = torch_c.to_builtin_tensor %4381 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %4385 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_6483, %4383, %4384) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_6484 = tensor.cast %4385 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %4386 = torch_c.from_builtin_tensor %cast_6484 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %4387 = torch.aten.silu %4386 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_6485 = torch.constant.int 2
    %4388 = torch.aten.view.dtype %203, %int2_6485 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %4389 = torch.aten.detach %4388 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_6486 = torch.constant.int -1
    %int17_6487 = torch.constant.int 17
    %4390 = torch.prim.ListConstruct %int-1_6486, %int17_6487 : (!torch.int, !torch.int) -> !torch.list<int>
    %4391 = torch.aten.view %4389, %4390 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_6488 = torch.constant.int 5632
    %int-1_6489 = torch.constant.int -1
    %int17_6490 = torch.constant.int 17
    %4392 = torch.prim.ListConstruct %int5632_6488, %int-1_6489, %int17_6490 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4393 = torch.aten.view %4391, %4392 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_6491 = torch.constant.int 2
    %int0_6492 = torch.constant.int 0
    %int1_6493 = torch.constant.int 1
    %int1_6494 = torch.constant.int 1
    %4394 = torch.aten.slice.Tensor %4393, %int2_6491, %int0_6492, %int1_6493, %int1_6494 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_6495 = torch.constant.int 5
    %4395 = torch.aten.view.dtype %4394, %int5_6495 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %4396 = torch.aten.detach %4395 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_6496 = torch.constant.int 2
    %int1_6497 = torch.constant.int 1
    %int9223372036854775807_6498 = torch.constant.int 9223372036854775807
    %int1_6499 = torch.constant.int 1
    %4397 = torch.aten.slice.Tensor %4393, %int2_6496, %int1_6497, %int9223372036854775807_6498, %int1_6499 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_6500 = torch.constant.int 1
    %4398 = torch.aten.view.dtype %4397, %int1_6500 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %4399 = torch.aten.detach %4398 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %4400 = torch_c.to_builtin_tensor %4369 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_6501 = tensor.cast %4400 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %4401 = torch_c.to_builtin_tensor %4396 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %4402 = torch_c.to_builtin_tensor %4399 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %4403 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_6501, %4401, %4402) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_6502 = tensor.cast %4403 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %4404 = torch_c.from_builtin_tensor %cast_6502 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %4405 = torch.aten.mul.Tensor %4387, %4404 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_6503 = torch.constant.int 2
    %4406 = torch.aten.view.dtype %204, %int2_6503 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %4407 = torch.aten.detach %4406 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_6504 = torch.constant.int -1
    %int17_6505 = torch.constant.int 17
    %4408 = torch.prim.ListConstruct %int-1_6504, %int17_6505 : (!torch.int, !torch.int) -> !torch.list<int>
    %4409 = torch.aten.view %4407, %4408 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_6506 = torch.constant.int 2048
    %int-1_6507 = torch.constant.int -1
    %int17_6508 = torch.constant.int 17
    %4410 = torch.prim.ListConstruct %int2048_6506, %int-1_6507, %int17_6508 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4411 = torch.aten.view %4409, %4410 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_6509 = torch.constant.int 2
    %int0_6510 = torch.constant.int 0
    %int1_6511 = torch.constant.int 1
    %int1_6512 = torch.constant.int 1
    %4412 = torch.aten.slice.Tensor %4411, %int2_6509, %int0_6510, %int1_6511, %int1_6512 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_6513 = torch.constant.int 5
    %4413 = torch.aten.view.dtype %4412, %int5_6513 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %4414 = torch.aten.detach %4413 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_6514 = torch.constant.int 2
    %int1_6515 = torch.constant.int 1
    %int9223372036854775807_6516 = torch.constant.int 9223372036854775807
    %int1_6517 = torch.constant.int 1
    %4415 = torch.aten.slice.Tensor %4411, %int2_6514, %int1_6515, %int9223372036854775807_6516, %int1_6517 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_6518 = torch.constant.int 1
    %4416 = torch.aten.view.dtype %4415, %int1_6518 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %4417 = torch.aten.detach %4416 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %4418 = torch_c.to_builtin_tensor %4405 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_6519 = tensor.cast %4418 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %4419 = torch_c.to_builtin_tensor %4414 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %4420 = torch_c.to_builtin_tensor %4417 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %4421 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_6519, %4419, %4420) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_6520 = tensor.cast %4421 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %4422 = torch_c.from_builtin_tensor %cast_6520 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_6521 = torch.constant.int 1
    %4423 = torch.aten.add.Tensor %4359, %4422, %int1_6521 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_6522 = torch.constant.none
    %none_6523 = torch.constant.none
    %int5_6524 = torch.constant.int 5
    %cpu_6525 = torch.constant.device "cpu"
    %int0_6526 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4423, %none_6522, %none_6523, %int5_6524, %cpu_6525, %int0_6526 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6527 = torch.constant.int 6
    %4424 = torch.prims.convert_element_type %4423, %int6_6527 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_6528 = torch.constant.int 2
    %4425 = torch.aten.pow.Tensor_Scalar %4424, %int2_6528 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_6529 = torch.constant.int -1
    %4426 = torch.prim.ListConstruct %int-1_6529 : (!torch.int) -> !torch.list<int>
    %true_6530 = torch.constant.bool true
    %none_6531 = torch.constant.none
    %4427 = torch.aten.mean.dim %4425, %4426, %true_6530, %none_6531 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_6532 = torch.constant.float 9.9999997473787516E-6
    %int1_6533 = torch.constant.int 1
    %4428 = torch.aten.add.Scalar %4427, %float9.999990e-06_6532, %int1_6533 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4429 = torch.aten.rsqrt %4428 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %4430 = torch.aten.mul.Tensor %4424, %4429 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_6534 = torch.constant.none
    %none_6535 = torch.constant.none
    %int6_6536 = torch.constant.int 6
    %cpu_6537 = torch.constant.device "cpu"
    %int0_6538 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4430, %none_6534, %none_6535, %int6_6536, %cpu_6537, %int0_6538 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6539 = torch.constant.int 5
    %4431 = torch.prims.convert_element_type %4430, %int5_6539 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %4432 = torch.aten.mul.Tensor %213, %4431 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_6540 = torch.constant.none
    %none_6541 = torch.constant.none
    %int6_6542 = torch.constant.int 6
    %cpu_6543 = torch.constant.device "cpu"
    %int0_6544 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4432, %none_6540, %none_6541, %int6_6542, %cpu_6543, %int0_6544 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6545 = torch.constant.int 5
    %4433 = torch.prims.convert_element_type %4432, %int5_6545 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_6546 = torch.constant.int 2
    %4434 = torch.aten.view.dtype %214, %int2_6546 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %4435 = torch.aten.detach %4434 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_6547 = torch.constant.int -1
    %int17_6548 = torch.constant.int 17
    %4436 = torch.prim.ListConstruct %int-1_6547, %int17_6548 : (!torch.int, !torch.int) -> !torch.list<int>
    %4437 = torch.aten.view %4435, %4436 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_6549 = torch.constant.int 2048
    %int-1_6550 = torch.constant.int -1
    %int17_6551 = torch.constant.int 17
    %4438 = torch.prim.ListConstruct %int2048_6549, %int-1_6550, %int17_6551 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4439 = torch.aten.view %4437, %4438 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_6552 = torch.constant.int 2
    %int0_6553 = torch.constant.int 0
    %int1_6554 = torch.constant.int 1
    %int1_6555 = torch.constant.int 1
    %4440 = torch.aten.slice.Tensor %4439, %int2_6552, %int0_6553, %int1_6554, %int1_6555 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_6556 = torch.constant.int 5
    %4441 = torch.aten.view.dtype %4440, %int5_6556 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %4442 = torch.aten.detach %4441 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_6557 = torch.constant.int 2
    %int1_6558 = torch.constant.int 1
    %int9223372036854775807_6559 = torch.constant.int 9223372036854775807
    %int1_6560 = torch.constant.int 1
    %4443 = torch.aten.slice.Tensor %4439, %int2_6557, %int1_6558, %int9223372036854775807_6559, %int1_6560 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_6561 = torch.constant.int 1
    %4444 = torch.aten.view.dtype %4443, %int1_6561 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %4445 = torch.aten.detach %4444 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %4446 = torch_c.to_builtin_tensor %4433 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_6562 = tensor.cast %4446 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %4447 = torch_c.to_builtin_tensor %4442 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %4448 = torch_c.to_builtin_tensor %4445 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %4449 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_6562, %4447, %4448) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_6563 = tensor.cast %4449 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %4450 = torch_c.from_builtin_tensor %cast_6563 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_6564 = torch.constant.int 2
    %4451 = torch.aten.view.dtype %215, %int2_6564 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %4452 = torch.aten.detach %4451 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_6565 = torch.constant.int -1
    %int17_6566 = torch.constant.int 17
    %4453 = torch.prim.ListConstruct %int-1_6565, %int17_6566 : (!torch.int, !torch.int) -> !torch.list<int>
    %4454 = torch.aten.view %4452, %4453 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_6567 = torch.constant.int 256
    %int-1_6568 = torch.constant.int -1
    %int17_6569 = torch.constant.int 17
    %4455 = torch.prim.ListConstruct %int256_6567, %int-1_6568, %int17_6569 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4456 = torch.aten.view %4454, %4455 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_6570 = torch.constant.int 2
    %int0_6571 = torch.constant.int 0
    %int1_6572 = torch.constant.int 1
    %int1_6573 = torch.constant.int 1
    %4457 = torch.aten.slice.Tensor %4456, %int2_6570, %int0_6571, %int1_6572, %int1_6573 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_6574 = torch.constant.int 5
    %4458 = torch.aten.view.dtype %4457, %int5_6574 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %4459 = torch.aten.detach %4458 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_6575 = torch.constant.int 2
    %int1_6576 = torch.constant.int 1
    %int9223372036854775807_6577 = torch.constant.int 9223372036854775807
    %int1_6578 = torch.constant.int 1
    %4460 = torch.aten.slice.Tensor %4456, %int2_6575, %int1_6576, %int9223372036854775807_6577, %int1_6578 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_6579 = torch.constant.int 1
    %4461 = torch.aten.view.dtype %4460, %int1_6579 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %4462 = torch.aten.detach %4461 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %4463 = torch_c.to_builtin_tensor %4433 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_6580 = tensor.cast %4463 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %4464 = torch_c.to_builtin_tensor %4459 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %4465 = torch_c.to_builtin_tensor %4462 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %4466 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_6580, %4464, %4465) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_6581 = tensor.cast %4466 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %4467 = torch_c.from_builtin_tensor %cast_6581 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_6582 = torch.constant.int 2
    %4468 = torch.aten.view.dtype %216, %int2_6582 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %4469 = torch.aten.detach %4468 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_6583 = torch.constant.int -1
    %int17_6584 = torch.constant.int 17
    %4470 = torch.prim.ListConstruct %int-1_6583, %int17_6584 : (!torch.int, !torch.int) -> !torch.list<int>
    %4471 = torch.aten.view %4469, %4470 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_6585 = torch.constant.int 256
    %int-1_6586 = torch.constant.int -1
    %int17_6587 = torch.constant.int 17
    %4472 = torch.prim.ListConstruct %int256_6585, %int-1_6586, %int17_6587 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4473 = torch.aten.view %4471, %4472 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_6588 = torch.constant.int 2
    %int0_6589 = torch.constant.int 0
    %int1_6590 = torch.constant.int 1
    %int1_6591 = torch.constant.int 1
    %4474 = torch.aten.slice.Tensor %4473, %int2_6588, %int0_6589, %int1_6590, %int1_6591 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_6592 = torch.constant.int 5
    %4475 = torch.aten.view.dtype %4474, %int5_6592 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %4476 = torch.aten.detach %4475 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_6593 = torch.constant.int 2
    %int1_6594 = torch.constant.int 1
    %int9223372036854775807_6595 = torch.constant.int 9223372036854775807
    %int1_6596 = torch.constant.int 1
    %4477 = torch.aten.slice.Tensor %4473, %int2_6593, %int1_6594, %int9223372036854775807_6595, %int1_6596 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_6597 = torch.constant.int 1
    %4478 = torch.aten.view.dtype %4477, %int1_6597 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %4479 = torch.aten.detach %4478 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %4480 = torch_c.to_builtin_tensor %4433 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_6598 = tensor.cast %4480 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %4481 = torch_c.to_builtin_tensor %4476 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %4482 = torch_c.to_builtin_tensor %4479 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %4483 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_6598, %4481, %4482) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_6599 = tensor.cast %4483 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %4484 = torch_c.from_builtin_tensor %cast_6599 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_6600 = torch.constant.int 4
    %int1_6601 = torch.constant.int 1
    %int32_6602 = torch.constant.int 32
    %int64_6603 = torch.constant.int 64
    %4485 = torch.prim.ListConstruct %int4_6600, %int1_6601, %int32_6602, %int64_6603 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4486 = torch.aten.view %4450, %4485 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_6604 = torch.constant.int 4
    %int1_6605 = torch.constant.int 1
    %int4_6606 = torch.constant.int 4
    %int64_6607 = torch.constant.int 64
    %4487 = torch.prim.ListConstruct %int4_6604, %int1_6605, %int4_6606, %int64_6607 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4488 = torch.aten.view %4467, %4487 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_6608 = torch.constant.int 4
    %int1_6609 = torch.constant.int 1
    %int4_6610 = torch.constant.int 4
    %int64_6611 = torch.constant.int 64
    %4489 = torch.prim.ListConstruct %int4_6608, %int1_6609, %int4_6610, %int64_6611 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4490 = torch.aten.view %4484, %4489 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_6612 = torch.constant.int 0
    %int1_6613 = torch.constant.int 1
    %none_6614 = torch.constant.none
    %none_6615 = torch.constant.none
    %cpu_6616 = torch.constant.device "cpu"
    %false_6617 = torch.constant.bool false
    %4491 = torch.aten.arange.start %int0_6612, %int1_6613, %none_6614, %none_6615, %cpu_6616, %false_6617 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_6618 = torch.constant.int 0
    %4492 = torch.aten.unsqueeze %4491, %int0_6618 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_6619 = torch.constant.int 1
    %4493 = torch.aten.unsqueeze %arg2, %int1_6619 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6620 = torch.constant.int 1
    %4494 = torch.aten.add.Tensor %4492, %4493, %int1_6620 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_6621 = torch.constant.int 0
    %int64_6622 = torch.constant.int 64
    %int2_6623 = torch.constant.int 2
    %none_6624 = torch.constant.none
    %none_6625 = torch.constant.none
    %cpu_6626 = torch.constant.device "cpu"
    %false_6627 = torch.constant.bool false
    %4495 = torch.aten.arange.start_step %int0_6621, %int64_6622, %int2_6623, %none_6624, %none_6625, %cpu_6626, %false_6627 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_6628 = torch.constant.none
    %none_6629 = torch.constant.none
    %int4_6630 = torch.constant.int 4
    %cpu_6631 = torch.constant.device "cpu"
    %int0_6632 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4495, %none_6628, %none_6629, %int4_6630, %cpu_6631, %int0_6632 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6633 = torch.constant.int 6
    %4496 = torch.prims.convert_element_type %4495, %int6_6633 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_6634 = torch.constant.int 64
    %4497 = torch.aten.div.Scalar %4496, %int64_6634 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_6635 = torch.constant.float 1.000000e+04
    %4498 = torch.aten.pow.Scalar %float1.000000e04_6635, %4497 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %4499 = torch.aten.reciprocal %4498 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_6636 = torch.constant.float 1.000000e+00
    %4500 = torch.aten.mul.Scalar %4499, %float1.000000e00_6636 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_6637 = torch.constant.none
    %4501 = torch.aten.clone %205, %none_6637 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_6638 = torch.constant.int 0
    %4502 = torch.aten.unsqueeze %4500, %int0_6638 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_6639 = torch.constant.int 2
    %4503 = torch.aten.unsqueeze %4502, %int2_6639 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_6640 = torch.constant.none
    %none_6641 = torch.constant.none
    %int6_6642 = torch.constant.int 6
    %cpu_6643 = torch.constant.device "cpu"
    %int0_6644 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4503, %none_6640, %none_6641, %int6_6642, %cpu_6643, %int0_6644 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_6645 = torch.constant.int 4
    %int-1_6646 = torch.constant.int -1
    %int1_6647 = torch.constant.int 1
    %4504 = torch.prim.ListConstruct %int4_6645, %int-1_6646, %int1_6647 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6648 = torch.constant.bool false
    %4505 = torch.aten.expand %4503, %4504, %false_6648 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_6649 = torch.constant.int 1
    %4506 = torch.aten.unsqueeze %4494, %int1_6649 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_6650 = torch.constant.none
    %none_6651 = torch.constant.none
    %int4_6652 = torch.constant.int 4
    %cpu_6653 = torch.constant.device "cpu"
    %int0_6654 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4506, %none_6650, %none_6651, %int4_6652, %cpu_6653, %int0_6654 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6655 = torch.constant.int 6
    %4507 = torch.prims.convert_element_type %4506, %int6_6655 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4508 = torch.aten.matmul %4505, %4507 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_6656 = torch.constant.int 1
    %int2_6657 = torch.constant.int 2
    %4509 = torch.aten.transpose.int %4508, %int1_6656, %int2_6657 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %4510 = torch.aten.cos %4509 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %4511 = torch.aten.mul.Tensor %4510, %4501 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_6658 = torch.constant.none
    %none_6659 = torch.constant.none
    %int6_6660 = torch.constant.int 6
    %cpu_6661 = torch.constant.device "cpu"
    %int0_6662 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4511, %none_6658, %none_6659, %int6_6660, %cpu_6661, %int0_6662 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6663 = torch.constant.int 5
    %4512 = torch.prims.convert_element_type %4511, %int5_6663 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %4513 = torch.aten.sin %4509 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %4514 = torch.aten.mul.Tensor %4513, %4501 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_6664 = torch.constant.none
    %none_6665 = torch.constant.none
    %int6_6666 = torch.constant.int 6
    %cpu_6667 = torch.constant.device "cpu"
    %int0_6668 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4514, %none_6664, %none_6665, %int6_6666, %cpu_6667, %int0_6668 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6669 = torch.constant.int 5
    %4515 = torch.prims.convert_element_type %4514, %int5_6669 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_6670 = torch.constant.int 2
    %4516 = torch.aten.unsqueeze %4512, %int2_6670 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_6671 = torch.constant.int 2
    %4517 = torch.aten.unsqueeze %4515, %int2_6671 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_6672 = torch.constant.none
    %none_6673 = torch.constant.none
    %int5_6674 = torch.constant.int 5
    %cpu_6675 = torch.constant.device "cpu"
    %int0_6676 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4516, %none_6672, %none_6673, %int5_6674, %cpu_6675, %int0_6676 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_6677 = torch.constant.none
    %none_6678 = torch.constant.none
    %int5_6679 = torch.constant.int 5
    %cpu_6680 = torch.constant.device "cpu"
    %int0_6681 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4517, %none_6677, %none_6678, %int5_6679, %cpu_6680, %int0_6681 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_6682 = torch.constant.none
    %none_6683 = torch.constant.none
    %int5_6684 = torch.constant.int 5
    %cpu_6685 = torch.constant.device "cpu"
    %int0_6686 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4486, %none_6682, %none_6683, %int5_6684, %cpu_6685, %int0_6686 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_6687 = torch.constant.int 3
    %int0_6688 = torch.constant.int 0
    %int64_6689 = torch.constant.int 64
    %int2_6690 = torch.constant.int 2
    %4518 = torch.aten.slice.Tensor %4486, %int3_6687, %int0_6688, %int64_6689, %int2_6690 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_6691 = torch.constant.int 3
    %int1_6692 = torch.constant.int 1
    %int64_6693 = torch.constant.int 64
    %int2_6694 = torch.constant.int 2
    %4519 = torch.aten.slice.Tensor %4486, %int3_6691, %int1_6692, %int64_6693, %int2_6694 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %4520 = torch.aten.mul.Tensor %4518, %4516 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %4521 = torch.aten.mul.Tensor %4519, %4517 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_6695 = torch.constant.int 1
    %4522 = torch.aten.sub.Tensor %4520, %4521, %int1_6695 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %4523 = torch.aten.mul.Tensor %4519, %4516 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %4524 = torch.aten.mul.Tensor %4518, %4517 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_6696 = torch.constant.int 1
    %4525 = torch.aten.add.Tensor %4523, %4524, %int1_6696 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %4526 = torch_c.to_builtin_tensor %4522 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_6697 = tensor.cast %4526 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %4527 = torch_c.to_builtin_tensor %4525 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_6698 = tensor.cast %4527 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %4528 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_6697, %cast_6698) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_6699 = tensor.cast %4528 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %4529 = torch_c.from_builtin_tensor %cast_6699 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_6700 = torch.constant.int 4
    %int1_6701 = torch.constant.int 1
    %int32_6702 = torch.constant.int 32
    %int64_6703 = torch.constant.int 64
    %4530 = torch.prim.ListConstruct %int4_6700, %int1_6701, %int32_6702, %int64_6703 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4531 = torch.aten.view %4529, %4530 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_6704 = torch.constant.none
    %none_6705 = torch.constant.none
    %int5_6706 = torch.constant.int 5
    %cpu_6707 = torch.constant.device "cpu"
    %int0_6708 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4531, %none_6704, %none_6705, %int5_6706, %cpu_6707, %int0_6708 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_6709 = torch.constant.int 0
    %int1_6710 = torch.constant.int 1
    %none_6711 = torch.constant.none
    %none_6712 = torch.constant.none
    %cpu_6713 = torch.constant.device "cpu"
    %false_6714 = torch.constant.bool false
    %4532 = torch.aten.arange.start %int0_6709, %int1_6710, %none_6711, %none_6712, %cpu_6713, %false_6714 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_6715 = torch.constant.int 0
    %4533 = torch.aten.unsqueeze %4532, %int0_6715 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_6716 = torch.constant.int 1
    %4534 = torch.aten.unsqueeze %arg2, %int1_6716 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6717 = torch.constant.int 1
    %4535 = torch.aten.add.Tensor %4533, %4534, %int1_6717 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_6718 = torch.constant.int 0
    %int64_6719 = torch.constant.int 64
    %int2_6720 = torch.constant.int 2
    %none_6721 = torch.constant.none
    %none_6722 = torch.constant.none
    %cpu_6723 = torch.constant.device "cpu"
    %false_6724 = torch.constant.bool false
    %4536 = torch.aten.arange.start_step %int0_6718, %int64_6719, %int2_6720, %none_6721, %none_6722, %cpu_6723, %false_6724 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_6725 = torch.constant.none
    %none_6726 = torch.constant.none
    %int4_6727 = torch.constant.int 4
    %cpu_6728 = torch.constant.device "cpu"
    %int0_6729 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4536, %none_6725, %none_6726, %int4_6727, %cpu_6728, %int0_6729 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6730 = torch.constant.int 6
    %4537 = torch.prims.convert_element_type %4536, %int6_6730 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_6731 = torch.constant.int 64
    %4538 = torch.aten.div.Scalar %4537, %int64_6731 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_6732 = torch.constant.float 1.000000e+04
    %4539 = torch.aten.pow.Scalar %float1.000000e04_6732, %4538 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %4540 = torch.aten.reciprocal %4539 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_6733 = torch.constant.float 1.000000e+00
    %4541 = torch.aten.mul.Scalar %4540, %float1.000000e00_6733 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_6734 = torch.constant.none
    %4542 = torch.aten.clone %206, %none_6734 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_6735 = torch.constant.int 0
    %4543 = torch.aten.unsqueeze %4541, %int0_6735 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_6736 = torch.constant.int 2
    %4544 = torch.aten.unsqueeze %4543, %int2_6736 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_6737 = torch.constant.none
    %none_6738 = torch.constant.none
    %int6_6739 = torch.constant.int 6
    %cpu_6740 = torch.constant.device "cpu"
    %int0_6741 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4544, %none_6737, %none_6738, %int6_6739, %cpu_6740, %int0_6741 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_6742 = torch.constant.int 4
    %int-1_6743 = torch.constant.int -1
    %int1_6744 = torch.constant.int 1
    %4545 = torch.prim.ListConstruct %int4_6742, %int-1_6743, %int1_6744 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6745 = torch.constant.bool false
    %4546 = torch.aten.expand %4544, %4545, %false_6745 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_6746 = torch.constant.int 1
    %4547 = torch.aten.unsqueeze %4535, %int1_6746 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_6747 = torch.constant.none
    %none_6748 = torch.constant.none
    %int4_6749 = torch.constant.int 4
    %cpu_6750 = torch.constant.device "cpu"
    %int0_6751 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4547, %none_6747, %none_6748, %int4_6749, %cpu_6750, %int0_6751 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6752 = torch.constant.int 6
    %4548 = torch.prims.convert_element_type %4547, %int6_6752 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4549 = torch.aten.matmul %4546, %4548 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_6753 = torch.constant.int 1
    %int2_6754 = torch.constant.int 2
    %4550 = torch.aten.transpose.int %4549, %int1_6753, %int2_6754 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %4551 = torch.aten.cos %4550 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %4552 = torch.aten.mul.Tensor %4551, %4542 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_6755 = torch.constant.none
    %none_6756 = torch.constant.none
    %int6_6757 = torch.constant.int 6
    %cpu_6758 = torch.constant.device "cpu"
    %int0_6759 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4552, %none_6755, %none_6756, %int6_6757, %cpu_6758, %int0_6759 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6760 = torch.constant.int 5
    %4553 = torch.prims.convert_element_type %4552, %int5_6760 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %4554 = torch.aten.sin %4550 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %4555 = torch.aten.mul.Tensor %4554, %4542 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_6761 = torch.constant.none
    %none_6762 = torch.constant.none
    %int6_6763 = torch.constant.int 6
    %cpu_6764 = torch.constant.device "cpu"
    %int0_6765 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4555, %none_6761, %none_6762, %int6_6763, %cpu_6764, %int0_6765 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_6766 = torch.constant.int 5
    %4556 = torch.prims.convert_element_type %4555, %int5_6766 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_6767 = torch.constant.int 2
    %4557 = torch.aten.unsqueeze %4553, %int2_6767 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_6768 = torch.constant.int 2
    %4558 = torch.aten.unsqueeze %4556, %int2_6768 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_6769 = torch.constant.none
    %none_6770 = torch.constant.none
    %int5_6771 = torch.constant.int 5
    %cpu_6772 = torch.constant.device "cpu"
    %int0_6773 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4557, %none_6769, %none_6770, %int5_6771, %cpu_6772, %int0_6773 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_6774 = torch.constant.none
    %none_6775 = torch.constant.none
    %int5_6776 = torch.constant.int 5
    %cpu_6777 = torch.constant.device "cpu"
    %int0_6778 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4558, %none_6774, %none_6775, %int5_6776, %cpu_6777, %int0_6778 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_6779 = torch.constant.none
    %none_6780 = torch.constant.none
    %int5_6781 = torch.constant.int 5
    %cpu_6782 = torch.constant.device "cpu"
    %int0_6783 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4488, %none_6779, %none_6780, %int5_6781, %cpu_6782, %int0_6783 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_6784 = torch.constant.int 3
    %int0_6785 = torch.constant.int 0
    %int64_6786 = torch.constant.int 64
    %int2_6787 = torch.constant.int 2
    %4559 = torch.aten.slice.Tensor %4488, %int3_6784, %int0_6785, %int64_6786, %int2_6787 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_6788 = torch.constant.int 3
    %int1_6789 = torch.constant.int 1
    %int64_6790 = torch.constant.int 64
    %int2_6791 = torch.constant.int 2
    %4560 = torch.aten.slice.Tensor %4488, %int3_6788, %int1_6789, %int64_6790, %int2_6791 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %4561 = torch.aten.mul.Tensor %4559, %4557 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %4562 = torch.aten.mul.Tensor %4560, %4558 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_6792 = torch.constant.int 1
    %4563 = torch.aten.sub.Tensor %4561, %4562, %int1_6792 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %4564 = torch.aten.mul.Tensor %4560, %4557 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %4565 = torch.aten.mul.Tensor %4559, %4558 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_6793 = torch.constant.int 1
    %4566 = torch.aten.add.Tensor %4564, %4565, %int1_6793 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %4567 = torch_c.to_builtin_tensor %4563 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_6794 = tensor.cast %4567 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %4568 = torch_c.to_builtin_tensor %4566 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_6795 = tensor.cast %4568 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %4569 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_6794, %cast_6795) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_6796 = tensor.cast %4569 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %4570 = torch_c.from_builtin_tensor %cast_6796 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_6797 = torch.constant.int 4
    %int1_6798 = torch.constant.int 1
    %int4_6799 = torch.constant.int 4
    %int64_6800 = torch.constant.int 64
    %4571 = torch.prim.ListConstruct %int4_6797, %int1_6798, %int4_6799, %int64_6800 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4572 = torch.aten.view %4570, %4571 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_6801 = torch.constant.none
    %none_6802 = torch.constant.none
    %int5_6803 = torch.constant.int 5
    %cpu_6804 = torch.constant.device "cpu"
    %int0_6805 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4572, %none_6801, %none_6802, %int5_6803, %cpu_6804, %int0_6805 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_6806 = torch.constant.int 32
    %4573 = torch.aten.floor_divide.Scalar %arg2, %int32_6806 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_6807 = torch.constant.int 1
    %4574 = torch.aten.unsqueeze %4573, %int1_6807 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6808 = torch.constant.int 1
    %false_6809 = torch.constant.bool false
    %4575 = torch.aten.gather %arg3, %int1_6808, %4574, %false_6809 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_6810 = torch.constant.int 4
    %int1_6811 = torch.constant.int 1
    %int1_6812 = torch.constant.int 1
    %4576 = torch.prim.ListConstruct %int4_6810, %int1_6811, %int1_6812 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4577 = torch.aten.view %4575, %4576 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_6813 = torch.constant.int 32
    %4578 = torch.aten.remainder.Scalar %arg2, %int32_6813 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_6814 = torch.constant.int 4
    %int1_6815 = torch.constant.int 1
    %int1_6816 = torch.constant.int 1
    %4579 = torch.prim.ListConstruct %int4_6814, %int1_6815, %int1_6816 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4580 = torch.aten.view %4578, %4579 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_6817 = torch.constant.int 4
    %none_6818 = torch.constant.none
    %none_6819 = torch.constant.none
    %cpu_6820 = torch.constant.device "cpu"
    %false_6821 = torch.constant.bool false
    %4581 = torch.aten.arange %int4_6817, %none_6818, %none_6819, %cpu_6820, %false_6821 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_6822 = torch.constant.int 1
    %int1_6823 = torch.constant.int 1
    %int4_6824 = torch.constant.int 4
    %4582 = torch.prim.ListConstruct %int1_6822, %int1_6823, %int4_6824 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4583 = torch.aten.view %4581, %4582 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_6825 = torch.constant.none
    %4584 = torch.aten.clone %207, %none_6825 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_6826 = torch.constant.int 1
    %int1_6827 = torch.constant.int 1
    %int1_6828 = torch.constant.int 1
    %4585 = torch.prim.ListConstruct %int1_6826, %int1_6827, %int1_6828 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4586 = torch.aten.view %4584, %4585 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_6829 = torch.constant.int 22
    %4587 = torch.aten.mul.Scalar %4577, %int22_6829 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int12 = torch.constant.int 12
    %int1_6830 = torch.constant.int 1
    %4588 = torch.aten.add.Scalar %4587, %int12, %int1_6830 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_6831 = torch.constant.int 2
    %4589 = torch.aten.mul.Scalar %4588, %int2_6831 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_6832 = torch.constant.int 1
    %4590 = torch.aten.add.Tensor %4589, %4586, %int1_6832 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_6833 = torch.constant.int 4
    %4591 = torch.aten.mul.Scalar %4590, %int4_6833 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_6834 = torch.constant.int 1
    %4592 = torch.aten.add.Tensor %4591, %4583, %int1_6834 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_6835 = torch.constant.int 32
    %4593 = torch.aten.mul.Scalar %4592, %int32_6835 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_6836 = torch.constant.int 1
    %4594 = torch.aten.add.Tensor %4593, %4580, %int1_6836 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_6837 = torch.constant.none
    %none_6838 = torch.constant.none
    %int5_6839 = torch.constant.int 5
    %cpu_6840 = torch.constant.device "cpu"
    %int0_6841 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4572, %none_6837, %none_6838, %int5_6839, %cpu_6840, %int0_6841 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_6842 = torch.constant.int 22
    %int2_6843 = torch.constant.int 2
    %int4_6844 = torch.constant.int 4
    %int32_6845 = torch.constant.int 32
    %int64_6846 = torch.constant.int 64
    %4595 = torch.prim.ListConstruct %381, %int22_6842, %int2_6843, %int4_6844, %int32_6845, %int64_6846 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4596 = torch.aten.view %4290, %4595 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4596, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_6847 = torch.constant.int 64
    %4597 = torch.prim.ListConstruct %553, %int64_6847 : (!torch.int, !torch.int) -> !torch.list<int>
    %4598 = torch.aten.view %4596, %4597 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %4598, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %4599 = torch.prim.ListConstruct %4594 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_6848 = torch.constant.bool false
    %4600 = torch.aten.index_put %4598, %4599, %4572, %false_6848 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %4600, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_6849 = torch.constant.int 22
    %int2_6850 = torch.constant.int 2
    %int4_6851 = torch.constant.int 4
    %int32_6852 = torch.constant.int 32
    %int64_6853 = torch.constant.int 64
    %4601 = torch.prim.ListConstruct %381, %int22_6849, %int2_6850, %int4_6851, %int32_6852, %int64_6853 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4602 = torch.aten.view %4600, %4601 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4602, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_6854 = torch.constant.int 360448
    %4603 = torch.prim.ListConstruct %381, %int360448_6854 : (!torch.int, !torch.int) -> !torch.list<int>
    %4604 = torch.aten.view %4602, %4603 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %4604, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_6855 = torch.constant.int 22
    %int2_6856 = torch.constant.int 2
    %int4_6857 = torch.constant.int 4
    %int32_6858 = torch.constant.int 32
    %int64_6859 = torch.constant.int 64
    %4605 = torch.prim.ListConstruct %381, %int22_6855, %int2_6856, %int4_6857, %int32_6858, %int64_6859 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4606 = torch.aten.view %4604, %4605 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4606, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_6860 = torch.constant.int 64
    %4607 = torch.prim.ListConstruct %553, %int64_6860 : (!torch.int, !torch.int) -> !torch.list<int>
    %4608 = torch.aten.view %4606, %4607 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %4608, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_6861 = torch.constant.none
    %4609 = torch.aten.clone %208, %none_6861 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_6862 = torch.constant.int 1
    %int1_6863 = torch.constant.int 1
    %int1_6864 = torch.constant.int 1
    %4610 = torch.prim.ListConstruct %int1_6862, %int1_6863, %int1_6864 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4611 = torch.aten.view %4609, %4610 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_6865 = torch.constant.int 22
    %4612 = torch.aten.mul.Scalar %4577, %int22_6865 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int12_6866 = torch.constant.int 12
    %int1_6867 = torch.constant.int 1
    %4613 = torch.aten.add.Scalar %4612, %int12_6866, %int1_6867 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_6868 = torch.constant.int 2
    %4614 = torch.aten.mul.Scalar %4613, %int2_6868 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_6869 = torch.constant.int 1
    %4615 = torch.aten.add.Tensor %4614, %4611, %int1_6869 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_6870 = torch.constant.int 4
    %4616 = torch.aten.mul.Scalar %4615, %int4_6870 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_6871 = torch.constant.int 1
    %4617 = torch.aten.add.Tensor %4616, %4583, %int1_6871 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_6872 = torch.constant.int 32
    %4618 = torch.aten.mul.Scalar %4617, %int32_6872 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_6873 = torch.constant.int 1
    %4619 = torch.aten.add.Tensor %4618, %4580, %int1_6873 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_6874 = torch.constant.none
    %none_6875 = torch.constant.none
    %int5_6876 = torch.constant.int 5
    %cpu_6877 = torch.constant.device "cpu"
    %int0_6878 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4490, %none_6874, %none_6875, %int5_6876, %cpu_6877, %int0_6878 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %4620 = torch.prim.ListConstruct %4619 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_6879 = torch.constant.bool false
    %4621 = torch.aten.index_put %4608, %4620, %4490, %false_6879 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %4621, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_6880 = torch.constant.int 22
    %int2_6881 = torch.constant.int 2
    %int4_6882 = torch.constant.int 4
    %int32_6883 = torch.constant.int 32
    %int64_6884 = torch.constant.int 64
    %4622 = torch.prim.ListConstruct %381, %int22_6880, %int2_6881, %int4_6882, %int32_6883, %int64_6884 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4623 = torch.aten.view %4621, %4622 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4623, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_6885 = torch.constant.int 360448
    %4624 = torch.prim.ListConstruct %381, %int360448_6885 : (!torch.int, !torch.int) -> !torch.list<int>
    %4625 = torch.aten.view %4623, %4624 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %4625, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_6886 = torch.constant.none
    %4626 = torch.aten.clone %209, %none_6886 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_6887 = torch.constant.none
    %4627 = torch.aten.clone %210, %none_6887 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_6888 = torch.constant.none
    %4628 = torch.aten.clone %211, %none_6888 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_6889 = torch.constant.int 22
    %int2_6890 = torch.constant.int 2
    %int4_6891 = torch.constant.int 4
    %int32_6892 = torch.constant.int 32
    %int64_6893 = torch.constant.int 64
    %4629 = torch.prim.ListConstruct %381, %int22_6889, %int2_6890, %int4_6891, %int32_6892, %int64_6893 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4630 = torch.aten.view %4625, %4629 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4630, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %4631 = torch_c.to_builtin_tensor %4630 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %4632 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_6894 = tensor.cast %4632 : tensor<4x?xi64> to tensor<?x?xi64>
    %4633 = torch_c.to_builtin_tensor %4626 : !torch.vtensor<[],si64> -> tensor<i64>
    %4634 = torch_c.to_builtin_tensor %4627 : !torch.vtensor<[],si64> -> tensor<i64>
    %4635 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%4631, %cast_6894, %4633, %4634) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_6895 = tensor.cast %4635 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %4636 = torch_c.from_builtin_tensor %cast_6895 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %4636, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %4637 = torch_c.to_builtin_tensor %4630 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %4638 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_6896 = tensor.cast %4638 : tensor<4x?xi64> to tensor<?x?xi64>
    %4639 = torch_c.to_builtin_tensor %4626 : !torch.vtensor<[],si64> -> tensor<i64>
    %4640 = torch_c.to_builtin_tensor %4628 : !torch.vtensor<[],si64> -> tensor<i64>
    %4641 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%4637, %cast_6896, %4639, %4640) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_6897 = tensor.cast %4641 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %4642 = torch_c.from_builtin_tensor %cast_6897 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %4642, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_6898 = torch.constant.int 2
    %int3_6899 = torch.constant.int 3
    %4643 = torch.aten.transpose.int %4636, %int2_6898, %int3_6899 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %4643, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_6900 = torch.constant.int 0
    %4644 = torch.aten.clone %4643, %int0_6900 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %4644, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_6901 = torch.constant.int 4
    %int4_6902 = torch.constant.int 4
    %int64_6903 = torch.constant.int 64
    %4645 = torch.prim.ListConstruct %int4_6901, %623, %int4_6902, %int64_6903 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4646 = torch.aten._unsafe_view %4644, %4645 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %4646, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_6904 = torch.constant.int 2
    %int3_6905 = torch.constant.int 3
    %4647 = torch.aten.transpose.int %4642, %int2_6904, %int3_6905 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %4647, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_6906 = torch.constant.int 0
    %4648 = torch.aten.clone %4647, %int0_6906 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %4648, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_6907 = torch.constant.int 4
    %int4_6908 = torch.constant.int 4
    %int64_6909 = torch.constant.int 64
    %4649 = torch.prim.ListConstruct %int4_6907, %623, %int4_6908, %int64_6909 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4650 = torch.aten._unsafe_view %4648, %4649 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %4650, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_6910 = torch.constant.int 0
    %int1_6911 = torch.constant.int 1
    %none_6912 = torch.constant.none
    %none_6913 = torch.constant.none
    %cpu_6914 = torch.constant.device "cpu"
    %false_6915 = torch.constant.bool false
    %4651 = torch.aten.arange.start_step %int0_6910, %623, %int1_6911, %none_6912, %none_6913, %cpu_6914, %false_6915 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4651, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_6916 = torch.constant.int -1
    %4652 = torch.aten.unsqueeze %arg1, %int-1_6916 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %4653 = torch.aten.ge.Tensor %4651, %4652 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %4653, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_6917 = torch.constant.none
    %4654 = torch.aten.clone %212, %none_6917 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_6918 = torch.constant.int 0
    %4655 = torch.aten.where.ScalarOther %4653, %4654, %int0_6918 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %4655, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_6919 = torch.constant.none
    %none_6920 = torch.constant.none
    %int5_6921 = torch.constant.int 5
    %cpu_6922 = torch.constant.device "cpu"
    %int0_6923 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4655, %none_6919, %none_6920, %int5_6921, %cpu_6922, %int0_6923 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_6924 = torch.constant.int 1
    %4656 = torch.aten.unsqueeze %4655, %int1_6924 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %4656, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_6925 = torch.constant.int 1
    %4657 = torch.aten.unsqueeze %4656, %int1_6925 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %4657, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_6926 = torch.constant.int -2
    %4658 = torch.aten.unsqueeze %4646, %int-2_6926 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %4658, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_6927 = torch.constant.int 4
    %int4_6928 = torch.constant.int 4
    %int8_6929 = torch.constant.int 8
    %int64_6930 = torch.constant.int 64
    %4659 = torch.prim.ListConstruct %int4_6927, %623, %int4_6928, %int8_6929, %int64_6930 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6931 = torch.constant.bool false
    %4660 = torch.aten.expand %4658, %4659, %false_6931 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4660, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_6932 = torch.constant.int 0
    %4661 = torch.aten.clone %4660, %int0_6932 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4661, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_6933 = torch.constant.int 4
    %int32_6934 = torch.constant.int 32
    %int64_6935 = torch.constant.int 64
    %4662 = torch.prim.ListConstruct %int4_6933, %623, %int32_6934, %int64_6935 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4663 = torch.aten._unsafe_view %4661, %4662 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4663, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_6936 = torch.constant.int -2
    %4664 = torch.aten.unsqueeze %4650, %int-2_6936 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %4664, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_6937 = torch.constant.int 4
    %int4_6938 = torch.constant.int 4
    %int8_6939 = torch.constant.int 8
    %int64_6940 = torch.constant.int 64
    %4665 = torch.prim.ListConstruct %int4_6937, %623, %int4_6938, %int8_6939, %int64_6940 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6941 = torch.constant.bool false
    %4666 = torch.aten.expand %4664, %4665, %false_6941 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4666, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_6942 = torch.constant.int 0
    %4667 = torch.aten.clone %4666, %int0_6942 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4667, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_6943 = torch.constant.int 4
    %int32_6944 = torch.constant.int 32
    %int64_6945 = torch.constant.int 64
    %4668 = torch.prim.ListConstruct %int4_6943, %623, %int32_6944, %int64_6945 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4669 = torch.aten._unsafe_view %4667, %4668 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4669, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_6946 = torch.constant.int 1
    %int2_6947 = torch.constant.int 2
    %4670 = torch.aten.transpose.int %4531, %int1_6946, %int2_6947 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_6948 = torch.constant.int 1
    %int2_6949 = torch.constant.int 2
    %4671 = torch.aten.transpose.int %4663, %int1_6948, %int2_6949 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %4671, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_6950 = torch.constant.int 1
    %int2_6951 = torch.constant.int 2
    %4672 = torch.aten.transpose.int %4669, %int1_6950, %int2_6951 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %4672, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_6952 = torch.constant.float 0.000000e+00
    %false_6953 = torch.constant.bool false
    %none_6954 = torch.constant.none
    %false_6955 = torch.constant.bool false
    %4673 = torch.aten.scaled_dot_product_attention %4670, %4671, %4672, %4657, %float0.000000e00_6952, %false_6953, %none_6954, %false_6955 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_6956 = torch.constant.int 1
    %int2_6957 = torch.constant.int 2
    %4674 = torch.aten.transpose.int %4673, %int1_6956, %int2_6957 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_6958 = torch.constant.int 4
    %int1_6959 = torch.constant.int 1
    %int2048_6960 = torch.constant.int 2048
    %4675 = torch.prim.ListConstruct %int4_6958, %int1_6959, %int2048_6960 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4676 = torch.aten.view %4674, %4675 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_6961 = torch.constant.int 2
    %4677 = torch.aten.view.dtype %217, %int2_6961 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %4678 = torch.aten.detach %4677 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_6962 = torch.constant.int -1
    %int17_6963 = torch.constant.int 17
    %4679 = torch.prim.ListConstruct %int-1_6962, %int17_6963 : (!torch.int, !torch.int) -> !torch.list<int>
    %4680 = torch.aten.view %4678, %4679 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_6964 = torch.constant.int 2048
    %int-1_6965 = torch.constant.int -1
    %int17_6966 = torch.constant.int 17
    %4681 = torch.prim.ListConstruct %int2048_6964, %int-1_6965, %int17_6966 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4682 = torch.aten.view %4680, %4681 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_6967 = torch.constant.int 2
    %int0_6968 = torch.constant.int 0
    %int1_6969 = torch.constant.int 1
    %int1_6970 = torch.constant.int 1
    %4683 = torch.aten.slice.Tensor %4682, %int2_6967, %int0_6968, %int1_6969, %int1_6970 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_6971 = torch.constant.int 5
    %4684 = torch.aten.view.dtype %4683, %int5_6971 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %4685 = torch.aten.detach %4684 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_6972 = torch.constant.int 2
    %int1_6973 = torch.constant.int 1
    %int9223372036854775807_6974 = torch.constant.int 9223372036854775807
    %int1_6975 = torch.constant.int 1
    %4686 = torch.aten.slice.Tensor %4682, %int2_6972, %int1_6973, %int9223372036854775807_6974, %int1_6975 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_6976 = torch.constant.int 1
    %4687 = torch.aten.view.dtype %4686, %int1_6976 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %4688 = torch.aten.detach %4687 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %4689 = torch_c.to_builtin_tensor %4676 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_6977 = tensor.cast %4689 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %4690 = torch_c.to_builtin_tensor %4685 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %4691 = torch_c.to_builtin_tensor %4688 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %4692 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_6977, %4690, %4691) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_6978 = tensor.cast %4692 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %4693 = torch_c.from_builtin_tensor %cast_6978 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_6979 = torch.constant.none
    %none_6980 = torch.constant.none
    %int5_6981 = torch.constant.int 5
    %cpu_6982 = torch.constant.device "cpu"
    %int0_6983 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4693, %none_6979, %none_6980, %int5_6981, %cpu_6982, %int0_6983 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_6984 = torch.constant.int 1
    %4694 = torch.aten.add.Tensor %4423, %4693, %int1_6984 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_6985 = torch.constant.none
    %none_6986 = torch.constant.none
    %int5_6987 = torch.constant.int 5
    %cpu_6988 = torch.constant.device "cpu"
    %int0_6989 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4694, %none_6985, %none_6986, %int5_6987, %cpu_6988, %int0_6989 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_6990 = torch.constant.int 6
    %4695 = torch.prims.convert_element_type %4694, %int6_6990 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_6991 = torch.constant.int 2
    %4696 = torch.aten.pow.Tensor_Scalar %4695, %int2_6991 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_6992 = torch.constant.int -1
    %4697 = torch.prim.ListConstruct %int-1_6992 : (!torch.int) -> !torch.list<int>
    %true_6993 = torch.constant.bool true
    %none_6994 = torch.constant.none
    %4698 = torch.aten.mean.dim %4696, %4697, %true_6993, %none_6994 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_6995 = torch.constant.float 9.9999997473787516E-6
    %int1_6996 = torch.constant.int 1
    %4699 = torch.aten.add.Scalar %4698, %float9.999990e-06_6995, %int1_6996 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4700 = torch.aten.rsqrt %4699 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %4701 = torch.aten.mul.Tensor %4695, %4700 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_6997 = torch.constant.none
    %none_6998 = torch.constant.none
    %int6_6999 = torch.constant.int 6
    %cpu_7000 = torch.constant.device "cpu"
    %int0_7001 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4701, %none_6997, %none_6998, %int6_6999, %cpu_7000, %int0_7001 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7002 = torch.constant.int 5
    %4702 = torch.prims.convert_element_type %4701, %int5_7002 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %4703 = torch.aten.mul.Tensor %218, %4702 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_7003 = torch.constant.none
    %none_7004 = torch.constant.none
    %int6_7005 = torch.constant.int 6
    %cpu_7006 = torch.constant.device "cpu"
    %int0_7007 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4703, %none_7003, %none_7004, %int6_7005, %cpu_7006, %int0_7007 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7008 = torch.constant.int 5
    %4704 = torch.prims.convert_element_type %4703, %int5_7008 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_7009 = torch.constant.int 2
    %4705 = torch.aten.view.dtype %219, %int2_7009 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %4706 = torch.aten.detach %4705 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_7010 = torch.constant.int -1
    %int17_7011 = torch.constant.int 17
    %4707 = torch.prim.ListConstruct %int-1_7010, %int17_7011 : (!torch.int, !torch.int) -> !torch.list<int>
    %4708 = torch.aten.view %4706, %4707 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_7012 = torch.constant.int 5632
    %int-1_7013 = torch.constant.int -1
    %int17_7014 = torch.constant.int 17
    %4709 = torch.prim.ListConstruct %int5632_7012, %int-1_7013, %int17_7014 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4710 = torch.aten.view %4708, %4709 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_7015 = torch.constant.int 2
    %int0_7016 = torch.constant.int 0
    %int1_7017 = torch.constant.int 1
    %int1_7018 = torch.constant.int 1
    %4711 = torch.aten.slice.Tensor %4710, %int2_7015, %int0_7016, %int1_7017, %int1_7018 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_7019 = torch.constant.int 5
    %4712 = torch.aten.view.dtype %4711, %int5_7019 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %4713 = torch.aten.detach %4712 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_7020 = torch.constant.int 2
    %int1_7021 = torch.constant.int 1
    %int9223372036854775807_7022 = torch.constant.int 9223372036854775807
    %int1_7023 = torch.constant.int 1
    %4714 = torch.aten.slice.Tensor %4710, %int2_7020, %int1_7021, %int9223372036854775807_7022, %int1_7023 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_7024 = torch.constant.int 1
    %4715 = torch.aten.view.dtype %4714, %int1_7024 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %4716 = torch.aten.detach %4715 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %4717 = torch_c.to_builtin_tensor %4704 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_7025 = tensor.cast %4717 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %4718 = torch_c.to_builtin_tensor %4713 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %4719 = torch_c.to_builtin_tensor %4716 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %4720 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_7025, %4718, %4719) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_7026 = tensor.cast %4720 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %4721 = torch_c.from_builtin_tensor %cast_7026 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %4722 = torch.aten.silu %4721 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_7027 = torch.constant.int 2
    %4723 = torch.aten.view.dtype %220, %int2_7027 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %4724 = torch.aten.detach %4723 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_7028 = torch.constant.int -1
    %int17_7029 = torch.constant.int 17
    %4725 = torch.prim.ListConstruct %int-1_7028, %int17_7029 : (!torch.int, !torch.int) -> !torch.list<int>
    %4726 = torch.aten.view %4724, %4725 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_7030 = torch.constant.int 5632
    %int-1_7031 = torch.constant.int -1
    %int17_7032 = torch.constant.int 17
    %4727 = torch.prim.ListConstruct %int5632_7030, %int-1_7031, %int17_7032 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4728 = torch.aten.view %4726, %4727 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_7033 = torch.constant.int 2
    %int0_7034 = torch.constant.int 0
    %int1_7035 = torch.constant.int 1
    %int1_7036 = torch.constant.int 1
    %4729 = torch.aten.slice.Tensor %4728, %int2_7033, %int0_7034, %int1_7035, %int1_7036 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_7037 = torch.constant.int 5
    %4730 = torch.aten.view.dtype %4729, %int5_7037 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %4731 = torch.aten.detach %4730 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_7038 = torch.constant.int 2
    %int1_7039 = torch.constant.int 1
    %int9223372036854775807_7040 = torch.constant.int 9223372036854775807
    %int1_7041 = torch.constant.int 1
    %4732 = torch.aten.slice.Tensor %4728, %int2_7038, %int1_7039, %int9223372036854775807_7040, %int1_7041 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_7042 = torch.constant.int 1
    %4733 = torch.aten.view.dtype %4732, %int1_7042 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %4734 = torch.aten.detach %4733 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %4735 = torch_c.to_builtin_tensor %4704 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_7043 = tensor.cast %4735 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %4736 = torch_c.to_builtin_tensor %4731 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %4737 = torch_c.to_builtin_tensor %4734 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %4738 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_7043, %4736, %4737) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_7044 = tensor.cast %4738 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %4739 = torch_c.from_builtin_tensor %cast_7044 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %4740 = torch.aten.mul.Tensor %4722, %4739 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_7045 = torch.constant.int 2
    %4741 = torch.aten.view.dtype %221, %int2_7045 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %4742 = torch.aten.detach %4741 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_7046 = torch.constant.int -1
    %int17_7047 = torch.constant.int 17
    %4743 = torch.prim.ListConstruct %int-1_7046, %int17_7047 : (!torch.int, !torch.int) -> !torch.list<int>
    %4744 = torch.aten.view %4742, %4743 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_7048 = torch.constant.int 2048
    %int-1_7049 = torch.constant.int -1
    %int17_7050 = torch.constant.int 17
    %4745 = torch.prim.ListConstruct %int2048_7048, %int-1_7049, %int17_7050 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4746 = torch.aten.view %4744, %4745 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_7051 = torch.constant.int 2
    %int0_7052 = torch.constant.int 0
    %int1_7053 = torch.constant.int 1
    %int1_7054 = torch.constant.int 1
    %4747 = torch.aten.slice.Tensor %4746, %int2_7051, %int0_7052, %int1_7053, %int1_7054 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_7055 = torch.constant.int 5
    %4748 = torch.aten.view.dtype %4747, %int5_7055 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %4749 = torch.aten.detach %4748 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_7056 = torch.constant.int 2
    %int1_7057 = torch.constant.int 1
    %int9223372036854775807_7058 = torch.constant.int 9223372036854775807
    %int1_7059 = torch.constant.int 1
    %4750 = torch.aten.slice.Tensor %4746, %int2_7056, %int1_7057, %int9223372036854775807_7058, %int1_7059 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_7060 = torch.constant.int 1
    %4751 = torch.aten.view.dtype %4750, %int1_7060 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %4752 = torch.aten.detach %4751 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %4753 = torch_c.to_builtin_tensor %4740 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_7061 = tensor.cast %4753 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %4754 = torch_c.to_builtin_tensor %4749 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %4755 = torch_c.to_builtin_tensor %4752 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %4756 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_7061, %4754, %4755) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_7062 = tensor.cast %4756 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %4757 = torch_c.from_builtin_tensor %cast_7062 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_7063 = torch.constant.int 1
    %4758 = torch.aten.add.Tensor %4694, %4757, %int1_7063 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_7064 = torch.constant.none
    %none_7065 = torch.constant.none
    %int5_7066 = torch.constant.int 5
    %cpu_7067 = torch.constant.device "cpu"
    %int0_7068 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4758, %none_7064, %none_7065, %int5_7066, %cpu_7067, %int0_7068 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7069 = torch.constant.int 6
    %4759 = torch.prims.convert_element_type %4758, %int6_7069 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_7070 = torch.constant.int 2
    %4760 = torch.aten.pow.Tensor_Scalar %4759, %int2_7070 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_7071 = torch.constant.int -1
    %4761 = torch.prim.ListConstruct %int-1_7071 : (!torch.int) -> !torch.list<int>
    %true_7072 = torch.constant.bool true
    %none_7073 = torch.constant.none
    %4762 = torch.aten.mean.dim %4760, %4761, %true_7072, %none_7073 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_7074 = torch.constant.float 9.9999997473787516E-6
    %int1_7075 = torch.constant.int 1
    %4763 = torch.aten.add.Scalar %4762, %float9.999990e-06_7074, %int1_7075 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4764 = torch.aten.rsqrt %4763 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %4765 = torch.aten.mul.Tensor %4759, %4764 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_7076 = torch.constant.none
    %none_7077 = torch.constant.none
    %int6_7078 = torch.constant.int 6
    %cpu_7079 = torch.constant.device "cpu"
    %int0_7080 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4765, %none_7076, %none_7077, %int6_7078, %cpu_7079, %int0_7080 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7081 = torch.constant.int 5
    %4766 = torch.prims.convert_element_type %4765, %int5_7081 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %4767 = torch.aten.mul.Tensor %230, %4766 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_7082 = torch.constant.none
    %none_7083 = torch.constant.none
    %int6_7084 = torch.constant.int 6
    %cpu_7085 = torch.constant.device "cpu"
    %int0_7086 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4767, %none_7082, %none_7083, %int6_7084, %cpu_7085, %int0_7086 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7087 = torch.constant.int 5
    %4768 = torch.prims.convert_element_type %4767, %int5_7087 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_7088 = torch.constant.int 2
    %4769 = torch.aten.view.dtype %231, %int2_7088 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %4770 = torch.aten.detach %4769 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_7089 = torch.constant.int -1
    %int17_7090 = torch.constant.int 17
    %4771 = torch.prim.ListConstruct %int-1_7089, %int17_7090 : (!torch.int, !torch.int) -> !torch.list<int>
    %4772 = torch.aten.view %4770, %4771 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_7091 = torch.constant.int 2048
    %int-1_7092 = torch.constant.int -1
    %int17_7093 = torch.constant.int 17
    %4773 = torch.prim.ListConstruct %int2048_7091, %int-1_7092, %int17_7093 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4774 = torch.aten.view %4772, %4773 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_7094 = torch.constant.int 2
    %int0_7095 = torch.constant.int 0
    %int1_7096 = torch.constant.int 1
    %int1_7097 = torch.constant.int 1
    %4775 = torch.aten.slice.Tensor %4774, %int2_7094, %int0_7095, %int1_7096, %int1_7097 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_7098 = torch.constant.int 5
    %4776 = torch.aten.view.dtype %4775, %int5_7098 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %4777 = torch.aten.detach %4776 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_7099 = torch.constant.int 2
    %int1_7100 = torch.constant.int 1
    %int9223372036854775807_7101 = torch.constant.int 9223372036854775807
    %int1_7102 = torch.constant.int 1
    %4778 = torch.aten.slice.Tensor %4774, %int2_7099, %int1_7100, %int9223372036854775807_7101, %int1_7102 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_7103 = torch.constant.int 1
    %4779 = torch.aten.view.dtype %4778, %int1_7103 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %4780 = torch.aten.detach %4779 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %4781 = torch_c.to_builtin_tensor %4768 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_7104 = tensor.cast %4781 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %4782 = torch_c.to_builtin_tensor %4777 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %4783 = torch_c.to_builtin_tensor %4780 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %4784 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_7104, %4782, %4783) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_7105 = tensor.cast %4784 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %4785 = torch_c.from_builtin_tensor %cast_7105 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_7106 = torch.constant.int 2
    %4786 = torch.aten.view.dtype %232, %int2_7106 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %4787 = torch.aten.detach %4786 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_7107 = torch.constant.int -1
    %int17_7108 = torch.constant.int 17
    %4788 = torch.prim.ListConstruct %int-1_7107, %int17_7108 : (!torch.int, !torch.int) -> !torch.list<int>
    %4789 = torch.aten.view %4787, %4788 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_7109 = torch.constant.int 256
    %int-1_7110 = torch.constant.int -1
    %int17_7111 = torch.constant.int 17
    %4790 = torch.prim.ListConstruct %int256_7109, %int-1_7110, %int17_7111 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4791 = torch.aten.view %4789, %4790 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_7112 = torch.constant.int 2
    %int0_7113 = torch.constant.int 0
    %int1_7114 = torch.constant.int 1
    %int1_7115 = torch.constant.int 1
    %4792 = torch.aten.slice.Tensor %4791, %int2_7112, %int0_7113, %int1_7114, %int1_7115 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_7116 = torch.constant.int 5
    %4793 = torch.aten.view.dtype %4792, %int5_7116 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %4794 = torch.aten.detach %4793 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_7117 = torch.constant.int 2
    %int1_7118 = torch.constant.int 1
    %int9223372036854775807_7119 = torch.constant.int 9223372036854775807
    %int1_7120 = torch.constant.int 1
    %4795 = torch.aten.slice.Tensor %4791, %int2_7117, %int1_7118, %int9223372036854775807_7119, %int1_7120 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_7121 = torch.constant.int 1
    %4796 = torch.aten.view.dtype %4795, %int1_7121 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %4797 = torch.aten.detach %4796 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %4798 = torch_c.to_builtin_tensor %4768 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_7122 = tensor.cast %4798 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %4799 = torch_c.to_builtin_tensor %4794 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %4800 = torch_c.to_builtin_tensor %4797 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %4801 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_7122, %4799, %4800) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_7123 = tensor.cast %4801 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %4802 = torch_c.from_builtin_tensor %cast_7123 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_7124 = torch.constant.int 2
    %4803 = torch.aten.view.dtype %233, %int2_7124 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %4804 = torch.aten.detach %4803 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_7125 = torch.constant.int -1
    %int17_7126 = torch.constant.int 17
    %4805 = torch.prim.ListConstruct %int-1_7125, %int17_7126 : (!torch.int, !torch.int) -> !torch.list<int>
    %4806 = torch.aten.view %4804, %4805 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_7127 = torch.constant.int 256
    %int-1_7128 = torch.constant.int -1
    %int17_7129 = torch.constant.int 17
    %4807 = torch.prim.ListConstruct %int256_7127, %int-1_7128, %int17_7129 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4808 = torch.aten.view %4806, %4807 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_7130 = torch.constant.int 2
    %int0_7131 = torch.constant.int 0
    %int1_7132 = torch.constant.int 1
    %int1_7133 = torch.constant.int 1
    %4809 = torch.aten.slice.Tensor %4808, %int2_7130, %int0_7131, %int1_7132, %int1_7133 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_7134 = torch.constant.int 5
    %4810 = torch.aten.view.dtype %4809, %int5_7134 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %4811 = torch.aten.detach %4810 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_7135 = torch.constant.int 2
    %int1_7136 = torch.constant.int 1
    %int9223372036854775807_7137 = torch.constant.int 9223372036854775807
    %int1_7138 = torch.constant.int 1
    %4812 = torch.aten.slice.Tensor %4808, %int2_7135, %int1_7136, %int9223372036854775807_7137, %int1_7138 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_7139 = torch.constant.int 1
    %4813 = torch.aten.view.dtype %4812, %int1_7139 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %4814 = torch.aten.detach %4813 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %4815 = torch_c.to_builtin_tensor %4768 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_7140 = tensor.cast %4815 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %4816 = torch_c.to_builtin_tensor %4811 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %4817 = torch_c.to_builtin_tensor %4814 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %4818 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_7140, %4816, %4817) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_7141 = tensor.cast %4818 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %4819 = torch_c.from_builtin_tensor %cast_7141 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_7142 = torch.constant.int 4
    %int1_7143 = torch.constant.int 1
    %int32_7144 = torch.constant.int 32
    %int64_7145 = torch.constant.int 64
    %4820 = torch.prim.ListConstruct %int4_7142, %int1_7143, %int32_7144, %int64_7145 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4821 = torch.aten.view %4785, %4820 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_7146 = torch.constant.int 4
    %int1_7147 = torch.constant.int 1
    %int4_7148 = torch.constant.int 4
    %int64_7149 = torch.constant.int 64
    %4822 = torch.prim.ListConstruct %int4_7146, %int1_7147, %int4_7148, %int64_7149 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4823 = torch.aten.view %4802, %4822 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_7150 = torch.constant.int 4
    %int1_7151 = torch.constant.int 1
    %int4_7152 = torch.constant.int 4
    %int64_7153 = torch.constant.int 64
    %4824 = torch.prim.ListConstruct %int4_7150, %int1_7151, %int4_7152, %int64_7153 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4825 = torch.aten.view %4819, %4824 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_7154 = torch.constant.int 0
    %int1_7155 = torch.constant.int 1
    %none_7156 = torch.constant.none
    %none_7157 = torch.constant.none
    %cpu_7158 = torch.constant.device "cpu"
    %false_7159 = torch.constant.bool false
    %4826 = torch.aten.arange.start %int0_7154, %int1_7155, %none_7156, %none_7157, %cpu_7158, %false_7159 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_7160 = torch.constant.int 0
    %4827 = torch.aten.unsqueeze %4826, %int0_7160 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_7161 = torch.constant.int 1
    %4828 = torch.aten.unsqueeze %arg2, %int1_7161 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7162 = torch.constant.int 1
    %4829 = torch.aten.add.Tensor %4827, %4828, %int1_7162 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_7163 = torch.constant.int 0
    %int64_7164 = torch.constant.int 64
    %int2_7165 = torch.constant.int 2
    %none_7166 = torch.constant.none
    %none_7167 = torch.constant.none
    %cpu_7168 = torch.constant.device "cpu"
    %false_7169 = torch.constant.bool false
    %4830 = torch.aten.arange.start_step %int0_7163, %int64_7164, %int2_7165, %none_7166, %none_7167, %cpu_7168, %false_7169 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_7170 = torch.constant.none
    %none_7171 = torch.constant.none
    %int4_7172 = torch.constant.int 4
    %cpu_7173 = torch.constant.device "cpu"
    %int0_7174 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4830, %none_7170, %none_7171, %int4_7172, %cpu_7173, %int0_7174 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7175 = torch.constant.int 6
    %4831 = torch.prims.convert_element_type %4830, %int6_7175 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_7176 = torch.constant.int 64
    %4832 = torch.aten.div.Scalar %4831, %int64_7176 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_7177 = torch.constant.float 1.000000e+04
    %4833 = torch.aten.pow.Scalar %float1.000000e04_7177, %4832 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %4834 = torch.aten.reciprocal %4833 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_7178 = torch.constant.float 1.000000e+00
    %4835 = torch.aten.mul.Scalar %4834, %float1.000000e00_7178 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_7179 = torch.constant.none
    %4836 = torch.aten.clone %222, %none_7179 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_7180 = torch.constant.int 0
    %4837 = torch.aten.unsqueeze %4835, %int0_7180 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_7181 = torch.constant.int 2
    %4838 = torch.aten.unsqueeze %4837, %int2_7181 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_7182 = torch.constant.none
    %none_7183 = torch.constant.none
    %int6_7184 = torch.constant.int 6
    %cpu_7185 = torch.constant.device "cpu"
    %int0_7186 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4838, %none_7182, %none_7183, %int6_7184, %cpu_7185, %int0_7186 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_7187 = torch.constant.int 4
    %int-1_7188 = torch.constant.int -1
    %int1_7189 = torch.constant.int 1
    %4839 = torch.prim.ListConstruct %int4_7187, %int-1_7188, %int1_7189 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7190 = torch.constant.bool false
    %4840 = torch.aten.expand %4838, %4839, %false_7190 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_7191 = torch.constant.int 1
    %4841 = torch.aten.unsqueeze %4829, %int1_7191 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_7192 = torch.constant.none
    %none_7193 = torch.constant.none
    %int4_7194 = torch.constant.int 4
    %cpu_7195 = torch.constant.device "cpu"
    %int0_7196 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4841, %none_7192, %none_7193, %int4_7194, %cpu_7195, %int0_7196 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7197 = torch.constant.int 6
    %4842 = torch.prims.convert_element_type %4841, %int6_7197 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4843 = torch.aten.matmul %4840, %4842 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_7198 = torch.constant.int 1
    %int2_7199 = torch.constant.int 2
    %4844 = torch.aten.transpose.int %4843, %int1_7198, %int2_7199 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %4845 = torch.aten.cos %4844 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %4846 = torch.aten.mul.Tensor %4845, %4836 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_7200 = torch.constant.none
    %none_7201 = torch.constant.none
    %int6_7202 = torch.constant.int 6
    %cpu_7203 = torch.constant.device "cpu"
    %int0_7204 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4846, %none_7200, %none_7201, %int6_7202, %cpu_7203, %int0_7204 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7205 = torch.constant.int 5
    %4847 = torch.prims.convert_element_type %4846, %int5_7205 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %4848 = torch.aten.sin %4844 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %4849 = torch.aten.mul.Tensor %4848, %4836 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_7206 = torch.constant.none
    %none_7207 = torch.constant.none
    %int6_7208 = torch.constant.int 6
    %cpu_7209 = torch.constant.device "cpu"
    %int0_7210 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4849, %none_7206, %none_7207, %int6_7208, %cpu_7209, %int0_7210 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7211 = torch.constant.int 5
    %4850 = torch.prims.convert_element_type %4849, %int5_7211 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_7212 = torch.constant.int 2
    %4851 = torch.aten.unsqueeze %4847, %int2_7212 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_7213 = torch.constant.int 2
    %4852 = torch.aten.unsqueeze %4850, %int2_7213 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_7214 = torch.constant.none
    %none_7215 = torch.constant.none
    %int5_7216 = torch.constant.int 5
    %cpu_7217 = torch.constant.device "cpu"
    %int0_7218 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4851, %none_7214, %none_7215, %int5_7216, %cpu_7217, %int0_7218 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_7219 = torch.constant.none
    %none_7220 = torch.constant.none
    %int5_7221 = torch.constant.int 5
    %cpu_7222 = torch.constant.device "cpu"
    %int0_7223 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4852, %none_7219, %none_7220, %int5_7221, %cpu_7222, %int0_7223 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_7224 = torch.constant.none
    %none_7225 = torch.constant.none
    %int5_7226 = torch.constant.int 5
    %cpu_7227 = torch.constant.device "cpu"
    %int0_7228 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4821, %none_7224, %none_7225, %int5_7226, %cpu_7227, %int0_7228 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_7229 = torch.constant.int 3
    %int0_7230 = torch.constant.int 0
    %int64_7231 = torch.constant.int 64
    %int2_7232 = torch.constant.int 2
    %4853 = torch.aten.slice.Tensor %4821, %int3_7229, %int0_7230, %int64_7231, %int2_7232 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_7233 = torch.constant.int 3
    %int1_7234 = torch.constant.int 1
    %int64_7235 = torch.constant.int 64
    %int2_7236 = torch.constant.int 2
    %4854 = torch.aten.slice.Tensor %4821, %int3_7233, %int1_7234, %int64_7235, %int2_7236 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %4855 = torch.aten.mul.Tensor %4853, %4851 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %4856 = torch.aten.mul.Tensor %4854, %4852 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_7237 = torch.constant.int 1
    %4857 = torch.aten.sub.Tensor %4855, %4856, %int1_7237 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %4858 = torch.aten.mul.Tensor %4854, %4851 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %4859 = torch.aten.mul.Tensor %4853, %4852 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_7238 = torch.constant.int 1
    %4860 = torch.aten.add.Tensor %4858, %4859, %int1_7238 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %4861 = torch_c.to_builtin_tensor %4857 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_7239 = tensor.cast %4861 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %4862 = torch_c.to_builtin_tensor %4860 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_7240 = tensor.cast %4862 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %4863 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_7239, %cast_7240) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_7241 = tensor.cast %4863 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %4864 = torch_c.from_builtin_tensor %cast_7241 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_7242 = torch.constant.int 4
    %int1_7243 = torch.constant.int 1
    %int32_7244 = torch.constant.int 32
    %int64_7245 = torch.constant.int 64
    %4865 = torch.prim.ListConstruct %int4_7242, %int1_7243, %int32_7244, %int64_7245 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4866 = torch.aten.view %4864, %4865 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_7246 = torch.constant.none
    %none_7247 = torch.constant.none
    %int5_7248 = torch.constant.int 5
    %cpu_7249 = torch.constant.device "cpu"
    %int0_7250 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4866, %none_7246, %none_7247, %int5_7248, %cpu_7249, %int0_7250 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_7251 = torch.constant.int 0
    %int1_7252 = torch.constant.int 1
    %none_7253 = torch.constant.none
    %none_7254 = torch.constant.none
    %cpu_7255 = torch.constant.device "cpu"
    %false_7256 = torch.constant.bool false
    %4867 = torch.aten.arange.start %int0_7251, %int1_7252, %none_7253, %none_7254, %cpu_7255, %false_7256 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_7257 = torch.constant.int 0
    %4868 = torch.aten.unsqueeze %4867, %int0_7257 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_7258 = torch.constant.int 1
    %4869 = torch.aten.unsqueeze %arg2, %int1_7258 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7259 = torch.constant.int 1
    %4870 = torch.aten.add.Tensor %4868, %4869, %int1_7259 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_7260 = torch.constant.int 0
    %int64_7261 = torch.constant.int 64
    %int2_7262 = torch.constant.int 2
    %none_7263 = torch.constant.none
    %none_7264 = torch.constant.none
    %cpu_7265 = torch.constant.device "cpu"
    %false_7266 = torch.constant.bool false
    %4871 = torch.aten.arange.start_step %int0_7260, %int64_7261, %int2_7262, %none_7263, %none_7264, %cpu_7265, %false_7266 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_7267 = torch.constant.none
    %none_7268 = torch.constant.none
    %int4_7269 = torch.constant.int 4
    %cpu_7270 = torch.constant.device "cpu"
    %int0_7271 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4871, %none_7267, %none_7268, %int4_7269, %cpu_7270, %int0_7271 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7272 = torch.constant.int 6
    %4872 = torch.prims.convert_element_type %4871, %int6_7272 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_7273 = torch.constant.int 64
    %4873 = torch.aten.div.Scalar %4872, %int64_7273 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_7274 = torch.constant.float 1.000000e+04
    %4874 = torch.aten.pow.Scalar %float1.000000e04_7274, %4873 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %4875 = torch.aten.reciprocal %4874 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_7275 = torch.constant.float 1.000000e+00
    %4876 = torch.aten.mul.Scalar %4875, %float1.000000e00_7275 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_7276 = torch.constant.none
    %4877 = torch.aten.clone %223, %none_7276 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_7277 = torch.constant.int 0
    %4878 = torch.aten.unsqueeze %4876, %int0_7277 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_7278 = torch.constant.int 2
    %4879 = torch.aten.unsqueeze %4878, %int2_7278 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_7279 = torch.constant.none
    %none_7280 = torch.constant.none
    %int6_7281 = torch.constant.int 6
    %cpu_7282 = torch.constant.device "cpu"
    %int0_7283 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4879, %none_7279, %none_7280, %int6_7281, %cpu_7282, %int0_7283 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_7284 = torch.constant.int 4
    %int-1_7285 = torch.constant.int -1
    %int1_7286 = torch.constant.int 1
    %4880 = torch.prim.ListConstruct %int4_7284, %int-1_7285, %int1_7286 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7287 = torch.constant.bool false
    %4881 = torch.aten.expand %4879, %4880, %false_7287 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_7288 = torch.constant.int 1
    %4882 = torch.aten.unsqueeze %4870, %int1_7288 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_7289 = torch.constant.none
    %none_7290 = torch.constant.none
    %int4_7291 = torch.constant.int 4
    %cpu_7292 = torch.constant.device "cpu"
    %int0_7293 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4882, %none_7289, %none_7290, %int4_7291, %cpu_7292, %int0_7293 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7294 = torch.constant.int 6
    %4883 = torch.prims.convert_element_type %4882, %int6_7294 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4884 = torch.aten.matmul %4881, %4883 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_7295 = torch.constant.int 1
    %int2_7296 = torch.constant.int 2
    %4885 = torch.aten.transpose.int %4884, %int1_7295, %int2_7296 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %4886 = torch.aten.cos %4885 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %4887 = torch.aten.mul.Tensor %4886, %4877 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_7297 = torch.constant.none
    %none_7298 = torch.constant.none
    %int6_7299 = torch.constant.int 6
    %cpu_7300 = torch.constant.device "cpu"
    %int0_7301 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4887, %none_7297, %none_7298, %int6_7299, %cpu_7300, %int0_7301 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7302 = torch.constant.int 5
    %4888 = torch.prims.convert_element_type %4887, %int5_7302 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %4889 = torch.aten.sin %4885 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %4890 = torch.aten.mul.Tensor %4889, %4877 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_7303 = torch.constant.none
    %none_7304 = torch.constant.none
    %int6_7305 = torch.constant.int 6
    %cpu_7306 = torch.constant.device "cpu"
    %int0_7307 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4890, %none_7303, %none_7304, %int6_7305, %cpu_7306, %int0_7307 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7308 = torch.constant.int 5
    %4891 = torch.prims.convert_element_type %4890, %int5_7308 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_7309 = torch.constant.int 2
    %4892 = torch.aten.unsqueeze %4888, %int2_7309 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_7310 = torch.constant.int 2
    %4893 = torch.aten.unsqueeze %4891, %int2_7310 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_7311 = torch.constant.none
    %none_7312 = torch.constant.none
    %int5_7313 = torch.constant.int 5
    %cpu_7314 = torch.constant.device "cpu"
    %int0_7315 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4892, %none_7311, %none_7312, %int5_7313, %cpu_7314, %int0_7315 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_7316 = torch.constant.none
    %none_7317 = torch.constant.none
    %int5_7318 = torch.constant.int 5
    %cpu_7319 = torch.constant.device "cpu"
    %int0_7320 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4893, %none_7316, %none_7317, %int5_7318, %cpu_7319, %int0_7320 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_7321 = torch.constant.none
    %none_7322 = torch.constant.none
    %int5_7323 = torch.constant.int 5
    %cpu_7324 = torch.constant.device "cpu"
    %int0_7325 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4823, %none_7321, %none_7322, %int5_7323, %cpu_7324, %int0_7325 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_7326 = torch.constant.int 3
    %int0_7327 = torch.constant.int 0
    %int64_7328 = torch.constant.int 64
    %int2_7329 = torch.constant.int 2
    %4894 = torch.aten.slice.Tensor %4823, %int3_7326, %int0_7327, %int64_7328, %int2_7329 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_7330 = torch.constant.int 3
    %int1_7331 = torch.constant.int 1
    %int64_7332 = torch.constant.int 64
    %int2_7333 = torch.constant.int 2
    %4895 = torch.aten.slice.Tensor %4823, %int3_7330, %int1_7331, %int64_7332, %int2_7333 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %4896 = torch.aten.mul.Tensor %4894, %4892 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %4897 = torch.aten.mul.Tensor %4895, %4893 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_7334 = torch.constant.int 1
    %4898 = torch.aten.sub.Tensor %4896, %4897, %int1_7334 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %4899 = torch.aten.mul.Tensor %4895, %4892 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %4900 = torch.aten.mul.Tensor %4894, %4893 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_7335 = torch.constant.int 1
    %4901 = torch.aten.add.Tensor %4899, %4900, %int1_7335 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %4902 = torch_c.to_builtin_tensor %4898 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_7336 = tensor.cast %4902 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %4903 = torch_c.to_builtin_tensor %4901 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_7337 = tensor.cast %4903 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %4904 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_7336, %cast_7337) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_7338 = tensor.cast %4904 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %4905 = torch_c.from_builtin_tensor %cast_7338 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_7339 = torch.constant.int 4
    %int1_7340 = torch.constant.int 1
    %int4_7341 = torch.constant.int 4
    %int64_7342 = torch.constant.int 64
    %4906 = torch.prim.ListConstruct %int4_7339, %int1_7340, %int4_7341, %int64_7342 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4907 = torch.aten.view %4905, %4906 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_7343 = torch.constant.none
    %none_7344 = torch.constant.none
    %int5_7345 = torch.constant.int 5
    %cpu_7346 = torch.constant.device "cpu"
    %int0_7347 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4907, %none_7343, %none_7344, %int5_7345, %cpu_7346, %int0_7347 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_7348 = torch.constant.int 32
    %4908 = torch.aten.floor_divide.Scalar %arg2, %int32_7348 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_7349 = torch.constant.int 1
    %4909 = torch.aten.unsqueeze %4908, %int1_7349 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7350 = torch.constant.int 1
    %false_7351 = torch.constant.bool false
    %4910 = torch.aten.gather %arg3, %int1_7350, %4909, %false_7351 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_7352 = torch.constant.int 4
    %int1_7353 = torch.constant.int 1
    %int1_7354 = torch.constant.int 1
    %4911 = torch.prim.ListConstruct %int4_7352, %int1_7353, %int1_7354 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4912 = torch.aten.view %4910, %4911 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_7355 = torch.constant.int 32
    %4913 = torch.aten.remainder.Scalar %arg2, %int32_7355 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_7356 = torch.constant.int 4
    %int1_7357 = torch.constant.int 1
    %int1_7358 = torch.constant.int 1
    %4914 = torch.prim.ListConstruct %int4_7356, %int1_7357, %int1_7358 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4915 = torch.aten.view %4913, %4914 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_7359 = torch.constant.int 4
    %none_7360 = torch.constant.none
    %none_7361 = torch.constant.none
    %cpu_7362 = torch.constant.device "cpu"
    %false_7363 = torch.constant.bool false
    %4916 = torch.aten.arange %int4_7359, %none_7360, %none_7361, %cpu_7362, %false_7363 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_7364 = torch.constant.int 1
    %int1_7365 = torch.constant.int 1
    %int4_7366 = torch.constant.int 4
    %4917 = torch.prim.ListConstruct %int1_7364, %int1_7365, %int4_7366 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4918 = torch.aten.view %4916, %4917 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_7367 = torch.constant.none
    %4919 = torch.aten.clone %224, %none_7367 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_7368 = torch.constant.int 1
    %int1_7369 = torch.constant.int 1
    %int1_7370 = torch.constant.int 1
    %4920 = torch.prim.ListConstruct %int1_7368, %int1_7369, %int1_7370 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4921 = torch.aten.view %4919, %4920 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_7371 = torch.constant.int 22
    %4922 = torch.aten.mul.Scalar %4912, %int22_7371 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int13 = torch.constant.int 13
    %int1_7372 = torch.constant.int 1
    %4923 = torch.aten.add.Scalar %4922, %int13, %int1_7372 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_7373 = torch.constant.int 2
    %4924 = torch.aten.mul.Scalar %4923, %int2_7373 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_7374 = torch.constant.int 1
    %4925 = torch.aten.add.Tensor %4924, %4921, %int1_7374 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_7375 = torch.constant.int 4
    %4926 = torch.aten.mul.Scalar %4925, %int4_7375 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_7376 = torch.constant.int 1
    %4927 = torch.aten.add.Tensor %4926, %4918, %int1_7376 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_7377 = torch.constant.int 32
    %4928 = torch.aten.mul.Scalar %4927, %int32_7377 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_7378 = torch.constant.int 1
    %4929 = torch.aten.add.Tensor %4928, %4915, %int1_7378 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_7379 = torch.constant.none
    %none_7380 = torch.constant.none
    %int5_7381 = torch.constant.int 5
    %cpu_7382 = torch.constant.device "cpu"
    %int0_7383 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4907, %none_7379, %none_7380, %int5_7381, %cpu_7382, %int0_7383 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_7384 = torch.constant.int 22
    %int2_7385 = torch.constant.int 2
    %int4_7386 = torch.constant.int 4
    %int32_7387 = torch.constant.int 32
    %int64_7388 = torch.constant.int 64
    %4930 = torch.prim.ListConstruct %381, %int22_7384, %int2_7385, %int4_7386, %int32_7387, %int64_7388 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4931 = torch.aten.view %4625, %4930 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4931, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_7389 = torch.constant.int 64
    %4932 = torch.prim.ListConstruct %553, %int64_7389 : (!torch.int, !torch.int) -> !torch.list<int>
    %4933 = torch.aten.view %4931, %4932 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %4933, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %4934 = torch.prim.ListConstruct %4929 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_7390 = torch.constant.bool false
    %4935 = torch.aten.index_put %4933, %4934, %4907, %false_7390 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %4935, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_7391 = torch.constant.int 22
    %int2_7392 = torch.constant.int 2
    %int4_7393 = torch.constant.int 4
    %int32_7394 = torch.constant.int 32
    %int64_7395 = torch.constant.int 64
    %4936 = torch.prim.ListConstruct %381, %int22_7391, %int2_7392, %int4_7393, %int32_7394, %int64_7395 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4937 = torch.aten.view %4935, %4936 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4937, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_7396 = torch.constant.int 360448
    %4938 = torch.prim.ListConstruct %381, %int360448_7396 : (!torch.int, !torch.int) -> !torch.list<int>
    %4939 = torch.aten.view %4937, %4938 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %4939, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_7397 = torch.constant.int 22
    %int2_7398 = torch.constant.int 2
    %int4_7399 = torch.constant.int 4
    %int32_7400 = torch.constant.int 32
    %int64_7401 = torch.constant.int 64
    %4940 = torch.prim.ListConstruct %381, %int22_7397, %int2_7398, %int4_7399, %int32_7400, %int64_7401 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4941 = torch.aten.view %4939, %4940 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4941, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_7402 = torch.constant.int 64
    %4942 = torch.prim.ListConstruct %553, %int64_7402 : (!torch.int, !torch.int) -> !torch.list<int>
    %4943 = torch.aten.view %4941, %4942 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %4943, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_7403 = torch.constant.none
    %4944 = torch.aten.clone %225, %none_7403 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_7404 = torch.constant.int 1
    %int1_7405 = torch.constant.int 1
    %int1_7406 = torch.constant.int 1
    %4945 = torch.prim.ListConstruct %int1_7404, %int1_7405, %int1_7406 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4946 = torch.aten.view %4944, %4945 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_7407 = torch.constant.int 22
    %4947 = torch.aten.mul.Scalar %4912, %int22_7407 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int13_7408 = torch.constant.int 13
    %int1_7409 = torch.constant.int 1
    %4948 = torch.aten.add.Scalar %4947, %int13_7408, %int1_7409 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_7410 = torch.constant.int 2
    %4949 = torch.aten.mul.Scalar %4948, %int2_7410 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_7411 = torch.constant.int 1
    %4950 = torch.aten.add.Tensor %4949, %4946, %int1_7411 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_7412 = torch.constant.int 4
    %4951 = torch.aten.mul.Scalar %4950, %int4_7412 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_7413 = torch.constant.int 1
    %4952 = torch.aten.add.Tensor %4951, %4918, %int1_7413 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_7414 = torch.constant.int 32
    %4953 = torch.aten.mul.Scalar %4952, %int32_7414 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_7415 = torch.constant.int 1
    %4954 = torch.aten.add.Tensor %4953, %4915, %int1_7415 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_7416 = torch.constant.none
    %none_7417 = torch.constant.none
    %int5_7418 = torch.constant.int 5
    %cpu_7419 = torch.constant.device "cpu"
    %int0_7420 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4825, %none_7416, %none_7417, %int5_7418, %cpu_7419, %int0_7420 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %4955 = torch.prim.ListConstruct %4954 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_7421 = torch.constant.bool false
    %4956 = torch.aten.index_put %4943, %4955, %4825, %false_7421 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %4956, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_7422 = torch.constant.int 22
    %int2_7423 = torch.constant.int 2
    %int4_7424 = torch.constant.int 4
    %int32_7425 = torch.constant.int 32
    %int64_7426 = torch.constant.int 64
    %4957 = torch.prim.ListConstruct %381, %int22_7422, %int2_7423, %int4_7424, %int32_7425, %int64_7426 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4958 = torch.aten.view %4956, %4957 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4958, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_7427 = torch.constant.int 360448
    %4959 = torch.prim.ListConstruct %381, %int360448_7427 : (!torch.int, !torch.int) -> !torch.list<int>
    %4960 = torch.aten.view %4958, %4959 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %4960, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_7428 = torch.constant.none
    %4961 = torch.aten.clone %226, %none_7428 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_7429 = torch.constant.none
    %4962 = torch.aten.clone %227, %none_7429 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_7430 = torch.constant.none
    %4963 = torch.aten.clone %228, %none_7430 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_7431 = torch.constant.int 22
    %int2_7432 = torch.constant.int 2
    %int4_7433 = torch.constant.int 4
    %int32_7434 = torch.constant.int 32
    %int64_7435 = torch.constant.int 64
    %4964 = torch.prim.ListConstruct %381, %int22_7431, %int2_7432, %int4_7433, %int32_7434, %int64_7435 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4965 = torch.aten.view %4960, %4964 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %4965, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %4966 = torch_c.to_builtin_tensor %4965 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %4967 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_7436 = tensor.cast %4967 : tensor<4x?xi64> to tensor<?x?xi64>
    %4968 = torch_c.to_builtin_tensor %4961 : !torch.vtensor<[],si64> -> tensor<i64>
    %4969 = torch_c.to_builtin_tensor %4962 : !torch.vtensor<[],si64> -> tensor<i64>
    %4970 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%4966, %cast_7436, %4968, %4969) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_7437 = tensor.cast %4970 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %4971 = torch_c.from_builtin_tensor %cast_7437 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %4971, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %4972 = torch_c.to_builtin_tensor %4965 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %4973 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_7438 = tensor.cast %4973 : tensor<4x?xi64> to tensor<?x?xi64>
    %4974 = torch_c.to_builtin_tensor %4961 : !torch.vtensor<[],si64> -> tensor<i64>
    %4975 = torch_c.to_builtin_tensor %4963 : !torch.vtensor<[],si64> -> tensor<i64>
    %4976 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%4972, %cast_7438, %4974, %4975) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_7439 = tensor.cast %4976 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %4977 = torch_c.from_builtin_tensor %cast_7439 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %4977, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_7440 = torch.constant.int 2
    %int3_7441 = torch.constant.int 3
    %4978 = torch.aten.transpose.int %4971, %int2_7440, %int3_7441 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %4978, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_7442 = torch.constant.int 0
    %4979 = torch.aten.clone %4978, %int0_7442 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %4979, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_7443 = torch.constant.int 4
    %int4_7444 = torch.constant.int 4
    %int64_7445 = torch.constant.int 64
    %4980 = torch.prim.ListConstruct %int4_7443, %623, %int4_7444, %int64_7445 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4981 = torch.aten._unsafe_view %4979, %4980 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %4981, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_7446 = torch.constant.int 2
    %int3_7447 = torch.constant.int 3
    %4982 = torch.aten.transpose.int %4977, %int2_7446, %int3_7447 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %4982, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_7448 = torch.constant.int 0
    %4983 = torch.aten.clone %4982, %int0_7448 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %4983, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_7449 = torch.constant.int 4
    %int4_7450 = torch.constant.int 4
    %int64_7451 = torch.constant.int 64
    %4984 = torch.prim.ListConstruct %int4_7449, %623, %int4_7450, %int64_7451 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4985 = torch.aten._unsafe_view %4983, %4984 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %4985, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_7452 = torch.constant.int 0
    %int1_7453 = torch.constant.int 1
    %none_7454 = torch.constant.none
    %none_7455 = torch.constant.none
    %cpu_7456 = torch.constant.device "cpu"
    %false_7457 = torch.constant.bool false
    %4986 = torch.aten.arange.start_step %int0_7452, %623, %int1_7453, %none_7454, %none_7455, %cpu_7456, %false_7457 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4986, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_7458 = torch.constant.int -1
    %4987 = torch.aten.unsqueeze %arg1, %int-1_7458 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %4988 = torch.aten.ge.Tensor %4986, %4987 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %4988, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_7459 = torch.constant.none
    %4989 = torch.aten.clone %229, %none_7459 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_7460 = torch.constant.int 0
    %4990 = torch.aten.where.ScalarOther %4988, %4989, %int0_7460 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %4990, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_7461 = torch.constant.none
    %none_7462 = torch.constant.none
    %int5_7463 = torch.constant.int 5
    %cpu_7464 = torch.constant.device "cpu"
    %int0_7465 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %4990, %none_7461, %none_7462, %int5_7463, %cpu_7464, %int0_7465 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_7466 = torch.constant.int 1
    %4991 = torch.aten.unsqueeze %4990, %int1_7466 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %4991, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_7467 = torch.constant.int 1
    %4992 = torch.aten.unsqueeze %4991, %int1_7467 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %4992, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_7468 = torch.constant.int -2
    %4993 = torch.aten.unsqueeze %4981, %int-2_7468 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %4993, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_7469 = torch.constant.int 4
    %int4_7470 = torch.constant.int 4
    %int8_7471 = torch.constant.int 8
    %int64_7472 = torch.constant.int 64
    %4994 = torch.prim.ListConstruct %int4_7469, %623, %int4_7470, %int8_7471, %int64_7472 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7473 = torch.constant.bool false
    %4995 = torch.aten.expand %4993, %4994, %false_7473 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4995, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_7474 = torch.constant.int 0
    %4996 = torch.aten.clone %4995, %int0_7474 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %4996, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_7475 = torch.constant.int 4
    %int32_7476 = torch.constant.int 32
    %int64_7477 = torch.constant.int 64
    %4997 = torch.prim.ListConstruct %int4_7475, %623, %int32_7476, %int64_7477 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4998 = torch.aten._unsafe_view %4996, %4997 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %4998, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_7478 = torch.constant.int -2
    %4999 = torch.aten.unsqueeze %4985, %int-2_7478 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %4999, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_7479 = torch.constant.int 4
    %int4_7480 = torch.constant.int 4
    %int8_7481 = torch.constant.int 8
    %int64_7482 = torch.constant.int 64
    %5000 = torch.prim.ListConstruct %int4_7479, %623, %int4_7480, %int8_7481, %int64_7482 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7483 = torch.constant.bool false
    %5001 = torch.aten.expand %4999, %5000, %false_7483 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5001, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_7484 = torch.constant.int 0
    %5002 = torch.aten.clone %5001, %int0_7484 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5002, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_7485 = torch.constant.int 4
    %int32_7486 = torch.constant.int 32
    %int64_7487 = torch.constant.int 64
    %5003 = torch.prim.ListConstruct %int4_7485, %623, %int32_7486, %int64_7487 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5004 = torch.aten._unsafe_view %5002, %5003 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5004, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_7488 = torch.constant.int 1
    %int2_7489 = torch.constant.int 2
    %5005 = torch.aten.transpose.int %4866, %int1_7488, %int2_7489 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_7490 = torch.constant.int 1
    %int2_7491 = torch.constant.int 2
    %5006 = torch.aten.transpose.int %4998, %int1_7490, %int2_7491 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %5006, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_7492 = torch.constant.int 1
    %int2_7493 = torch.constant.int 2
    %5007 = torch.aten.transpose.int %5004, %int1_7492, %int2_7493 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %5007, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_7494 = torch.constant.float 0.000000e+00
    %false_7495 = torch.constant.bool false
    %none_7496 = torch.constant.none
    %false_7497 = torch.constant.bool false
    %5008 = torch.aten.scaled_dot_product_attention %5005, %5006, %5007, %4992, %float0.000000e00_7494, %false_7495, %none_7496, %false_7497 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_7498 = torch.constant.int 1
    %int2_7499 = torch.constant.int 2
    %5009 = torch.aten.transpose.int %5008, %int1_7498, %int2_7499 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_7500 = torch.constant.int 4
    %int1_7501 = torch.constant.int 1
    %int2048_7502 = torch.constant.int 2048
    %5010 = torch.prim.ListConstruct %int4_7500, %int1_7501, %int2048_7502 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5011 = torch.aten.view %5009, %5010 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_7503 = torch.constant.int 2
    %5012 = torch.aten.view.dtype %234, %int2_7503 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %5013 = torch.aten.detach %5012 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_7504 = torch.constant.int -1
    %int17_7505 = torch.constant.int 17
    %5014 = torch.prim.ListConstruct %int-1_7504, %int17_7505 : (!torch.int, !torch.int) -> !torch.list<int>
    %5015 = torch.aten.view %5013, %5014 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_7506 = torch.constant.int 2048
    %int-1_7507 = torch.constant.int -1
    %int17_7508 = torch.constant.int 17
    %5016 = torch.prim.ListConstruct %int2048_7506, %int-1_7507, %int17_7508 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5017 = torch.aten.view %5015, %5016 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_7509 = torch.constant.int 2
    %int0_7510 = torch.constant.int 0
    %int1_7511 = torch.constant.int 1
    %int1_7512 = torch.constant.int 1
    %5018 = torch.aten.slice.Tensor %5017, %int2_7509, %int0_7510, %int1_7511, %int1_7512 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_7513 = torch.constant.int 5
    %5019 = torch.aten.view.dtype %5018, %int5_7513 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %5020 = torch.aten.detach %5019 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_7514 = torch.constant.int 2
    %int1_7515 = torch.constant.int 1
    %int9223372036854775807_7516 = torch.constant.int 9223372036854775807
    %int1_7517 = torch.constant.int 1
    %5021 = torch.aten.slice.Tensor %5017, %int2_7514, %int1_7515, %int9223372036854775807_7516, %int1_7517 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_7518 = torch.constant.int 1
    %5022 = torch.aten.view.dtype %5021, %int1_7518 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %5023 = torch.aten.detach %5022 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %5024 = torch_c.to_builtin_tensor %5011 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_7519 = tensor.cast %5024 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %5025 = torch_c.to_builtin_tensor %5020 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %5026 = torch_c.to_builtin_tensor %5023 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %5027 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_7519, %5025, %5026) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_7520 = tensor.cast %5027 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %5028 = torch_c.from_builtin_tensor %cast_7520 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_7521 = torch.constant.none
    %none_7522 = torch.constant.none
    %int5_7523 = torch.constant.int 5
    %cpu_7524 = torch.constant.device "cpu"
    %int0_7525 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5028, %none_7521, %none_7522, %int5_7523, %cpu_7524, %int0_7525 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_7526 = torch.constant.int 1
    %5029 = torch.aten.add.Tensor %4758, %5028, %int1_7526 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_7527 = torch.constant.none
    %none_7528 = torch.constant.none
    %int5_7529 = torch.constant.int 5
    %cpu_7530 = torch.constant.device "cpu"
    %int0_7531 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5029, %none_7527, %none_7528, %int5_7529, %cpu_7530, %int0_7531 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7532 = torch.constant.int 6
    %5030 = torch.prims.convert_element_type %5029, %int6_7532 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_7533 = torch.constant.int 2
    %5031 = torch.aten.pow.Tensor_Scalar %5030, %int2_7533 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_7534 = torch.constant.int -1
    %5032 = torch.prim.ListConstruct %int-1_7534 : (!torch.int) -> !torch.list<int>
    %true_7535 = torch.constant.bool true
    %none_7536 = torch.constant.none
    %5033 = torch.aten.mean.dim %5031, %5032, %true_7535, %none_7536 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_7537 = torch.constant.float 9.9999997473787516E-6
    %int1_7538 = torch.constant.int 1
    %5034 = torch.aten.add.Scalar %5033, %float9.999990e-06_7537, %int1_7538 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5035 = torch.aten.rsqrt %5034 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %5036 = torch.aten.mul.Tensor %5030, %5035 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_7539 = torch.constant.none
    %none_7540 = torch.constant.none
    %int6_7541 = torch.constant.int 6
    %cpu_7542 = torch.constant.device "cpu"
    %int0_7543 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5036, %none_7539, %none_7540, %int6_7541, %cpu_7542, %int0_7543 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7544 = torch.constant.int 5
    %5037 = torch.prims.convert_element_type %5036, %int5_7544 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %5038 = torch.aten.mul.Tensor %235, %5037 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_7545 = torch.constant.none
    %none_7546 = torch.constant.none
    %int6_7547 = torch.constant.int 6
    %cpu_7548 = torch.constant.device "cpu"
    %int0_7549 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5038, %none_7545, %none_7546, %int6_7547, %cpu_7548, %int0_7549 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7550 = torch.constant.int 5
    %5039 = torch.prims.convert_element_type %5038, %int5_7550 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_7551 = torch.constant.int 2
    %5040 = torch.aten.view.dtype %236, %int2_7551 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %5041 = torch.aten.detach %5040 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_7552 = torch.constant.int -1
    %int17_7553 = torch.constant.int 17
    %5042 = torch.prim.ListConstruct %int-1_7552, %int17_7553 : (!torch.int, !torch.int) -> !torch.list<int>
    %5043 = torch.aten.view %5041, %5042 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_7554 = torch.constant.int 5632
    %int-1_7555 = torch.constant.int -1
    %int17_7556 = torch.constant.int 17
    %5044 = torch.prim.ListConstruct %int5632_7554, %int-1_7555, %int17_7556 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5045 = torch.aten.view %5043, %5044 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_7557 = torch.constant.int 2
    %int0_7558 = torch.constant.int 0
    %int1_7559 = torch.constant.int 1
    %int1_7560 = torch.constant.int 1
    %5046 = torch.aten.slice.Tensor %5045, %int2_7557, %int0_7558, %int1_7559, %int1_7560 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_7561 = torch.constant.int 5
    %5047 = torch.aten.view.dtype %5046, %int5_7561 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %5048 = torch.aten.detach %5047 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_7562 = torch.constant.int 2
    %int1_7563 = torch.constant.int 1
    %int9223372036854775807_7564 = torch.constant.int 9223372036854775807
    %int1_7565 = torch.constant.int 1
    %5049 = torch.aten.slice.Tensor %5045, %int2_7562, %int1_7563, %int9223372036854775807_7564, %int1_7565 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_7566 = torch.constant.int 1
    %5050 = torch.aten.view.dtype %5049, %int1_7566 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %5051 = torch.aten.detach %5050 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %5052 = torch_c.to_builtin_tensor %5039 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_7567 = tensor.cast %5052 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %5053 = torch_c.to_builtin_tensor %5048 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %5054 = torch_c.to_builtin_tensor %5051 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %5055 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_7567, %5053, %5054) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_7568 = tensor.cast %5055 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %5056 = torch_c.from_builtin_tensor %cast_7568 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %5057 = torch.aten.silu %5056 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_7569 = torch.constant.int 2
    %5058 = torch.aten.view.dtype %237, %int2_7569 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %5059 = torch.aten.detach %5058 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_7570 = torch.constant.int -1
    %int17_7571 = torch.constant.int 17
    %5060 = torch.prim.ListConstruct %int-1_7570, %int17_7571 : (!torch.int, !torch.int) -> !torch.list<int>
    %5061 = torch.aten.view %5059, %5060 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_7572 = torch.constant.int 5632
    %int-1_7573 = torch.constant.int -1
    %int17_7574 = torch.constant.int 17
    %5062 = torch.prim.ListConstruct %int5632_7572, %int-1_7573, %int17_7574 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5063 = torch.aten.view %5061, %5062 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_7575 = torch.constant.int 2
    %int0_7576 = torch.constant.int 0
    %int1_7577 = torch.constant.int 1
    %int1_7578 = torch.constant.int 1
    %5064 = torch.aten.slice.Tensor %5063, %int2_7575, %int0_7576, %int1_7577, %int1_7578 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_7579 = torch.constant.int 5
    %5065 = torch.aten.view.dtype %5064, %int5_7579 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %5066 = torch.aten.detach %5065 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_7580 = torch.constant.int 2
    %int1_7581 = torch.constant.int 1
    %int9223372036854775807_7582 = torch.constant.int 9223372036854775807
    %int1_7583 = torch.constant.int 1
    %5067 = torch.aten.slice.Tensor %5063, %int2_7580, %int1_7581, %int9223372036854775807_7582, %int1_7583 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_7584 = torch.constant.int 1
    %5068 = torch.aten.view.dtype %5067, %int1_7584 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %5069 = torch.aten.detach %5068 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %5070 = torch_c.to_builtin_tensor %5039 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_7585 = tensor.cast %5070 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %5071 = torch_c.to_builtin_tensor %5066 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %5072 = torch_c.to_builtin_tensor %5069 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %5073 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_7585, %5071, %5072) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_7586 = tensor.cast %5073 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %5074 = torch_c.from_builtin_tensor %cast_7586 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %5075 = torch.aten.mul.Tensor %5057, %5074 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_7587 = torch.constant.int 2
    %5076 = torch.aten.view.dtype %238, %int2_7587 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %5077 = torch.aten.detach %5076 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_7588 = torch.constant.int -1
    %int17_7589 = torch.constant.int 17
    %5078 = torch.prim.ListConstruct %int-1_7588, %int17_7589 : (!torch.int, !torch.int) -> !torch.list<int>
    %5079 = torch.aten.view %5077, %5078 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_7590 = torch.constant.int 2048
    %int-1_7591 = torch.constant.int -1
    %int17_7592 = torch.constant.int 17
    %5080 = torch.prim.ListConstruct %int2048_7590, %int-1_7591, %int17_7592 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5081 = torch.aten.view %5079, %5080 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_7593 = torch.constant.int 2
    %int0_7594 = torch.constant.int 0
    %int1_7595 = torch.constant.int 1
    %int1_7596 = torch.constant.int 1
    %5082 = torch.aten.slice.Tensor %5081, %int2_7593, %int0_7594, %int1_7595, %int1_7596 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_7597 = torch.constant.int 5
    %5083 = torch.aten.view.dtype %5082, %int5_7597 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %5084 = torch.aten.detach %5083 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_7598 = torch.constant.int 2
    %int1_7599 = torch.constant.int 1
    %int9223372036854775807_7600 = torch.constant.int 9223372036854775807
    %int1_7601 = torch.constant.int 1
    %5085 = torch.aten.slice.Tensor %5081, %int2_7598, %int1_7599, %int9223372036854775807_7600, %int1_7601 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_7602 = torch.constant.int 1
    %5086 = torch.aten.view.dtype %5085, %int1_7602 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %5087 = torch.aten.detach %5086 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %5088 = torch_c.to_builtin_tensor %5075 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_7603 = tensor.cast %5088 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %5089 = torch_c.to_builtin_tensor %5084 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %5090 = torch_c.to_builtin_tensor %5087 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %5091 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_7603, %5089, %5090) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_7604 = tensor.cast %5091 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %5092 = torch_c.from_builtin_tensor %cast_7604 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_7605 = torch.constant.int 1
    %5093 = torch.aten.add.Tensor %5029, %5092, %int1_7605 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_7606 = torch.constant.none
    %none_7607 = torch.constant.none
    %int5_7608 = torch.constant.int 5
    %cpu_7609 = torch.constant.device "cpu"
    %int0_7610 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5093, %none_7606, %none_7607, %int5_7608, %cpu_7609, %int0_7610 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7611 = torch.constant.int 6
    %5094 = torch.prims.convert_element_type %5093, %int6_7611 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_7612 = torch.constant.int 2
    %5095 = torch.aten.pow.Tensor_Scalar %5094, %int2_7612 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_7613 = torch.constant.int -1
    %5096 = torch.prim.ListConstruct %int-1_7613 : (!torch.int) -> !torch.list<int>
    %true_7614 = torch.constant.bool true
    %none_7615 = torch.constant.none
    %5097 = torch.aten.mean.dim %5095, %5096, %true_7614, %none_7615 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_7616 = torch.constant.float 9.9999997473787516E-6
    %int1_7617 = torch.constant.int 1
    %5098 = torch.aten.add.Scalar %5097, %float9.999990e-06_7616, %int1_7617 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5099 = torch.aten.rsqrt %5098 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %5100 = torch.aten.mul.Tensor %5094, %5099 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_7618 = torch.constant.none
    %none_7619 = torch.constant.none
    %int6_7620 = torch.constant.int 6
    %cpu_7621 = torch.constant.device "cpu"
    %int0_7622 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5100, %none_7618, %none_7619, %int6_7620, %cpu_7621, %int0_7622 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7623 = torch.constant.int 5
    %5101 = torch.prims.convert_element_type %5100, %int5_7623 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %5102 = torch.aten.mul.Tensor %247, %5101 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_7624 = torch.constant.none
    %none_7625 = torch.constant.none
    %int6_7626 = torch.constant.int 6
    %cpu_7627 = torch.constant.device "cpu"
    %int0_7628 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5102, %none_7624, %none_7625, %int6_7626, %cpu_7627, %int0_7628 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7629 = torch.constant.int 5
    %5103 = torch.prims.convert_element_type %5102, %int5_7629 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_7630 = torch.constant.int 2
    %5104 = torch.aten.view.dtype %248, %int2_7630 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %5105 = torch.aten.detach %5104 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_7631 = torch.constant.int -1
    %int17_7632 = torch.constant.int 17
    %5106 = torch.prim.ListConstruct %int-1_7631, %int17_7632 : (!torch.int, !torch.int) -> !torch.list<int>
    %5107 = torch.aten.view %5105, %5106 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_7633 = torch.constant.int 2048
    %int-1_7634 = torch.constant.int -1
    %int17_7635 = torch.constant.int 17
    %5108 = torch.prim.ListConstruct %int2048_7633, %int-1_7634, %int17_7635 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5109 = torch.aten.view %5107, %5108 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_7636 = torch.constant.int 2
    %int0_7637 = torch.constant.int 0
    %int1_7638 = torch.constant.int 1
    %int1_7639 = torch.constant.int 1
    %5110 = torch.aten.slice.Tensor %5109, %int2_7636, %int0_7637, %int1_7638, %int1_7639 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_7640 = torch.constant.int 5
    %5111 = torch.aten.view.dtype %5110, %int5_7640 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %5112 = torch.aten.detach %5111 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_7641 = torch.constant.int 2
    %int1_7642 = torch.constant.int 1
    %int9223372036854775807_7643 = torch.constant.int 9223372036854775807
    %int1_7644 = torch.constant.int 1
    %5113 = torch.aten.slice.Tensor %5109, %int2_7641, %int1_7642, %int9223372036854775807_7643, %int1_7644 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_7645 = torch.constant.int 1
    %5114 = torch.aten.view.dtype %5113, %int1_7645 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %5115 = torch.aten.detach %5114 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %5116 = torch_c.to_builtin_tensor %5103 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_7646 = tensor.cast %5116 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %5117 = torch_c.to_builtin_tensor %5112 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %5118 = torch_c.to_builtin_tensor %5115 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %5119 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_7646, %5117, %5118) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_7647 = tensor.cast %5119 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %5120 = torch_c.from_builtin_tensor %cast_7647 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_7648 = torch.constant.int 2
    %5121 = torch.aten.view.dtype %249, %int2_7648 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %5122 = torch.aten.detach %5121 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_7649 = torch.constant.int -1
    %int17_7650 = torch.constant.int 17
    %5123 = torch.prim.ListConstruct %int-1_7649, %int17_7650 : (!torch.int, !torch.int) -> !torch.list<int>
    %5124 = torch.aten.view %5122, %5123 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_7651 = torch.constant.int 256
    %int-1_7652 = torch.constant.int -1
    %int17_7653 = torch.constant.int 17
    %5125 = torch.prim.ListConstruct %int256_7651, %int-1_7652, %int17_7653 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5126 = torch.aten.view %5124, %5125 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_7654 = torch.constant.int 2
    %int0_7655 = torch.constant.int 0
    %int1_7656 = torch.constant.int 1
    %int1_7657 = torch.constant.int 1
    %5127 = torch.aten.slice.Tensor %5126, %int2_7654, %int0_7655, %int1_7656, %int1_7657 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_7658 = torch.constant.int 5
    %5128 = torch.aten.view.dtype %5127, %int5_7658 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %5129 = torch.aten.detach %5128 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_7659 = torch.constant.int 2
    %int1_7660 = torch.constant.int 1
    %int9223372036854775807_7661 = torch.constant.int 9223372036854775807
    %int1_7662 = torch.constant.int 1
    %5130 = torch.aten.slice.Tensor %5126, %int2_7659, %int1_7660, %int9223372036854775807_7661, %int1_7662 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_7663 = torch.constant.int 1
    %5131 = torch.aten.view.dtype %5130, %int1_7663 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %5132 = torch.aten.detach %5131 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %5133 = torch_c.to_builtin_tensor %5103 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_7664 = tensor.cast %5133 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %5134 = torch_c.to_builtin_tensor %5129 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %5135 = torch_c.to_builtin_tensor %5132 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %5136 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_7664, %5134, %5135) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_7665 = tensor.cast %5136 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %5137 = torch_c.from_builtin_tensor %cast_7665 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_7666 = torch.constant.int 2
    %5138 = torch.aten.view.dtype %250, %int2_7666 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %5139 = torch.aten.detach %5138 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_7667 = torch.constant.int -1
    %int17_7668 = torch.constant.int 17
    %5140 = torch.prim.ListConstruct %int-1_7667, %int17_7668 : (!torch.int, !torch.int) -> !torch.list<int>
    %5141 = torch.aten.view %5139, %5140 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_7669 = torch.constant.int 256
    %int-1_7670 = torch.constant.int -1
    %int17_7671 = torch.constant.int 17
    %5142 = torch.prim.ListConstruct %int256_7669, %int-1_7670, %int17_7671 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5143 = torch.aten.view %5141, %5142 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_7672 = torch.constant.int 2
    %int0_7673 = torch.constant.int 0
    %int1_7674 = torch.constant.int 1
    %int1_7675 = torch.constant.int 1
    %5144 = torch.aten.slice.Tensor %5143, %int2_7672, %int0_7673, %int1_7674, %int1_7675 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_7676 = torch.constant.int 5
    %5145 = torch.aten.view.dtype %5144, %int5_7676 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %5146 = torch.aten.detach %5145 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_7677 = torch.constant.int 2
    %int1_7678 = torch.constant.int 1
    %int9223372036854775807_7679 = torch.constant.int 9223372036854775807
    %int1_7680 = torch.constant.int 1
    %5147 = torch.aten.slice.Tensor %5143, %int2_7677, %int1_7678, %int9223372036854775807_7679, %int1_7680 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_7681 = torch.constant.int 1
    %5148 = torch.aten.view.dtype %5147, %int1_7681 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %5149 = torch.aten.detach %5148 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %5150 = torch_c.to_builtin_tensor %5103 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_7682 = tensor.cast %5150 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %5151 = torch_c.to_builtin_tensor %5146 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %5152 = torch_c.to_builtin_tensor %5149 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %5153 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_7682, %5151, %5152) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_7683 = tensor.cast %5153 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %5154 = torch_c.from_builtin_tensor %cast_7683 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_7684 = torch.constant.int 4
    %int1_7685 = torch.constant.int 1
    %int32_7686 = torch.constant.int 32
    %int64_7687 = torch.constant.int 64
    %5155 = torch.prim.ListConstruct %int4_7684, %int1_7685, %int32_7686, %int64_7687 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5156 = torch.aten.view %5120, %5155 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_7688 = torch.constant.int 4
    %int1_7689 = torch.constant.int 1
    %int4_7690 = torch.constant.int 4
    %int64_7691 = torch.constant.int 64
    %5157 = torch.prim.ListConstruct %int4_7688, %int1_7689, %int4_7690, %int64_7691 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5158 = torch.aten.view %5137, %5157 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_7692 = torch.constant.int 4
    %int1_7693 = torch.constant.int 1
    %int4_7694 = torch.constant.int 4
    %int64_7695 = torch.constant.int 64
    %5159 = torch.prim.ListConstruct %int4_7692, %int1_7693, %int4_7694, %int64_7695 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5160 = torch.aten.view %5154, %5159 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_7696 = torch.constant.int 0
    %int1_7697 = torch.constant.int 1
    %none_7698 = torch.constant.none
    %none_7699 = torch.constant.none
    %cpu_7700 = torch.constant.device "cpu"
    %false_7701 = torch.constant.bool false
    %5161 = torch.aten.arange.start %int0_7696, %int1_7697, %none_7698, %none_7699, %cpu_7700, %false_7701 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_7702 = torch.constant.int 0
    %5162 = torch.aten.unsqueeze %5161, %int0_7702 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_7703 = torch.constant.int 1
    %5163 = torch.aten.unsqueeze %arg2, %int1_7703 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7704 = torch.constant.int 1
    %5164 = torch.aten.add.Tensor %5162, %5163, %int1_7704 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_7705 = torch.constant.int 0
    %int64_7706 = torch.constant.int 64
    %int2_7707 = torch.constant.int 2
    %none_7708 = torch.constant.none
    %none_7709 = torch.constant.none
    %cpu_7710 = torch.constant.device "cpu"
    %false_7711 = torch.constant.bool false
    %5165 = torch.aten.arange.start_step %int0_7705, %int64_7706, %int2_7707, %none_7708, %none_7709, %cpu_7710, %false_7711 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_7712 = torch.constant.none
    %none_7713 = torch.constant.none
    %int4_7714 = torch.constant.int 4
    %cpu_7715 = torch.constant.device "cpu"
    %int0_7716 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5165, %none_7712, %none_7713, %int4_7714, %cpu_7715, %int0_7716 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7717 = torch.constant.int 6
    %5166 = torch.prims.convert_element_type %5165, %int6_7717 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_7718 = torch.constant.int 64
    %5167 = torch.aten.div.Scalar %5166, %int64_7718 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_7719 = torch.constant.float 1.000000e+04
    %5168 = torch.aten.pow.Scalar %float1.000000e04_7719, %5167 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %5169 = torch.aten.reciprocal %5168 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_7720 = torch.constant.float 1.000000e+00
    %5170 = torch.aten.mul.Scalar %5169, %float1.000000e00_7720 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_7721 = torch.constant.none
    %5171 = torch.aten.clone %239, %none_7721 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_7722 = torch.constant.int 0
    %5172 = torch.aten.unsqueeze %5170, %int0_7722 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_7723 = torch.constant.int 2
    %5173 = torch.aten.unsqueeze %5172, %int2_7723 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_7724 = torch.constant.none
    %none_7725 = torch.constant.none
    %int6_7726 = torch.constant.int 6
    %cpu_7727 = torch.constant.device "cpu"
    %int0_7728 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5173, %none_7724, %none_7725, %int6_7726, %cpu_7727, %int0_7728 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_7729 = torch.constant.int 4
    %int-1_7730 = torch.constant.int -1
    %int1_7731 = torch.constant.int 1
    %5174 = torch.prim.ListConstruct %int4_7729, %int-1_7730, %int1_7731 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7732 = torch.constant.bool false
    %5175 = torch.aten.expand %5173, %5174, %false_7732 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_7733 = torch.constant.int 1
    %5176 = torch.aten.unsqueeze %5164, %int1_7733 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_7734 = torch.constant.none
    %none_7735 = torch.constant.none
    %int4_7736 = torch.constant.int 4
    %cpu_7737 = torch.constant.device "cpu"
    %int0_7738 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5176, %none_7734, %none_7735, %int4_7736, %cpu_7737, %int0_7738 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7739 = torch.constant.int 6
    %5177 = torch.prims.convert_element_type %5176, %int6_7739 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5178 = torch.aten.matmul %5175, %5177 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_7740 = torch.constant.int 1
    %int2_7741 = torch.constant.int 2
    %5179 = torch.aten.transpose.int %5178, %int1_7740, %int2_7741 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %5180 = torch.aten.cos %5179 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %5181 = torch.aten.mul.Tensor %5180, %5171 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_7742 = torch.constant.none
    %none_7743 = torch.constant.none
    %int6_7744 = torch.constant.int 6
    %cpu_7745 = torch.constant.device "cpu"
    %int0_7746 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5181, %none_7742, %none_7743, %int6_7744, %cpu_7745, %int0_7746 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7747 = torch.constant.int 5
    %5182 = torch.prims.convert_element_type %5181, %int5_7747 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %5183 = torch.aten.sin %5179 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %5184 = torch.aten.mul.Tensor %5183, %5171 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_7748 = torch.constant.none
    %none_7749 = torch.constant.none
    %int6_7750 = torch.constant.int 6
    %cpu_7751 = torch.constant.device "cpu"
    %int0_7752 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5184, %none_7748, %none_7749, %int6_7750, %cpu_7751, %int0_7752 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7753 = torch.constant.int 5
    %5185 = torch.prims.convert_element_type %5184, %int5_7753 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_7754 = torch.constant.int 2
    %5186 = torch.aten.unsqueeze %5182, %int2_7754 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_7755 = torch.constant.int 2
    %5187 = torch.aten.unsqueeze %5185, %int2_7755 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_7756 = torch.constant.none
    %none_7757 = torch.constant.none
    %int5_7758 = torch.constant.int 5
    %cpu_7759 = torch.constant.device "cpu"
    %int0_7760 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5186, %none_7756, %none_7757, %int5_7758, %cpu_7759, %int0_7760 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_7761 = torch.constant.none
    %none_7762 = torch.constant.none
    %int5_7763 = torch.constant.int 5
    %cpu_7764 = torch.constant.device "cpu"
    %int0_7765 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5187, %none_7761, %none_7762, %int5_7763, %cpu_7764, %int0_7765 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_7766 = torch.constant.none
    %none_7767 = torch.constant.none
    %int5_7768 = torch.constant.int 5
    %cpu_7769 = torch.constant.device "cpu"
    %int0_7770 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5156, %none_7766, %none_7767, %int5_7768, %cpu_7769, %int0_7770 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_7771 = torch.constant.int 3
    %int0_7772 = torch.constant.int 0
    %int64_7773 = torch.constant.int 64
    %int2_7774 = torch.constant.int 2
    %5188 = torch.aten.slice.Tensor %5156, %int3_7771, %int0_7772, %int64_7773, %int2_7774 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_7775 = torch.constant.int 3
    %int1_7776 = torch.constant.int 1
    %int64_7777 = torch.constant.int 64
    %int2_7778 = torch.constant.int 2
    %5189 = torch.aten.slice.Tensor %5156, %int3_7775, %int1_7776, %int64_7777, %int2_7778 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %5190 = torch.aten.mul.Tensor %5188, %5186 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %5191 = torch.aten.mul.Tensor %5189, %5187 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_7779 = torch.constant.int 1
    %5192 = torch.aten.sub.Tensor %5190, %5191, %int1_7779 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %5193 = torch.aten.mul.Tensor %5189, %5186 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %5194 = torch.aten.mul.Tensor %5188, %5187 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_7780 = torch.constant.int 1
    %5195 = torch.aten.add.Tensor %5193, %5194, %int1_7780 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %5196 = torch_c.to_builtin_tensor %5192 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_7781 = tensor.cast %5196 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %5197 = torch_c.to_builtin_tensor %5195 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_7782 = tensor.cast %5197 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %5198 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_7781, %cast_7782) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_7783 = tensor.cast %5198 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %5199 = torch_c.from_builtin_tensor %cast_7783 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_7784 = torch.constant.int 4
    %int1_7785 = torch.constant.int 1
    %int32_7786 = torch.constant.int 32
    %int64_7787 = torch.constant.int 64
    %5200 = torch.prim.ListConstruct %int4_7784, %int1_7785, %int32_7786, %int64_7787 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5201 = torch.aten.view %5199, %5200 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_7788 = torch.constant.none
    %none_7789 = torch.constant.none
    %int5_7790 = torch.constant.int 5
    %cpu_7791 = torch.constant.device "cpu"
    %int0_7792 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5201, %none_7788, %none_7789, %int5_7790, %cpu_7791, %int0_7792 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_7793 = torch.constant.int 0
    %int1_7794 = torch.constant.int 1
    %none_7795 = torch.constant.none
    %none_7796 = torch.constant.none
    %cpu_7797 = torch.constant.device "cpu"
    %false_7798 = torch.constant.bool false
    %5202 = torch.aten.arange.start %int0_7793, %int1_7794, %none_7795, %none_7796, %cpu_7797, %false_7798 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_7799 = torch.constant.int 0
    %5203 = torch.aten.unsqueeze %5202, %int0_7799 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_7800 = torch.constant.int 1
    %5204 = torch.aten.unsqueeze %arg2, %int1_7800 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7801 = torch.constant.int 1
    %5205 = torch.aten.add.Tensor %5203, %5204, %int1_7801 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_7802 = torch.constant.int 0
    %int64_7803 = torch.constant.int 64
    %int2_7804 = torch.constant.int 2
    %none_7805 = torch.constant.none
    %none_7806 = torch.constant.none
    %cpu_7807 = torch.constant.device "cpu"
    %false_7808 = torch.constant.bool false
    %5206 = torch.aten.arange.start_step %int0_7802, %int64_7803, %int2_7804, %none_7805, %none_7806, %cpu_7807, %false_7808 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_7809 = torch.constant.none
    %none_7810 = torch.constant.none
    %int4_7811 = torch.constant.int 4
    %cpu_7812 = torch.constant.device "cpu"
    %int0_7813 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5206, %none_7809, %none_7810, %int4_7811, %cpu_7812, %int0_7813 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7814 = torch.constant.int 6
    %5207 = torch.prims.convert_element_type %5206, %int6_7814 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_7815 = torch.constant.int 64
    %5208 = torch.aten.div.Scalar %5207, %int64_7815 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_7816 = torch.constant.float 1.000000e+04
    %5209 = torch.aten.pow.Scalar %float1.000000e04_7816, %5208 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %5210 = torch.aten.reciprocal %5209 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_7817 = torch.constant.float 1.000000e+00
    %5211 = torch.aten.mul.Scalar %5210, %float1.000000e00_7817 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_7818 = torch.constant.none
    %5212 = torch.aten.clone %240, %none_7818 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_7819 = torch.constant.int 0
    %5213 = torch.aten.unsqueeze %5211, %int0_7819 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_7820 = torch.constant.int 2
    %5214 = torch.aten.unsqueeze %5213, %int2_7820 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_7821 = torch.constant.none
    %none_7822 = torch.constant.none
    %int6_7823 = torch.constant.int 6
    %cpu_7824 = torch.constant.device "cpu"
    %int0_7825 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5214, %none_7821, %none_7822, %int6_7823, %cpu_7824, %int0_7825 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_7826 = torch.constant.int 4
    %int-1_7827 = torch.constant.int -1
    %int1_7828 = torch.constant.int 1
    %5215 = torch.prim.ListConstruct %int4_7826, %int-1_7827, %int1_7828 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7829 = torch.constant.bool false
    %5216 = torch.aten.expand %5214, %5215, %false_7829 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_7830 = torch.constant.int 1
    %5217 = torch.aten.unsqueeze %5205, %int1_7830 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_7831 = torch.constant.none
    %none_7832 = torch.constant.none
    %int4_7833 = torch.constant.int 4
    %cpu_7834 = torch.constant.device "cpu"
    %int0_7835 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5217, %none_7831, %none_7832, %int4_7833, %cpu_7834, %int0_7835 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_7836 = torch.constant.int 6
    %5218 = torch.prims.convert_element_type %5217, %int6_7836 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5219 = torch.aten.matmul %5216, %5218 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_7837 = torch.constant.int 1
    %int2_7838 = torch.constant.int 2
    %5220 = torch.aten.transpose.int %5219, %int1_7837, %int2_7838 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %5221 = torch.aten.cos %5220 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %5222 = torch.aten.mul.Tensor %5221, %5212 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_7839 = torch.constant.none
    %none_7840 = torch.constant.none
    %int6_7841 = torch.constant.int 6
    %cpu_7842 = torch.constant.device "cpu"
    %int0_7843 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5222, %none_7839, %none_7840, %int6_7841, %cpu_7842, %int0_7843 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7844 = torch.constant.int 5
    %5223 = torch.prims.convert_element_type %5222, %int5_7844 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %5224 = torch.aten.sin %5220 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %5225 = torch.aten.mul.Tensor %5224, %5212 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_7845 = torch.constant.none
    %none_7846 = torch.constant.none
    %int6_7847 = torch.constant.int 6
    %cpu_7848 = torch.constant.device "cpu"
    %int0_7849 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5225, %none_7845, %none_7846, %int6_7847, %cpu_7848, %int0_7849 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_7850 = torch.constant.int 5
    %5226 = torch.prims.convert_element_type %5225, %int5_7850 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_7851 = torch.constant.int 2
    %5227 = torch.aten.unsqueeze %5223, %int2_7851 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_7852 = torch.constant.int 2
    %5228 = torch.aten.unsqueeze %5226, %int2_7852 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_7853 = torch.constant.none
    %none_7854 = torch.constant.none
    %int5_7855 = torch.constant.int 5
    %cpu_7856 = torch.constant.device "cpu"
    %int0_7857 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5227, %none_7853, %none_7854, %int5_7855, %cpu_7856, %int0_7857 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_7858 = torch.constant.none
    %none_7859 = torch.constant.none
    %int5_7860 = torch.constant.int 5
    %cpu_7861 = torch.constant.device "cpu"
    %int0_7862 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5228, %none_7858, %none_7859, %int5_7860, %cpu_7861, %int0_7862 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_7863 = torch.constant.none
    %none_7864 = torch.constant.none
    %int5_7865 = torch.constant.int 5
    %cpu_7866 = torch.constant.device "cpu"
    %int0_7867 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5158, %none_7863, %none_7864, %int5_7865, %cpu_7866, %int0_7867 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_7868 = torch.constant.int 3
    %int0_7869 = torch.constant.int 0
    %int64_7870 = torch.constant.int 64
    %int2_7871 = torch.constant.int 2
    %5229 = torch.aten.slice.Tensor %5158, %int3_7868, %int0_7869, %int64_7870, %int2_7871 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_7872 = torch.constant.int 3
    %int1_7873 = torch.constant.int 1
    %int64_7874 = torch.constant.int 64
    %int2_7875 = torch.constant.int 2
    %5230 = torch.aten.slice.Tensor %5158, %int3_7872, %int1_7873, %int64_7874, %int2_7875 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %5231 = torch.aten.mul.Tensor %5229, %5227 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %5232 = torch.aten.mul.Tensor %5230, %5228 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_7876 = torch.constant.int 1
    %5233 = torch.aten.sub.Tensor %5231, %5232, %int1_7876 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %5234 = torch.aten.mul.Tensor %5230, %5227 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %5235 = torch.aten.mul.Tensor %5229, %5228 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_7877 = torch.constant.int 1
    %5236 = torch.aten.add.Tensor %5234, %5235, %int1_7877 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %5237 = torch_c.to_builtin_tensor %5233 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_7878 = tensor.cast %5237 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %5238 = torch_c.to_builtin_tensor %5236 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_7879 = tensor.cast %5238 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %5239 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_7878, %cast_7879) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_7880 = tensor.cast %5239 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %5240 = torch_c.from_builtin_tensor %cast_7880 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_7881 = torch.constant.int 4
    %int1_7882 = torch.constant.int 1
    %int4_7883 = torch.constant.int 4
    %int64_7884 = torch.constant.int 64
    %5241 = torch.prim.ListConstruct %int4_7881, %int1_7882, %int4_7883, %int64_7884 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5242 = torch.aten.view %5240, %5241 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_7885 = torch.constant.none
    %none_7886 = torch.constant.none
    %int5_7887 = torch.constant.int 5
    %cpu_7888 = torch.constant.device "cpu"
    %int0_7889 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5242, %none_7885, %none_7886, %int5_7887, %cpu_7888, %int0_7889 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_7890 = torch.constant.int 32
    %5243 = torch.aten.floor_divide.Scalar %arg2, %int32_7890 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_7891 = torch.constant.int 1
    %5244 = torch.aten.unsqueeze %5243, %int1_7891 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7892 = torch.constant.int 1
    %false_7893 = torch.constant.bool false
    %5245 = torch.aten.gather %arg3, %int1_7892, %5244, %false_7893 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_7894 = torch.constant.int 4
    %int1_7895 = torch.constant.int 1
    %int1_7896 = torch.constant.int 1
    %5246 = torch.prim.ListConstruct %int4_7894, %int1_7895, %int1_7896 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5247 = torch.aten.view %5245, %5246 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_7897 = torch.constant.int 32
    %5248 = torch.aten.remainder.Scalar %arg2, %int32_7897 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_7898 = torch.constant.int 4
    %int1_7899 = torch.constant.int 1
    %int1_7900 = torch.constant.int 1
    %5249 = torch.prim.ListConstruct %int4_7898, %int1_7899, %int1_7900 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5250 = torch.aten.view %5248, %5249 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_7901 = torch.constant.int 4
    %none_7902 = torch.constant.none
    %none_7903 = torch.constant.none
    %cpu_7904 = torch.constant.device "cpu"
    %false_7905 = torch.constant.bool false
    %5251 = torch.aten.arange %int4_7901, %none_7902, %none_7903, %cpu_7904, %false_7905 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_7906 = torch.constant.int 1
    %int1_7907 = torch.constant.int 1
    %int4_7908 = torch.constant.int 4
    %5252 = torch.prim.ListConstruct %int1_7906, %int1_7907, %int4_7908 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5253 = torch.aten.view %5251, %5252 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_7909 = torch.constant.none
    %5254 = torch.aten.clone %241, %none_7909 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_7910 = torch.constant.int 1
    %int1_7911 = torch.constant.int 1
    %int1_7912 = torch.constant.int 1
    %5255 = torch.prim.ListConstruct %int1_7910, %int1_7911, %int1_7912 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5256 = torch.aten.view %5254, %5255 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_7913 = torch.constant.int 22
    %5257 = torch.aten.mul.Scalar %5247, %int22_7913 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int14 = torch.constant.int 14
    %int1_7914 = torch.constant.int 1
    %5258 = torch.aten.add.Scalar %5257, %int14, %int1_7914 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_7915 = torch.constant.int 2
    %5259 = torch.aten.mul.Scalar %5258, %int2_7915 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_7916 = torch.constant.int 1
    %5260 = torch.aten.add.Tensor %5259, %5256, %int1_7916 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_7917 = torch.constant.int 4
    %5261 = torch.aten.mul.Scalar %5260, %int4_7917 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_7918 = torch.constant.int 1
    %5262 = torch.aten.add.Tensor %5261, %5253, %int1_7918 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_7919 = torch.constant.int 32
    %5263 = torch.aten.mul.Scalar %5262, %int32_7919 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_7920 = torch.constant.int 1
    %5264 = torch.aten.add.Tensor %5263, %5250, %int1_7920 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_7921 = torch.constant.none
    %none_7922 = torch.constant.none
    %int5_7923 = torch.constant.int 5
    %cpu_7924 = torch.constant.device "cpu"
    %int0_7925 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5242, %none_7921, %none_7922, %int5_7923, %cpu_7924, %int0_7925 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_7926 = torch.constant.int 22
    %int2_7927 = torch.constant.int 2
    %int4_7928 = torch.constant.int 4
    %int32_7929 = torch.constant.int 32
    %int64_7930 = torch.constant.int 64
    %5265 = torch.prim.ListConstruct %381, %int22_7926, %int2_7927, %int4_7928, %int32_7929, %int64_7930 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5266 = torch.aten.view %4960, %5265 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5266, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_7931 = torch.constant.int 64
    %5267 = torch.prim.ListConstruct %553, %int64_7931 : (!torch.int, !torch.int) -> !torch.list<int>
    %5268 = torch.aten.view %5266, %5267 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %5268, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %5269 = torch.prim.ListConstruct %5264 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_7932 = torch.constant.bool false
    %5270 = torch.aten.index_put %5268, %5269, %5242, %false_7932 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %5270, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_7933 = torch.constant.int 22
    %int2_7934 = torch.constant.int 2
    %int4_7935 = torch.constant.int 4
    %int32_7936 = torch.constant.int 32
    %int64_7937 = torch.constant.int 64
    %5271 = torch.prim.ListConstruct %381, %int22_7933, %int2_7934, %int4_7935, %int32_7936, %int64_7937 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5272 = torch.aten.view %5270, %5271 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5272, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_7938 = torch.constant.int 360448
    %5273 = torch.prim.ListConstruct %381, %int360448_7938 : (!torch.int, !torch.int) -> !torch.list<int>
    %5274 = torch.aten.view %5272, %5273 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %5274, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_7939 = torch.constant.int 22
    %int2_7940 = torch.constant.int 2
    %int4_7941 = torch.constant.int 4
    %int32_7942 = torch.constant.int 32
    %int64_7943 = torch.constant.int 64
    %5275 = torch.prim.ListConstruct %381, %int22_7939, %int2_7940, %int4_7941, %int32_7942, %int64_7943 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5276 = torch.aten.view %5274, %5275 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5276, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_7944 = torch.constant.int 64
    %5277 = torch.prim.ListConstruct %553, %int64_7944 : (!torch.int, !torch.int) -> !torch.list<int>
    %5278 = torch.aten.view %5276, %5277 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %5278, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_7945 = torch.constant.none
    %5279 = torch.aten.clone %242, %none_7945 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_7946 = torch.constant.int 1
    %int1_7947 = torch.constant.int 1
    %int1_7948 = torch.constant.int 1
    %5280 = torch.prim.ListConstruct %int1_7946, %int1_7947, %int1_7948 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5281 = torch.aten.view %5279, %5280 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_7949 = torch.constant.int 22
    %5282 = torch.aten.mul.Scalar %5247, %int22_7949 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int14_7950 = torch.constant.int 14
    %int1_7951 = torch.constant.int 1
    %5283 = torch.aten.add.Scalar %5282, %int14_7950, %int1_7951 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_7952 = torch.constant.int 2
    %5284 = torch.aten.mul.Scalar %5283, %int2_7952 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_7953 = torch.constant.int 1
    %5285 = torch.aten.add.Tensor %5284, %5281, %int1_7953 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_7954 = torch.constant.int 4
    %5286 = torch.aten.mul.Scalar %5285, %int4_7954 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_7955 = torch.constant.int 1
    %5287 = torch.aten.add.Tensor %5286, %5253, %int1_7955 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_7956 = torch.constant.int 32
    %5288 = torch.aten.mul.Scalar %5287, %int32_7956 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_7957 = torch.constant.int 1
    %5289 = torch.aten.add.Tensor %5288, %5250, %int1_7957 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_7958 = torch.constant.none
    %none_7959 = torch.constant.none
    %int5_7960 = torch.constant.int 5
    %cpu_7961 = torch.constant.device "cpu"
    %int0_7962 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5160, %none_7958, %none_7959, %int5_7960, %cpu_7961, %int0_7962 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %5290 = torch.prim.ListConstruct %5289 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_7963 = torch.constant.bool false
    %5291 = torch.aten.index_put %5278, %5290, %5160, %false_7963 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %5291, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_7964 = torch.constant.int 22
    %int2_7965 = torch.constant.int 2
    %int4_7966 = torch.constant.int 4
    %int32_7967 = torch.constant.int 32
    %int64_7968 = torch.constant.int 64
    %5292 = torch.prim.ListConstruct %381, %int22_7964, %int2_7965, %int4_7966, %int32_7967, %int64_7968 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5293 = torch.aten.view %5291, %5292 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5293, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_7969 = torch.constant.int 360448
    %5294 = torch.prim.ListConstruct %381, %int360448_7969 : (!torch.int, !torch.int) -> !torch.list<int>
    %5295 = torch.aten.view %5293, %5294 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %5295, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_7970 = torch.constant.none
    %5296 = torch.aten.clone %243, %none_7970 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_7971 = torch.constant.none
    %5297 = torch.aten.clone %244, %none_7971 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_7972 = torch.constant.none
    %5298 = torch.aten.clone %245, %none_7972 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_7973 = torch.constant.int 22
    %int2_7974 = torch.constant.int 2
    %int4_7975 = torch.constant.int 4
    %int32_7976 = torch.constant.int 32
    %int64_7977 = torch.constant.int 64
    %5299 = torch.prim.ListConstruct %381, %int22_7973, %int2_7974, %int4_7975, %int32_7976, %int64_7977 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5300 = torch.aten.view %5295, %5299 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5300, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %5301 = torch_c.to_builtin_tensor %5300 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %5302 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_7978 = tensor.cast %5302 : tensor<4x?xi64> to tensor<?x?xi64>
    %5303 = torch_c.to_builtin_tensor %5296 : !torch.vtensor<[],si64> -> tensor<i64>
    %5304 = torch_c.to_builtin_tensor %5297 : !torch.vtensor<[],si64> -> tensor<i64>
    %5305 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%5301, %cast_7978, %5303, %5304) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_7979 = tensor.cast %5305 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %5306 = torch_c.from_builtin_tensor %cast_7979 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %5306, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %5307 = torch_c.to_builtin_tensor %5300 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %5308 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_7980 = tensor.cast %5308 : tensor<4x?xi64> to tensor<?x?xi64>
    %5309 = torch_c.to_builtin_tensor %5296 : !torch.vtensor<[],si64> -> tensor<i64>
    %5310 = torch_c.to_builtin_tensor %5298 : !torch.vtensor<[],si64> -> tensor<i64>
    %5311 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%5307, %cast_7980, %5309, %5310) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_7981 = tensor.cast %5311 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %5312 = torch_c.from_builtin_tensor %cast_7981 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %5312, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_7982 = torch.constant.int 2
    %int3_7983 = torch.constant.int 3
    %5313 = torch.aten.transpose.int %5306, %int2_7982, %int3_7983 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5313, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_7984 = torch.constant.int 0
    %5314 = torch.aten.clone %5313, %int0_7984 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5314, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_7985 = torch.constant.int 4
    %int4_7986 = torch.constant.int 4
    %int64_7987 = torch.constant.int 64
    %5315 = torch.prim.ListConstruct %int4_7985, %623, %int4_7986, %int64_7987 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5316 = torch.aten._unsafe_view %5314, %5315 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %5316, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_7988 = torch.constant.int 2
    %int3_7989 = torch.constant.int 3
    %5317 = torch.aten.transpose.int %5312, %int2_7988, %int3_7989 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5317, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_7990 = torch.constant.int 0
    %5318 = torch.aten.clone %5317, %int0_7990 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5318, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_7991 = torch.constant.int 4
    %int4_7992 = torch.constant.int 4
    %int64_7993 = torch.constant.int 64
    %5319 = torch.prim.ListConstruct %int4_7991, %623, %int4_7992, %int64_7993 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5320 = torch.aten._unsafe_view %5318, %5319 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %5320, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_7994 = torch.constant.int 0
    %int1_7995 = torch.constant.int 1
    %none_7996 = torch.constant.none
    %none_7997 = torch.constant.none
    %cpu_7998 = torch.constant.device "cpu"
    %false_7999 = torch.constant.bool false
    %5321 = torch.aten.arange.start_step %int0_7994, %623, %int1_7995, %none_7996, %none_7997, %cpu_7998, %false_7999 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5321, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_8000 = torch.constant.int -1
    %5322 = torch.aten.unsqueeze %arg1, %int-1_8000 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %5323 = torch.aten.ge.Tensor %5321, %5322 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %5323, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_8001 = torch.constant.none
    %5324 = torch.aten.clone %246, %none_8001 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_8002 = torch.constant.int 0
    %5325 = torch.aten.where.ScalarOther %5323, %5324, %int0_8002 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %5325, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_8003 = torch.constant.none
    %none_8004 = torch.constant.none
    %int5_8005 = torch.constant.int 5
    %cpu_8006 = torch.constant.device "cpu"
    %int0_8007 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5325, %none_8003, %none_8004, %int5_8005, %cpu_8006, %int0_8007 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_8008 = torch.constant.int 1
    %5326 = torch.aten.unsqueeze %5325, %int1_8008 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %5326, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_8009 = torch.constant.int 1
    %5327 = torch.aten.unsqueeze %5326, %int1_8009 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %5327, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_8010 = torch.constant.int -2
    %5328 = torch.aten.unsqueeze %5316, %int-2_8010 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %5328, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_8011 = torch.constant.int 4
    %int4_8012 = torch.constant.int 4
    %int8_8013 = torch.constant.int 8
    %int64_8014 = torch.constant.int 64
    %5329 = torch.prim.ListConstruct %int4_8011, %623, %int4_8012, %int8_8013, %int64_8014 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8015 = torch.constant.bool false
    %5330 = torch.aten.expand %5328, %5329, %false_8015 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5330, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_8016 = torch.constant.int 0
    %5331 = torch.aten.clone %5330, %int0_8016 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5331, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_8017 = torch.constant.int 4
    %int32_8018 = torch.constant.int 32
    %int64_8019 = torch.constant.int 64
    %5332 = torch.prim.ListConstruct %int4_8017, %623, %int32_8018, %int64_8019 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5333 = torch.aten._unsafe_view %5331, %5332 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5333, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_8020 = torch.constant.int -2
    %5334 = torch.aten.unsqueeze %5320, %int-2_8020 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %5334, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_8021 = torch.constant.int 4
    %int4_8022 = torch.constant.int 4
    %int8_8023 = torch.constant.int 8
    %int64_8024 = torch.constant.int 64
    %5335 = torch.prim.ListConstruct %int4_8021, %623, %int4_8022, %int8_8023, %int64_8024 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8025 = torch.constant.bool false
    %5336 = torch.aten.expand %5334, %5335, %false_8025 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5336, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_8026 = torch.constant.int 0
    %5337 = torch.aten.clone %5336, %int0_8026 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5337, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_8027 = torch.constant.int 4
    %int32_8028 = torch.constant.int 32
    %int64_8029 = torch.constant.int 64
    %5338 = torch.prim.ListConstruct %int4_8027, %623, %int32_8028, %int64_8029 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5339 = torch.aten._unsafe_view %5337, %5338 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5339, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_8030 = torch.constant.int 1
    %int2_8031 = torch.constant.int 2
    %5340 = torch.aten.transpose.int %5201, %int1_8030, %int2_8031 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_8032 = torch.constant.int 1
    %int2_8033 = torch.constant.int 2
    %5341 = torch.aten.transpose.int %5333, %int1_8032, %int2_8033 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %5341, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_8034 = torch.constant.int 1
    %int2_8035 = torch.constant.int 2
    %5342 = torch.aten.transpose.int %5339, %int1_8034, %int2_8035 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %5342, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_8036 = torch.constant.float 0.000000e+00
    %false_8037 = torch.constant.bool false
    %none_8038 = torch.constant.none
    %false_8039 = torch.constant.bool false
    %5343 = torch.aten.scaled_dot_product_attention %5340, %5341, %5342, %5327, %float0.000000e00_8036, %false_8037, %none_8038, %false_8039 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_8040 = torch.constant.int 1
    %int2_8041 = torch.constant.int 2
    %5344 = torch.aten.transpose.int %5343, %int1_8040, %int2_8041 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_8042 = torch.constant.int 4
    %int1_8043 = torch.constant.int 1
    %int2048_8044 = torch.constant.int 2048
    %5345 = torch.prim.ListConstruct %int4_8042, %int1_8043, %int2048_8044 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5346 = torch.aten.view %5344, %5345 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_8045 = torch.constant.int 2
    %5347 = torch.aten.view.dtype %251, %int2_8045 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %5348 = torch.aten.detach %5347 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_8046 = torch.constant.int -1
    %int17_8047 = torch.constant.int 17
    %5349 = torch.prim.ListConstruct %int-1_8046, %int17_8047 : (!torch.int, !torch.int) -> !torch.list<int>
    %5350 = torch.aten.view %5348, %5349 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_8048 = torch.constant.int 2048
    %int-1_8049 = torch.constant.int -1
    %int17_8050 = torch.constant.int 17
    %5351 = torch.prim.ListConstruct %int2048_8048, %int-1_8049, %int17_8050 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5352 = torch.aten.view %5350, %5351 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_8051 = torch.constant.int 2
    %int0_8052 = torch.constant.int 0
    %int1_8053 = torch.constant.int 1
    %int1_8054 = torch.constant.int 1
    %5353 = torch.aten.slice.Tensor %5352, %int2_8051, %int0_8052, %int1_8053, %int1_8054 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_8055 = torch.constant.int 5
    %5354 = torch.aten.view.dtype %5353, %int5_8055 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %5355 = torch.aten.detach %5354 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_8056 = torch.constant.int 2
    %int1_8057 = torch.constant.int 1
    %int9223372036854775807_8058 = torch.constant.int 9223372036854775807
    %int1_8059 = torch.constant.int 1
    %5356 = torch.aten.slice.Tensor %5352, %int2_8056, %int1_8057, %int9223372036854775807_8058, %int1_8059 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_8060 = torch.constant.int 1
    %5357 = torch.aten.view.dtype %5356, %int1_8060 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %5358 = torch.aten.detach %5357 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %5359 = torch_c.to_builtin_tensor %5346 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_8061 = tensor.cast %5359 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %5360 = torch_c.to_builtin_tensor %5355 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %5361 = torch_c.to_builtin_tensor %5358 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %5362 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_8061, %5360, %5361) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_8062 = tensor.cast %5362 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %5363 = torch_c.from_builtin_tensor %cast_8062 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_8063 = torch.constant.none
    %none_8064 = torch.constant.none
    %int5_8065 = torch.constant.int 5
    %cpu_8066 = torch.constant.device "cpu"
    %int0_8067 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5363, %none_8063, %none_8064, %int5_8065, %cpu_8066, %int0_8067 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_8068 = torch.constant.int 1
    %5364 = torch.aten.add.Tensor %5093, %5363, %int1_8068 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_8069 = torch.constant.none
    %none_8070 = torch.constant.none
    %int5_8071 = torch.constant.int 5
    %cpu_8072 = torch.constant.device "cpu"
    %int0_8073 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5364, %none_8069, %none_8070, %int5_8071, %cpu_8072, %int0_8073 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8074 = torch.constant.int 6
    %5365 = torch.prims.convert_element_type %5364, %int6_8074 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_8075 = torch.constant.int 2
    %5366 = torch.aten.pow.Tensor_Scalar %5365, %int2_8075 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_8076 = torch.constant.int -1
    %5367 = torch.prim.ListConstruct %int-1_8076 : (!torch.int) -> !torch.list<int>
    %true_8077 = torch.constant.bool true
    %none_8078 = torch.constant.none
    %5368 = torch.aten.mean.dim %5366, %5367, %true_8077, %none_8078 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_8079 = torch.constant.float 9.9999997473787516E-6
    %int1_8080 = torch.constant.int 1
    %5369 = torch.aten.add.Scalar %5368, %float9.999990e-06_8079, %int1_8080 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5370 = torch.aten.rsqrt %5369 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %5371 = torch.aten.mul.Tensor %5365, %5370 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_8081 = torch.constant.none
    %none_8082 = torch.constant.none
    %int6_8083 = torch.constant.int 6
    %cpu_8084 = torch.constant.device "cpu"
    %int0_8085 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5371, %none_8081, %none_8082, %int6_8083, %cpu_8084, %int0_8085 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8086 = torch.constant.int 5
    %5372 = torch.prims.convert_element_type %5371, %int5_8086 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %5373 = torch.aten.mul.Tensor %252, %5372 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_8087 = torch.constant.none
    %none_8088 = torch.constant.none
    %int6_8089 = torch.constant.int 6
    %cpu_8090 = torch.constant.device "cpu"
    %int0_8091 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5373, %none_8087, %none_8088, %int6_8089, %cpu_8090, %int0_8091 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8092 = torch.constant.int 5
    %5374 = torch.prims.convert_element_type %5373, %int5_8092 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_8093 = torch.constant.int 2
    %5375 = torch.aten.view.dtype %253, %int2_8093 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %5376 = torch.aten.detach %5375 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_8094 = torch.constant.int -1
    %int17_8095 = torch.constant.int 17
    %5377 = torch.prim.ListConstruct %int-1_8094, %int17_8095 : (!torch.int, !torch.int) -> !torch.list<int>
    %5378 = torch.aten.view %5376, %5377 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_8096 = torch.constant.int 5632
    %int-1_8097 = torch.constant.int -1
    %int17_8098 = torch.constant.int 17
    %5379 = torch.prim.ListConstruct %int5632_8096, %int-1_8097, %int17_8098 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5380 = torch.aten.view %5378, %5379 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_8099 = torch.constant.int 2
    %int0_8100 = torch.constant.int 0
    %int1_8101 = torch.constant.int 1
    %int1_8102 = torch.constant.int 1
    %5381 = torch.aten.slice.Tensor %5380, %int2_8099, %int0_8100, %int1_8101, %int1_8102 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_8103 = torch.constant.int 5
    %5382 = torch.aten.view.dtype %5381, %int5_8103 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %5383 = torch.aten.detach %5382 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_8104 = torch.constant.int 2
    %int1_8105 = torch.constant.int 1
    %int9223372036854775807_8106 = torch.constant.int 9223372036854775807
    %int1_8107 = torch.constant.int 1
    %5384 = torch.aten.slice.Tensor %5380, %int2_8104, %int1_8105, %int9223372036854775807_8106, %int1_8107 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_8108 = torch.constant.int 1
    %5385 = torch.aten.view.dtype %5384, %int1_8108 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %5386 = torch.aten.detach %5385 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %5387 = torch_c.to_builtin_tensor %5374 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_8109 = tensor.cast %5387 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %5388 = torch_c.to_builtin_tensor %5383 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %5389 = torch_c.to_builtin_tensor %5386 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %5390 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_8109, %5388, %5389) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_8110 = tensor.cast %5390 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %5391 = torch_c.from_builtin_tensor %cast_8110 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %5392 = torch.aten.silu %5391 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_8111 = torch.constant.int 2
    %5393 = torch.aten.view.dtype %254, %int2_8111 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %5394 = torch.aten.detach %5393 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_8112 = torch.constant.int -1
    %int17_8113 = torch.constant.int 17
    %5395 = torch.prim.ListConstruct %int-1_8112, %int17_8113 : (!torch.int, !torch.int) -> !torch.list<int>
    %5396 = torch.aten.view %5394, %5395 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_8114 = torch.constant.int 5632
    %int-1_8115 = torch.constant.int -1
    %int17_8116 = torch.constant.int 17
    %5397 = torch.prim.ListConstruct %int5632_8114, %int-1_8115, %int17_8116 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5398 = torch.aten.view %5396, %5397 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_8117 = torch.constant.int 2
    %int0_8118 = torch.constant.int 0
    %int1_8119 = torch.constant.int 1
    %int1_8120 = torch.constant.int 1
    %5399 = torch.aten.slice.Tensor %5398, %int2_8117, %int0_8118, %int1_8119, %int1_8120 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_8121 = torch.constant.int 5
    %5400 = torch.aten.view.dtype %5399, %int5_8121 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %5401 = torch.aten.detach %5400 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_8122 = torch.constant.int 2
    %int1_8123 = torch.constant.int 1
    %int9223372036854775807_8124 = torch.constant.int 9223372036854775807
    %int1_8125 = torch.constant.int 1
    %5402 = torch.aten.slice.Tensor %5398, %int2_8122, %int1_8123, %int9223372036854775807_8124, %int1_8125 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_8126 = torch.constant.int 1
    %5403 = torch.aten.view.dtype %5402, %int1_8126 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %5404 = torch.aten.detach %5403 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %5405 = torch_c.to_builtin_tensor %5374 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_8127 = tensor.cast %5405 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %5406 = torch_c.to_builtin_tensor %5401 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %5407 = torch_c.to_builtin_tensor %5404 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %5408 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_8127, %5406, %5407) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_8128 = tensor.cast %5408 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %5409 = torch_c.from_builtin_tensor %cast_8128 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %5410 = torch.aten.mul.Tensor %5392, %5409 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_8129 = torch.constant.int 2
    %5411 = torch.aten.view.dtype %255, %int2_8129 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %5412 = torch.aten.detach %5411 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_8130 = torch.constant.int -1
    %int17_8131 = torch.constant.int 17
    %5413 = torch.prim.ListConstruct %int-1_8130, %int17_8131 : (!torch.int, !torch.int) -> !torch.list<int>
    %5414 = torch.aten.view %5412, %5413 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_8132 = torch.constant.int 2048
    %int-1_8133 = torch.constant.int -1
    %int17_8134 = torch.constant.int 17
    %5415 = torch.prim.ListConstruct %int2048_8132, %int-1_8133, %int17_8134 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5416 = torch.aten.view %5414, %5415 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_8135 = torch.constant.int 2
    %int0_8136 = torch.constant.int 0
    %int1_8137 = torch.constant.int 1
    %int1_8138 = torch.constant.int 1
    %5417 = torch.aten.slice.Tensor %5416, %int2_8135, %int0_8136, %int1_8137, %int1_8138 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_8139 = torch.constant.int 5
    %5418 = torch.aten.view.dtype %5417, %int5_8139 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %5419 = torch.aten.detach %5418 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_8140 = torch.constant.int 2
    %int1_8141 = torch.constant.int 1
    %int9223372036854775807_8142 = torch.constant.int 9223372036854775807
    %int1_8143 = torch.constant.int 1
    %5420 = torch.aten.slice.Tensor %5416, %int2_8140, %int1_8141, %int9223372036854775807_8142, %int1_8143 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_8144 = torch.constant.int 1
    %5421 = torch.aten.view.dtype %5420, %int1_8144 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %5422 = torch.aten.detach %5421 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %5423 = torch_c.to_builtin_tensor %5410 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_8145 = tensor.cast %5423 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %5424 = torch_c.to_builtin_tensor %5419 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %5425 = torch_c.to_builtin_tensor %5422 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %5426 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_8145, %5424, %5425) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_8146 = tensor.cast %5426 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %5427 = torch_c.from_builtin_tensor %cast_8146 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_8147 = torch.constant.int 1
    %5428 = torch.aten.add.Tensor %5364, %5427, %int1_8147 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_8148 = torch.constant.none
    %none_8149 = torch.constant.none
    %int5_8150 = torch.constant.int 5
    %cpu_8151 = torch.constant.device "cpu"
    %int0_8152 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5428, %none_8148, %none_8149, %int5_8150, %cpu_8151, %int0_8152 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8153 = torch.constant.int 6
    %5429 = torch.prims.convert_element_type %5428, %int6_8153 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_8154 = torch.constant.int 2
    %5430 = torch.aten.pow.Tensor_Scalar %5429, %int2_8154 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_8155 = torch.constant.int -1
    %5431 = torch.prim.ListConstruct %int-1_8155 : (!torch.int) -> !torch.list<int>
    %true_8156 = torch.constant.bool true
    %none_8157 = torch.constant.none
    %5432 = torch.aten.mean.dim %5430, %5431, %true_8156, %none_8157 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_8158 = torch.constant.float 9.9999997473787516E-6
    %int1_8159 = torch.constant.int 1
    %5433 = torch.aten.add.Scalar %5432, %float9.999990e-06_8158, %int1_8159 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5434 = torch.aten.rsqrt %5433 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %5435 = torch.aten.mul.Tensor %5429, %5434 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_8160 = torch.constant.none
    %none_8161 = torch.constant.none
    %int6_8162 = torch.constant.int 6
    %cpu_8163 = torch.constant.device "cpu"
    %int0_8164 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5435, %none_8160, %none_8161, %int6_8162, %cpu_8163, %int0_8164 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8165 = torch.constant.int 5
    %5436 = torch.prims.convert_element_type %5435, %int5_8165 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %5437 = torch.aten.mul.Tensor %264, %5436 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_8166 = torch.constant.none
    %none_8167 = torch.constant.none
    %int6_8168 = torch.constant.int 6
    %cpu_8169 = torch.constant.device "cpu"
    %int0_8170 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5437, %none_8166, %none_8167, %int6_8168, %cpu_8169, %int0_8170 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8171 = torch.constant.int 5
    %5438 = torch.prims.convert_element_type %5437, %int5_8171 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_8172 = torch.constant.int 2
    %5439 = torch.aten.view.dtype %265, %int2_8172 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %5440 = torch.aten.detach %5439 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_8173 = torch.constant.int -1
    %int17_8174 = torch.constant.int 17
    %5441 = torch.prim.ListConstruct %int-1_8173, %int17_8174 : (!torch.int, !torch.int) -> !torch.list<int>
    %5442 = torch.aten.view %5440, %5441 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_8175 = torch.constant.int 2048
    %int-1_8176 = torch.constant.int -1
    %int17_8177 = torch.constant.int 17
    %5443 = torch.prim.ListConstruct %int2048_8175, %int-1_8176, %int17_8177 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5444 = torch.aten.view %5442, %5443 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_8178 = torch.constant.int 2
    %int0_8179 = torch.constant.int 0
    %int1_8180 = torch.constant.int 1
    %int1_8181 = torch.constant.int 1
    %5445 = torch.aten.slice.Tensor %5444, %int2_8178, %int0_8179, %int1_8180, %int1_8181 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_8182 = torch.constant.int 5
    %5446 = torch.aten.view.dtype %5445, %int5_8182 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %5447 = torch.aten.detach %5446 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_8183 = torch.constant.int 2
    %int1_8184 = torch.constant.int 1
    %int9223372036854775807_8185 = torch.constant.int 9223372036854775807
    %int1_8186 = torch.constant.int 1
    %5448 = torch.aten.slice.Tensor %5444, %int2_8183, %int1_8184, %int9223372036854775807_8185, %int1_8186 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_8187 = torch.constant.int 1
    %5449 = torch.aten.view.dtype %5448, %int1_8187 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %5450 = torch.aten.detach %5449 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %5451 = torch_c.to_builtin_tensor %5438 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_8188 = tensor.cast %5451 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %5452 = torch_c.to_builtin_tensor %5447 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %5453 = torch_c.to_builtin_tensor %5450 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %5454 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_8188, %5452, %5453) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_8189 = tensor.cast %5454 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %5455 = torch_c.from_builtin_tensor %cast_8189 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_8190 = torch.constant.int 2
    %5456 = torch.aten.view.dtype %266, %int2_8190 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %5457 = torch.aten.detach %5456 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_8191 = torch.constant.int -1
    %int17_8192 = torch.constant.int 17
    %5458 = torch.prim.ListConstruct %int-1_8191, %int17_8192 : (!torch.int, !torch.int) -> !torch.list<int>
    %5459 = torch.aten.view %5457, %5458 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_8193 = torch.constant.int 256
    %int-1_8194 = torch.constant.int -1
    %int17_8195 = torch.constant.int 17
    %5460 = torch.prim.ListConstruct %int256_8193, %int-1_8194, %int17_8195 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5461 = torch.aten.view %5459, %5460 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_8196 = torch.constant.int 2
    %int0_8197 = torch.constant.int 0
    %int1_8198 = torch.constant.int 1
    %int1_8199 = torch.constant.int 1
    %5462 = torch.aten.slice.Tensor %5461, %int2_8196, %int0_8197, %int1_8198, %int1_8199 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_8200 = torch.constant.int 5
    %5463 = torch.aten.view.dtype %5462, %int5_8200 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %5464 = torch.aten.detach %5463 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_8201 = torch.constant.int 2
    %int1_8202 = torch.constant.int 1
    %int9223372036854775807_8203 = torch.constant.int 9223372036854775807
    %int1_8204 = torch.constant.int 1
    %5465 = torch.aten.slice.Tensor %5461, %int2_8201, %int1_8202, %int9223372036854775807_8203, %int1_8204 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_8205 = torch.constant.int 1
    %5466 = torch.aten.view.dtype %5465, %int1_8205 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %5467 = torch.aten.detach %5466 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %5468 = torch_c.to_builtin_tensor %5438 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_8206 = tensor.cast %5468 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %5469 = torch_c.to_builtin_tensor %5464 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %5470 = torch_c.to_builtin_tensor %5467 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %5471 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_8206, %5469, %5470) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_8207 = tensor.cast %5471 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %5472 = torch_c.from_builtin_tensor %cast_8207 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_8208 = torch.constant.int 2
    %5473 = torch.aten.view.dtype %267, %int2_8208 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %5474 = torch.aten.detach %5473 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_8209 = torch.constant.int -1
    %int17_8210 = torch.constant.int 17
    %5475 = torch.prim.ListConstruct %int-1_8209, %int17_8210 : (!torch.int, !torch.int) -> !torch.list<int>
    %5476 = torch.aten.view %5474, %5475 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_8211 = torch.constant.int 256
    %int-1_8212 = torch.constant.int -1
    %int17_8213 = torch.constant.int 17
    %5477 = torch.prim.ListConstruct %int256_8211, %int-1_8212, %int17_8213 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5478 = torch.aten.view %5476, %5477 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_8214 = torch.constant.int 2
    %int0_8215 = torch.constant.int 0
    %int1_8216 = torch.constant.int 1
    %int1_8217 = torch.constant.int 1
    %5479 = torch.aten.slice.Tensor %5478, %int2_8214, %int0_8215, %int1_8216, %int1_8217 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_8218 = torch.constant.int 5
    %5480 = torch.aten.view.dtype %5479, %int5_8218 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %5481 = torch.aten.detach %5480 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_8219 = torch.constant.int 2
    %int1_8220 = torch.constant.int 1
    %int9223372036854775807_8221 = torch.constant.int 9223372036854775807
    %int1_8222 = torch.constant.int 1
    %5482 = torch.aten.slice.Tensor %5478, %int2_8219, %int1_8220, %int9223372036854775807_8221, %int1_8222 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_8223 = torch.constant.int 1
    %5483 = torch.aten.view.dtype %5482, %int1_8223 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %5484 = torch.aten.detach %5483 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %5485 = torch_c.to_builtin_tensor %5438 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_8224 = tensor.cast %5485 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %5486 = torch_c.to_builtin_tensor %5481 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %5487 = torch_c.to_builtin_tensor %5484 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %5488 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_8224, %5486, %5487) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_8225 = tensor.cast %5488 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %5489 = torch_c.from_builtin_tensor %cast_8225 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_8226 = torch.constant.int 4
    %int1_8227 = torch.constant.int 1
    %int32_8228 = torch.constant.int 32
    %int64_8229 = torch.constant.int 64
    %5490 = torch.prim.ListConstruct %int4_8226, %int1_8227, %int32_8228, %int64_8229 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5491 = torch.aten.view %5455, %5490 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_8230 = torch.constant.int 4
    %int1_8231 = torch.constant.int 1
    %int4_8232 = torch.constant.int 4
    %int64_8233 = torch.constant.int 64
    %5492 = torch.prim.ListConstruct %int4_8230, %int1_8231, %int4_8232, %int64_8233 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5493 = torch.aten.view %5472, %5492 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_8234 = torch.constant.int 4
    %int1_8235 = torch.constant.int 1
    %int4_8236 = torch.constant.int 4
    %int64_8237 = torch.constant.int 64
    %5494 = torch.prim.ListConstruct %int4_8234, %int1_8235, %int4_8236, %int64_8237 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5495 = torch.aten.view %5489, %5494 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_8238 = torch.constant.int 0
    %int1_8239 = torch.constant.int 1
    %none_8240 = torch.constant.none
    %none_8241 = torch.constant.none
    %cpu_8242 = torch.constant.device "cpu"
    %false_8243 = torch.constant.bool false
    %5496 = torch.aten.arange.start %int0_8238, %int1_8239, %none_8240, %none_8241, %cpu_8242, %false_8243 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_8244 = torch.constant.int 0
    %5497 = torch.aten.unsqueeze %5496, %int0_8244 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_8245 = torch.constant.int 1
    %5498 = torch.aten.unsqueeze %arg2, %int1_8245 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8246 = torch.constant.int 1
    %5499 = torch.aten.add.Tensor %5497, %5498, %int1_8246 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_8247 = torch.constant.int 0
    %int64_8248 = torch.constant.int 64
    %int2_8249 = torch.constant.int 2
    %none_8250 = torch.constant.none
    %none_8251 = torch.constant.none
    %cpu_8252 = torch.constant.device "cpu"
    %false_8253 = torch.constant.bool false
    %5500 = torch.aten.arange.start_step %int0_8247, %int64_8248, %int2_8249, %none_8250, %none_8251, %cpu_8252, %false_8253 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_8254 = torch.constant.none
    %none_8255 = torch.constant.none
    %int4_8256 = torch.constant.int 4
    %cpu_8257 = torch.constant.device "cpu"
    %int0_8258 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5500, %none_8254, %none_8255, %int4_8256, %cpu_8257, %int0_8258 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8259 = torch.constant.int 6
    %5501 = torch.prims.convert_element_type %5500, %int6_8259 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_8260 = torch.constant.int 64
    %5502 = torch.aten.div.Scalar %5501, %int64_8260 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_8261 = torch.constant.float 1.000000e+04
    %5503 = torch.aten.pow.Scalar %float1.000000e04_8261, %5502 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %5504 = torch.aten.reciprocal %5503 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_8262 = torch.constant.float 1.000000e+00
    %5505 = torch.aten.mul.Scalar %5504, %float1.000000e00_8262 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_8263 = torch.constant.none
    %5506 = torch.aten.clone %256, %none_8263 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_8264 = torch.constant.int 0
    %5507 = torch.aten.unsqueeze %5505, %int0_8264 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_8265 = torch.constant.int 2
    %5508 = torch.aten.unsqueeze %5507, %int2_8265 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_8266 = torch.constant.none
    %none_8267 = torch.constant.none
    %int6_8268 = torch.constant.int 6
    %cpu_8269 = torch.constant.device "cpu"
    %int0_8270 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5508, %none_8266, %none_8267, %int6_8268, %cpu_8269, %int0_8270 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_8271 = torch.constant.int 4
    %int-1_8272 = torch.constant.int -1
    %int1_8273 = torch.constant.int 1
    %5509 = torch.prim.ListConstruct %int4_8271, %int-1_8272, %int1_8273 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8274 = torch.constant.bool false
    %5510 = torch.aten.expand %5508, %5509, %false_8274 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_8275 = torch.constant.int 1
    %5511 = torch.aten.unsqueeze %5499, %int1_8275 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_8276 = torch.constant.none
    %none_8277 = torch.constant.none
    %int4_8278 = torch.constant.int 4
    %cpu_8279 = torch.constant.device "cpu"
    %int0_8280 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5511, %none_8276, %none_8277, %int4_8278, %cpu_8279, %int0_8280 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8281 = torch.constant.int 6
    %5512 = torch.prims.convert_element_type %5511, %int6_8281 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5513 = torch.aten.matmul %5510, %5512 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_8282 = torch.constant.int 1
    %int2_8283 = torch.constant.int 2
    %5514 = torch.aten.transpose.int %5513, %int1_8282, %int2_8283 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %5515 = torch.aten.cos %5514 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %5516 = torch.aten.mul.Tensor %5515, %5506 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_8284 = torch.constant.none
    %none_8285 = torch.constant.none
    %int6_8286 = torch.constant.int 6
    %cpu_8287 = torch.constant.device "cpu"
    %int0_8288 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5516, %none_8284, %none_8285, %int6_8286, %cpu_8287, %int0_8288 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8289 = torch.constant.int 5
    %5517 = torch.prims.convert_element_type %5516, %int5_8289 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %5518 = torch.aten.sin %5514 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %5519 = torch.aten.mul.Tensor %5518, %5506 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_8290 = torch.constant.none
    %none_8291 = torch.constant.none
    %int6_8292 = torch.constant.int 6
    %cpu_8293 = torch.constant.device "cpu"
    %int0_8294 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5519, %none_8290, %none_8291, %int6_8292, %cpu_8293, %int0_8294 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8295 = torch.constant.int 5
    %5520 = torch.prims.convert_element_type %5519, %int5_8295 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_8296 = torch.constant.int 2
    %5521 = torch.aten.unsqueeze %5517, %int2_8296 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_8297 = torch.constant.int 2
    %5522 = torch.aten.unsqueeze %5520, %int2_8297 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_8298 = torch.constant.none
    %none_8299 = torch.constant.none
    %int5_8300 = torch.constant.int 5
    %cpu_8301 = torch.constant.device "cpu"
    %int0_8302 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5521, %none_8298, %none_8299, %int5_8300, %cpu_8301, %int0_8302 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_8303 = torch.constant.none
    %none_8304 = torch.constant.none
    %int5_8305 = torch.constant.int 5
    %cpu_8306 = torch.constant.device "cpu"
    %int0_8307 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5522, %none_8303, %none_8304, %int5_8305, %cpu_8306, %int0_8307 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_8308 = torch.constant.none
    %none_8309 = torch.constant.none
    %int5_8310 = torch.constant.int 5
    %cpu_8311 = torch.constant.device "cpu"
    %int0_8312 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5491, %none_8308, %none_8309, %int5_8310, %cpu_8311, %int0_8312 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_8313 = torch.constant.int 3
    %int0_8314 = torch.constant.int 0
    %int64_8315 = torch.constant.int 64
    %int2_8316 = torch.constant.int 2
    %5523 = torch.aten.slice.Tensor %5491, %int3_8313, %int0_8314, %int64_8315, %int2_8316 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_8317 = torch.constant.int 3
    %int1_8318 = torch.constant.int 1
    %int64_8319 = torch.constant.int 64
    %int2_8320 = torch.constant.int 2
    %5524 = torch.aten.slice.Tensor %5491, %int3_8317, %int1_8318, %int64_8319, %int2_8320 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %5525 = torch.aten.mul.Tensor %5523, %5521 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %5526 = torch.aten.mul.Tensor %5524, %5522 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_8321 = torch.constant.int 1
    %5527 = torch.aten.sub.Tensor %5525, %5526, %int1_8321 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %5528 = torch.aten.mul.Tensor %5524, %5521 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %5529 = torch.aten.mul.Tensor %5523, %5522 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_8322 = torch.constant.int 1
    %5530 = torch.aten.add.Tensor %5528, %5529, %int1_8322 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %5531 = torch_c.to_builtin_tensor %5527 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_8323 = tensor.cast %5531 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %5532 = torch_c.to_builtin_tensor %5530 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_8324 = tensor.cast %5532 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %5533 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_8323, %cast_8324) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_8325 = tensor.cast %5533 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %5534 = torch_c.from_builtin_tensor %cast_8325 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_8326 = torch.constant.int 4
    %int1_8327 = torch.constant.int 1
    %int32_8328 = torch.constant.int 32
    %int64_8329 = torch.constant.int 64
    %5535 = torch.prim.ListConstruct %int4_8326, %int1_8327, %int32_8328, %int64_8329 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5536 = torch.aten.view %5534, %5535 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_8330 = torch.constant.none
    %none_8331 = torch.constant.none
    %int5_8332 = torch.constant.int 5
    %cpu_8333 = torch.constant.device "cpu"
    %int0_8334 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5536, %none_8330, %none_8331, %int5_8332, %cpu_8333, %int0_8334 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_8335 = torch.constant.int 0
    %int1_8336 = torch.constant.int 1
    %none_8337 = torch.constant.none
    %none_8338 = torch.constant.none
    %cpu_8339 = torch.constant.device "cpu"
    %false_8340 = torch.constant.bool false
    %5537 = torch.aten.arange.start %int0_8335, %int1_8336, %none_8337, %none_8338, %cpu_8339, %false_8340 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_8341 = torch.constant.int 0
    %5538 = torch.aten.unsqueeze %5537, %int0_8341 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_8342 = torch.constant.int 1
    %5539 = torch.aten.unsqueeze %arg2, %int1_8342 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8343 = torch.constant.int 1
    %5540 = torch.aten.add.Tensor %5538, %5539, %int1_8343 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_8344 = torch.constant.int 0
    %int64_8345 = torch.constant.int 64
    %int2_8346 = torch.constant.int 2
    %none_8347 = torch.constant.none
    %none_8348 = torch.constant.none
    %cpu_8349 = torch.constant.device "cpu"
    %false_8350 = torch.constant.bool false
    %5541 = torch.aten.arange.start_step %int0_8344, %int64_8345, %int2_8346, %none_8347, %none_8348, %cpu_8349, %false_8350 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_8351 = torch.constant.none
    %none_8352 = torch.constant.none
    %int4_8353 = torch.constant.int 4
    %cpu_8354 = torch.constant.device "cpu"
    %int0_8355 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5541, %none_8351, %none_8352, %int4_8353, %cpu_8354, %int0_8355 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8356 = torch.constant.int 6
    %5542 = torch.prims.convert_element_type %5541, %int6_8356 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_8357 = torch.constant.int 64
    %5543 = torch.aten.div.Scalar %5542, %int64_8357 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_8358 = torch.constant.float 1.000000e+04
    %5544 = torch.aten.pow.Scalar %float1.000000e04_8358, %5543 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %5545 = torch.aten.reciprocal %5544 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_8359 = torch.constant.float 1.000000e+00
    %5546 = torch.aten.mul.Scalar %5545, %float1.000000e00_8359 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_8360 = torch.constant.none
    %5547 = torch.aten.clone %257, %none_8360 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_8361 = torch.constant.int 0
    %5548 = torch.aten.unsqueeze %5546, %int0_8361 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_8362 = torch.constant.int 2
    %5549 = torch.aten.unsqueeze %5548, %int2_8362 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_8363 = torch.constant.none
    %none_8364 = torch.constant.none
    %int6_8365 = torch.constant.int 6
    %cpu_8366 = torch.constant.device "cpu"
    %int0_8367 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5549, %none_8363, %none_8364, %int6_8365, %cpu_8366, %int0_8367 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_8368 = torch.constant.int 4
    %int-1_8369 = torch.constant.int -1
    %int1_8370 = torch.constant.int 1
    %5550 = torch.prim.ListConstruct %int4_8368, %int-1_8369, %int1_8370 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8371 = torch.constant.bool false
    %5551 = torch.aten.expand %5549, %5550, %false_8371 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_8372 = torch.constant.int 1
    %5552 = torch.aten.unsqueeze %5540, %int1_8372 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_8373 = torch.constant.none
    %none_8374 = torch.constant.none
    %int4_8375 = torch.constant.int 4
    %cpu_8376 = torch.constant.device "cpu"
    %int0_8377 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5552, %none_8373, %none_8374, %int4_8375, %cpu_8376, %int0_8377 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8378 = torch.constant.int 6
    %5553 = torch.prims.convert_element_type %5552, %int6_8378 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5554 = torch.aten.matmul %5551, %5553 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_8379 = torch.constant.int 1
    %int2_8380 = torch.constant.int 2
    %5555 = torch.aten.transpose.int %5554, %int1_8379, %int2_8380 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %5556 = torch.aten.cos %5555 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %5557 = torch.aten.mul.Tensor %5556, %5547 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_8381 = torch.constant.none
    %none_8382 = torch.constant.none
    %int6_8383 = torch.constant.int 6
    %cpu_8384 = torch.constant.device "cpu"
    %int0_8385 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5557, %none_8381, %none_8382, %int6_8383, %cpu_8384, %int0_8385 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8386 = torch.constant.int 5
    %5558 = torch.prims.convert_element_type %5557, %int5_8386 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %5559 = torch.aten.sin %5555 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %5560 = torch.aten.mul.Tensor %5559, %5547 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_8387 = torch.constant.none
    %none_8388 = torch.constant.none
    %int6_8389 = torch.constant.int 6
    %cpu_8390 = torch.constant.device "cpu"
    %int0_8391 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5560, %none_8387, %none_8388, %int6_8389, %cpu_8390, %int0_8391 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8392 = torch.constant.int 5
    %5561 = torch.prims.convert_element_type %5560, %int5_8392 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_8393 = torch.constant.int 2
    %5562 = torch.aten.unsqueeze %5558, %int2_8393 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_8394 = torch.constant.int 2
    %5563 = torch.aten.unsqueeze %5561, %int2_8394 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_8395 = torch.constant.none
    %none_8396 = torch.constant.none
    %int5_8397 = torch.constant.int 5
    %cpu_8398 = torch.constant.device "cpu"
    %int0_8399 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5562, %none_8395, %none_8396, %int5_8397, %cpu_8398, %int0_8399 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_8400 = torch.constant.none
    %none_8401 = torch.constant.none
    %int5_8402 = torch.constant.int 5
    %cpu_8403 = torch.constant.device "cpu"
    %int0_8404 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5563, %none_8400, %none_8401, %int5_8402, %cpu_8403, %int0_8404 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_8405 = torch.constant.none
    %none_8406 = torch.constant.none
    %int5_8407 = torch.constant.int 5
    %cpu_8408 = torch.constant.device "cpu"
    %int0_8409 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5493, %none_8405, %none_8406, %int5_8407, %cpu_8408, %int0_8409 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_8410 = torch.constant.int 3
    %int0_8411 = torch.constant.int 0
    %int64_8412 = torch.constant.int 64
    %int2_8413 = torch.constant.int 2
    %5564 = torch.aten.slice.Tensor %5493, %int3_8410, %int0_8411, %int64_8412, %int2_8413 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_8414 = torch.constant.int 3
    %int1_8415 = torch.constant.int 1
    %int64_8416 = torch.constant.int 64
    %int2_8417 = torch.constant.int 2
    %5565 = torch.aten.slice.Tensor %5493, %int3_8414, %int1_8415, %int64_8416, %int2_8417 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %5566 = torch.aten.mul.Tensor %5564, %5562 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %5567 = torch.aten.mul.Tensor %5565, %5563 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_8418 = torch.constant.int 1
    %5568 = torch.aten.sub.Tensor %5566, %5567, %int1_8418 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %5569 = torch.aten.mul.Tensor %5565, %5562 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %5570 = torch.aten.mul.Tensor %5564, %5563 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_8419 = torch.constant.int 1
    %5571 = torch.aten.add.Tensor %5569, %5570, %int1_8419 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %5572 = torch_c.to_builtin_tensor %5568 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_8420 = tensor.cast %5572 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %5573 = torch_c.to_builtin_tensor %5571 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_8421 = tensor.cast %5573 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %5574 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_8420, %cast_8421) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_8422 = tensor.cast %5574 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %5575 = torch_c.from_builtin_tensor %cast_8422 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_8423 = torch.constant.int 4
    %int1_8424 = torch.constant.int 1
    %int4_8425 = torch.constant.int 4
    %int64_8426 = torch.constant.int 64
    %5576 = torch.prim.ListConstruct %int4_8423, %int1_8424, %int4_8425, %int64_8426 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5577 = torch.aten.view %5575, %5576 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_8427 = torch.constant.none
    %none_8428 = torch.constant.none
    %int5_8429 = torch.constant.int 5
    %cpu_8430 = torch.constant.device "cpu"
    %int0_8431 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5577, %none_8427, %none_8428, %int5_8429, %cpu_8430, %int0_8431 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_8432 = torch.constant.int 32
    %5578 = torch.aten.floor_divide.Scalar %arg2, %int32_8432 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_8433 = torch.constant.int 1
    %5579 = torch.aten.unsqueeze %5578, %int1_8433 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8434 = torch.constant.int 1
    %false_8435 = torch.constant.bool false
    %5580 = torch.aten.gather %arg3, %int1_8434, %5579, %false_8435 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_8436 = torch.constant.int 4
    %int1_8437 = torch.constant.int 1
    %int1_8438 = torch.constant.int 1
    %5581 = torch.prim.ListConstruct %int4_8436, %int1_8437, %int1_8438 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5582 = torch.aten.view %5580, %5581 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_8439 = torch.constant.int 32
    %5583 = torch.aten.remainder.Scalar %arg2, %int32_8439 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_8440 = torch.constant.int 4
    %int1_8441 = torch.constant.int 1
    %int1_8442 = torch.constant.int 1
    %5584 = torch.prim.ListConstruct %int4_8440, %int1_8441, %int1_8442 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5585 = torch.aten.view %5583, %5584 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_8443 = torch.constant.int 4
    %none_8444 = torch.constant.none
    %none_8445 = torch.constant.none
    %cpu_8446 = torch.constant.device "cpu"
    %false_8447 = torch.constant.bool false
    %5586 = torch.aten.arange %int4_8443, %none_8444, %none_8445, %cpu_8446, %false_8447 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_8448 = torch.constant.int 1
    %int1_8449 = torch.constant.int 1
    %int4_8450 = torch.constant.int 4
    %5587 = torch.prim.ListConstruct %int1_8448, %int1_8449, %int4_8450 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5588 = torch.aten.view %5586, %5587 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_8451 = torch.constant.none
    %5589 = torch.aten.clone %258, %none_8451 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_8452 = torch.constant.int 1
    %int1_8453 = torch.constant.int 1
    %int1_8454 = torch.constant.int 1
    %5590 = torch.prim.ListConstruct %int1_8452, %int1_8453, %int1_8454 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5591 = torch.aten.view %5589, %5590 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_8455 = torch.constant.int 22
    %5592 = torch.aten.mul.Scalar %5582, %int22_8455 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int15 = torch.constant.int 15
    %int1_8456 = torch.constant.int 1
    %5593 = torch.aten.add.Scalar %5592, %int15, %int1_8456 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_8457 = torch.constant.int 2
    %5594 = torch.aten.mul.Scalar %5593, %int2_8457 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_8458 = torch.constant.int 1
    %5595 = torch.aten.add.Tensor %5594, %5591, %int1_8458 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_8459 = torch.constant.int 4
    %5596 = torch.aten.mul.Scalar %5595, %int4_8459 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_8460 = torch.constant.int 1
    %5597 = torch.aten.add.Tensor %5596, %5588, %int1_8460 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_8461 = torch.constant.int 32
    %5598 = torch.aten.mul.Scalar %5597, %int32_8461 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_8462 = torch.constant.int 1
    %5599 = torch.aten.add.Tensor %5598, %5585, %int1_8462 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_8463 = torch.constant.none
    %none_8464 = torch.constant.none
    %int5_8465 = torch.constant.int 5
    %cpu_8466 = torch.constant.device "cpu"
    %int0_8467 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5577, %none_8463, %none_8464, %int5_8465, %cpu_8466, %int0_8467 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_8468 = torch.constant.int 22
    %int2_8469 = torch.constant.int 2
    %int4_8470 = torch.constant.int 4
    %int32_8471 = torch.constant.int 32
    %int64_8472 = torch.constant.int 64
    %5600 = torch.prim.ListConstruct %381, %int22_8468, %int2_8469, %int4_8470, %int32_8471, %int64_8472 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5601 = torch.aten.view %5295, %5600 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5601, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_8473 = torch.constant.int 64
    %5602 = torch.prim.ListConstruct %553, %int64_8473 : (!torch.int, !torch.int) -> !torch.list<int>
    %5603 = torch.aten.view %5601, %5602 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %5603, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %5604 = torch.prim.ListConstruct %5599 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_8474 = torch.constant.bool false
    %5605 = torch.aten.index_put %5603, %5604, %5577, %false_8474 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %5605, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_8475 = torch.constant.int 22
    %int2_8476 = torch.constant.int 2
    %int4_8477 = torch.constant.int 4
    %int32_8478 = torch.constant.int 32
    %int64_8479 = torch.constant.int 64
    %5606 = torch.prim.ListConstruct %381, %int22_8475, %int2_8476, %int4_8477, %int32_8478, %int64_8479 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5607 = torch.aten.view %5605, %5606 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5607, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_8480 = torch.constant.int 360448
    %5608 = torch.prim.ListConstruct %381, %int360448_8480 : (!torch.int, !torch.int) -> !torch.list<int>
    %5609 = torch.aten.view %5607, %5608 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %5609, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_8481 = torch.constant.int 22
    %int2_8482 = torch.constant.int 2
    %int4_8483 = torch.constant.int 4
    %int32_8484 = torch.constant.int 32
    %int64_8485 = torch.constant.int 64
    %5610 = torch.prim.ListConstruct %381, %int22_8481, %int2_8482, %int4_8483, %int32_8484, %int64_8485 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5611 = torch.aten.view %5609, %5610 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5611, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_8486 = torch.constant.int 64
    %5612 = torch.prim.ListConstruct %553, %int64_8486 : (!torch.int, !torch.int) -> !torch.list<int>
    %5613 = torch.aten.view %5611, %5612 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %5613, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_8487 = torch.constant.none
    %5614 = torch.aten.clone %259, %none_8487 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_8488 = torch.constant.int 1
    %int1_8489 = torch.constant.int 1
    %int1_8490 = torch.constant.int 1
    %5615 = torch.prim.ListConstruct %int1_8488, %int1_8489, %int1_8490 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5616 = torch.aten.view %5614, %5615 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_8491 = torch.constant.int 22
    %5617 = torch.aten.mul.Scalar %5582, %int22_8491 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int15_8492 = torch.constant.int 15
    %int1_8493 = torch.constant.int 1
    %5618 = torch.aten.add.Scalar %5617, %int15_8492, %int1_8493 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_8494 = torch.constant.int 2
    %5619 = torch.aten.mul.Scalar %5618, %int2_8494 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_8495 = torch.constant.int 1
    %5620 = torch.aten.add.Tensor %5619, %5616, %int1_8495 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_8496 = torch.constant.int 4
    %5621 = torch.aten.mul.Scalar %5620, %int4_8496 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_8497 = torch.constant.int 1
    %5622 = torch.aten.add.Tensor %5621, %5588, %int1_8497 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_8498 = torch.constant.int 32
    %5623 = torch.aten.mul.Scalar %5622, %int32_8498 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_8499 = torch.constant.int 1
    %5624 = torch.aten.add.Tensor %5623, %5585, %int1_8499 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_8500 = torch.constant.none
    %none_8501 = torch.constant.none
    %int5_8502 = torch.constant.int 5
    %cpu_8503 = torch.constant.device "cpu"
    %int0_8504 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5495, %none_8500, %none_8501, %int5_8502, %cpu_8503, %int0_8504 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %5625 = torch.prim.ListConstruct %5624 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_8505 = torch.constant.bool false
    %5626 = torch.aten.index_put %5613, %5625, %5495, %false_8505 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %5626, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_8506 = torch.constant.int 22
    %int2_8507 = torch.constant.int 2
    %int4_8508 = torch.constant.int 4
    %int32_8509 = torch.constant.int 32
    %int64_8510 = torch.constant.int 64
    %5627 = torch.prim.ListConstruct %381, %int22_8506, %int2_8507, %int4_8508, %int32_8509, %int64_8510 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5628 = torch.aten.view %5626, %5627 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5628, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_8511 = torch.constant.int 360448
    %5629 = torch.prim.ListConstruct %381, %int360448_8511 : (!torch.int, !torch.int) -> !torch.list<int>
    %5630 = torch.aten.view %5628, %5629 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %5630, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_8512 = torch.constant.none
    %5631 = torch.aten.clone %260, %none_8512 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_8513 = torch.constant.none
    %5632 = torch.aten.clone %261, %none_8513 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_8514 = torch.constant.none
    %5633 = torch.aten.clone %262, %none_8514 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_8515 = torch.constant.int 22
    %int2_8516 = torch.constant.int 2
    %int4_8517 = torch.constant.int 4
    %int32_8518 = torch.constant.int 32
    %int64_8519 = torch.constant.int 64
    %5634 = torch.prim.ListConstruct %381, %int22_8515, %int2_8516, %int4_8517, %int32_8518, %int64_8519 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5635 = torch.aten.view %5630, %5634 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5635, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %5636 = torch_c.to_builtin_tensor %5635 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %5637 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_8520 = tensor.cast %5637 : tensor<4x?xi64> to tensor<?x?xi64>
    %5638 = torch_c.to_builtin_tensor %5631 : !torch.vtensor<[],si64> -> tensor<i64>
    %5639 = torch_c.to_builtin_tensor %5632 : !torch.vtensor<[],si64> -> tensor<i64>
    %5640 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%5636, %cast_8520, %5638, %5639) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_8521 = tensor.cast %5640 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %5641 = torch_c.from_builtin_tensor %cast_8521 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %5641, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %5642 = torch_c.to_builtin_tensor %5635 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %5643 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_8522 = tensor.cast %5643 : tensor<4x?xi64> to tensor<?x?xi64>
    %5644 = torch_c.to_builtin_tensor %5631 : !torch.vtensor<[],si64> -> tensor<i64>
    %5645 = torch_c.to_builtin_tensor %5633 : !torch.vtensor<[],si64> -> tensor<i64>
    %5646 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%5642, %cast_8522, %5644, %5645) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_8523 = tensor.cast %5646 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %5647 = torch_c.from_builtin_tensor %cast_8523 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %5647, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_8524 = torch.constant.int 2
    %int3_8525 = torch.constant.int 3
    %5648 = torch.aten.transpose.int %5641, %int2_8524, %int3_8525 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5648, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_8526 = torch.constant.int 0
    %5649 = torch.aten.clone %5648, %int0_8526 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5649, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_8527 = torch.constant.int 4
    %int4_8528 = torch.constant.int 4
    %int64_8529 = torch.constant.int 64
    %5650 = torch.prim.ListConstruct %int4_8527, %623, %int4_8528, %int64_8529 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5651 = torch.aten._unsafe_view %5649, %5650 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %5651, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_8530 = torch.constant.int 2
    %int3_8531 = torch.constant.int 3
    %5652 = torch.aten.transpose.int %5647, %int2_8530, %int3_8531 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5652, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_8532 = torch.constant.int 0
    %5653 = torch.aten.clone %5652, %int0_8532 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5653, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_8533 = torch.constant.int 4
    %int4_8534 = torch.constant.int 4
    %int64_8535 = torch.constant.int 64
    %5654 = torch.prim.ListConstruct %int4_8533, %623, %int4_8534, %int64_8535 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5655 = torch.aten._unsafe_view %5653, %5654 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %5655, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_8536 = torch.constant.int 0
    %int1_8537 = torch.constant.int 1
    %none_8538 = torch.constant.none
    %none_8539 = torch.constant.none
    %cpu_8540 = torch.constant.device "cpu"
    %false_8541 = torch.constant.bool false
    %5656 = torch.aten.arange.start_step %int0_8536, %623, %int1_8537, %none_8538, %none_8539, %cpu_8540, %false_8541 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5656, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_8542 = torch.constant.int -1
    %5657 = torch.aten.unsqueeze %arg1, %int-1_8542 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %5658 = torch.aten.ge.Tensor %5656, %5657 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %5658, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_8543 = torch.constant.none
    %5659 = torch.aten.clone %263, %none_8543 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_8544 = torch.constant.int 0
    %5660 = torch.aten.where.ScalarOther %5658, %5659, %int0_8544 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %5660, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_8545 = torch.constant.none
    %none_8546 = torch.constant.none
    %int5_8547 = torch.constant.int 5
    %cpu_8548 = torch.constant.device "cpu"
    %int0_8549 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5660, %none_8545, %none_8546, %int5_8547, %cpu_8548, %int0_8549 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_8550 = torch.constant.int 1
    %5661 = torch.aten.unsqueeze %5660, %int1_8550 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %5661, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_8551 = torch.constant.int 1
    %5662 = torch.aten.unsqueeze %5661, %int1_8551 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %5662, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_8552 = torch.constant.int -2
    %5663 = torch.aten.unsqueeze %5651, %int-2_8552 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %5663, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_8553 = torch.constant.int 4
    %int4_8554 = torch.constant.int 4
    %int8_8555 = torch.constant.int 8
    %int64_8556 = torch.constant.int 64
    %5664 = torch.prim.ListConstruct %int4_8553, %623, %int4_8554, %int8_8555, %int64_8556 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8557 = torch.constant.bool false
    %5665 = torch.aten.expand %5663, %5664, %false_8557 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5665, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_8558 = torch.constant.int 0
    %5666 = torch.aten.clone %5665, %int0_8558 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5666, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_8559 = torch.constant.int 4
    %int32_8560 = torch.constant.int 32
    %int64_8561 = torch.constant.int 64
    %5667 = torch.prim.ListConstruct %int4_8559, %623, %int32_8560, %int64_8561 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5668 = torch.aten._unsafe_view %5666, %5667 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5668, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_8562 = torch.constant.int -2
    %5669 = torch.aten.unsqueeze %5655, %int-2_8562 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %5669, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_8563 = torch.constant.int 4
    %int4_8564 = torch.constant.int 4
    %int8_8565 = torch.constant.int 8
    %int64_8566 = torch.constant.int 64
    %5670 = torch.prim.ListConstruct %int4_8563, %623, %int4_8564, %int8_8565, %int64_8566 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8567 = torch.constant.bool false
    %5671 = torch.aten.expand %5669, %5670, %false_8567 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5671, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_8568 = torch.constant.int 0
    %5672 = torch.aten.clone %5671, %int0_8568 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %5672, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_8569 = torch.constant.int 4
    %int32_8570 = torch.constant.int 32
    %int64_8571 = torch.constant.int 64
    %5673 = torch.prim.ListConstruct %int4_8569, %623, %int32_8570, %int64_8571 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5674 = torch.aten._unsafe_view %5672, %5673 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %5674, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_8572 = torch.constant.int 1
    %int2_8573 = torch.constant.int 2
    %5675 = torch.aten.transpose.int %5536, %int1_8572, %int2_8573 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_8574 = torch.constant.int 1
    %int2_8575 = torch.constant.int 2
    %5676 = torch.aten.transpose.int %5668, %int1_8574, %int2_8575 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %5676, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_8576 = torch.constant.int 1
    %int2_8577 = torch.constant.int 2
    %5677 = torch.aten.transpose.int %5674, %int1_8576, %int2_8577 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %5677, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_8578 = torch.constant.float 0.000000e+00
    %false_8579 = torch.constant.bool false
    %none_8580 = torch.constant.none
    %false_8581 = torch.constant.bool false
    %5678 = torch.aten.scaled_dot_product_attention %5675, %5676, %5677, %5662, %float0.000000e00_8578, %false_8579, %none_8580, %false_8581 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_8582 = torch.constant.int 1
    %int2_8583 = torch.constant.int 2
    %5679 = torch.aten.transpose.int %5678, %int1_8582, %int2_8583 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_8584 = torch.constant.int 4
    %int1_8585 = torch.constant.int 1
    %int2048_8586 = torch.constant.int 2048
    %5680 = torch.prim.ListConstruct %int4_8584, %int1_8585, %int2048_8586 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5681 = torch.aten.view %5679, %5680 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_8587 = torch.constant.int 2
    %5682 = torch.aten.view.dtype %268, %int2_8587 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %5683 = torch.aten.detach %5682 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_8588 = torch.constant.int -1
    %int17_8589 = torch.constant.int 17
    %5684 = torch.prim.ListConstruct %int-1_8588, %int17_8589 : (!torch.int, !torch.int) -> !torch.list<int>
    %5685 = torch.aten.view %5683, %5684 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_8590 = torch.constant.int 2048
    %int-1_8591 = torch.constant.int -1
    %int17_8592 = torch.constant.int 17
    %5686 = torch.prim.ListConstruct %int2048_8590, %int-1_8591, %int17_8592 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5687 = torch.aten.view %5685, %5686 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_8593 = torch.constant.int 2
    %int0_8594 = torch.constant.int 0
    %int1_8595 = torch.constant.int 1
    %int1_8596 = torch.constant.int 1
    %5688 = torch.aten.slice.Tensor %5687, %int2_8593, %int0_8594, %int1_8595, %int1_8596 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_8597 = torch.constant.int 5
    %5689 = torch.aten.view.dtype %5688, %int5_8597 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %5690 = torch.aten.detach %5689 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_8598 = torch.constant.int 2
    %int1_8599 = torch.constant.int 1
    %int9223372036854775807_8600 = torch.constant.int 9223372036854775807
    %int1_8601 = torch.constant.int 1
    %5691 = torch.aten.slice.Tensor %5687, %int2_8598, %int1_8599, %int9223372036854775807_8600, %int1_8601 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_8602 = torch.constant.int 1
    %5692 = torch.aten.view.dtype %5691, %int1_8602 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %5693 = torch.aten.detach %5692 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %5694 = torch_c.to_builtin_tensor %5681 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_8603 = tensor.cast %5694 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %5695 = torch_c.to_builtin_tensor %5690 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %5696 = torch_c.to_builtin_tensor %5693 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %5697 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_8603, %5695, %5696) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_8604 = tensor.cast %5697 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %5698 = torch_c.from_builtin_tensor %cast_8604 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_8605 = torch.constant.none
    %none_8606 = torch.constant.none
    %int5_8607 = torch.constant.int 5
    %cpu_8608 = torch.constant.device "cpu"
    %int0_8609 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5698, %none_8605, %none_8606, %int5_8607, %cpu_8608, %int0_8609 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_8610 = torch.constant.int 1
    %5699 = torch.aten.add.Tensor %5428, %5698, %int1_8610 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_8611 = torch.constant.none
    %none_8612 = torch.constant.none
    %int5_8613 = torch.constant.int 5
    %cpu_8614 = torch.constant.device "cpu"
    %int0_8615 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5699, %none_8611, %none_8612, %int5_8613, %cpu_8614, %int0_8615 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8616 = torch.constant.int 6
    %5700 = torch.prims.convert_element_type %5699, %int6_8616 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_8617 = torch.constant.int 2
    %5701 = torch.aten.pow.Tensor_Scalar %5700, %int2_8617 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_8618 = torch.constant.int -1
    %5702 = torch.prim.ListConstruct %int-1_8618 : (!torch.int) -> !torch.list<int>
    %true_8619 = torch.constant.bool true
    %none_8620 = torch.constant.none
    %5703 = torch.aten.mean.dim %5701, %5702, %true_8619, %none_8620 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_8621 = torch.constant.float 9.9999997473787516E-6
    %int1_8622 = torch.constant.int 1
    %5704 = torch.aten.add.Scalar %5703, %float9.999990e-06_8621, %int1_8622 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5705 = torch.aten.rsqrt %5704 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %5706 = torch.aten.mul.Tensor %5700, %5705 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_8623 = torch.constant.none
    %none_8624 = torch.constant.none
    %int6_8625 = torch.constant.int 6
    %cpu_8626 = torch.constant.device "cpu"
    %int0_8627 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5706, %none_8623, %none_8624, %int6_8625, %cpu_8626, %int0_8627 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8628 = torch.constant.int 5
    %5707 = torch.prims.convert_element_type %5706, %int5_8628 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %5708 = torch.aten.mul.Tensor %269, %5707 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_8629 = torch.constant.none
    %none_8630 = torch.constant.none
    %int6_8631 = torch.constant.int 6
    %cpu_8632 = torch.constant.device "cpu"
    %int0_8633 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5708, %none_8629, %none_8630, %int6_8631, %cpu_8632, %int0_8633 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8634 = torch.constant.int 5
    %5709 = torch.prims.convert_element_type %5708, %int5_8634 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_8635 = torch.constant.int 2
    %5710 = torch.aten.view.dtype %270, %int2_8635 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %5711 = torch.aten.detach %5710 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_8636 = torch.constant.int -1
    %int17_8637 = torch.constant.int 17
    %5712 = torch.prim.ListConstruct %int-1_8636, %int17_8637 : (!torch.int, !torch.int) -> !torch.list<int>
    %5713 = torch.aten.view %5711, %5712 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_8638 = torch.constant.int 5632
    %int-1_8639 = torch.constant.int -1
    %int17_8640 = torch.constant.int 17
    %5714 = torch.prim.ListConstruct %int5632_8638, %int-1_8639, %int17_8640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5715 = torch.aten.view %5713, %5714 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_8641 = torch.constant.int 2
    %int0_8642 = torch.constant.int 0
    %int1_8643 = torch.constant.int 1
    %int1_8644 = torch.constant.int 1
    %5716 = torch.aten.slice.Tensor %5715, %int2_8641, %int0_8642, %int1_8643, %int1_8644 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_8645 = torch.constant.int 5
    %5717 = torch.aten.view.dtype %5716, %int5_8645 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %5718 = torch.aten.detach %5717 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_8646 = torch.constant.int 2
    %int1_8647 = torch.constant.int 1
    %int9223372036854775807_8648 = torch.constant.int 9223372036854775807
    %int1_8649 = torch.constant.int 1
    %5719 = torch.aten.slice.Tensor %5715, %int2_8646, %int1_8647, %int9223372036854775807_8648, %int1_8649 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_8650 = torch.constant.int 1
    %5720 = torch.aten.view.dtype %5719, %int1_8650 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %5721 = torch.aten.detach %5720 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %5722 = torch_c.to_builtin_tensor %5709 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_8651 = tensor.cast %5722 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %5723 = torch_c.to_builtin_tensor %5718 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %5724 = torch_c.to_builtin_tensor %5721 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %5725 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_8651, %5723, %5724) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_8652 = tensor.cast %5725 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %5726 = torch_c.from_builtin_tensor %cast_8652 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %5727 = torch.aten.silu %5726 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_8653 = torch.constant.int 2
    %5728 = torch.aten.view.dtype %271, %int2_8653 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %5729 = torch.aten.detach %5728 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_8654 = torch.constant.int -1
    %int17_8655 = torch.constant.int 17
    %5730 = torch.prim.ListConstruct %int-1_8654, %int17_8655 : (!torch.int, !torch.int) -> !torch.list<int>
    %5731 = torch.aten.view %5729, %5730 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_8656 = torch.constant.int 5632
    %int-1_8657 = torch.constant.int -1
    %int17_8658 = torch.constant.int 17
    %5732 = torch.prim.ListConstruct %int5632_8656, %int-1_8657, %int17_8658 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5733 = torch.aten.view %5731, %5732 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_8659 = torch.constant.int 2
    %int0_8660 = torch.constant.int 0
    %int1_8661 = torch.constant.int 1
    %int1_8662 = torch.constant.int 1
    %5734 = torch.aten.slice.Tensor %5733, %int2_8659, %int0_8660, %int1_8661, %int1_8662 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_8663 = torch.constant.int 5
    %5735 = torch.aten.view.dtype %5734, %int5_8663 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %5736 = torch.aten.detach %5735 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_8664 = torch.constant.int 2
    %int1_8665 = torch.constant.int 1
    %int9223372036854775807_8666 = torch.constant.int 9223372036854775807
    %int1_8667 = torch.constant.int 1
    %5737 = torch.aten.slice.Tensor %5733, %int2_8664, %int1_8665, %int9223372036854775807_8666, %int1_8667 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_8668 = torch.constant.int 1
    %5738 = torch.aten.view.dtype %5737, %int1_8668 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %5739 = torch.aten.detach %5738 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %5740 = torch_c.to_builtin_tensor %5709 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_8669 = tensor.cast %5740 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %5741 = torch_c.to_builtin_tensor %5736 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %5742 = torch_c.to_builtin_tensor %5739 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %5743 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_8669, %5741, %5742) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_8670 = tensor.cast %5743 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %5744 = torch_c.from_builtin_tensor %cast_8670 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %5745 = torch.aten.mul.Tensor %5727, %5744 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_8671 = torch.constant.int 2
    %5746 = torch.aten.view.dtype %272, %int2_8671 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %5747 = torch.aten.detach %5746 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_8672 = torch.constant.int -1
    %int17_8673 = torch.constant.int 17
    %5748 = torch.prim.ListConstruct %int-1_8672, %int17_8673 : (!torch.int, !torch.int) -> !torch.list<int>
    %5749 = torch.aten.view %5747, %5748 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_8674 = torch.constant.int 2048
    %int-1_8675 = torch.constant.int -1
    %int17_8676 = torch.constant.int 17
    %5750 = torch.prim.ListConstruct %int2048_8674, %int-1_8675, %int17_8676 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5751 = torch.aten.view %5749, %5750 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_8677 = torch.constant.int 2
    %int0_8678 = torch.constant.int 0
    %int1_8679 = torch.constant.int 1
    %int1_8680 = torch.constant.int 1
    %5752 = torch.aten.slice.Tensor %5751, %int2_8677, %int0_8678, %int1_8679, %int1_8680 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_8681 = torch.constant.int 5
    %5753 = torch.aten.view.dtype %5752, %int5_8681 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %5754 = torch.aten.detach %5753 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_8682 = torch.constant.int 2
    %int1_8683 = torch.constant.int 1
    %int9223372036854775807_8684 = torch.constant.int 9223372036854775807
    %int1_8685 = torch.constant.int 1
    %5755 = torch.aten.slice.Tensor %5751, %int2_8682, %int1_8683, %int9223372036854775807_8684, %int1_8685 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_8686 = torch.constant.int 1
    %5756 = torch.aten.view.dtype %5755, %int1_8686 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %5757 = torch.aten.detach %5756 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %5758 = torch_c.to_builtin_tensor %5745 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_8687 = tensor.cast %5758 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %5759 = torch_c.to_builtin_tensor %5754 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %5760 = torch_c.to_builtin_tensor %5757 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %5761 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_8687, %5759, %5760) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_8688 = tensor.cast %5761 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %5762 = torch_c.from_builtin_tensor %cast_8688 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_8689 = torch.constant.int 1
    %5763 = torch.aten.add.Tensor %5699, %5762, %int1_8689 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_8690 = torch.constant.none
    %none_8691 = torch.constant.none
    %int5_8692 = torch.constant.int 5
    %cpu_8693 = torch.constant.device "cpu"
    %int0_8694 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5763, %none_8690, %none_8691, %int5_8692, %cpu_8693, %int0_8694 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8695 = torch.constant.int 6
    %5764 = torch.prims.convert_element_type %5763, %int6_8695 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_8696 = torch.constant.int 2
    %5765 = torch.aten.pow.Tensor_Scalar %5764, %int2_8696 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_8697 = torch.constant.int -1
    %5766 = torch.prim.ListConstruct %int-1_8697 : (!torch.int) -> !torch.list<int>
    %true_8698 = torch.constant.bool true
    %none_8699 = torch.constant.none
    %5767 = torch.aten.mean.dim %5765, %5766, %true_8698, %none_8699 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_8700 = torch.constant.float 9.9999997473787516E-6
    %int1_8701 = torch.constant.int 1
    %5768 = torch.aten.add.Scalar %5767, %float9.999990e-06_8700, %int1_8701 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5769 = torch.aten.rsqrt %5768 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %5770 = torch.aten.mul.Tensor %5764, %5769 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_8702 = torch.constant.none
    %none_8703 = torch.constant.none
    %int6_8704 = torch.constant.int 6
    %cpu_8705 = torch.constant.device "cpu"
    %int0_8706 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5770, %none_8702, %none_8703, %int6_8704, %cpu_8705, %int0_8706 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8707 = torch.constant.int 5
    %5771 = torch.prims.convert_element_type %5770, %int5_8707 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %5772 = torch.aten.mul.Tensor %281, %5771 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_8708 = torch.constant.none
    %none_8709 = torch.constant.none
    %int6_8710 = torch.constant.int 6
    %cpu_8711 = torch.constant.device "cpu"
    %int0_8712 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5772, %none_8708, %none_8709, %int6_8710, %cpu_8711, %int0_8712 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8713 = torch.constant.int 5
    %5773 = torch.prims.convert_element_type %5772, %int5_8713 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_8714 = torch.constant.int 2
    %5774 = torch.aten.view.dtype %282, %int2_8714 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %5775 = torch.aten.detach %5774 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_8715 = torch.constant.int -1
    %int17_8716 = torch.constant.int 17
    %5776 = torch.prim.ListConstruct %int-1_8715, %int17_8716 : (!torch.int, !torch.int) -> !torch.list<int>
    %5777 = torch.aten.view %5775, %5776 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_8717 = torch.constant.int 2048
    %int-1_8718 = torch.constant.int -1
    %int17_8719 = torch.constant.int 17
    %5778 = torch.prim.ListConstruct %int2048_8717, %int-1_8718, %int17_8719 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5779 = torch.aten.view %5777, %5778 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_8720 = torch.constant.int 2
    %int0_8721 = torch.constant.int 0
    %int1_8722 = torch.constant.int 1
    %int1_8723 = torch.constant.int 1
    %5780 = torch.aten.slice.Tensor %5779, %int2_8720, %int0_8721, %int1_8722, %int1_8723 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_8724 = torch.constant.int 5
    %5781 = torch.aten.view.dtype %5780, %int5_8724 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %5782 = torch.aten.detach %5781 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_8725 = torch.constant.int 2
    %int1_8726 = torch.constant.int 1
    %int9223372036854775807_8727 = torch.constant.int 9223372036854775807
    %int1_8728 = torch.constant.int 1
    %5783 = torch.aten.slice.Tensor %5779, %int2_8725, %int1_8726, %int9223372036854775807_8727, %int1_8728 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_8729 = torch.constant.int 1
    %5784 = torch.aten.view.dtype %5783, %int1_8729 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %5785 = torch.aten.detach %5784 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %5786 = torch_c.to_builtin_tensor %5773 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_8730 = tensor.cast %5786 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %5787 = torch_c.to_builtin_tensor %5782 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %5788 = torch_c.to_builtin_tensor %5785 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %5789 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_8730, %5787, %5788) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_8731 = tensor.cast %5789 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %5790 = torch_c.from_builtin_tensor %cast_8731 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_8732 = torch.constant.int 2
    %5791 = torch.aten.view.dtype %283, %int2_8732 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %5792 = torch.aten.detach %5791 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_8733 = torch.constant.int -1
    %int17_8734 = torch.constant.int 17
    %5793 = torch.prim.ListConstruct %int-1_8733, %int17_8734 : (!torch.int, !torch.int) -> !torch.list<int>
    %5794 = torch.aten.view %5792, %5793 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_8735 = torch.constant.int 256
    %int-1_8736 = torch.constant.int -1
    %int17_8737 = torch.constant.int 17
    %5795 = torch.prim.ListConstruct %int256_8735, %int-1_8736, %int17_8737 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5796 = torch.aten.view %5794, %5795 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_8738 = torch.constant.int 2
    %int0_8739 = torch.constant.int 0
    %int1_8740 = torch.constant.int 1
    %int1_8741 = torch.constant.int 1
    %5797 = torch.aten.slice.Tensor %5796, %int2_8738, %int0_8739, %int1_8740, %int1_8741 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_8742 = torch.constant.int 5
    %5798 = torch.aten.view.dtype %5797, %int5_8742 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %5799 = torch.aten.detach %5798 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_8743 = torch.constant.int 2
    %int1_8744 = torch.constant.int 1
    %int9223372036854775807_8745 = torch.constant.int 9223372036854775807
    %int1_8746 = torch.constant.int 1
    %5800 = torch.aten.slice.Tensor %5796, %int2_8743, %int1_8744, %int9223372036854775807_8745, %int1_8746 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_8747 = torch.constant.int 1
    %5801 = torch.aten.view.dtype %5800, %int1_8747 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %5802 = torch.aten.detach %5801 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %5803 = torch_c.to_builtin_tensor %5773 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_8748 = tensor.cast %5803 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %5804 = torch_c.to_builtin_tensor %5799 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %5805 = torch_c.to_builtin_tensor %5802 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %5806 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_8748, %5804, %5805) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_8749 = tensor.cast %5806 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %5807 = torch_c.from_builtin_tensor %cast_8749 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_8750 = torch.constant.int 2
    %5808 = torch.aten.view.dtype %284, %int2_8750 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %5809 = torch.aten.detach %5808 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_8751 = torch.constant.int -1
    %int17_8752 = torch.constant.int 17
    %5810 = torch.prim.ListConstruct %int-1_8751, %int17_8752 : (!torch.int, !torch.int) -> !torch.list<int>
    %5811 = torch.aten.view %5809, %5810 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_8753 = torch.constant.int 256
    %int-1_8754 = torch.constant.int -1
    %int17_8755 = torch.constant.int 17
    %5812 = torch.prim.ListConstruct %int256_8753, %int-1_8754, %int17_8755 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5813 = torch.aten.view %5811, %5812 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_8756 = torch.constant.int 2
    %int0_8757 = torch.constant.int 0
    %int1_8758 = torch.constant.int 1
    %int1_8759 = torch.constant.int 1
    %5814 = torch.aten.slice.Tensor %5813, %int2_8756, %int0_8757, %int1_8758, %int1_8759 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_8760 = torch.constant.int 5
    %5815 = torch.aten.view.dtype %5814, %int5_8760 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %5816 = torch.aten.detach %5815 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_8761 = torch.constant.int 2
    %int1_8762 = torch.constant.int 1
    %int9223372036854775807_8763 = torch.constant.int 9223372036854775807
    %int1_8764 = torch.constant.int 1
    %5817 = torch.aten.slice.Tensor %5813, %int2_8761, %int1_8762, %int9223372036854775807_8763, %int1_8764 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_8765 = torch.constant.int 1
    %5818 = torch.aten.view.dtype %5817, %int1_8765 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %5819 = torch.aten.detach %5818 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %5820 = torch_c.to_builtin_tensor %5773 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_8766 = tensor.cast %5820 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %5821 = torch_c.to_builtin_tensor %5816 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %5822 = torch_c.to_builtin_tensor %5819 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %5823 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_8766, %5821, %5822) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_8767 = tensor.cast %5823 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %5824 = torch_c.from_builtin_tensor %cast_8767 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_8768 = torch.constant.int 4
    %int1_8769 = torch.constant.int 1
    %int32_8770 = torch.constant.int 32
    %int64_8771 = torch.constant.int 64
    %5825 = torch.prim.ListConstruct %int4_8768, %int1_8769, %int32_8770, %int64_8771 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5826 = torch.aten.view %5790, %5825 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_8772 = torch.constant.int 4
    %int1_8773 = torch.constant.int 1
    %int4_8774 = torch.constant.int 4
    %int64_8775 = torch.constant.int 64
    %5827 = torch.prim.ListConstruct %int4_8772, %int1_8773, %int4_8774, %int64_8775 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5828 = torch.aten.view %5807, %5827 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_8776 = torch.constant.int 4
    %int1_8777 = torch.constant.int 1
    %int4_8778 = torch.constant.int 4
    %int64_8779 = torch.constant.int 64
    %5829 = torch.prim.ListConstruct %int4_8776, %int1_8777, %int4_8778, %int64_8779 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5830 = torch.aten.view %5824, %5829 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_8780 = torch.constant.int 0
    %int1_8781 = torch.constant.int 1
    %none_8782 = torch.constant.none
    %none_8783 = torch.constant.none
    %cpu_8784 = torch.constant.device "cpu"
    %false_8785 = torch.constant.bool false
    %5831 = torch.aten.arange.start %int0_8780, %int1_8781, %none_8782, %none_8783, %cpu_8784, %false_8785 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_8786 = torch.constant.int 0
    %5832 = torch.aten.unsqueeze %5831, %int0_8786 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_8787 = torch.constant.int 1
    %5833 = torch.aten.unsqueeze %arg2, %int1_8787 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8788 = torch.constant.int 1
    %5834 = torch.aten.add.Tensor %5832, %5833, %int1_8788 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_8789 = torch.constant.int 0
    %int64_8790 = torch.constant.int 64
    %int2_8791 = torch.constant.int 2
    %none_8792 = torch.constant.none
    %none_8793 = torch.constant.none
    %cpu_8794 = torch.constant.device "cpu"
    %false_8795 = torch.constant.bool false
    %5835 = torch.aten.arange.start_step %int0_8789, %int64_8790, %int2_8791, %none_8792, %none_8793, %cpu_8794, %false_8795 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_8796 = torch.constant.none
    %none_8797 = torch.constant.none
    %int4_8798 = torch.constant.int 4
    %cpu_8799 = torch.constant.device "cpu"
    %int0_8800 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5835, %none_8796, %none_8797, %int4_8798, %cpu_8799, %int0_8800 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8801 = torch.constant.int 6
    %5836 = torch.prims.convert_element_type %5835, %int6_8801 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_8802 = torch.constant.int 64
    %5837 = torch.aten.div.Scalar %5836, %int64_8802 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_8803 = torch.constant.float 1.000000e+04
    %5838 = torch.aten.pow.Scalar %float1.000000e04_8803, %5837 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %5839 = torch.aten.reciprocal %5838 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_8804 = torch.constant.float 1.000000e+00
    %5840 = torch.aten.mul.Scalar %5839, %float1.000000e00_8804 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_8805 = torch.constant.none
    %5841 = torch.aten.clone %273, %none_8805 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_8806 = torch.constant.int 0
    %5842 = torch.aten.unsqueeze %5840, %int0_8806 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_8807 = torch.constant.int 2
    %5843 = torch.aten.unsqueeze %5842, %int2_8807 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_8808 = torch.constant.none
    %none_8809 = torch.constant.none
    %int6_8810 = torch.constant.int 6
    %cpu_8811 = torch.constant.device "cpu"
    %int0_8812 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5843, %none_8808, %none_8809, %int6_8810, %cpu_8811, %int0_8812 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_8813 = torch.constant.int 4
    %int-1_8814 = torch.constant.int -1
    %int1_8815 = torch.constant.int 1
    %5844 = torch.prim.ListConstruct %int4_8813, %int-1_8814, %int1_8815 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8816 = torch.constant.bool false
    %5845 = torch.aten.expand %5843, %5844, %false_8816 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_8817 = torch.constant.int 1
    %5846 = torch.aten.unsqueeze %5834, %int1_8817 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_8818 = torch.constant.none
    %none_8819 = torch.constant.none
    %int4_8820 = torch.constant.int 4
    %cpu_8821 = torch.constant.device "cpu"
    %int0_8822 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5846, %none_8818, %none_8819, %int4_8820, %cpu_8821, %int0_8822 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8823 = torch.constant.int 6
    %5847 = torch.prims.convert_element_type %5846, %int6_8823 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5848 = torch.aten.matmul %5845, %5847 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_8824 = torch.constant.int 1
    %int2_8825 = torch.constant.int 2
    %5849 = torch.aten.transpose.int %5848, %int1_8824, %int2_8825 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %5850 = torch.aten.cos %5849 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %5851 = torch.aten.mul.Tensor %5850, %5841 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_8826 = torch.constant.none
    %none_8827 = torch.constant.none
    %int6_8828 = torch.constant.int 6
    %cpu_8829 = torch.constant.device "cpu"
    %int0_8830 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5851, %none_8826, %none_8827, %int6_8828, %cpu_8829, %int0_8830 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8831 = torch.constant.int 5
    %5852 = torch.prims.convert_element_type %5851, %int5_8831 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %5853 = torch.aten.sin %5849 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %5854 = torch.aten.mul.Tensor %5853, %5841 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_8832 = torch.constant.none
    %none_8833 = torch.constant.none
    %int6_8834 = torch.constant.int 6
    %cpu_8835 = torch.constant.device "cpu"
    %int0_8836 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5854, %none_8832, %none_8833, %int6_8834, %cpu_8835, %int0_8836 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8837 = torch.constant.int 5
    %5855 = torch.prims.convert_element_type %5854, %int5_8837 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_8838 = torch.constant.int 2
    %5856 = torch.aten.unsqueeze %5852, %int2_8838 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_8839 = torch.constant.int 2
    %5857 = torch.aten.unsqueeze %5855, %int2_8839 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_8840 = torch.constant.none
    %none_8841 = torch.constant.none
    %int5_8842 = torch.constant.int 5
    %cpu_8843 = torch.constant.device "cpu"
    %int0_8844 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5856, %none_8840, %none_8841, %int5_8842, %cpu_8843, %int0_8844 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_8845 = torch.constant.none
    %none_8846 = torch.constant.none
    %int5_8847 = torch.constant.int 5
    %cpu_8848 = torch.constant.device "cpu"
    %int0_8849 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5857, %none_8845, %none_8846, %int5_8847, %cpu_8848, %int0_8849 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_8850 = torch.constant.none
    %none_8851 = torch.constant.none
    %int5_8852 = torch.constant.int 5
    %cpu_8853 = torch.constant.device "cpu"
    %int0_8854 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5826, %none_8850, %none_8851, %int5_8852, %cpu_8853, %int0_8854 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_8855 = torch.constant.int 3
    %int0_8856 = torch.constant.int 0
    %int64_8857 = torch.constant.int 64
    %int2_8858 = torch.constant.int 2
    %5858 = torch.aten.slice.Tensor %5826, %int3_8855, %int0_8856, %int64_8857, %int2_8858 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_8859 = torch.constant.int 3
    %int1_8860 = torch.constant.int 1
    %int64_8861 = torch.constant.int 64
    %int2_8862 = torch.constant.int 2
    %5859 = torch.aten.slice.Tensor %5826, %int3_8859, %int1_8860, %int64_8861, %int2_8862 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %5860 = torch.aten.mul.Tensor %5858, %5856 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %5861 = torch.aten.mul.Tensor %5859, %5857 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_8863 = torch.constant.int 1
    %5862 = torch.aten.sub.Tensor %5860, %5861, %int1_8863 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %5863 = torch.aten.mul.Tensor %5859, %5856 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %5864 = torch.aten.mul.Tensor %5858, %5857 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_8864 = torch.constant.int 1
    %5865 = torch.aten.add.Tensor %5863, %5864, %int1_8864 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %5866 = torch_c.to_builtin_tensor %5862 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_8865 = tensor.cast %5866 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %5867 = torch_c.to_builtin_tensor %5865 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_8866 = tensor.cast %5867 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %5868 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_8865, %cast_8866) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_8867 = tensor.cast %5868 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %5869 = torch_c.from_builtin_tensor %cast_8867 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_8868 = torch.constant.int 4
    %int1_8869 = torch.constant.int 1
    %int32_8870 = torch.constant.int 32
    %int64_8871 = torch.constant.int 64
    %5870 = torch.prim.ListConstruct %int4_8868, %int1_8869, %int32_8870, %int64_8871 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5871 = torch.aten.view %5869, %5870 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_8872 = torch.constant.none
    %none_8873 = torch.constant.none
    %int5_8874 = torch.constant.int 5
    %cpu_8875 = torch.constant.device "cpu"
    %int0_8876 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5871, %none_8872, %none_8873, %int5_8874, %cpu_8875, %int0_8876 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_8877 = torch.constant.int 0
    %int1_8878 = torch.constant.int 1
    %none_8879 = torch.constant.none
    %none_8880 = torch.constant.none
    %cpu_8881 = torch.constant.device "cpu"
    %false_8882 = torch.constant.bool false
    %5872 = torch.aten.arange.start %int0_8877, %int1_8878, %none_8879, %none_8880, %cpu_8881, %false_8882 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_8883 = torch.constant.int 0
    %5873 = torch.aten.unsqueeze %5872, %int0_8883 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_8884 = torch.constant.int 1
    %5874 = torch.aten.unsqueeze %arg2, %int1_8884 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8885 = torch.constant.int 1
    %5875 = torch.aten.add.Tensor %5873, %5874, %int1_8885 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_8886 = torch.constant.int 0
    %int64_8887 = torch.constant.int 64
    %int2_8888 = torch.constant.int 2
    %none_8889 = torch.constant.none
    %none_8890 = torch.constant.none
    %cpu_8891 = torch.constant.device "cpu"
    %false_8892 = torch.constant.bool false
    %5876 = torch.aten.arange.start_step %int0_8886, %int64_8887, %int2_8888, %none_8889, %none_8890, %cpu_8891, %false_8892 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_8893 = torch.constant.none
    %none_8894 = torch.constant.none
    %int4_8895 = torch.constant.int 4
    %cpu_8896 = torch.constant.device "cpu"
    %int0_8897 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5876, %none_8893, %none_8894, %int4_8895, %cpu_8896, %int0_8897 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8898 = torch.constant.int 6
    %5877 = torch.prims.convert_element_type %5876, %int6_8898 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_8899 = torch.constant.int 64
    %5878 = torch.aten.div.Scalar %5877, %int64_8899 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_8900 = torch.constant.float 1.000000e+04
    %5879 = torch.aten.pow.Scalar %float1.000000e04_8900, %5878 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %5880 = torch.aten.reciprocal %5879 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_8901 = torch.constant.float 1.000000e+00
    %5881 = torch.aten.mul.Scalar %5880, %float1.000000e00_8901 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_8902 = torch.constant.none
    %5882 = torch.aten.clone %274, %none_8902 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_8903 = torch.constant.int 0
    %5883 = torch.aten.unsqueeze %5881, %int0_8903 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_8904 = torch.constant.int 2
    %5884 = torch.aten.unsqueeze %5883, %int2_8904 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_8905 = torch.constant.none
    %none_8906 = torch.constant.none
    %int6_8907 = torch.constant.int 6
    %cpu_8908 = torch.constant.device "cpu"
    %int0_8909 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5884, %none_8905, %none_8906, %int6_8907, %cpu_8908, %int0_8909 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_8910 = torch.constant.int 4
    %int-1_8911 = torch.constant.int -1
    %int1_8912 = torch.constant.int 1
    %5885 = torch.prim.ListConstruct %int4_8910, %int-1_8911, %int1_8912 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8913 = torch.constant.bool false
    %5886 = torch.aten.expand %5884, %5885, %false_8913 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_8914 = torch.constant.int 1
    %5887 = torch.aten.unsqueeze %5875, %int1_8914 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_8915 = torch.constant.none
    %none_8916 = torch.constant.none
    %int4_8917 = torch.constant.int 4
    %cpu_8918 = torch.constant.device "cpu"
    %int0_8919 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5887, %none_8915, %none_8916, %int4_8917, %cpu_8918, %int0_8919 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_8920 = torch.constant.int 6
    %5888 = torch.prims.convert_element_type %5887, %int6_8920 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5889 = torch.aten.matmul %5886, %5888 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_8921 = torch.constant.int 1
    %int2_8922 = torch.constant.int 2
    %5890 = torch.aten.transpose.int %5889, %int1_8921, %int2_8922 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %5891 = torch.aten.cos %5890 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %5892 = torch.aten.mul.Tensor %5891, %5882 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_8923 = torch.constant.none
    %none_8924 = torch.constant.none
    %int6_8925 = torch.constant.int 6
    %cpu_8926 = torch.constant.device "cpu"
    %int0_8927 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5892, %none_8923, %none_8924, %int6_8925, %cpu_8926, %int0_8927 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8928 = torch.constant.int 5
    %5893 = torch.prims.convert_element_type %5892, %int5_8928 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %5894 = torch.aten.sin %5890 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %5895 = torch.aten.mul.Tensor %5894, %5882 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_8929 = torch.constant.none
    %none_8930 = torch.constant.none
    %int6_8931 = torch.constant.int 6
    %cpu_8932 = torch.constant.device "cpu"
    %int0_8933 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5895, %none_8929, %none_8930, %int6_8931, %cpu_8932, %int0_8933 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_8934 = torch.constant.int 5
    %5896 = torch.prims.convert_element_type %5895, %int5_8934 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_8935 = torch.constant.int 2
    %5897 = torch.aten.unsqueeze %5893, %int2_8935 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_8936 = torch.constant.int 2
    %5898 = torch.aten.unsqueeze %5896, %int2_8936 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_8937 = torch.constant.none
    %none_8938 = torch.constant.none
    %int5_8939 = torch.constant.int 5
    %cpu_8940 = torch.constant.device "cpu"
    %int0_8941 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5897, %none_8937, %none_8938, %int5_8939, %cpu_8940, %int0_8941 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_8942 = torch.constant.none
    %none_8943 = torch.constant.none
    %int5_8944 = torch.constant.int 5
    %cpu_8945 = torch.constant.device "cpu"
    %int0_8946 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5898, %none_8942, %none_8943, %int5_8944, %cpu_8945, %int0_8946 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_8947 = torch.constant.none
    %none_8948 = torch.constant.none
    %int5_8949 = torch.constant.int 5
    %cpu_8950 = torch.constant.device "cpu"
    %int0_8951 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5828, %none_8947, %none_8948, %int5_8949, %cpu_8950, %int0_8951 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_8952 = torch.constant.int 3
    %int0_8953 = torch.constant.int 0
    %int64_8954 = torch.constant.int 64
    %int2_8955 = torch.constant.int 2
    %5899 = torch.aten.slice.Tensor %5828, %int3_8952, %int0_8953, %int64_8954, %int2_8955 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_8956 = torch.constant.int 3
    %int1_8957 = torch.constant.int 1
    %int64_8958 = torch.constant.int 64
    %int2_8959 = torch.constant.int 2
    %5900 = torch.aten.slice.Tensor %5828, %int3_8956, %int1_8957, %int64_8958, %int2_8959 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %5901 = torch.aten.mul.Tensor %5899, %5897 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %5902 = torch.aten.mul.Tensor %5900, %5898 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_8960 = torch.constant.int 1
    %5903 = torch.aten.sub.Tensor %5901, %5902, %int1_8960 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %5904 = torch.aten.mul.Tensor %5900, %5897 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %5905 = torch.aten.mul.Tensor %5899, %5898 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_8961 = torch.constant.int 1
    %5906 = torch.aten.add.Tensor %5904, %5905, %int1_8961 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %5907 = torch_c.to_builtin_tensor %5903 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_8962 = tensor.cast %5907 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %5908 = torch_c.to_builtin_tensor %5906 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_8963 = tensor.cast %5908 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %5909 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_8962, %cast_8963) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_8964 = tensor.cast %5909 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %5910 = torch_c.from_builtin_tensor %cast_8964 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_8965 = torch.constant.int 4
    %int1_8966 = torch.constant.int 1
    %int4_8967 = torch.constant.int 4
    %int64_8968 = torch.constant.int 64
    %5911 = torch.prim.ListConstruct %int4_8965, %int1_8966, %int4_8967, %int64_8968 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5912 = torch.aten.view %5910, %5911 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_8969 = torch.constant.none
    %none_8970 = torch.constant.none
    %int5_8971 = torch.constant.int 5
    %cpu_8972 = torch.constant.device "cpu"
    %int0_8973 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5912, %none_8969, %none_8970, %int5_8971, %cpu_8972, %int0_8973 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_8974 = torch.constant.int 32
    %5913 = torch.aten.floor_divide.Scalar %arg2, %int32_8974 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_8975 = torch.constant.int 1
    %5914 = torch.aten.unsqueeze %5913, %int1_8975 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8976 = torch.constant.int 1
    %false_8977 = torch.constant.bool false
    %5915 = torch.aten.gather %arg3, %int1_8976, %5914, %false_8977 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_8978 = torch.constant.int 4
    %int1_8979 = torch.constant.int 1
    %int1_8980 = torch.constant.int 1
    %5916 = torch.prim.ListConstruct %int4_8978, %int1_8979, %int1_8980 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5917 = torch.aten.view %5915, %5916 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_8981 = torch.constant.int 32
    %5918 = torch.aten.remainder.Scalar %arg2, %int32_8981 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_8982 = torch.constant.int 4
    %int1_8983 = torch.constant.int 1
    %int1_8984 = torch.constant.int 1
    %5919 = torch.prim.ListConstruct %int4_8982, %int1_8983, %int1_8984 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5920 = torch.aten.view %5918, %5919 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_8985 = torch.constant.int 4
    %none_8986 = torch.constant.none
    %none_8987 = torch.constant.none
    %cpu_8988 = torch.constant.device "cpu"
    %false_8989 = torch.constant.bool false
    %5921 = torch.aten.arange %int4_8985, %none_8986, %none_8987, %cpu_8988, %false_8989 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_8990 = torch.constant.int 1
    %int1_8991 = torch.constant.int 1
    %int4_8992 = torch.constant.int 4
    %5922 = torch.prim.ListConstruct %int1_8990, %int1_8991, %int4_8992 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5923 = torch.aten.view %5921, %5922 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_8993 = torch.constant.none
    %5924 = torch.aten.clone %275, %none_8993 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_8994 = torch.constant.int 1
    %int1_8995 = torch.constant.int 1
    %int1_8996 = torch.constant.int 1
    %5925 = torch.prim.ListConstruct %int1_8994, %int1_8995, %int1_8996 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5926 = torch.aten.view %5924, %5925 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_8997 = torch.constant.int 22
    %5927 = torch.aten.mul.Scalar %5917, %int22_8997 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int16 = torch.constant.int 16
    %int1_8998 = torch.constant.int 1
    %5928 = torch.aten.add.Scalar %5927, %int16, %int1_8998 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_8999 = torch.constant.int 2
    %5929 = torch.aten.mul.Scalar %5928, %int2_8999 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_9000 = torch.constant.int 1
    %5930 = torch.aten.add.Tensor %5929, %5926, %int1_9000 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_9001 = torch.constant.int 4
    %5931 = torch.aten.mul.Scalar %5930, %int4_9001 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_9002 = torch.constant.int 1
    %5932 = torch.aten.add.Tensor %5931, %5923, %int1_9002 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_9003 = torch.constant.int 32
    %5933 = torch.aten.mul.Scalar %5932, %int32_9003 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_9004 = torch.constant.int 1
    %5934 = torch.aten.add.Tensor %5933, %5920, %int1_9004 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_9005 = torch.constant.none
    %none_9006 = torch.constant.none
    %int5_9007 = torch.constant.int 5
    %cpu_9008 = torch.constant.device "cpu"
    %int0_9009 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5912, %none_9005, %none_9006, %int5_9007, %cpu_9008, %int0_9009 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_9010 = torch.constant.int 22
    %int2_9011 = torch.constant.int 2
    %int4_9012 = torch.constant.int 4
    %int32_9013 = torch.constant.int 32
    %int64_9014 = torch.constant.int 64
    %5935 = torch.prim.ListConstruct %381, %int22_9010, %int2_9011, %int4_9012, %int32_9013, %int64_9014 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5936 = torch.aten.view %5630, %5935 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5936, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_9015 = torch.constant.int 64
    %5937 = torch.prim.ListConstruct %553, %int64_9015 : (!torch.int, !torch.int) -> !torch.list<int>
    %5938 = torch.aten.view %5936, %5937 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %5938, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %5939 = torch.prim.ListConstruct %5934 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_9016 = torch.constant.bool false
    %5940 = torch.aten.index_put %5938, %5939, %5912, %false_9016 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %5940, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_9017 = torch.constant.int 22
    %int2_9018 = torch.constant.int 2
    %int4_9019 = torch.constant.int 4
    %int32_9020 = torch.constant.int 32
    %int64_9021 = torch.constant.int 64
    %5941 = torch.prim.ListConstruct %381, %int22_9017, %int2_9018, %int4_9019, %int32_9020, %int64_9021 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5942 = torch.aten.view %5940, %5941 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5942, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_9022 = torch.constant.int 360448
    %5943 = torch.prim.ListConstruct %381, %int360448_9022 : (!torch.int, !torch.int) -> !torch.list<int>
    %5944 = torch.aten.view %5942, %5943 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %5944, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_9023 = torch.constant.int 22
    %int2_9024 = torch.constant.int 2
    %int4_9025 = torch.constant.int 4
    %int32_9026 = torch.constant.int 32
    %int64_9027 = torch.constant.int 64
    %5945 = torch.prim.ListConstruct %381, %int22_9023, %int2_9024, %int4_9025, %int32_9026, %int64_9027 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5946 = torch.aten.view %5944, %5945 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5946, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_9028 = torch.constant.int 64
    %5947 = torch.prim.ListConstruct %553, %int64_9028 : (!torch.int, !torch.int) -> !torch.list<int>
    %5948 = torch.aten.view %5946, %5947 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %5948, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_9029 = torch.constant.none
    %5949 = torch.aten.clone %276, %none_9029 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_9030 = torch.constant.int 1
    %int1_9031 = torch.constant.int 1
    %int1_9032 = torch.constant.int 1
    %5950 = torch.prim.ListConstruct %int1_9030, %int1_9031, %int1_9032 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5951 = torch.aten.view %5949, %5950 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_9033 = torch.constant.int 22
    %5952 = torch.aten.mul.Scalar %5917, %int22_9033 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int16_9034 = torch.constant.int 16
    %int1_9035 = torch.constant.int 1
    %5953 = torch.aten.add.Scalar %5952, %int16_9034, %int1_9035 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_9036 = torch.constant.int 2
    %5954 = torch.aten.mul.Scalar %5953, %int2_9036 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_9037 = torch.constant.int 1
    %5955 = torch.aten.add.Tensor %5954, %5951, %int1_9037 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_9038 = torch.constant.int 4
    %5956 = torch.aten.mul.Scalar %5955, %int4_9038 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_9039 = torch.constant.int 1
    %5957 = torch.aten.add.Tensor %5956, %5923, %int1_9039 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_9040 = torch.constant.int 32
    %5958 = torch.aten.mul.Scalar %5957, %int32_9040 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_9041 = torch.constant.int 1
    %5959 = torch.aten.add.Tensor %5958, %5920, %int1_9041 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_9042 = torch.constant.none
    %none_9043 = torch.constant.none
    %int5_9044 = torch.constant.int 5
    %cpu_9045 = torch.constant.device "cpu"
    %int0_9046 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5830, %none_9042, %none_9043, %int5_9044, %cpu_9045, %int0_9046 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %5960 = torch.prim.ListConstruct %5959 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_9047 = torch.constant.bool false
    %5961 = torch.aten.index_put %5948, %5960, %5830, %false_9047 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %5961, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_9048 = torch.constant.int 22
    %int2_9049 = torch.constant.int 2
    %int4_9050 = torch.constant.int 4
    %int32_9051 = torch.constant.int 32
    %int64_9052 = torch.constant.int 64
    %5962 = torch.prim.ListConstruct %381, %int22_9048, %int2_9049, %int4_9050, %int32_9051, %int64_9052 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5963 = torch.aten.view %5961, %5962 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5963, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_9053 = torch.constant.int 360448
    %5964 = torch.prim.ListConstruct %381, %int360448_9053 : (!torch.int, !torch.int) -> !torch.list<int>
    %5965 = torch.aten.view %5963, %5964 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %5965, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_9054 = torch.constant.none
    %5966 = torch.aten.clone %277, %none_9054 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_9055 = torch.constant.none
    %5967 = torch.aten.clone %278, %none_9055 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_9056 = torch.constant.none
    %5968 = torch.aten.clone %279, %none_9056 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_9057 = torch.constant.int 22
    %int2_9058 = torch.constant.int 2
    %int4_9059 = torch.constant.int 4
    %int32_9060 = torch.constant.int 32
    %int64_9061 = torch.constant.int 64
    %5969 = torch.prim.ListConstruct %381, %int22_9057, %int2_9058, %int4_9059, %int32_9060, %int64_9061 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5970 = torch.aten.view %5965, %5969 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %5970, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %5971 = torch_c.to_builtin_tensor %5970 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %5972 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_9062 = tensor.cast %5972 : tensor<4x?xi64> to tensor<?x?xi64>
    %5973 = torch_c.to_builtin_tensor %5966 : !torch.vtensor<[],si64> -> tensor<i64>
    %5974 = torch_c.to_builtin_tensor %5967 : !torch.vtensor<[],si64> -> tensor<i64>
    %5975 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%5971, %cast_9062, %5973, %5974) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_9063 = tensor.cast %5975 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %5976 = torch_c.from_builtin_tensor %cast_9063 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %5976, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %5977 = torch_c.to_builtin_tensor %5970 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %5978 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_9064 = tensor.cast %5978 : tensor<4x?xi64> to tensor<?x?xi64>
    %5979 = torch_c.to_builtin_tensor %5966 : !torch.vtensor<[],si64> -> tensor<i64>
    %5980 = torch_c.to_builtin_tensor %5968 : !torch.vtensor<[],si64> -> tensor<i64>
    %5981 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%5977, %cast_9064, %5979, %5980) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_9065 = tensor.cast %5981 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %5982 = torch_c.from_builtin_tensor %cast_9065 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %5982, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_9066 = torch.constant.int 2
    %int3_9067 = torch.constant.int 3
    %5983 = torch.aten.transpose.int %5976, %int2_9066, %int3_9067 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5983, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_9068 = torch.constant.int 0
    %5984 = torch.aten.clone %5983, %int0_9068 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5984, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_9069 = torch.constant.int 4
    %int4_9070 = torch.constant.int 4
    %int64_9071 = torch.constant.int 64
    %5985 = torch.prim.ListConstruct %int4_9069, %623, %int4_9070, %int64_9071 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5986 = torch.aten._unsafe_view %5984, %5985 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %5986, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_9072 = torch.constant.int 2
    %int3_9073 = torch.constant.int 3
    %5987 = torch.aten.transpose.int %5982, %int2_9072, %int3_9073 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5987, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_9074 = torch.constant.int 0
    %5988 = torch.aten.clone %5987, %int0_9074 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %5988, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_9075 = torch.constant.int 4
    %int4_9076 = torch.constant.int 4
    %int64_9077 = torch.constant.int 64
    %5989 = torch.prim.ListConstruct %int4_9075, %623, %int4_9076, %int64_9077 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5990 = torch.aten._unsafe_view %5988, %5989 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %5990, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_9078 = torch.constant.int 0
    %int1_9079 = torch.constant.int 1
    %none_9080 = torch.constant.none
    %none_9081 = torch.constant.none
    %cpu_9082 = torch.constant.device "cpu"
    %false_9083 = torch.constant.bool false
    %5991 = torch.aten.arange.start_step %int0_9078, %623, %int1_9079, %none_9080, %none_9081, %cpu_9082, %false_9083 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5991, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_9084 = torch.constant.int -1
    %5992 = torch.aten.unsqueeze %arg1, %int-1_9084 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %5993 = torch.aten.ge.Tensor %5991, %5992 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %5993, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_9085 = torch.constant.none
    %5994 = torch.aten.clone %280, %none_9085 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_9086 = torch.constant.int 0
    %5995 = torch.aten.where.ScalarOther %5993, %5994, %int0_9086 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %5995, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_9087 = torch.constant.none
    %none_9088 = torch.constant.none
    %int5_9089 = torch.constant.int 5
    %cpu_9090 = torch.constant.device "cpu"
    %int0_9091 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %5995, %none_9087, %none_9088, %int5_9089, %cpu_9090, %int0_9091 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_9092 = torch.constant.int 1
    %5996 = torch.aten.unsqueeze %5995, %int1_9092 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %5996, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_9093 = torch.constant.int 1
    %5997 = torch.aten.unsqueeze %5996, %int1_9093 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %5997, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_9094 = torch.constant.int -2
    %5998 = torch.aten.unsqueeze %5986, %int-2_9094 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %5998, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_9095 = torch.constant.int 4
    %int4_9096 = torch.constant.int 4
    %int8_9097 = torch.constant.int 8
    %int64_9098 = torch.constant.int 64
    %5999 = torch.prim.ListConstruct %int4_9095, %623, %int4_9096, %int8_9097, %int64_9098 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_9099 = torch.constant.bool false
    %6000 = torch.aten.expand %5998, %5999, %false_9099 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6000, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_9100 = torch.constant.int 0
    %6001 = torch.aten.clone %6000, %int0_9100 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6001, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_9101 = torch.constant.int 4
    %int32_9102 = torch.constant.int 32
    %int64_9103 = torch.constant.int 64
    %6002 = torch.prim.ListConstruct %int4_9101, %623, %int32_9102, %int64_9103 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6003 = torch.aten._unsafe_view %6001, %6002 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6003, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_9104 = torch.constant.int -2
    %6004 = torch.aten.unsqueeze %5990, %int-2_9104 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %6004, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_9105 = torch.constant.int 4
    %int4_9106 = torch.constant.int 4
    %int8_9107 = torch.constant.int 8
    %int64_9108 = torch.constant.int 64
    %6005 = torch.prim.ListConstruct %int4_9105, %623, %int4_9106, %int8_9107, %int64_9108 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_9109 = torch.constant.bool false
    %6006 = torch.aten.expand %6004, %6005, %false_9109 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6006, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_9110 = torch.constant.int 0
    %6007 = torch.aten.clone %6006, %int0_9110 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6007, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_9111 = torch.constant.int 4
    %int32_9112 = torch.constant.int 32
    %int64_9113 = torch.constant.int 64
    %6008 = torch.prim.ListConstruct %int4_9111, %623, %int32_9112, %int64_9113 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6009 = torch.aten._unsafe_view %6007, %6008 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6009, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_9114 = torch.constant.int 1
    %int2_9115 = torch.constant.int 2
    %6010 = torch.aten.transpose.int %5871, %int1_9114, %int2_9115 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_9116 = torch.constant.int 1
    %int2_9117 = torch.constant.int 2
    %6011 = torch.aten.transpose.int %6003, %int1_9116, %int2_9117 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6011, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_9118 = torch.constant.int 1
    %int2_9119 = torch.constant.int 2
    %6012 = torch.aten.transpose.int %6009, %int1_9118, %int2_9119 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6012, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_9120 = torch.constant.float 0.000000e+00
    %false_9121 = torch.constant.bool false
    %none_9122 = torch.constant.none
    %false_9123 = torch.constant.bool false
    %6013 = torch.aten.scaled_dot_product_attention %6010, %6011, %6012, %5997, %float0.000000e00_9120, %false_9121, %none_9122, %false_9123 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_9124 = torch.constant.int 1
    %int2_9125 = torch.constant.int 2
    %6014 = torch.aten.transpose.int %6013, %int1_9124, %int2_9125 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_9126 = torch.constant.int 4
    %int1_9127 = torch.constant.int 1
    %int2048_9128 = torch.constant.int 2048
    %6015 = torch.prim.ListConstruct %int4_9126, %int1_9127, %int2048_9128 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6016 = torch.aten.view %6014, %6015 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_9129 = torch.constant.int 2
    %6017 = torch.aten.view.dtype %285, %int2_9129 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %6018 = torch.aten.detach %6017 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_9130 = torch.constant.int -1
    %int17_9131 = torch.constant.int 17
    %6019 = torch.prim.ListConstruct %int-1_9130, %int17_9131 : (!torch.int, !torch.int) -> !torch.list<int>
    %6020 = torch.aten.view %6018, %6019 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_9132 = torch.constant.int 2048
    %int-1_9133 = torch.constant.int -1
    %int17_9134 = torch.constant.int 17
    %6021 = torch.prim.ListConstruct %int2048_9132, %int-1_9133, %int17_9134 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6022 = torch.aten.view %6020, %6021 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_9135 = torch.constant.int 2
    %int0_9136 = torch.constant.int 0
    %int1_9137 = torch.constant.int 1
    %int1_9138 = torch.constant.int 1
    %6023 = torch.aten.slice.Tensor %6022, %int2_9135, %int0_9136, %int1_9137, %int1_9138 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_9139 = torch.constant.int 5
    %6024 = torch.aten.view.dtype %6023, %int5_9139 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %6025 = torch.aten.detach %6024 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_9140 = torch.constant.int 2
    %int1_9141 = torch.constant.int 1
    %int9223372036854775807_9142 = torch.constant.int 9223372036854775807
    %int1_9143 = torch.constant.int 1
    %6026 = torch.aten.slice.Tensor %6022, %int2_9140, %int1_9141, %int9223372036854775807_9142, %int1_9143 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_9144 = torch.constant.int 1
    %6027 = torch.aten.view.dtype %6026, %int1_9144 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %6028 = torch.aten.detach %6027 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %6029 = torch_c.to_builtin_tensor %6016 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_9145 = tensor.cast %6029 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %6030 = torch_c.to_builtin_tensor %6025 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %6031 = torch_c.to_builtin_tensor %6028 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %6032 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_9145, %6030, %6031) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_9146 = tensor.cast %6032 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %6033 = torch_c.from_builtin_tensor %cast_9146 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_9147 = torch.constant.none
    %none_9148 = torch.constant.none
    %int5_9149 = torch.constant.int 5
    %cpu_9150 = torch.constant.device "cpu"
    %int0_9151 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6033, %none_9147, %none_9148, %int5_9149, %cpu_9150, %int0_9151 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_9152 = torch.constant.int 1
    %6034 = torch.aten.add.Tensor %5763, %6033, %int1_9152 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_9153 = torch.constant.none
    %none_9154 = torch.constant.none
    %int5_9155 = torch.constant.int 5
    %cpu_9156 = torch.constant.device "cpu"
    %int0_9157 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6034, %none_9153, %none_9154, %int5_9155, %cpu_9156, %int0_9157 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9158 = torch.constant.int 6
    %6035 = torch.prims.convert_element_type %6034, %int6_9158 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_9159 = torch.constant.int 2
    %6036 = torch.aten.pow.Tensor_Scalar %6035, %int2_9159 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_9160 = torch.constant.int -1
    %6037 = torch.prim.ListConstruct %int-1_9160 : (!torch.int) -> !torch.list<int>
    %true_9161 = torch.constant.bool true
    %none_9162 = torch.constant.none
    %6038 = torch.aten.mean.dim %6036, %6037, %true_9161, %none_9162 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_9163 = torch.constant.float 9.9999997473787516E-6
    %int1_9164 = torch.constant.int 1
    %6039 = torch.aten.add.Scalar %6038, %float9.999990e-06_9163, %int1_9164 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6040 = torch.aten.rsqrt %6039 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %6041 = torch.aten.mul.Tensor %6035, %6040 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_9165 = torch.constant.none
    %none_9166 = torch.constant.none
    %int6_9167 = torch.constant.int 6
    %cpu_9168 = torch.constant.device "cpu"
    %int0_9169 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6041, %none_9165, %none_9166, %int6_9167, %cpu_9168, %int0_9169 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9170 = torch.constant.int 5
    %6042 = torch.prims.convert_element_type %6041, %int5_9170 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %6043 = torch.aten.mul.Tensor %286, %6042 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_9171 = torch.constant.none
    %none_9172 = torch.constant.none
    %int6_9173 = torch.constant.int 6
    %cpu_9174 = torch.constant.device "cpu"
    %int0_9175 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6043, %none_9171, %none_9172, %int6_9173, %cpu_9174, %int0_9175 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9176 = torch.constant.int 5
    %6044 = torch.prims.convert_element_type %6043, %int5_9176 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_9177 = torch.constant.int 2
    %6045 = torch.aten.view.dtype %287, %int2_9177 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %6046 = torch.aten.detach %6045 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_9178 = torch.constant.int -1
    %int17_9179 = torch.constant.int 17
    %6047 = torch.prim.ListConstruct %int-1_9178, %int17_9179 : (!torch.int, !torch.int) -> !torch.list<int>
    %6048 = torch.aten.view %6046, %6047 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_9180 = torch.constant.int 5632
    %int-1_9181 = torch.constant.int -1
    %int17_9182 = torch.constant.int 17
    %6049 = torch.prim.ListConstruct %int5632_9180, %int-1_9181, %int17_9182 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6050 = torch.aten.view %6048, %6049 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_9183 = torch.constant.int 2
    %int0_9184 = torch.constant.int 0
    %int1_9185 = torch.constant.int 1
    %int1_9186 = torch.constant.int 1
    %6051 = torch.aten.slice.Tensor %6050, %int2_9183, %int0_9184, %int1_9185, %int1_9186 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_9187 = torch.constant.int 5
    %6052 = torch.aten.view.dtype %6051, %int5_9187 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %6053 = torch.aten.detach %6052 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_9188 = torch.constant.int 2
    %int1_9189 = torch.constant.int 1
    %int9223372036854775807_9190 = torch.constant.int 9223372036854775807
    %int1_9191 = torch.constant.int 1
    %6054 = torch.aten.slice.Tensor %6050, %int2_9188, %int1_9189, %int9223372036854775807_9190, %int1_9191 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_9192 = torch.constant.int 1
    %6055 = torch.aten.view.dtype %6054, %int1_9192 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %6056 = torch.aten.detach %6055 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %6057 = torch_c.to_builtin_tensor %6044 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_9193 = tensor.cast %6057 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %6058 = torch_c.to_builtin_tensor %6053 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %6059 = torch_c.to_builtin_tensor %6056 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %6060 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_9193, %6058, %6059) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_9194 = tensor.cast %6060 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %6061 = torch_c.from_builtin_tensor %cast_9194 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %6062 = torch.aten.silu %6061 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_9195 = torch.constant.int 2
    %6063 = torch.aten.view.dtype %288, %int2_9195 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %6064 = torch.aten.detach %6063 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_9196 = torch.constant.int -1
    %int17_9197 = torch.constant.int 17
    %6065 = torch.prim.ListConstruct %int-1_9196, %int17_9197 : (!torch.int, !torch.int) -> !torch.list<int>
    %6066 = torch.aten.view %6064, %6065 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_9198 = torch.constant.int 5632
    %int-1_9199 = torch.constant.int -1
    %int17_9200 = torch.constant.int 17
    %6067 = torch.prim.ListConstruct %int5632_9198, %int-1_9199, %int17_9200 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6068 = torch.aten.view %6066, %6067 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_9201 = torch.constant.int 2
    %int0_9202 = torch.constant.int 0
    %int1_9203 = torch.constant.int 1
    %int1_9204 = torch.constant.int 1
    %6069 = torch.aten.slice.Tensor %6068, %int2_9201, %int0_9202, %int1_9203, %int1_9204 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_9205 = torch.constant.int 5
    %6070 = torch.aten.view.dtype %6069, %int5_9205 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %6071 = torch.aten.detach %6070 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_9206 = torch.constant.int 2
    %int1_9207 = torch.constant.int 1
    %int9223372036854775807_9208 = torch.constant.int 9223372036854775807
    %int1_9209 = torch.constant.int 1
    %6072 = torch.aten.slice.Tensor %6068, %int2_9206, %int1_9207, %int9223372036854775807_9208, %int1_9209 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_9210 = torch.constant.int 1
    %6073 = torch.aten.view.dtype %6072, %int1_9210 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %6074 = torch.aten.detach %6073 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %6075 = torch_c.to_builtin_tensor %6044 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_9211 = tensor.cast %6075 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %6076 = torch_c.to_builtin_tensor %6071 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %6077 = torch_c.to_builtin_tensor %6074 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %6078 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_9211, %6076, %6077) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_9212 = tensor.cast %6078 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %6079 = torch_c.from_builtin_tensor %cast_9212 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %6080 = torch.aten.mul.Tensor %6062, %6079 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_9213 = torch.constant.int 2
    %6081 = torch.aten.view.dtype %289, %int2_9213 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %6082 = torch.aten.detach %6081 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_9214 = torch.constant.int -1
    %int17_9215 = torch.constant.int 17
    %6083 = torch.prim.ListConstruct %int-1_9214, %int17_9215 : (!torch.int, !torch.int) -> !torch.list<int>
    %6084 = torch.aten.view %6082, %6083 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_9216 = torch.constant.int 2048
    %int-1_9217 = torch.constant.int -1
    %int17_9218 = torch.constant.int 17
    %6085 = torch.prim.ListConstruct %int2048_9216, %int-1_9217, %int17_9218 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6086 = torch.aten.view %6084, %6085 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_9219 = torch.constant.int 2
    %int0_9220 = torch.constant.int 0
    %int1_9221 = torch.constant.int 1
    %int1_9222 = torch.constant.int 1
    %6087 = torch.aten.slice.Tensor %6086, %int2_9219, %int0_9220, %int1_9221, %int1_9222 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_9223 = torch.constant.int 5
    %6088 = torch.aten.view.dtype %6087, %int5_9223 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %6089 = torch.aten.detach %6088 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_9224 = torch.constant.int 2
    %int1_9225 = torch.constant.int 1
    %int9223372036854775807_9226 = torch.constant.int 9223372036854775807
    %int1_9227 = torch.constant.int 1
    %6090 = torch.aten.slice.Tensor %6086, %int2_9224, %int1_9225, %int9223372036854775807_9226, %int1_9227 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_9228 = torch.constant.int 1
    %6091 = torch.aten.view.dtype %6090, %int1_9228 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %6092 = torch.aten.detach %6091 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %6093 = torch_c.to_builtin_tensor %6080 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_9229 = tensor.cast %6093 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %6094 = torch_c.to_builtin_tensor %6089 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %6095 = torch_c.to_builtin_tensor %6092 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %6096 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_9229, %6094, %6095) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_9230 = tensor.cast %6096 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %6097 = torch_c.from_builtin_tensor %cast_9230 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_9231 = torch.constant.int 1
    %6098 = torch.aten.add.Tensor %6034, %6097, %int1_9231 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_9232 = torch.constant.none
    %none_9233 = torch.constant.none
    %int5_9234 = torch.constant.int 5
    %cpu_9235 = torch.constant.device "cpu"
    %int0_9236 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6098, %none_9232, %none_9233, %int5_9234, %cpu_9235, %int0_9236 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9237 = torch.constant.int 6
    %6099 = torch.prims.convert_element_type %6098, %int6_9237 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_9238 = torch.constant.int 2
    %6100 = torch.aten.pow.Tensor_Scalar %6099, %int2_9238 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_9239 = torch.constant.int -1
    %6101 = torch.prim.ListConstruct %int-1_9239 : (!torch.int) -> !torch.list<int>
    %true_9240 = torch.constant.bool true
    %none_9241 = torch.constant.none
    %6102 = torch.aten.mean.dim %6100, %6101, %true_9240, %none_9241 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_9242 = torch.constant.float 9.9999997473787516E-6
    %int1_9243 = torch.constant.int 1
    %6103 = torch.aten.add.Scalar %6102, %float9.999990e-06_9242, %int1_9243 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6104 = torch.aten.rsqrt %6103 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %6105 = torch.aten.mul.Tensor %6099, %6104 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_9244 = torch.constant.none
    %none_9245 = torch.constant.none
    %int6_9246 = torch.constant.int 6
    %cpu_9247 = torch.constant.device "cpu"
    %int0_9248 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6105, %none_9244, %none_9245, %int6_9246, %cpu_9247, %int0_9248 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9249 = torch.constant.int 5
    %6106 = torch.prims.convert_element_type %6105, %int5_9249 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %6107 = torch.aten.mul.Tensor %298, %6106 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_9250 = torch.constant.none
    %none_9251 = torch.constant.none
    %int6_9252 = torch.constant.int 6
    %cpu_9253 = torch.constant.device "cpu"
    %int0_9254 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6107, %none_9250, %none_9251, %int6_9252, %cpu_9253, %int0_9254 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9255 = torch.constant.int 5
    %6108 = torch.prims.convert_element_type %6107, %int5_9255 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_9256 = torch.constant.int 2
    %6109 = torch.aten.view.dtype %299, %int2_9256 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %6110 = torch.aten.detach %6109 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_9257 = torch.constant.int -1
    %int17_9258 = torch.constant.int 17
    %6111 = torch.prim.ListConstruct %int-1_9257, %int17_9258 : (!torch.int, !torch.int) -> !torch.list<int>
    %6112 = torch.aten.view %6110, %6111 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_9259 = torch.constant.int 2048
    %int-1_9260 = torch.constant.int -1
    %int17_9261 = torch.constant.int 17
    %6113 = torch.prim.ListConstruct %int2048_9259, %int-1_9260, %int17_9261 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6114 = torch.aten.view %6112, %6113 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_9262 = torch.constant.int 2
    %int0_9263 = torch.constant.int 0
    %int1_9264 = torch.constant.int 1
    %int1_9265 = torch.constant.int 1
    %6115 = torch.aten.slice.Tensor %6114, %int2_9262, %int0_9263, %int1_9264, %int1_9265 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_9266 = torch.constant.int 5
    %6116 = torch.aten.view.dtype %6115, %int5_9266 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %6117 = torch.aten.detach %6116 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_9267 = torch.constant.int 2
    %int1_9268 = torch.constant.int 1
    %int9223372036854775807_9269 = torch.constant.int 9223372036854775807
    %int1_9270 = torch.constant.int 1
    %6118 = torch.aten.slice.Tensor %6114, %int2_9267, %int1_9268, %int9223372036854775807_9269, %int1_9270 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_9271 = torch.constant.int 1
    %6119 = torch.aten.view.dtype %6118, %int1_9271 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %6120 = torch.aten.detach %6119 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %6121 = torch_c.to_builtin_tensor %6108 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_9272 = tensor.cast %6121 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %6122 = torch_c.to_builtin_tensor %6117 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %6123 = torch_c.to_builtin_tensor %6120 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %6124 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_9272, %6122, %6123) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_9273 = tensor.cast %6124 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %6125 = torch_c.from_builtin_tensor %cast_9273 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_9274 = torch.constant.int 2
    %6126 = torch.aten.view.dtype %300, %int2_9274 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %6127 = torch.aten.detach %6126 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_9275 = torch.constant.int -1
    %int17_9276 = torch.constant.int 17
    %6128 = torch.prim.ListConstruct %int-1_9275, %int17_9276 : (!torch.int, !torch.int) -> !torch.list<int>
    %6129 = torch.aten.view %6127, %6128 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_9277 = torch.constant.int 256
    %int-1_9278 = torch.constant.int -1
    %int17_9279 = torch.constant.int 17
    %6130 = torch.prim.ListConstruct %int256_9277, %int-1_9278, %int17_9279 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6131 = torch.aten.view %6129, %6130 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_9280 = torch.constant.int 2
    %int0_9281 = torch.constant.int 0
    %int1_9282 = torch.constant.int 1
    %int1_9283 = torch.constant.int 1
    %6132 = torch.aten.slice.Tensor %6131, %int2_9280, %int0_9281, %int1_9282, %int1_9283 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_9284 = torch.constant.int 5
    %6133 = torch.aten.view.dtype %6132, %int5_9284 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %6134 = torch.aten.detach %6133 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_9285 = torch.constant.int 2
    %int1_9286 = torch.constant.int 1
    %int9223372036854775807_9287 = torch.constant.int 9223372036854775807
    %int1_9288 = torch.constant.int 1
    %6135 = torch.aten.slice.Tensor %6131, %int2_9285, %int1_9286, %int9223372036854775807_9287, %int1_9288 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_9289 = torch.constant.int 1
    %6136 = torch.aten.view.dtype %6135, %int1_9289 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %6137 = torch.aten.detach %6136 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %6138 = torch_c.to_builtin_tensor %6108 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_9290 = tensor.cast %6138 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %6139 = torch_c.to_builtin_tensor %6134 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %6140 = torch_c.to_builtin_tensor %6137 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %6141 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_9290, %6139, %6140) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_9291 = tensor.cast %6141 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %6142 = torch_c.from_builtin_tensor %cast_9291 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_9292 = torch.constant.int 2
    %6143 = torch.aten.view.dtype %301, %int2_9292 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %6144 = torch.aten.detach %6143 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_9293 = torch.constant.int -1
    %int17_9294 = torch.constant.int 17
    %6145 = torch.prim.ListConstruct %int-1_9293, %int17_9294 : (!torch.int, !torch.int) -> !torch.list<int>
    %6146 = torch.aten.view %6144, %6145 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_9295 = torch.constant.int 256
    %int-1_9296 = torch.constant.int -1
    %int17_9297 = torch.constant.int 17
    %6147 = torch.prim.ListConstruct %int256_9295, %int-1_9296, %int17_9297 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6148 = torch.aten.view %6146, %6147 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_9298 = torch.constant.int 2
    %int0_9299 = torch.constant.int 0
    %int1_9300 = torch.constant.int 1
    %int1_9301 = torch.constant.int 1
    %6149 = torch.aten.slice.Tensor %6148, %int2_9298, %int0_9299, %int1_9300, %int1_9301 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_9302 = torch.constant.int 5
    %6150 = torch.aten.view.dtype %6149, %int5_9302 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %6151 = torch.aten.detach %6150 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_9303 = torch.constant.int 2
    %int1_9304 = torch.constant.int 1
    %int9223372036854775807_9305 = torch.constant.int 9223372036854775807
    %int1_9306 = torch.constant.int 1
    %6152 = torch.aten.slice.Tensor %6148, %int2_9303, %int1_9304, %int9223372036854775807_9305, %int1_9306 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_9307 = torch.constant.int 1
    %6153 = torch.aten.view.dtype %6152, %int1_9307 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %6154 = torch.aten.detach %6153 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %6155 = torch_c.to_builtin_tensor %6108 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_9308 = tensor.cast %6155 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %6156 = torch_c.to_builtin_tensor %6151 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %6157 = torch_c.to_builtin_tensor %6154 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %6158 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_9308, %6156, %6157) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_9309 = tensor.cast %6158 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %6159 = torch_c.from_builtin_tensor %cast_9309 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_9310 = torch.constant.int 4
    %int1_9311 = torch.constant.int 1
    %int32_9312 = torch.constant.int 32
    %int64_9313 = torch.constant.int 64
    %6160 = torch.prim.ListConstruct %int4_9310, %int1_9311, %int32_9312, %int64_9313 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6161 = torch.aten.view %6125, %6160 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_9314 = torch.constant.int 4
    %int1_9315 = torch.constant.int 1
    %int4_9316 = torch.constant.int 4
    %int64_9317 = torch.constant.int 64
    %6162 = torch.prim.ListConstruct %int4_9314, %int1_9315, %int4_9316, %int64_9317 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6163 = torch.aten.view %6142, %6162 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_9318 = torch.constant.int 4
    %int1_9319 = torch.constant.int 1
    %int4_9320 = torch.constant.int 4
    %int64_9321 = torch.constant.int 64
    %6164 = torch.prim.ListConstruct %int4_9318, %int1_9319, %int4_9320, %int64_9321 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6165 = torch.aten.view %6159, %6164 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_9322 = torch.constant.int 0
    %int1_9323 = torch.constant.int 1
    %none_9324 = torch.constant.none
    %none_9325 = torch.constant.none
    %cpu_9326 = torch.constant.device "cpu"
    %false_9327 = torch.constant.bool false
    %6166 = torch.aten.arange.start %int0_9322, %int1_9323, %none_9324, %none_9325, %cpu_9326, %false_9327 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_9328 = torch.constant.int 0
    %6167 = torch.aten.unsqueeze %6166, %int0_9328 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_9329 = torch.constant.int 1
    %6168 = torch.aten.unsqueeze %arg2, %int1_9329 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_9330 = torch.constant.int 1
    %6169 = torch.aten.add.Tensor %6167, %6168, %int1_9330 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_9331 = torch.constant.int 0
    %int64_9332 = torch.constant.int 64
    %int2_9333 = torch.constant.int 2
    %none_9334 = torch.constant.none
    %none_9335 = torch.constant.none
    %cpu_9336 = torch.constant.device "cpu"
    %false_9337 = torch.constant.bool false
    %6170 = torch.aten.arange.start_step %int0_9331, %int64_9332, %int2_9333, %none_9334, %none_9335, %cpu_9336, %false_9337 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_9338 = torch.constant.none
    %none_9339 = torch.constant.none
    %int4_9340 = torch.constant.int 4
    %cpu_9341 = torch.constant.device "cpu"
    %int0_9342 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6170, %none_9338, %none_9339, %int4_9340, %cpu_9341, %int0_9342 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9343 = torch.constant.int 6
    %6171 = torch.prims.convert_element_type %6170, %int6_9343 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_9344 = torch.constant.int 64
    %6172 = torch.aten.div.Scalar %6171, %int64_9344 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_9345 = torch.constant.float 1.000000e+04
    %6173 = torch.aten.pow.Scalar %float1.000000e04_9345, %6172 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %6174 = torch.aten.reciprocal %6173 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_9346 = torch.constant.float 1.000000e+00
    %6175 = torch.aten.mul.Scalar %6174, %float1.000000e00_9346 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_9347 = torch.constant.none
    %6176 = torch.aten.clone %290, %none_9347 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_9348 = torch.constant.int 0
    %6177 = torch.aten.unsqueeze %6175, %int0_9348 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_9349 = torch.constant.int 2
    %6178 = torch.aten.unsqueeze %6177, %int2_9349 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_9350 = torch.constant.none
    %none_9351 = torch.constant.none
    %int6_9352 = torch.constant.int 6
    %cpu_9353 = torch.constant.device "cpu"
    %int0_9354 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6178, %none_9350, %none_9351, %int6_9352, %cpu_9353, %int0_9354 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_9355 = torch.constant.int 4
    %int-1_9356 = torch.constant.int -1
    %int1_9357 = torch.constant.int 1
    %6179 = torch.prim.ListConstruct %int4_9355, %int-1_9356, %int1_9357 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_9358 = torch.constant.bool false
    %6180 = torch.aten.expand %6178, %6179, %false_9358 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_9359 = torch.constant.int 1
    %6181 = torch.aten.unsqueeze %6169, %int1_9359 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_9360 = torch.constant.none
    %none_9361 = torch.constant.none
    %int4_9362 = torch.constant.int 4
    %cpu_9363 = torch.constant.device "cpu"
    %int0_9364 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6181, %none_9360, %none_9361, %int4_9362, %cpu_9363, %int0_9364 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9365 = torch.constant.int 6
    %6182 = torch.prims.convert_element_type %6181, %int6_9365 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6183 = torch.aten.matmul %6180, %6182 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_9366 = torch.constant.int 1
    %int2_9367 = torch.constant.int 2
    %6184 = torch.aten.transpose.int %6183, %int1_9366, %int2_9367 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %6185 = torch.aten.cos %6184 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %6186 = torch.aten.mul.Tensor %6185, %6176 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_9368 = torch.constant.none
    %none_9369 = torch.constant.none
    %int6_9370 = torch.constant.int 6
    %cpu_9371 = torch.constant.device "cpu"
    %int0_9372 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6186, %none_9368, %none_9369, %int6_9370, %cpu_9371, %int0_9372 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9373 = torch.constant.int 5
    %6187 = torch.prims.convert_element_type %6186, %int5_9373 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %6188 = torch.aten.sin %6184 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %6189 = torch.aten.mul.Tensor %6188, %6176 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_9374 = torch.constant.none
    %none_9375 = torch.constant.none
    %int6_9376 = torch.constant.int 6
    %cpu_9377 = torch.constant.device "cpu"
    %int0_9378 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6189, %none_9374, %none_9375, %int6_9376, %cpu_9377, %int0_9378 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9379 = torch.constant.int 5
    %6190 = torch.prims.convert_element_type %6189, %int5_9379 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_9380 = torch.constant.int 2
    %6191 = torch.aten.unsqueeze %6187, %int2_9380 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_9381 = torch.constant.int 2
    %6192 = torch.aten.unsqueeze %6190, %int2_9381 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_9382 = torch.constant.none
    %none_9383 = torch.constant.none
    %int5_9384 = torch.constant.int 5
    %cpu_9385 = torch.constant.device "cpu"
    %int0_9386 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6191, %none_9382, %none_9383, %int5_9384, %cpu_9385, %int0_9386 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_9387 = torch.constant.none
    %none_9388 = torch.constant.none
    %int5_9389 = torch.constant.int 5
    %cpu_9390 = torch.constant.device "cpu"
    %int0_9391 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6192, %none_9387, %none_9388, %int5_9389, %cpu_9390, %int0_9391 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_9392 = torch.constant.none
    %none_9393 = torch.constant.none
    %int5_9394 = torch.constant.int 5
    %cpu_9395 = torch.constant.device "cpu"
    %int0_9396 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6161, %none_9392, %none_9393, %int5_9394, %cpu_9395, %int0_9396 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_9397 = torch.constant.int 3
    %int0_9398 = torch.constant.int 0
    %int64_9399 = torch.constant.int 64
    %int2_9400 = torch.constant.int 2
    %6193 = torch.aten.slice.Tensor %6161, %int3_9397, %int0_9398, %int64_9399, %int2_9400 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_9401 = torch.constant.int 3
    %int1_9402 = torch.constant.int 1
    %int64_9403 = torch.constant.int 64
    %int2_9404 = torch.constant.int 2
    %6194 = torch.aten.slice.Tensor %6161, %int3_9401, %int1_9402, %int64_9403, %int2_9404 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %6195 = torch.aten.mul.Tensor %6193, %6191 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %6196 = torch.aten.mul.Tensor %6194, %6192 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_9405 = torch.constant.int 1
    %6197 = torch.aten.sub.Tensor %6195, %6196, %int1_9405 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %6198 = torch.aten.mul.Tensor %6194, %6191 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %6199 = torch.aten.mul.Tensor %6193, %6192 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_9406 = torch.constant.int 1
    %6200 = torch.aten.add.Tensor %6198, %6199, %int1_9406 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %6201 = torch_c.to_builtin_tensor %6197 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_9407 = tensor.cast %6201 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %6202 = torch_c.to_builtin_tensor %6200 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_9408 = tensor.cast %6202 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %6203 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_9407, %cast_9408) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_9409 = tensor.cast %6203 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %6204 = torch_c.from_builtin_tensor %cast_9409 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_9410 = torch.constant.int 4
    %int1_9411 = torch.constant.int 1
    %int32_9412 = torch.constant.int 32
    %int64_9413 = torch.constant.int 64
    %6205 = torch.prim.ListConstruct %int4_9410, %int1_9411, %int32_9412, %int64_9413 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6206 = torch.aten.view %6204, %6205 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_9414 = torch.constant.none
    %none_9415 = torch.constant.none
    %int5_9416 = torch.constant.int 5
    %cpu_9417 = torch.constant.device "cpu"
    %int0_9418 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6206, %none_9414, %none_9415, %int5_9416, %cpu_9417, %int0_9418 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_9419 = torch.constant.int 0
    %int1_9420 = torch.constant.int 1
    %none_9421 = torch.constant.none
    %none_9422 = torch.constant.none
    %cpu_9423 = torch.constant.device "cpu"
    %false_9424 = torch.constant.bool false
    %6207 = torch.aten.arange.start %int0_9419, %int1_9420, %none_9421, %none_9422, %cpu_9423, %false_9424 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_9425 = torch.constant.int 0
    %6208 = torch.aten.unsqueeze %6207, %int0_9425 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_9426 = torch.constant.int 1
    %6209 = torch.aten.unsqueeze %arg2, %int1_9426 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_9427 = torch.constant.int 1
    %6210 = torch.aten.add.Tensor %6208, %6209, %int1_9427 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_9428 = torch.constant.int 0
    %int64_9429 = torch.constant.int 64
    %int2_9430 = torch.constant.int 2
    %none_9431 = torch.constant.none
    %none_9432 = torch.constant.none
    %cpu_9433 = torch.constant.device "cpu"
    %false_9434 = torch.constant.bool false
    %6211 = torch.aten.arange.start_step %int0_9428, %int64_9429, %int2_9430, %none_9431, %none_9432, %cpu_9433, %false_9434 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_9435 = torch.constant.none
    %none_9436 = torch.constant.none
    %int4_9437 = torch.constant.int 4
    %cpu_9438 = torch.constant.device "cpu"
    %int0_9439 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6211, %none_9435, %none_9436, %int4_9437, %cpu_9438, %int0_9439 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9440 = torch.constant.int 6
    %6212 = torch.prims.convert_element_type %6211, %int6_9440 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_9441 = torch.constant.int 64
    %6213 = torch.aten.div.Scalar %6212, %int64_9441 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_9442 = torch.constant.float 1.000000e+04
    %6214 = torch.aten.pow.Scalar %float1.000000e04_9442, %6213 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %6215 = torch.aten.reciprocal %6214 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_9443 = torch.constant.float 1.000000e+00
    %6216 = torch.aten.mul.Scalar %6215, %float1.000000e00_9443 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_9444 = torch.constant.none
    %6217 = torch.aten.clone %291, %none_9444 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_9445 = torch.constant.int 0
    %6218 = torch.aten.unsqueeze %6216, %int0_9445 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_9446 = torch.constant.int 2
    %6219 = torch.aten.unsqueeze %6218, %int2_9446 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_9447 = torch.constant.none
    %none_9448 = torch.constant.none
    %int6_9449 = torch.constant.int 6
    %cpu_9450 = torch.constant.device "cpu"
    %int0_9451 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6219, %none_9447, %none_9448, %int6_9449, %cpu_9450, %int0_9451 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_9452 = torch.constant.int 4
    %int-1_9453 = torch.constant.int -1
    %int1_9454 = torch.constant.int 1
    %6220 = torch.prim.ListConstruct %int4_9452, %int-1_9453, %int1_9454 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_9455 = torch.constant.bool false
    %6221 = torch.aten.expand %6219, %6220, %false_9455 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_9456 = torch.constant.int 1
    %6222 = torch.aten.unsqueeze %6210, %int1_9456 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_9457 = torch.constant.none
    %none_9458 = torch.constant.none
    %int4_9459 = torch.constant.int 4
    %cpu_9460 = torch.constant.device "cpu"
    %int0_9461 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6222, %none_9457, %none_9458, %int4_9459, %cpu_9460, %int0_9461 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9462 = torch.constant.int 6
    %6223 = torch.prims.convert_element_type %6222, %int6_9462 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6224 = torch.aten.matmul %6221, %6223 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_9463 = torch.constant.int 1
    %int2_9464 = torch.constant.int 2
    %6225 = torch.aten.transpose.int %6224, %int1_9463, %int2_9464 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %6226 = torch.aten.cos %6225 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %6227 = torch.aten.mul.Tensor %6226, %6217 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_9465 = torch.constant.none
    %none_9466 = torch.constant.none
    %int6_9467 = torch.constant.int 6
    %cpu_9468 = torch.constant.device "cpu"
    %int0_9469 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6227, %none_9465, %none_9466, %int6_9467, %cpu_9468, %int0_9469 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9470 = torch.constant.int 5
    %6228 = torch.prims.convert_element_type %6227, %int5_9470 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %6229 = torch.aten.sin %6225 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %6230 = torch.aten.mul.Tensor %6229, %6217 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_9471 = torch.constant.none
    %none_9472 = torch.constant.none
    %int6_9473 = torch.constant.int 6
    %cpu_9474 = torch.constant.device "cpu"
    %int0_9475 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6230, %none_9471, %none_9472, %int6_9473, %cpu_9474, %int0_9475 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9476 = torch.constant.int 5
    %6231 = torch.prims.convert_element_type %6230, %int5_9476 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_9477 = torch.constant.int 2
    %6232 = torch.aten.unsqueeze %6228, %int2_9477 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_9478 = torch.constant.int 2
    %6233 = torch.aten.unsqueeze %6231, %int2_9478 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_9479 = torch.constant.none
    %none_9480 = torch.constant.none
    %int5_9481 = torch.constant.int 5
    %cpu_9482 = torch.constant.device "cpu"
    %int0_9483 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6232, %none_9479, %none_9480, %int5_9481, %cpu_9482, %int0_9483 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_9484 = torch.constant.none
    %none_9485 = torch.constant.none
    %int5_9486 = torch.constant.int 5
    %cpu_9487 = torch.constant.device "cpu"
    %int0_9488 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6233, %none_9484, %none_9485, %int5_9486, %cpu_9487, %int0_9488 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_9489 = torch.constant.none
    %none_9490 = torch.constant.none
    %int5_9491 = torch.constant.int 5
    %cpu_9492 = torch.constant.device "cpu"
    %int0_9493 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6163, %none_9489, %none_9490, %int5_9491, %cpu_9492, %int0_9493 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_9494 = torch.constant.int 3
    %int0_9495 = torch.constant.int 0
    %int64_9496 = torch.constant.int 64
    %int2_9497 = torch.constant.int 2
    %6234 = torch.aten.slice.Tensor %6163, %int3_9494, %int0_9495, %int64_9496, %int2_9497 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_9498 = torch.constant.int 3
    %int1_9499 = torch.constant.int 1
    %int64_9500 = torch.constant.int 64
    %int2_9501 = torch.constant.int 2
    %6235 = torch.aten.slice.Tensor %6163, %int3_9498, %int1_9499, %int64_9500, %int2_9501 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %6236 = torch.aten.mul.Tensor %6234, %6232 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %6237 = torch.aten.mul.Tensor %6235, %6233 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_9502 = torch.constant.int 1
    %6238 = torch.aten.sub.Tensor %6236, %6237, %int1_9502 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %6239 = torch.aten.mul.Tensor %6235, %6232 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %6240 = torch.aten.mul.Tensor %6234, %6233 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_9503 = torch.constant.int 1
    %6241 = torch.aten.add.Tensor %6239, %6240, %int1_9503 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %6242 = torch_c.to_builtin_tensor %6238 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_9504 = tensor.cast %6242 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %6243 = torch_c.to_builtin_tensor %6241 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_9505 = tensor.cast %6243 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %6244 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_9504, %cast_9505) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_9506 = tensor.cast %6244 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %6245 = torch_c.from_builtin_tensor %cast_9506 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_9507 = torch.constant.int 4
    %int1_9508 = torch.constant.int 1
    %int4_9509 = torch.constant.int 4
    %int64_9510 = torch.constant.int 64
    %6246 = torch.prim.ListConstruct %int4_9507, %int1_9508, %int4_9509, %int64_9510 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6247 = torch.aten.view %6245, %6246 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_9511 = torch.constant.none
    %none_9512 = torch.constant.none
    %int5_9513 = torch.constant.int 5
    %cpu_9514 = torch.constant.device "cpu"
    %int0_9515 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6247, %none_9511, %none_9512, %int5_9513, %cpu_9514, %int0_9515 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_9516 = torch.constant.int 32
    %6248 = torch.aten.floor_divide.Scalar %arg2, %int32_9516 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_9517 = torch.constant.int 1
    %6249 = torch.aten.unsqueeze %6248, %int1_9517 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_9518 = torch.constant.int 1
    %false_9519 = torch.constant.bool false
    %6250 = torch.aten.gather %arg3, %int1_9518, %6249, %false_9519 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_9520 = torch.constant.int 4
    %int1_9521 = torch.constant.int 1
    %int1_9522 = torch.constant.int 1
    %6251 = torch.prim.ListConstruct %int4_9520, %int1_9521, %int1_9522 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6252 = torch.aten.view %6250, %6251 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_9523 = torch.constant.int 32
    %6253 = torch.aten.remainder.Scalar %arg2, %int32_9523 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_9524 = torch.constant.int 4
    %int1_9525 = torch.constant.int 1
    %int1_9526 = torch.constant.int 1
    %6254 = torch.prim.ListConstruct %int4_9524, %int1_9525, %int1_9526 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6255 = torch.aten.view %6253, %6254 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_9527 = torch.constant.int 4
    %none_9528 = torch.constant.none
    %none_9529 = torch.constant.none
    %cpu_9530 = torch.constant.device "cpu"
    %false_9531 = torch.constant.bool false
    %6256 = torch.aten.arange %int4_9527, %none_9528, %none_9529, %cpu_9530, %false_9531 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_9532 = torch.constant.int 1
    %int1_9533 = torch.constant.int 1
    %int4_9534 = torch.constant.int 4
    %6257 = torch.prim.ListConstruct %int1_9532, %int1_9533, %int4_9534 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6258 = torch.aten.view %6256, %6257 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_9535 = torch.constant.none
    %6259 = torch.aten.clone %292, %none_9535 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_9536 = torch.constant.int 1
    %int1_9537 = torch.constant.int 1
    %int1_9538 = torch.constant.int 1
    %6260 = torch.prim.ListConstruct %int1_9536, %int1_9537, %int1_9538 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6261 = torch.aten.view %6259, %6260 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_9539 = torch.constant.int 22
    %6262 = torch.aten.mul.Scalar %6252, %int22_9539 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int17_9540 = torch.constant.int 17
    %int1_9541 = torch.constant.int 1
    %6263 = torch.aten.add.Scalar %6262, %int17_9540, %int1_9541 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_9542 = torch.constant.int 2
    %6264 = torch.aten.mul.Scalar %6263, %int2_9542 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_9543 = torch.constant.int 1
    %6265 = torch.aten.add.Tensor %6264, %6261, %int1_9543 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_9544 = torch.constant.int 4
    %6266 = torch.aten.mul.Scalar %6265, %int4_9544 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_9545 = torch.constant.int 1
    %6267 = torch.aten.add.Tensor %6266, %6258, %int1_9545 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_9546 = torch.constant.int 32
    %6268 = torch.aten.mul.Scalar %6267, %int32_9546 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_9547 = torch.constant.int 1
    %6269 = torch.aten.add.Tensor %6268, %6255, %int1_9547 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_9548 = torch.constant.none
    %none_9549 = torch.constant.none
    %int5_9550 = torch.constant.int 5
    %cpu_9551 = torch.constant.device "cpu"
    %int0_9552 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6247, %none_9548, %none_9549, %int5_9550, %cpu_9551, %int0_9552 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_9553 = torch.constant.int 22
    %int2_9554 = torch.constant.int 2
    %int4_9555 = torch.constant.int 4
    %int32_9556 = torch.constant.int 32
    %int64_9557 = torch.constant.int 64
    %6270 = torch.prim.ListConstruct %381, %int22_9553, %int2_9554, %int4_9555, %int32_9556, %int64_9557 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6271 = torch.aten.view %5965, %6270 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6271, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_9558 = torch.constant.int 64
    %6272 = torch.prim.ListConstruct %553, %int64_9558 : (!torch.int, !torch.int) -> !torch.list<int>
    %6273 = torch.aten.view %6271, %6272 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %6273, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %6274 = torch.prim.ListConstruct %6269 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_9559 = torch.constant.bool false
    %6275 = torch.aten.index_put %6273, %6274, %6247, %false_9559 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %6275, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_9560 = torch.constant.int 22
    %int2_9561 = torch.constant.int 2
    %int4_9562 = torch.constant.int 4
    %int32_9563 = torch.constant.int 32
    %int64_9564 = torch.constant.int 64
    %6276 = torch.prim.ListConstruct %381, %int22_9560, %int2_9561, %int4_9562, %int32_9563, %int64_9564 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6277 = torch.aten.view %6275, %6276 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6277, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_9565 = torch.constant.int 360448
    %6278 = torch.prim.ListConstruct %381, %int360448_9565 : (!torch.int, !torch.int) -> !torch.list<int>
    %6279 = torch.aten.view %6277, %6278 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %6279, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_9566 = torch.constant.int 22
    %int2_9567 = torch.constant.int 2
    %int4_9568 = torch.constant.int 4
    %int32_9569 = torch.constant.int 32
    %int64_9570 = torch.constant.int 64
    %6280 = torch.prim.ListConstruct %381, %int22_9566, %int2_9567, %int4_9568, %int32_9569, %int64_9570 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6281 = torch.aten.view %6279, %6280 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6281, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_9571 = torch.constant.int 64
    %6282 = torch.prim.ListConstruct %553, %int64_9571 : (!torch.int, !torch.int) -> !torch.list<int>
    %6283 = torch.aten.view %6281, %6282 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %6283, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_9572 = torch.constant.none
    %6284 = torch.aten.clone %293, %none_9572 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_9573 = torch.constant.int 1
    %int1_9574 = torch.constant.int 1
    %int1_9575 = torch.constant.int 1
    %6285 = torch.prim.ListConstruct %int1_9573, %int1_9574, %int1_9575 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6286 = torch.aten.view %6284, %6285 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_9576 = torch.constant.int 22
    %6287 = torch.aten.mul.Scalar %6252, %int22_9576 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int17_9577 = torch.constant.int 17
    %int1_9578 = torch.constant.int 1
    %6288 = torch.aten.add.Scalar %6287, %int17_9577, %int1_9578 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_9579 = torch.constant.int 2
    %6289 = torch.aten.mul.Scalar %6288, %int2_9579 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_9580 = torch.constant.int 1
    %6290 = torch.aten.add.Tensor %6289, %6286, %int1_9580 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_9581 = torch.constant.int 4
    %6291 = torch.aten.mul.Scalar %6290, %int4_9581 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_9582 = torch.constant.int 1
    %6292 = torch.aten.add.Tensor %6291, %6258, %int1_9582 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_9583 = torch.constant.int 32
    %6293 = torch.aten.mul.Scalar %6292, %int32_9583 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_9584 = torch.constant.int 1
    %6294 = torch.aten.add.Tensor %6293, %6255, %int1_9584 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_9585 = torch.constant.none
    %none_9586 = torch.constant.none
    %int5_9587 = torch.constant.int 5
    %cpu_9588 = torch.constant.device "cpu"
    %int0_9589 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6165, %none_9585, %none_9586, %int5_9587, %cpu_9588, %int0_9589 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %6295 = torch.prim.ListConstruct %6294 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_9590 = torch.constant.bool false
    %6296 = torch.aten.index_put %6283, %6295, %6165, %false_9590 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %6296, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_9591 = torch.constant.int 22
    %int2_9592 = torch.constant.int 2
    %int4_9593 = torch.constant.int 4
    %int32_9594 = torch.constant.int 32
    %int64_9595 = torch.constant.int 64
    %6297 = torch.prim.ListConstruct %381, %int22_9591, %int2_9592, %int4_9593, %int32_9594, %int64_9595 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6298 = torch.aten.view %6296, %6297 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6298, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_9596 = torch.constant.int 360448
    %6299 = torch.prim.ListConstruct %381, %int360448_9596 : (!torch.int, !torch.int) -> !torch.list<int>
    %6300 = torch.aten.view %6298, %6299 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %6300, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_9597 = torch.constant.none
    %6301 = torch.aten.clone %294, %none_9597 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_9598 = torch.constant.none
    %6302 = torch.aten.clone %295, %none_9598 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_9599 = torch.constant.none
    %6303 = torch.aten.clone %296, %none_9599 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_9600 = torch.constant.int 22
    %int2_9601 = torch.constant.int 2
    %int4_9602 = torch.constant.int 4
    %int32_9603 = torch.constant.int 32
    %int64_9604 = torch.constant.int 64
    %6304 = torch.prim.ListConstruct %381, %int22_9600, %int2_9601, %int4_9602, %int32_9603, %int64_9604 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6305 = torch.aten.view %6300, %6304 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6305, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %6306 = torch_c.to_builtin_tensor %6305 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %6307 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_9605 = tensor.cast %6307 : tensor<4x?xi64> to tensor<?x?xi64>
    %6308 = torch_c.to_builtin_tensor %6301 : !torch.vtensor<[],si64> -> tensor<i64>
    %6309 = torch_c.to_builtin_tensor %6302 : !torch.vtensor<[],si64> -> tensor<i64>
    %6310 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%6306, %cast_9605, %6308, %6309) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_9606 = tensor.cast %6310 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %6311 = torch_c.from_builtin_tensor %cast_9606 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %6311, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %6312 = torch_c.to_builtin_tensor %6305 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %6313 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_9607 = tensor.cast %6313 : tensor<4x?xi64> to tensor<?x?xi64>
    %6314 = torch_c.to_builtin_tensor %6301 : !torch.vtensor<[],si64> -> tensor<i64>
    %6315 = torch_c.to_builtin_tensor %6303 : !torch.vtensor<[],si64> -> tensor<i64>
    %6316 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%6312, %cast_9607, %6314, %6315) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_9608 = tensor.cast %6316 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %6317 = torch_c.from_builtin_tensor %cast_9608 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %6317, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_9609 = torch.constant.int 2
    %int3_9610 = torch.constant.int 3
    %6318 = torch.aten.transpose.int %6311, %int2_9609, %int3_9610 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %6318, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_9611 = torch.constant.int 0
    %6319 = torch.aten.clone %6318, %int0_9611 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %6319, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_9612 = torch.constant.int 4
    %int4_9613 = torch.constant.int 4
    %int64_9614 = torch.constant.int 64
    %6320 = torch.prim.ListConstruct %int4_9612, %623, %int4_9613, %int64_9614 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6321 = torch.aten._unsafe_view %6319, %6320 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %6321, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_9615 = torch.constant.int 2
    %int3_9616 = torch.constant.int 3
    %6322 = torch.aten.transpose.int %6317, %int2_9615, %int3_9616 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %6322, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_9617 = torch.constant.int 0
    %6323 = torch.aten.clone %6322, %int0_9617 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %6323, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_9618 = torch.constant.int 4
    %int4_9619 = torch.constant.int 4
    %int64_9620 = torch.constant.int 64
    %6324 = torch.prim.ListConstruct %int4_9618, %623, %int4_9619, %int64_9620 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6325 = torch.aten._unsafe_view %6323, %6324 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %6325, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_9621 = torch.constant.int 0
    %int1_9622 = torch.constant.int 1
    %none_9623 = torch.constant.none
    %none_9624 = torch.constant.none
    %cpu_9625 = torch.constant.device "cpu"
    %false_9626 = torch.constant.bool false
    %6326 = torch.aten.arange.start_step %int0_9621, %623, %int1_9622, %none_9623, %none_9624, %cpu_9625, %false_9626 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6326, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_9627 = torch.constant.int -1
    %6327 = torch.aten.unsqueeze %arg1, %int-1_9627 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %6328 = torch.aten.ge.Tensor %6326, %6327 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %6328, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_9628 = torch.constant.none
    %6329 = torch.aten.clone %297, %none_9628 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_9629 = torch.constant.int 0
    %6330 = torch.aten.where.ScalarOther %6328, %6329, %int0_9629 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %6330, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_9630 = torch.constant.none
    %none_9631 = torch.constant.none
    %int5_9632 = torch.constant.int 5
    %cpu_9633 = torch.constant.device "cpu"
    %int0_9634 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6330, %none_9630, %none_9631, %int5_9632, %cpu_9633, %int0_9634 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_9635 = torch.constant.int 1
    %6331 = torch.aten.unsqueeze %6330, %int1_9635 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %6331, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_9636 = torch.constant.int 1
    %6332 = torch.aten.unsqueeze %6331, %int1_9636 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %6332, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_9637 = torch.constant.int -2
    %6333 = torch.aten.unsqueeze %6321, %int-2_9637 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %6333, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_9638 = torch.constant.int 4
    %int4_9639 = torch.constant.int 4
    %int8_9640 = torch.constant.int 8
    %int64_9641 = torch.constant.int 64
    %6334 = torch.prim.ListConstruct %int4_9638, %623, %int4_9639, %int8_9640, %int64_9641 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_9642 = torch.constant.bool false
    %6335 = torch.aten.expand %6333, %6334, %false_9642 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6335, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_9643 = torch.constant.int 0
    %6336 = torch.aten.clone %6335, %int0_9643 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6336, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_9644 = torch.constant.int 4
    %int32_9645 = torch.constant.int 32
    %int64_9646 = torch.constant.int 64
    %6337 = torch.prim.ListConstruct %int4_9644, %623, %int32_9645, %int64_9646 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6338 = torch.aten._unsafe_view %6336, %6337 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6338, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_9647 = torch.constant.int -2
    %6339 = torch.aten.unsqueeze %6325, %int-2_9647 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %6339, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_9648 = torch.constant.int 4
    %int4_9649 = torch.constant.int 4
    %int8_9650 = torch.constant.int 8
    %int64_9651 = torch.constant.int 64
    %6340 = torch.prim.ListConstruct %int4_9648, %623, %int4_9649, %int8_9650, %int64_9651 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_9652 = torch.constant.bool false
    %6341 = torch.aten.expand %6339, %6340, %false_9652 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6341, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_9653 = torch.constant.int 0
    %6342 = torch.aten.clone %6341, %int0_9653 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6342, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_9654 = torch.constant.int 4
    %int32_9655 = torch.constant.int 32
    %int64_9656 = torch.constant.int 64
    %6343 = torch.prim.ListConstruct %int4_9654, %623, %int32_9655, %int64_9656 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6344 = torch.aten._unsafe_view %6342, %6343 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6344, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_9657 = torch.constant.int 1
    %int2_9658 = torch.constant.int 2
    %6345 = torch.aten.transpose.int %6206, %int1_9657, %int2_9658 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_9659 = torch.constant.int 1
    %int2_9660 = torch.constant.int 2
    %6346 = torch.aten.transpose.int %6338, %int1_9659, %int2_9660 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6346, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_9661 = torch.constant.int 1
    %int2_9662 = torch.constant.int 2
    %6347 = torch.aten.transpose.int %6344, %int1_9661, %int2_9662 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6347, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_9663 = torch.constant.float 0.000000e+00
    %false_9664 = torch.constant.bool false
    %none_9665 = torch.constant.none
    %false_9666 = torch.constant.bool false
    %6348 = torch.aten.scaled_dot_product_attention %6345, %6346, %6347, %6332, %float0.000000e00_9663, %false_9664, %none_9665, %false_9666 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_9667 = torch.constant.int 1
    %int2_9668 = torch.constant.int 2
    %6349 = torch.aten.transpose.int %6348, %int1_9667, %int2_9668 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_9669 = torch.constant.int 4
    %int1_9670 = torch.constant.int 1
    %int2048_9671 = torch.constant.int 2048
    %6350 = torch.prim.ListConstruct %int4_9669, %int1_9670, %int2048_9671 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6351 = torch.aten.view %6349, %6350 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_9672 = torch.constant.int 2
    %6352 = torch.aten.view.dtype %302, %int2_9672 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %6353 = torch.aten.detach %6352 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_9673 = torch.constant.int -1
    %int17_9674 = torch.constant.int 17
    %6354 = torch.prim.ListConstruct %int-1_9673, %int17_9674 : (!torch.int, !torch.int) -> !torch.list<int>
    %6355 = torch.aten.view %6353, %6354 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_9675 = torch.constant.int 2048
    %int-1_9676 = torch.constant.int -1
    %int17_9677 = torch.constant.int 17
    %6356 = torch.prim.ListConstruct %int2048_9675, %int-1_9676, %int17_9677 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6357 = torch.aten.view %6355, %6356 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_9678 = torch.constant.int 2
    %int0_9679 = torch.constant.int 0
    %int1_9680 = torch.constant.int 1
    %int1_9681 = torch.constant.int 1
    %6358 = torch.aten.slice.Tensor %6357, %int2_9678, %int0_9679, %int1_9680, %int1_9681 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_9682 = torch.constant.int 5
    %6359 = torch.aten.view.dtype %6358, %int5_9682 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %6360 = torch.aten.detach %6359 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_9683 = torch.constant.int 2
    %int1_9684 = torch.constant.int 1
    %int9223372036854775807_9685 = torch.constant.int 9223372036854775807
    %int1_9686 = torch.constant.int 1
    %6361 = torch.aten.slice.Tensor %6357, %int2_9683, %int1_9684, %int9223372036854775807_9685, %int1_9686 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_9687 = torch.constant.int 1
    %6362 = torch.aten.view.dtype %6361, %int1_9687 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %6363 = torch.aten.detach %6362 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %6364 = torch_c.to_builtin_tensor %6351 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_9688 = tensor.cast %6364 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %6365 = torch_c.to_builtin_tensor %6360 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %6366 = torch_c.to_builtin_tensor %6363 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %6367 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_9688, %6365, %6366) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_9689 = tensor.cast %6367 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %6368 = torch_c.from_builtin_tensor %cast_9689 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_9690 = torch.constant.none
    %none_9691 = torch.constant.none
    %int5_9692 = torch.constant.int 5
    %cpu_9693 = torch.constant.device "cpu"
    %int0_9694 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6368, %none_9690, %none_9691, %int5_9692, %cpu_9693, %int0_9694 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_9695 = torch.constant.int 1
    %6369 = torch.aten.add.Tensor %6098, %6368, %int1_9695 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_9696 = torch.constant.none
    %none_9697 = torch.constant.none
    %int5_9698 = torch.constant.int 5
    %cpu_9699 = torch.constant.device "cpu"
    %int0_9700 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6369, %none_9696, %none_9697, %int5_9698, %cpu_9699, %int0_9700 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9701 = torch.constant.int 6
    %6370 = torch.prims.convert_element_type %6369, %int6_9701 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_9702 = torch.constant.int 2
    %6371 = torch.aten.pow.Tensor_Scalar %6370, %int2_9702 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_9703 = torch.constant.int -1
    %6372 = torch.prim.ListConstruct %int-1_9703 : (!torch.int) -> !torch.list<int>
    %true_9704 = torch.constant.bool true
    %none_9705 = torch.constant.none
    %6373 = torch.aten.mean.dim %6371, %6372, %true_9704, %none_9705 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_9706 = torch.constant.float 9.9999997473787516E-6
    %int1_9707 = torch.constant.int 1
    %6374 = torch.aten.add.Scalar %6373, %float9.999990e-06_9706, %int1_9707 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6375 = torch.aten.rsqrt %6374 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %6376 = torch.aten.mul.Tensor %6370, %6375 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_9708 = torch.constant.none
    %none_9709 = torch.constant.none
    %int6_9710 = torch.constant.int 6
    %cpu_9711 = torch.constant.device "cpu"
    %int0_9712 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6376, %none_9708, %none_9709, %int6_9710, %cpu_9711, %int0_9712 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9713 = torch.constant.int 5
    %6377 = torch.prims.convert_element_type %6376, %int5_9713 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %6378 = torch.aten.mul.Tensor %303, %6377 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_9714 = torch.constant.none
    %none_9715 = torch.constant.none
    %int6_9716 = torch.constant.int 6
    %cpu_9717 = torch.constant.device "cpu"
    %int0_9718 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6378, %none_9714, %none_9715, %int6_9716, %cpu_9717, %int0_9718 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9719 = torch.constant.int 5
    %6379 = torch.prims.convert_element_type %6378, %int5_9719 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_9720 = torch.constant.int 2
    %6380 = torch.aten.view.dtype %304, %int2_9720 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %6381 = torch.aten.detach %6380 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_9721 = torch.constant.int -1
    %int17_9722 = torch.constant.int 17
    %6382 = torch.prim.ListConstruct %int-1_9721, %int17_9722 : (!torch.int, !torch.int) -> !torch.list<int>
    %6383 = torch.aten.view %6381, %6382 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_9723 = torch.constant.int 5632
    %int-1_9724 = torch.constant.int -1
    %int17_9725 = torch.constant.int 17
    %6384 = torch.prim.ListConstruct %int5632_9723, %int-1_9724, %int17_9725 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6385 = torch.aten.view %6383, %6384 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_9726 = torch.constant.int 2
    %int0_9727 = torch.constant.int 0
    %int1_9728 = torch.constant.int 1
    %int1_9729 = torch.constant.int 1
    %6386 = torch.aten.slice.Tensor %6385, %int2_9726, %int0_9727, %int1_9728, %int1_9729 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_9730 = torch.constant.int 5
    %6387 = torch.aten.view.dtype %6386, %int5_9730 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %6388 = torch.aten.detach %6387 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_9731 = torch.constant.int 2
    %int1_9732 = torch.constant.int 1
    %int9223372036854775807_9733 = torch.constant.int 9223372036854775807
    %int1_9734 = torch.constant.int 1
    %6389 = torch.aten.slice.Tensor %6385, %int2_9731, %int1_9732, %int9223372036854775807_9733, %int1_9734 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_9735 = torch.constant.int 1
    %6390 = torch.aten.view.dtype %6389, %int1_9735 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %6391 = torch.aten.detach %6390 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %6392 = torch_c.to_builtin_tensor %6379 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_9736 = tensor.cast %6392 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %6393 = torch_c.to_builtin_tensor %6388 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %6394 = torch_c.to_builtin_tensor %6391 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %6395 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_9736, %6393, %6394) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_9737 = tensor.cast %6395 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %6396 = torch_c.from_builtin_tensor %cast_9737 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %6397 = torch.aten.silu %6396 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_9738 = torch.constant.int 2
    %6398 = torch.aten.view.dtype %305, %int2_9738 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %6399 = torch.aten.detach %6398 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_9739 = torch.constant.int -1
    %int17_9740 = torch.constant.int 17
    %6400 = torch.prim.ListConstruct %int-1_9739, %int17_9740 : (!torch.int, !torch.int) -> !torch.list<int>
    %6401 = torch.aten.view %6399, %6400 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_9741 = torch.constant.int 5632
    %int-1_9742 = torch.constant.int -1
    %int17_9743 = torch.constant.int 17
    %6402 = torch.prim.ListConstruct %int5632_9741, %int-1_9742, %int17_9743 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6403 = torch.aten.view %6401, %6402 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_9744 = torch.constant.int 2
    %int0_9745 = torch.constant.int 0
    %int1_9746 = torch.constant.int 1
    %int1_9747 = torch.constant.int 1
    %6404 = torch.aten.slice.Tensor %6403, %int2_9744, %int0_9745, %int1_9746, %int1_9747 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_9748 = torch.constant.int 5
    %6405 = torch.aten.view.dtype %6404, %int5_9748 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %6406 = torch.aten.detach %6405 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_9749 = torch.constant.int 2
    %int1_9750 = torch.constant.int 1
    %int9223372036854775807_9751 = torch.constant.int 9223372036854775807
    %int1_9752 = torch.constant.int 1
    %6407 = torch.aten.slice.Tensor %6403, %int2_9749, %int1_9750, %int9223372036854775807_9751, %int1_9752 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_9753 = torch.constant.int 1
    %6408 = torch.aten.view.dtype %6407, %int1_9753 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %6409 = torch.aten.detach %6408 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %6410 = torch_c.to_builtin_tensor %6379 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_9754 = tensor.cast %6410 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %6411 = torch_c.to_builtin_tensor %6406 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %6412 = torch_c.to_builtin_tensor %6409 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %6413 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_9754, %6411, %6412) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_9755 = tensor.cast %6413 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %6414 = torch_c.from_builtin_tensor %cast_9755 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %6415 = torch.aten.mul.Tensor %6397, %6414 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_9756 = torch.constant.int 2
    %6416 = torch.aten.view.dtype %306, %int2_9756 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %6417 = torch.aten.detach %6416 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_9757 = torch.constant.int -1
    %int17_9758 = torch.constant.int 17
    %6418 = torch.prim.ListConstruct %int-1_9757, %int17_9758 : (!torch.int, !torch.int) -> !torch.list<int>
    %6419 = torch.aten.view %6417, %6418 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_9759 = torch.constant.int 2048
    %int-1_9760 = torch.constant.int -1
    %int17_9761 = torch.constant.int 17
    %6420 = torch.prim.ListConstruct %int2048_9759, %int-1_9760, %int17_9761 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6421 = torch.aten.view %6419, %6420 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_9762 = torch.constant.int 2
    %int0_9763 = torch.constant.int 0
    %int1_9764 = torch.constant.int 1
    %int1_9765 = torch.constant.int 1
    %6422 = torch.aten.slice.Tensor %6421, %int2_9762, %int0_9763, %int1_9764, %int1_9765 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_9766 = torch.constant.int 5
    %6423 = torch.aten.view.dtype %6422, %int5_9766 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %6424 = torch.aten.detach %6423 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_9767 = torch.constant.int 2
    %int1_9768 = torch.constant.int 1
    %int9223372036854775807_9769 = torch.constant.int 9223372036854775807
    %int1_9770 = torch.constant.int 1
    %6425 = torch.aten.slice.Tensor %6421, %int2_9767, %int1_9768, %int9223372036854775807_9769, %int1_9770 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_9771 = torch.constant.int 1
    %6426 = torch.aten.view.dtype %6425, %int1_9771 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %6427 = torch.aten.detach %6426 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %6428 = torch_c.to_builtin_tensor %6415 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_9772 = tensor.cast %6428 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %6429 = torch_c.to_builtin_tensor %6424 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %6430 = torch_c.to_builtin_tensor %6427 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %6431 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_9772, %6429, %6430) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_9773 = tensor.cast %6431 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %6432 = torch_c.from_builtin_tensor %cast_9773 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_9774 = torch.constant.int 1
    %6433 = torch.aten.add.Tensor %6369, %6432, %int1_9774 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_9775 = torch.constant.none
    %none_9776 = torch.constant.none
    %int5_9777 = torch.constant.int 5
    %cpu_9778 = torch.constant.device "cpu"
    %int0_9779 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6433, %none_9775, %none_9776, %int5_9777, %cpu_9778, %int0_9779 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9780 = torch.constant.int 6
    %6434 = torch.prims.convert_element_type %6433, %int6_9780 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_9781 = torch.constant.int 2
    %6435 = torch.aten.pow.Tensor_Scalar %6434, %int2_9781 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_9782 = torch.constant.int -1
    %6436 = torch.prim.ListConstruct %int-1_9782 : (!torch.int) -> !torch.list<int>
    %true_9783 = torch.constant.bool true
    %none_9784 = torch.constant.none
    %6437 = torch.aten.mean.dim %6435, %6436, %true_9783, %none_9784 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_9785 = torch.constant.float 9.9999997473787516E-6
    %int1_9786 = torch.constant.int 1
    %6438 = torch.aten.add.Scalar %6437, %float9.999990e-06_9785, %int1_9786 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6439 = torch.aten.rsqrt %6438 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %6440 = torch.aten.mul.Tensor %6434, %6439 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_9787 = torch.constant.none
    %none_9788 = torch.constant.none
    %int6_9789 = torch.constant.int 6
    %cpu_9790 = torch.constant.device "cpu"
    %int0_9791 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6440, %none_9787, %none_9788, %int6_9789, %cpu_9790, %int0_9791 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9792 = torch.constant.int 5
    %6441 = torch.prims.convert_element_type %6440, %int5_9792 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %6442 = torch.aten.mul.Tensor %315, %6441 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_9793 = torch.constant.none
    %none_9794 = torch.constant.none
    %int6_9795 = torch.constant.int 6
    %cpu_9796 = torch.constant.device "cpu"
    %int0_9797 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6442, %none_9793, %none_9794, %int6_9795, %cpu_9796, %int0_9797 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9798 = torch.constant.int 5
    %6443 = torch.prims.convert_element_type %6442, %int5_9798 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_9799 = torch.constant.int 2
    %6444 = torch.aten.view.dtype %316, %int2_9799 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %6445 = torch.aten.detach %6444 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_9800 = torch.constant.int -1
    %int17_9801 = torch.constant.int 17
    %6446 = torch.prim.ListConstruct %int-1_9800, %int17_9801 : (!torch.int, !torch.int) -> !torch.list<int>
    %6447 = torch.aten.view %6445, %6446 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_9802 = torch.constant.int 2048
    %int-1_9803 = torch.constant.int -1
    %int17_9804 = torch.constant.int 17
    %6448 = torch.prim.ListConstruct %int2048_9802, %int-1_9803, %int17_9804 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6449 = torch.aten.view %6447, %6448 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_9805 = torch.constant.int 2
    %int0_9806 = torch.constant.int 0
    %int1_9807 = torch.constant.int 1
    %int1_9808 = torch.constant.int 1
    %6450 = torch.aten.slice.Tensor %6449, %int2_9805, %int0_9806, %int1_9807, %int1_9808 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_9809 = torch.constant.int 5
    %6451 = torch.aten.view.dtype %6450, %int5_9809 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %6452 = torch.aten.detach %6451 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_9810 = torch.constant.int 2
    %int1_9811 = torch.constant.int 1
    %int9223372036854775807_9812 = torch.constant.int 9223372036854775807
    %int1_9813 = torch.constant.int 1
    %6453 = torch.aten.slice.Tensor %6449, %int2_9810, %int1_9811, %int9223372036854775807_9812, %int1_9813 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_9814 = torch.constant.int 1
    %6454 = torch.aten.view.dtype %6453, %int1_9814 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %6455 = torch.aten.detach %6454 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %6456 = torch_c.to_builtin_tensor %6443 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_9815 = tensor.cast %6456 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %6457 = torch_c.to_builtin_tensor %6452 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %6458 = torch_c.to_builtin_tensor %6455 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %6459 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_9815, %6457, %6458) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_9816 = tensor.cast %6459 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %6460 = torch_c.from_builtin_tensor %cast_9816 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_9817 = torch.constant.int 2
    %6461 = torch.aten.view.dtype %317, %int2_9817 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %6462 = torch.aten.detach %6461 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_9818 = torch.constant.int -1
    %int17_9819 = torch.constant.int 17
    %6463 = torch.prim.ListConstruct %int-1_9818, %int17_9819 : (!torch.int, !torch.int) -> !torch.list<int>
    %6464 = torch.aten.view %6462, %6463 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_9820 = torch.constant.int 256
    %int-1_9821 = torch.constant.int -1
    %int17_9822 = torch.constant.int 17
    %6465 = torch.prim.ListConstruct %int256_9820, %int-1_9821, %int17_9822 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6466 = torch.aten.view %6464, %6465 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_9823 = torch.constant.int 2
    %int0_9824 = torch.constant.int 0
    %int1_9825 = torch.constant.int 1
    %int1_9826 = torch.constant.int 1
    %6467 = torch.aten.slice.Tensor %6466, %int2_9823, %int0_9824, %int1_9825, %int1_9826 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_9827 = torch.constant.int 5
    %6468 = torch.aten.view.dtype %6467, %int5_9827 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %6469 = torch.aten.detach %6468 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_9828 = torch.constant.int 2
    %int1_9829 = torch.constant.int 1
    %int9223372036854775807_9830 = torch.constant.int 9223372036854775807
    %int1_9831 = torch.constant.int 1
    %6470 = torch.aten.slice.Tensor %6466, %int2_9828, %int1_9829, %int9223372036854775807_9830, %int1_9831 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_9832 = torch.constant.int 1
    %6471 = torch.aten.view.dtype %6470, %int1_9832 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %6472 = torch.aten.detach %6471 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %6473 = torch_c.to_builtin_tensor %6443 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_9833 = tensor.cast %6473 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %6474 = torch_c.to_builtin_tensor %6469 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %6475 = torch_c.to_builtin_tensor %6472 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %6476 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_9833, %6474, %6475) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_9834 = tensor.cast %6476 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %6477 = torch_c.from_builtin_tensor %cast_9834 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_9835 = torch.constant.int 2
    %6478 = torch.aten.view.dtype %318, %int2_9835 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %6479 = torch.aten.detach %6478 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_9836 = torch.constant.int -1
    %int17_9837 = torch.constant.int 17
    %6480 = torch.prim.ListConstruct %int-1_9836, %int17_9837 : (!torch.int, !torch.int) -> !torch.list<int>
    %6481 = torch.aten.view %6479, %6480 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_9838 = torch.constant.int 256
    %int-1_9839 = torch.constant.int -1
    %int17_9840 = torch.constant.int 17
    %6482 = torch.prim.ListConstruct %int256_9838, %int-1_9839, %int17_9840 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6483 = torch.aten.view %6481, %6482 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_9841 = torch.constant.int 2
    %int0_9842 = torch.constant.int 0
    %int1_9843 = torch.constant.int 1
    %int1_9844 = torch.constant.int 1
    %6484 = torch.aten.slice.Tensor %6483, %int2_9841, %int0_9842, %int1_9843, %int1_9844 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_9845 = torch.constant.int 5
    %6485 = torch.aten.view.dtype %6484, %int5_9845 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %6486 = torch.aten.detach %6485 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_9846 = torch.constant.int 2
    %int1_9847 = torch.constant.int 1
    %int9223372036854775807_9848 = torch.constant.int 9223372036854775807
    %int1_9849 = torch.constant.int 1
    %6487 = torch.aten.slice.Tensor %6483, %int2_9846, %int1_9847, %int9223372036854775807_9848, %int1_9849 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_9850 = torch.constant.int 1
    %6488 = torch.aten.view.dtype %6487, %int1_9850 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %6489 = torch.aten.detach %6488 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %6490 = torch_c.to_builtin_tensor %6443 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_9851 = tensor.cast %6490 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %6491 = torch_c.to_builtin_tensor %6486 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %6492 = torch_c.to_builtin_tensor %6489 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %6493 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_9851, %6491, %6492) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_9852 = tensor.cast %6493 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %6494 = torch_c.from_builtin_tensor %cast_9852 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_9853 = torch.constant.int 4
    %int1_9854 = torch.constant.int 1
    %int32_9855 = torch.constant.int 32
    %int64_9856 = torch.constant.int 64
    %6495 = torch.prim.ListConstruct %int4_9853, %int1_9854, %int32_9855, %int64_9856 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6496 = torch.aten.view %6460, %6495 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_9857 = torch.constant.int 4
    %int1_9858 = torch.constant.int 1
    %int4_9859 = torch.constant.int 4
    %int64_9860 = torch.constant.int 64
    %6497 = torch.prim.ListConstruct %int4_9857, %int1_9858, %int4_9859, %int64_9860 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6498 = torch.aten.view %6477, %6497 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_9861 = torch.constant.int 4
    %int1_9862 = torch.constant.int 1
    %int4_9863 = torch.constant.int 4
    %int64_9864 = torch.constant.int 64
    %6499 = torch.prim.ListConstruct %int4_9861, %int1_9862, %int4_9863, %int64_9864 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6500 = torch.aten.view %6494, %6499 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_9865 = torch.constant.int 0
    %int1_9866 = torch.constant.int 1
    %none_9867 = torch.constant.none
    %none_9868 = torch.constant.none
    %cpu_9869 = torch.constant.device "cpu"
    %false_9870 = torch.constant.bool false
    %6501 = torch.aten.arange.start %int0_9865, %int1_9866, %none_9867, %none_9868, %cpu_9869, %false_9870 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_9871 = torch.constant.int 0
    %6502 = torch.aten.unsqueeze %6501, %int0_9871 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_9872 = torch.constant.int 1
    %6503 = torch.aten.unsqueeze %arg2, %int1_9872 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_9873 = torch.constant.int 1
    %6504 = torch.aten.add.Tensor %6502, %6503, %int1_9873 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_9874 = torch.constant.int 0
    %int64_9875 = torch.constant.int 64
    %int2_9876 = torch.constant.int 2
    %none_9877 = torch.constant.none
    %none_9878 = torch.constant.none
    %cpu_9879 = torch.constant.device "cpu"
    %false_9880 = torch.constant.bool false
    %6505 = torch.aten.arange.start_step %int0_9874, %int64_9875, %int2_9876, %none_9877, %none_9878, %cpu_9879, %false_9880 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_9881 = torch.constant.none
    %none_9882 = torch.constant.none
    %int4_9883 = torch.constant.int 4
    %cpu_9884 = torch.constant.device "cpu"
    %int0_9885 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6505, %none_9881, %none_9882, %int4_9883, %cpu_9884, %int0_9885 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9886 = torch.constant.int 6
    %6506 = torch.prims.convert_element_type %6505, %int6_9886 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_9887 = torch.constant.int 64
    %6507 = torch.aten.div.Scalar %6506, %int64_9887 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_9888 = torch.constant.float 1.000000e+04
    %6508 = torch.aten.pow.Scalar %float1.000000e04_9888, %6507 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %6509 = torch.aten.reciprocal %6508 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_9889 = torch.constant.float 1.000000e+00
    %6510 = torch.aten.mul.Scalar %6509, %float1.000000e00_9889 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_9890 = torch.constant.none
    %6511 = torch.aten.clone %307, %none_9890 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_9891 = torch.constant.int 0
    %6512 = torch.aten.unsqueeze %6510, %int0_9891 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_9892 = torch.constant.int 2
    %6513 = torch.aten.unsqueeze %6512, %int2_9892 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_9893 = torch.constant.none
    %none_9894 = torch.constant.none
    %int6_9895 = torch.constant.int 6
    %cpu_9896 = torch.constant.device "cpu"
    %int0_9897 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6513, %none_9893, %none_9894, %int6_9895, %cpu_9896, %int0_9897 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_9898 = torch.constant.int 4
    %int-1_9899 = torch.constant.int -1
    %int1_9900 = torch.constant.int 1
    %6514 = torch.prim.ListConstruct %int4_9898, %int-1_9899, %int1_9900 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_9901 = torch.constant.bool false
    %6515 = torch.aten.expand %6513, %6514, %false_9901 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_9902 = torch.constant.int 1
    %6516 = torch.aten.unsqueeze %6504, %int1_9902 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_9903 = torch.constant.none
    %none_9904 = torch.constant.none
    %int4_9905 = torch.constant.int 4
    %cpu_9906 = torch.constant.device "cpu"
    %int0_9907 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6516, %none_9903, %none_9904, %int4_9905, %cpu_9906, %int0_9907 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9908 = torch.constant.int 6
    %6517 = torch.prims.convert_element_type %6516, %int6_9908 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6518 = torch.aten.matmul %6515, %6517 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_9909 = torch.constant.int 1
    %int2_9910 = torch.constant.int 2
    %6519 = torch.aten.transpose.int %6518, %int1_9909, %int2_9910 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %6520 = torch.aten.cos %6519 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %6521 = torch.aten.mul.Tensor %6520, %6511 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_9911 = torch.constant.none
    %none_9912 = torch.constant.none
    %int6_9913 = torch.constant.int 6
    %cpu_9914 = torch.constant.device "cpu"
    %int0_9915 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6521, %none_9911, %none_9912, %int6_9913, %cpu_9914, %int0_9915 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9916 = torch.constant.int 5
    %6522 = torch.prims.convert_element_type %6521, %int5_9916 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %6523 = torch.aten.sin %6519 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %6524 = torch.aten.mul.Tensor %6523, %6511 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_9917 = torch.constant.none
    %none_9918 = torch.constant.none
    %int6_9919 = torch.constant.int 6
    %cpu_9920 = torch.constant.device "cpu"
    %int0_9921 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6524, %none_9917, %none_9918, %int6_9919, %cpu_9920, %int0_9921 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_9922 = torch.constant.int 5
    %6525 = torch.prims.convert_element_type %6524, %int5_9922 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_9923 = torch.constant.int 2
    %6526 = torch.aten.unsqueeze %6522, %int2_9923 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_9924 = torch.constant.int 2
    %6527 = torch.aten.unsqueeze %6525, %int2_9924 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_9925 = torch.constant.none
    %none_9926 = torch.constant.none
    %int5_9927 = torch.constant.int 5
    %cpu_9928 = torch.constant.device "cpu"
    %int0_9929 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6526, %none_9925, %none_9926, %int5_9927, %cpu_9928, %int0_9929 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_9930 = torch.constant.none
    %none_9931 = torch.constant.none
    %int5_9932 = torch.constant.int 5
    %cpu_9933 = torch.constant.device "cpu"
    %int0_9934 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6527, %none_9930, %none_9931, %int5_9932, %cpu_9933, %int0_9934 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_9935 = torch.constant.none
    %none_9936 = torch.constant.none
    %int5_9937 = torch.constant.int 5
    %cpu_9938 = torch.constant.device "cpu"
    %int0_9939 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6496, %none_9935, %none_9936, %int5_9937, %cpu_9938, %int0_9939 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_9940 = torch.constant.int 3
    %int0_9941 = torch.constant.int 0
    %int64_9942 = torch.constant.int 64
    %int2_9943 = torch.constant.int 2
    %6528 = torch.aten.slice.Tensor %6496, %int3_9940, %int0_9941, %int64_9942, %int2_9943 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_9944 = torch.constant.int 3
    %int1_9945 = torch.constant.int 1
    %int64_9946 = torch.constant.int 64
    %int2_9947 = torch.constant.int 2
    %6529 = torch.aten.slice.Tensor %6496, %int3_9944, %int1_9945, %int64_9946, %int2_9947 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %6530 = torch.aten.mul.Tensor %6528, %6526 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %6531 = torch.aten.mul.Tensor %6529, %6527 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_9948 = torch.constant.int 1
    %6532 = torch.aten.sub.Tensor %6530, %6531, %int1_9948 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %6533 = torch.aten.mul.Tensor %6529, %6526 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %6534 = torch.aten.mul.Tensor %6528, %6527 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_9949 = torch.constant.int 1
    %6535 = torch.aten.add.Tensor %6533, %6534, %int1_9949 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %6536 = torch_c.to_builtin_tensor %6532 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_9950 = tensor.cast %6536 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %6537 = torch_c.to_builtin_tensor %6535 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_9951 = tensor.cast %6537 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %6538 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_9950, %cast_9951) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_9952 = tensor.cast %6538 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %6539 = torch_c.from_builtin_tensor %cast_9952 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_9953 = torch.constant.int 4
    %int1_9954 = torch.constant.int 1
    %int32_9955 = torch.constant.int 32
    %int64_9956 = torch.constant.int 64
    %6540 = torch.prim.ListConstruct %int4_9953, %int1_9954, %int32_9955, %int64_9956 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6541 = torch.aten.view %6539, %6540 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_9957 = torch.constant.none
    %none_9958 = torch.constant.none
    %int5_9959 = torch.constant.int 5
    %cpu_9960 = torch.constant.device "cpu"
    %int0_9961 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6541, %none_9957, %none_9958, %int5_9959, %cpu_9960, %int0_9961 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_9962 = torch.constant.int 0
    %int1_9963 = torch.constant.int 1
    %none_9964 = torch.constant.none
    %none_9965 = torch.constant.none
    %cpu_9966 = torch.constant.device "cpu"
    %false_9967 = torch.constant.bool false
    %6542 = torch.aten.arange.start %int0_9962, %int1_9963, %none_9964, %none_9965, %cpu_9966, %false_9967 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_9968 = torch.constant.int 0
    %6543 = torch.aten.unsqueeze %6542, %int0_9968 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_9969 = torch.constant.int 1
    %6544 = torch.aten.unsqueeze %arg2, %int1_9969 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_9970 = torch.constant.int 1
    %6545 = torch.aten.add.Tensor %6543, %6544, %int1_9970 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_9971 = torch.constant.int 0
    %int64_9972 = torch.constant.int 64
    %int2_9973 = torch.constant.int 2
    %none_9974 = torch.constant.none
    %none_9975 = torch.constant.none
    %cpu_9976 = torch.constant.device "cpu"
    %false_9977 = torch.constant.bool false
    %6546 = torch.aten.arange.start_step %int0_9971, %int64_9972, %int2_9973, %none_9974, %none_9975, %cpu_9976, %false_9977 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_9978 = torch.constant.none
    %none_9979 = torch.constant.none
    %int4_9980 = torch.constant.int 4
    %cpu_9981 = torch.constant.device "cpu"
    %int0_9982 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6546, %none_9978, %none_9979, %int4_9980, %cpu_9981, %int0_9982 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_9983 = torch.constant.int 6
    %6547 = torch.prims.convert_element_type %6546, %int6_9983 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_9984 = torch.constant.int 64
    %6548 = torch.aten.div.Scalar %6547, %int64_9984 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_9985 = torch.constant.float 1.000000e+04
    %6549 = torch.aten.pow.Scalar %float1.000000e04_9985, %6548 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %6550 = torch.aten.reciprocal %6549 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_9986 = torch.constant.float 1.000000e+00
    %6551 = torch.aten.mul.Scalar %6550, %float1.000000e00_9986 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_9987 = torch.constant.none
    %6552 = torch.aten.clone %308, %none_9987 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_9988 = torch.constant.int 0
    %6553 = torch.aten.unsqueeze %6551, %int0_9988 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_9989 = torch.constant.int 2
    %6554 = torch.aten.unsqueeze %6553, %int2_9989 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_9990 = torch.constant.none
    %none_9991 = torch.constant.none
    %int6_9992 = torch.constant.int 6
    %cpu_9993 = torch.constant.device "cpu"
    %int0_9994 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6554, %none_9990, %none_9991, %int6_9992, %cpu_9993, %int0_9994 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_9995 = torch.constant.int 4
    %int-1_9996 = torch.constant.int -1
    %int1_9997 = torch.constant.int 1
    %6555 = torch.prim.ListConstruct %int4_9995, %int-1_9996, %int1_9997 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_9998 = torch.constant.bool false
    %6556 = torch.aten.expand %6554, %6555, %false_9998 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_9999 = torch.constant.int 1
    %6557 = torch.aten.unsqueeze %6545, %int1_9999 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_10000 = torch.constant.none
    %none_10001 = torch.constant.none
    %int4_10002 = torch.constant.int 4
    %cpu_10003 = torch.constant.device "cpu"
    %int0_10004 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6557, %none_10000, %none_10001, %int4_10002, %cpu_10003, %int0_10004 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10005 = torch.constant.int 6
    %6558 = torch.prims.convert_element_type %6557, %int6_10005 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6559 = torch.aten.matmul %6556, %6558 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_10006 = torch.constant.int 1
    %int2_10007 = torch.constant.int 2
    %6560 = torch.aten.transpose.int %6559, %int1_10006, %int2_10007 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %6561 = torch.aten.cos %6560 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %6562 = torch.aten.mul.Tensor %6561, %6552 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_10008 = torch.constant.none
    %none_10009 = torch.constant.none
    %int6_10010 = torch.constant.int 6
    %cpu_10011 = torch.constant.device "cpu"
    %int0_10012 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6562, %none_10008, %none_10009, %int6_10010, %cpu_10011, %int0_10012 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10013 = torch.constant.int 5
    %6563 = torch.prims.convert_element_type %6562, %int5_10013 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %6564 = torch.aten.sin %6560 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %6565 = torch.aten.mul.Tensor %6564, %6552 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_10014 = torch.constant.none
    %none_10015 = torch.constant.none
    %int6_10016 = torch.constant.int 6
    %cpu_10017 = torch.constant.device "cpu"
    %int0_10018 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6565, %none_10014, %none_10015, %int6_10016, %cpu_10017, %int0_10018 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10019 = torch.constant.int 5
    %6566 = torch.prims.convert_element_type %6565, %int5_10019 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_10020 = torch.constant.int 2
    %6567 = torch.aten.unsqueeze %6563, %int2_10020 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_10021 = torch.constant.int 2
    %6568 = torch.aten.unsqueeze %6566, %int2_10021 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_10022 = torch.constant.none
    %none_10023 = torch.constant.none
    %int5_10024 = torch.constant.int 5
    %cpu_10025 = torch.constant.device "cpu"
    %int0_10026 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6567, %none_10022, %none_10023, %int5_10024, %cpu_10025, %int0_10026 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_10027 = torch.constant.none
    %none_10028 = torch.constant.none
    %int5_10029 = torch.constant.int 5
    %cpu_10030 = torch.constant.device "cpu"
    %int0_10031 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6568, %none_10027, %none_10028, %int5_10029, %cpu_10030, %int0_10031 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_10032 = torch.constant.none
    %none_10033 = torch.constant.none
    %int5_10034 = torch.constant.int 5
    %cpu_10035 = torch.constant.device "cpu"
    %int0_10036 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6498, %none_10032, %none_10033, %int5_10034, %cpu_10035, %int0_10036 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_10037 = torch.constant.int 3
    %int0_10038 = torch.constant.int 0
    %int64_10039 = torch.constant.int 64
    %int2_10040 = torch.constant.int 2
    %6569 = torch.aten.slice.Tensor %6498, %int3_10037, %int0_10038, %int64_10039, %int2_10040 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_10041 = torch.constant.int 3
    %int1_10042 = torch.constant.int 1
    %int64_10043 = torch.constant.int 64
    %int2_10044 = torch.constant.int 2
    %6570 = torch.aten.slice.Tensor %6498, %int3_10041, %int1_10042, %int64_10043, %int2_10044 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %6571 = torch.aten.mul.Tensor %6569, %6567 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %6572 = torch.aten.mul.Tensor %6570, %6568 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_10045 = torch.constant.int 1
    %6573 = torch.aten.sub.Tensor %6571, %6572, %int1_10045 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %6574 = torch.aten.mul.Tensor %6570, %6567 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %6575 = torch.aten.mul.Tensor %6569, %6568 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_10046 = torch.constant.int 1
    %6576 = torch.aten.add.Tensor %6574, %6575, %int1_10046 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %6577 = torch_c.to_builtin_tensor %6573 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_10047 = tensor.cast %6577 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %6578 = torch_c.to_builtin_tensor %6576 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_10048 = tensor.cast %6578 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %6579 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_10047, %cast_10048) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_10049 = tensor.cast %6579 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %6580 = torch_c.from_builtin_tensor %cast_10049 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_10050 = torch.constant.int 4
    %int1_10051 = torch.constant.int 1
    %int4_10052 = torch.constant.int 4
    %int64_10053 = torch.constant.int 64
    %6581 = torch.prim.ListConstruct %int4_10050, %int1_10051, %int4_10052, %int64_10053 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6582 = torch.aten.view %6580, %6581 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_10054 = torch.constant.none
    %none_10055 = torch.constant.none
    %int5_10056 = torch.constant.int 5
    %cpu_10057 = torch.constant.device "cpu"
    %int0_10058 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6582, %none_10054, %none_10055, %int5_10056, %cpu_10057, %int0_10058 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_10059 = torch.constant.int 32
    %6583 = torch.aten.floor_divide.Scalar %arg2, %int32_10059 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_10060 = torch.constant.int 1
    %6584 = torch.aten.unsqueeze %6583, %int1_10060 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_10061 = torch.constant.int 1
    %false_10062 = torch.constant.bool false
    %6585 = torch.aten.gather %arg3, %int1_10061, %6584, %false_10062 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_10063 = torch.constant.int 4
    %int1_10064 = torch.constant.int 1
    %int1_10065 = torch.constant.int 1
    %6586 = torch.prim.ListConstruct %int4_10063, %int1_10064, %int1_10065 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6587 = torch.aten.view %6585, %6586 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_10066 = torch.constant.int 32
    %6588 = torch.aten.remainder.Scalar %arg2, %int32_10066 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_10067 = torch.constant.int 4
    %int1_10068 = torch.constant.int 1
    %int1_10069 = torch.constant.int 1
    %6589 = torch.prim.ListConstruct %int4_10067, %int1_10068, %int1_10069 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6590 = torch.aten.view %6588, %6589 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_10070 = torch.constant.int 4
    %none_10071 = torch.constant.none
    %none_10072 = torch.constant.none
    %cpu_10073 = torch.constant.device "cpu"
    %false_10074 = torch.constant.bool false
    %6591 = torch.aten.arange %int4_10070, %none_10071, %none_10072, %cpu_10073, %false_10074 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_10075 = torch.constant.int 1
    %int1_10076 = torch.constant.int 1
    %int4_10077 = torch.constant.int 4
    %6592 = torch.prim.ListConstruct %int1_10075, %int1_10076, %int4_10077 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6593 = torch.aten.view %6591, %6592 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_10078 = torch.constant.none
    %6594 = torch.aten.clone %309, %none_10078 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_10079 = torch.constant.int 1
    %int1_10080 = torch.constant.int 1
    %int1_10081 = torch.constant.int 1
    %6595 = torch.prim.ListConstruct %int1_10079, %int1_10080, %int1_10081 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6596 = torch.aten.view %6594, %6595 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_10082 = torch.constant.int 22
    %6597 = torch.aten.mul.Scalar %6587, %int22_10082 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int18 = torch.constant.int 18
    %int1_10083 = torch.constant.int 1
    %6598 = torch.aten.add.Scalar %6597, %int18, %int1_10083 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_10084 = torch.constant.int 2
    %6599 = torch.aten.mul.Scalar %6598, %int2_10084 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_10085 = torch.constant.int 1
    %6600 = torch.aten.add.Tensor %6599, %6596, %int1_10085 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_10086 = torch.constant.int 4
    %6601 = torch.aten.mul.Scalar %6600, %int4_10086 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_10087 = torch.constant.int 1
    %6602 = torch.aten.add.Tensor %6601, %6593, %int1_10087 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_10088 = torch.constant.int 32
    %6603 = torch.aten.mul.Scalar %6602, %int32_10088 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_10089 = torch.constant.int 1
    %6604 = torch.aten.add.Tensor %6603, %6590, %int1_10089 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_10090 = torch.constant.none
    %none_10091 = torch.constant.none
    %int5_10092 = torch.constant.int 5
    %cpu_10093 = torch.constant.device "cpu"
    %int0_10094 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6582, %none_10090, %none_10091, %int5_10092, %cpu_10093, %int0_10094 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_10095 = torch.constant.int 22
    %int2_10096 = torch.constant.int 2
    %int4_10097 = torch.constant.int 4
    %int32_10098 = torch.constant.int 32
    %int64_10099 = torch.constant.int 64
    %6605 = torch.prim.ListConstruct %381, %int22_10095, %int2_10096, %int4_10097, %int32_10098, %int64_10099 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6606 = torch.aten.view %6300, %6605 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6606, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_10100 = torch.constant.int 64
    %6607 = torch.prim.ListConstruct %553, %int64_10100 : (!torch.int, !torch.int) -> !torch.list<int>
    %6608 = torch.aten.view %6606, %6607 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %6608, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %6609 = torch.prim.ListConstruct %6604 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_10101 = torch.constant.bool false
    %6610 = torch.aten.index_put %6608, %6609, %6582, %false_10101 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %6610, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_10102 = torch.constant.int 22
    %int2_10103 = torch.constant.int 2
    %int4_10104 = torch.constant.int 4
    %int32_10105 = torch.constant.int 32
    %int64_10106 = torch.constant.int 64
    %6611 = torch.prim.ListConstruct %381, %int22_10102, %int2_10103, %int4_10104, %int32_10105, %int64_10106 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6612 = torch.aten.view %6610, %6611 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6612, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_10107 = torch.constant.int 360448
    %6613 = torch.prim.ListConstruct %381, %int360448_10107 : (!torch.int, !torch.int) -> !torch.list<int>
    %6614 = torch.aten.view %6612, %6613 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %6614, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_10108 = torch.constant.int 22
    %int2_10109 = torch.constant.int 2
    %int4_10110 = torch.constant.int 4
    %int32_10111 = torch.constant.int 32
    %int64_10112 = torch.constant.int 64
    %6615 = torch.prim.ListConstruct %381, %int22_10108, %int2_10109, %int4_10110, %int32_10111, %int64_10112 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6616 = torch.aten.view %6614, %6615 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6616, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_10113 = torch.constant.int 64
    %6617 = torch.prim.ListConstruct %553, %int64_10113 : (!torch.int, !torch.int) -> !torch.list<int>
    %6618 = torch.aten.view %6616, %6617 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %6618, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_10114 = torch.constant.none
    %6619 = torch.aten.clone %310, %none_10114 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_10115 = torch.constant.int 1
    %int1_10116 = torch.constant.int 1
    %int1_10117 = torch.constant.int 1
    %6620 = torch.prim.ListConstruct %int1_10115, %int1_10116, %int1_10117 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6621 = torch.aten.view %6619, %6620 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_10118 = torch.constant.int 22
    %6622 = torch.aten.mul.Scalar %6587, %int22_10118 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int18_10119 = torch.constant.int 18
    %int1_10120 = torch.constant.int 1
    %6623 = torch.aten.add.Scalar %6622, %int18_10119, %int1_10120 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_10121 = torch.constant.int 2
    %6624 = torch.aten.mul.Scalar %6623, %int2_10121 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_10122 = torch.constant.int 1
    %6625 = torch.aten.add.Tensor %6624, %6621, %int1_10122 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_10123 = torch.constant.int 4
    %6626 = torch.aten.mul.Scalar %6625, %int4_10123 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_10124 = torch.constant.int 1
    %6627 = torch.aten.add.Tensor %6626, %6593, %int1_10124 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_10125 = torch.constant.int 32
    %6628 = torch.aten.mul.Scalar %6627, %int32_10125 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_10126 = torch.constant.int 1
    %6629 = torch.aten.add.Tensor %6628, %6590, %int1_10126 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_10127 = torch.constant.none
    %none_10128 = torch.constant.none
    %int5_10129 = torch.constant.int 5
    %cpu_10130 = torch.constant.device "cpu"
    %int0_10131 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6500, %none_10127, %none_10128, %int5_10129, %cpu_10130, %int0_10131 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %6630 = torch.prim.ListConstruct %6629 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_10132 = torch.constant.bool false
    %6631 = torch.aten.index_put %6618, %6630, %6500, %false_10132 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %6631, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_10133 = torch.constant.int 22
    %int2_10134 = torch.constant.int 2
    %int4_10135 = torch.constant.int 4
    %int32_10136 = torch.constant.int 32
    %int64_10137 = torch.constant.int 64
    %6632 = torch.prim.ListConstruct %381, %int22_10133, %int2_10134, %int4_10135, %int32_10136, %int64_10137 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6633 = torch.aten.view %6631, %6632 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6633, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_10138 = torch.constant.int 360448
    %6634 = torch.prim.ListConstruct %381, %int360448_10138 : (!torch.int, !torch.int) -> !torch.list<int>
    %6635 = torch.aten.view %6633, %6634 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %6635, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_10139 = torch.constant.none
    %6636 = torch.aten.clone %311, %none_10139 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_10140 = torch.constant.none
    %6637 = torch.aten.clone %312, %none_10140 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_10141 = torch.constant.none
    %6638 = torch.aten.clone %313, %none_10141 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_10142 = torch.constant.int 22
    %int2_10143 = torch.constant.int 2
    %int4_10144 = torch.constant.int 4
    %int32_10145 = torch.constant.int 32
    %int64_10146 = torch.constant.int 64
    %6639 = torch.prim.ListConstruct %381, %int22_10142, %int2_10143, %int4_10144, %int32_10145, %int64_10146 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6640 = torch.aten.view %6635, %6639 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6640, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %6641 = torch_c.to_builtin_tensor %6640 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %6642 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_10147 = tensor.cast %6642 : tensor<4x?xi64> to tensor<?x?xi64>
    %6643 = torch_c.to_builtin_tensor %6636 : !torch.vtensor<[],si64> -> tensor<i64>
    %6644 = torch_c.to_builtin_tensor %6637 : !torch.vtensor<[],si64> -> tensor<i64>
    %6645 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%6641, %cast_10147, %6643, %6644) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_10148 = tensor.cast %6645 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %6646 = torch_c.from_builtin_tensor %cast_10148 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %6646, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %6647 = torch_c.to_builtin_tensor %6640 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %6648 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_10149 = tensor.cast %6648 : tensor<4x?xi64> to tensor<?x?xi64>
    %6649 = torch_c.to_builtin_tensor %6636 : !torch.vtensor<[],si64> -> tensor<i64>
    %6650 = torch_c.to_builtin_tensor %6638 : !torch.vtensor<[],si64> -> tensor<i64>
    %6651 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%6647, %cast_10149, %6649, %6650) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_10150 = tensor.cast %6651 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %6652 = torch_c.from_builtin_tensor %cast_10150 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %6652, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_10151 = torch.constant.int 2
    %int3_10152 = torch.constant.int 3
    %6653 = torch.aten.transpose.int %6646, %int2_10151, %int3_10152 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %6653, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_10153 = torch.constant.int 0
    %6654 = torch.aten.clone %6653, %int0_10153 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %6654, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_10154 = torch.constant.int 4
    %int4_10155 = torch.constant.int 4
    %int64_10156 = torch.constant.int 64
    %6655 = torch.prim.ListConstruct %int4_10154, %623, %int4_10155, %int64_10156 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6656 = torch.aten._unsafe_view %6654, %6655 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %6656, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_10157 = torch.constant.int 2
    %int3_10158 = torch.constant.int 3
    %6657 = torch.aten.transpose.int %6652, %int2_10157, %int3_10158 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %6657, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_10159 = torch.constant.int 0
    %6658 = torch.aten.clone %6657, %int0_10159 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %6658, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_10160 = torch.constant.int 4
    %int4_10161 = torch.constant.int 4
    %int64_10162 = torch.constant.int 64
    %6659 = torch.prim.ListConstruct %int4_10160, %623, %int4_10161, %int64_10162 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6660 = torch.aten._unsafe_view %6658, %6659 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %6660, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_10163 = torch.constant.int 0
    %int1_10164 = torch.constant.int 1
    %none_10165 = torch.constant.none
    %none_10166 = torch.constant.none
    %cpu_10167 = torch.constant.device "cpu"
    %false_10168 = torch.constant.bool false
    %6661 = torch.aten.arange.start_step %int0_10163, %623, %int1_10164, %none_10165, %none_10166, %cpu_10167, %false_10168 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6661, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_10169 = torch.constant.int -1
    %6662 = torch.aten.unsqueeze %arg1, %int-1_10169 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %6663 = torch.aten.ge.Tensor %6661, %6662 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %6663, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_10170 = torch.constant.none
    %6664 = torch.aten.clone %314, %none_10170 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_10171 = torch.constant.int 0
    %6665 = torch.aten.where.ScalarOther %6663, %6664, %int0_10171 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %6665, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_10172 = torch.constant.none
    %none_10173 = torch.constant.none
    %int5_10174 = torch.constant.int 5
    %cpu_10175 = torch.constant.device "cpu"
    %int0_10176 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6665, %none_10172, %none_10173, %int5_10174, %cpu_10175, %int0_10176 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_10177 = torch.constant.int 1
    %6666 = torch.aten.unsqueeze %6665, %int1_10177 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %6666, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_10178 = torch.constant.int 1
    %6667 = torch.aten.unsqueeze %6666, %int1_10178 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %6667, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_10179 = torch.constant.int -2
    %6668 = torch.aten.unsqueeze %6656, %int-2_10179 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %6668, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_10180 = torch.constant.int 4
    %int4_10181 = torch.constant.int 4
    %int8_10182 = torch.constant.int 8
    %int64_10183 = torch.constant.int 64
    %6669 = torch.prim.ListConstruct %int4_10180, %623, %int4_10181, %int8_10182, %int64_10183 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_10184 = torch.constant.bool false
    %6670 = torch.aten.expand %6668, %6669, %false_10184 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6670, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_10185 = torch.constant.int 0
    %6671 = torch.aten.clone %6670, %int0_10185 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6671, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_10186 = torch.constant.int 4
    %int32_10187 = torch.constant.int 32
    %int64_10188 = torch.constant.int 64
    %6672 = torch.prim.ListConstruct %int4_10186, %623, %int32_10187, %int64_10188 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6673 = torch.aten._unsafe_view %6671, %6672 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6673, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_10189 = torch.constant.int -2
    %6674 = torch.aten.unsqueeze %6660, %int-2_10189 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %6674, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_10190 = torch.constant.int 4
    %int4_10191 = torch.constant.int 4
    %int8_10192 = torch.constant.int 8
    %int64_10193 = torch.constant.int 64
    %6675 = torch.prim.ListConstruct %int4_10190, %623, %int4_10191, %int8_10192, %int64_10193 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_10194 = torch.constant.bool false
    %6676 = torch.aten.expand %6674, %6675, %false_10194 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6676, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_10195 = torch.constant.int 0
    %6677 = torch.aten.clone %6676, %int0_10195 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %6677, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_10196 = torch.constant.int 4
    %int32_10197 = torch.constant.int 32
    %int64_10198 = torch.constant.int 64
    %6678 = torch.prim.ListConstruct %int4_10196, %623, %int32_10197, %int64_10198 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6679 = torch.aten._unsafe_view %6677, %6678 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %6679, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_10199 = torch.constant.int 1
    %int2_10200 = torch.constant.int 2
    %6680 = torch.aten.transpose.int %6541, %int1_10199, %int2_10200 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_10201 = torch.constant.int 1
    %int2_10202 = torch.constant.int 2
    %6681 = torch.aten.transpose.int %6673, %int1_10201, %int2_10202 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6681, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_10203 = torch.constant.int 1
    %int2_10204 = torch.constant.int 2
    %6682 = torch.aten.transpose.int %6679, %int1_10203, %int2_10204 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %6682, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_10205 = torch.constant.float 0.000000e+00
    %false_10206 = torch.constant.bool false
    %none_10207 = torch.constant.none
    %false_10208 = torch.constant.bool false
    %6683 = torch.aten.scaled_dot_product_attention %6680, %6681, %6682, %6667, %float0.000000e00_10205, %false_10206, %none_10207, %false_10208 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_10209 = torch.constant.int 1
    %int2_10210 = torch.constant.int 2
    %6684 = torch.aten.transpose.int %6683, %int1_10209, %int2_10210 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_10211 = torch.constant.int 4
    %int1_10212 = torch.constant.int 1
    %int2048_10213 = torch.constant.int 2048
    %6685 = torch.prim.ListConstruct %int4_10211, %int1_10212, %int2048_10213 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6686 = torch.aten.view %6684, %6685 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_10214 = torch.constant.int 2
    %6687 = torch.aten.view.dtype %319, %int2_10214 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %6688 = torch.aten.detach %6687 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_10215 = torch.constant.int -1
    %int17_10216 = torch.constant.int 17
    %6689 = torch.prim.ListConstruct %int-1_10215, %int17_10216 : (!torch.int, !torch.int) -> !torch.list<int>
    %6690 = torch.aten.view %6688, %6689 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_10217 = torch.constant.int 2048
    %int-1_10218 = torch.constant.int -1
    %int17_10219 = torch.constant.int 17
    %6691 = torch.prim.ListConstruct %int2048_10217, %int-1_10218, %int17_10219 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6692 = torch.aten.view %6690, %6691 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_10220 = torch.constant.int 2
    %int0_10221 = torch.constant.int 0
    %int1_10222 = torch.constant.int 1
    %int1_10223 = torch.constant.int 1
    %6693 = torch.aten.slice.Tensor %6692, %int2_10220, %int0_10221, %int1_10222, %int1_10223 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_10224 = torch.constant.int 5
    %6694 = torch.aten.view.dtype %6693, %int5_10224 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %6695 = torch.aten.detach %6694 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_10225 = torch.constant.int 2
    %int1_10226 = torch.constant.int 1
    %int9223372036854775807_10227 = torch.constant.int 9223372036854775807
    %int1_10228 = torch.constant.int 1
    %6696 = torch.aten.slice.Tensor %6692, %int2_10225, %int1_10226, %int9223372036854775807_10227, %int1_10228 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_10229 = torch.constant.int 1
    %6697 = torch.aten.view.dtype %6696, %int1_10229 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %6698 = torch.aten.detach %6697 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %6699 = torch_c.to_builtin_tensor %6686 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_10230 = tensor.cast %6699 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %6700 = torch_c.to_builtin_tensor %6695 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %6701 = torch_c.to_builtin_tensor %6698 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %6702 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_10230, %6700, %6701) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_10231 = tensor.cast %6702 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %6703 = torch_c.from_builtin_tensor %cast_10231 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_10232 = torch.constant.none
    %none_10233 = torch.constant.none
    %int5_10234 = torch.constant.int 5
    %cpu_10235 = torch.constant.device "cpu"
    %int0_10236 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6703, %none_10232, %none_10233, %int5_10234, %cpu_10235, %int0_10236 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_10237 = torch.constant.int 1
    %6704 = torch.aten.add.Tensor %6433, %6703, %int1_10237 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_10238 = torch.constant.none
    %none_10239 = torch.constant.none
    %int5_10240 = torch.constant.int 5
    %cpu_10241 = torch.constant.device "cpu"
    %int0_10242 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6704, %none_10238, %none_10239, %int5_10240, %cpu_10241, %int0_10242 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10243 = torch.constant.int 6
    %6705 = torch.prims.convert_element_type %6704, %int6_10243 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_10244 = torch.constant.int 2
    %6706 = torch.aten.pow.Tensor_Scalar %6705, %int2_10244 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_10245 = torch.constant.int -1
    %6707 = torch.prim.ListConstruct %int-1_10245 : (!torch.int) -> !torch.list<int>
    %true_10246 = torch.constant.bool true
    %none_10247 = torch.constant.none
    %6708 = torch.aten.mean.dim %6706, %6707, %true_10246, %none_10247 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_10248 = torch.constant.float 9.9999997473787516E-6
    %int1_10249 = torch.constant.int 1
    %6709 = torch.aten.add.Scalar %6708, %float9.999990e-06_10248, %int1_10249 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6710 = torch.aten.rsqrt %6709 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %6711 = torch.aten.mul.Tensor %6705, %6710 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_10250 = torch.constant.none
    %none_10251 = torch.constant.none
    %int6_10252 = torch.constant.int 6
    %cpu_10253 = torch.constant.device "cpu"
    %int0_10254 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6711, %none_10250, %none_10251, %int6_10252, %cpu_10253, %int0_10254 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10255 = torch.constant.int 5
    %6712 = torch.prims.convert_element_type %6711, %int5_10255 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %6713 = torch.aten.mul.Tensor %320, %6712 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_10256 = torch.constant.none
    %none_10257 = torch.constant.none
    %int6_10258 = torch.constant.int 6
    %cpu_10259 = torch.constant.device "cpu"
    %int0_10260 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6713, %none_10256, %none_10257, %int6_10258, %cpu_10259, %int0_10260 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10261 = torch.constant.int 5
    %6714 = torch.prims.convert_element_type %6713, %int5_10261 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_10262 = torch.constant.int 2
    %6715 = torch.aten.view.dtype %321, %int2_10262 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %6716 = torch.aten.detach %6715 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_10263 = torch.constant.int -1
    %int17_10264 = torch.constant.int 17
    %6717 = torch.prim.ListConstruct %int-1_10263, %int17_10264 : (!torch.int, !torch.int) -> !torch.list<int>
    %6718 = torch.aten.view %6716, %6717 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_10265 = torch.constant.int 5632
    %int-1_10266 = torch.constant.int -1
    %int17_10267 = torch.constant.int 17
    %6719 = torch.prim.ListConstruct %int5632_10265, %int-1_10266, %int17_10267 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6720 = torch.aten.view %6718, %6719 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_10268 = torch.constant.int 2
    %int0_10269 = torch.constant.int 0
    %int1_10270 = torch.constant.int 1
    %int1_10271 = torch.constant.int 1
    %6721 = torch.aten.slice.Tensor %6720, %int2_10268, %int0_10269, %int1_10270, %int1_10271 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_10272 = torch.constant.int 5
    %6722 = torch.aten.view.dtype %6721, %int5_10272 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %6723 = torch.aten.detach %6722 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_10273 = torch.constant.int 2
    %int1_10274 = torch.constant.int 1
    %int9223372036854775807_10275 = torch.constant.int 9223372036854775807
    %int1_10276 = torch.constant.int 1
    %6724 = torch.aten.slice.Tensor %6720, %int2_10273, %int1_10274, %int9223372036854775807_10275, %int1_10276 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_10277 = torch.constant.int 1
    %6725 = torch.aten.view.dtype %6724, %int1_10277 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %6726 = torch.aten.detach %6725 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %6727 = torch_c.to_builtin_tensor %6714 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_10278 = tensor.cast %6727 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %6728 = torch_c.to_builtin_tensor %6723 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %6729 = torch_c.to_builtin_tensor %6726 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %6730 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_10278, %6728, %6729) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_10279 = tensor.cast %6730 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %6731 = torch_c.from_builtin_tensor %cast_10279 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %6732 = torch.aten.silu %6731 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_10280 = torch.constant.int 2
    %6733 = torch.aten.view.dtype %322, %int2_10280 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %6734 = torch.aten.detach %6733 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_10281 = torch.constant.int -1
    %int17_10282 = torch.constant.int 17
    %6735 = torch.prim.ListConstruct %int-1_10281, %int17_10282 : (!torch.int, !torch.int) -> !torch.list<int>
    %6736 = torch.aten.view %6734, %6735 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_10283 = torch.constant.int 5632
    %int-1_10284 = torch.constant.int -1
    %int17_10285 = torch.constant.int 17
    %6737 = torch.prim.ListConstruct %int5632_10283, %int-1_10284, %int17_10285 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6738 = torch.aten.view %6736, %6737 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_10286 = torch.constant.int 2
    %int0_10287 = torch.constant.int 0
    %int1_10288 = torch.constant.int 1
    %int1_10289 = torch.constant.int 1
    %6739 = torch.aten.slice.Tensor %6738, %int2_10286, %int0_10287, %int1_10288, %int1_10289 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_10290 = torch.constant.int 5
    %6740 = torch.aten.view.dtype %6739, %int5_10290 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %6741 = torch.aten.detach %6740 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_10291 = torch.constant.int 2
    %int1_10292 = torch.constant.int 1
    %int9223372036854775807_10293 = torch.constant.int 9223372036854775807
    %int1_10294 = torch.constant.int 1
    %6742 = torch.aten.slice.Tensor %6738, %int2_10291, %int1_10292, %int9223372036854775807_10293, %int1_10294 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_10295 = torch.constant.int 1
    %6743 = torch.aten.view.dtype %6742, %int1_10295 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %6744 = torch.aten.detach %6743 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %6745 = torch_c.to_builtin_tensor %6714 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_10296 = tensor.cast %6745 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %6746 = torch_c.to_builtin_tensor %6741 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %6747 = torch_c.to_builtin_tensor %6744 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %6748 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_10296, %6746, %6747) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_10297 = tensor.cast %6748 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %6749 = torch_c.from_builtin_tensor %cast_10297 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %6750 = torch.aten.mul.Tensor %6732, %6749 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_10298 = torch.constant.int 2
    %6751 = torch.aten.view.dtype %323, %int2_10298 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %6752 = torch.aten.detach %6751 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_10299 = torch.constant.int -1
    %int17_10300 = torch.constant.int 17
    %6753 = torch.prim.ListConstruct %int-1_10299, %int17_10300 : (!torch.int, !torch.int) -> !torch.list<int>
    %6754 = torch.aten.view %6752, %6753 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_10301 = torch.constant.int 2048
    %int-1_10302 = torch.constant.int -1
    %int17_10303 = torch.constant.int 17
    %6755 = torch.prim.ListConstruct %int2048_10301, %int-1_10302, %int17_10303 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6756 = torch.aten.view %6754, %6755 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_10304 = torch.constant.int 2
    %int0_10305 = torch.constant.int 0
    %int1_10306 = torch.constant.int 1
    %int1_10307 = torch.constant.int 1
    %6757 = torch.aten.slice.Tensor %6756, %int2_10304, %int0_10305, %int1_10306, %int1_10307 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_10308 = torch.constant.int 5
    %6758 = torch.aten.view.dtype %6757, %int5_10308 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %6759 = torch.aten.detach %6758 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_10309 = torch.constant.int 2
    %int1_10310 = torch.constant.int 1
    %int9223372036854775807_10311 = torch.constant.int 9223372036854775807
    %int1_10312 = torch.constant.int 1
    %6760 = torch.aten.slice.Tensor %6756, %int2_10309, %int1_10310, %int9223372036854775807_10311, %int1_10312 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_10313 = torch.constant.int 1
    %6761 = torch.aten.view.dtype %6760, %int1_10313 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %6762 = torch.aten.detach %6761 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %6763 = torch_c.to_builtin_tensor %6750 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_10314 = tensor.cast %6763 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %6764 = torch_c.to_builtin_tensor %6759 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %6765 = torch_c.to_builtin_tensor %6762 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %6766 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_10314, %6764, %6765) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_10315 = tensor.cast %6766 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %6767 = torch_c.from_builtin_tensor %cast_10315 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_10316 = torch.constant.int 1
    %6768 = torch.aten.add.Tensor %6704, %6767, %int1_10316 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_10317 = torch.constant.none
    %none_10318 = torch.constant.none
    %int5_10319 = torch.constant.int 5
    %cpu_10320 = torch.constant.device "cpu"
    %int0_10321 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6768, %none_10317, %none_10318, %int5_10319, %cpu_10320, %int0_10321 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10322 = torch.constant.int 6
    %6769 = torch.prims.convert_element_type %6768, %int6_10322 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_10323 = torch.constant.int 2
    %6770 = torch.aten.pow.Tensor_Scalar %6769, %int2_10323 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_10324 = torch.constant.int -1
    %6771 = torch.prim.ListConstruct %int-1_10324 : (!torch.int) -> !torch.list<int>
    %true_10325 = torch.constant.bool true
    %none_10326 = torch.constant.none
    %6772 = torch.aten.mean.dim %6770, %6771, %true_10325, %none_10326 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_10327 = torch.constant.float 9.9999997473787516E-6
    %int1_10328 = torch.constant.int 1
    %6773 = torch.aten.add.Scalar %6772, %float9.999990e-06_10327, %int1_10328 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6774 = torch.aten.rsqrt %6773 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %6775 = torch.aten.mul.Tensor %6769, %6774 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_10329 = torch.constant.none
    %none_10330 = torch.constant.none
    %int6_10331 = torch.constant.int 6
    %cpu_10332 = torch.constant.device "cpu"
    %int0_10333 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6775, %none_10329, %none_10330, %int6_10331, %cpu_10332, %int0_10333 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10334 = torch.constant.int 5
    %6776 = torch.prims.convert_element_type %6775, %int5_10334 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %6777 = torch.aten.mul.Tensor %332, %6776 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_10335 = torch.constant.none
    %none_10336 = torch.constant.none
    %int6_10337 = torch.constant.int 6
    %cpu_10338 = torch.constant.device "cpu"
    %int0_10339 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6777, %none_10335, %none_10336, %int6_10337, %cpu_10338, %int0_10339 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10340 = torch.constant.int 5
    %6778 = torch.prims.convert_element_type %6777, %int5_10340 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_10341 = torch.constant.int 2
    %6779 = torch.aten.view.dtype %333, %int2_10341 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %6780 = torch.aten.detach %6779 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_10342 = torch.constant.int -1
    %int17_10343 = torch.constant.int 17
    %6781 = torch.prim.ListConstruct %int-1_10342, %int17_10343 : (!torch.int, !torch.int) -> !torch.list<int>
    %6782 = torch.aten.view %6780, %6781 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_10344 = torch.constant.int 2048
    %int-1_10345 = torch.constant.int -1
    %int17_10346 = torch.constant.int 17
    %6783 = torch.prim.ListConstruct %int2048_10344, %int-1_10345, %int17_10346 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6784 = torch.aten.view %6782, %6783 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_10347 = torch.constant.int 2
    %int0_10348 = torch.constant.int 0
    %int1_10349 = torch.constant.int 1
    %int1_10350 = torch.constant.int 1
    %6785 = torch.aten.slice.Tensor %6784, %int2_10347, %int0_10348, %int1_10349, %int1_10350 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_10351 = torch.constant.int 5
    %6786 = torch.aten.view.dtype %6785, %int5_10351 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %6787 = torch.aten.detach %6786 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_10352 = torch.constant.int 2
    %int1_10353 = torch.constant.int 1
    %int9223372036854775807_10354 = torch.constant.int 9223372036854775807
    %int1_10355 = torch.constant.int 1
    %6788 = torch.aten.slice.Tensor %6784, %int2_10352, %int1_10353, %int9223372036854775807_10354, %int1_10355 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_10356 = torch.constant.int 1
    %6789 = torch.aten.view.dtype %6788, %int1_10356 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %6790 = torch.aten.detach %6789 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %6791 = torch_c.to_builtin_tensor %6778 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_10357 = tensor.cast %6791 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %6792 = torch_c.to_builtin_tensor %6787 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %6793 = torch_c.to_builtin_tensor %6790 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %6794 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_10357, %6792, %6793) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_10358 = tensor.cast %6794 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %6795 = torch_c.from_builtin_tensor %cast_10358 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_10359 = torch.constant.int 2
    %6796 = torch.aten.view.dtype %334, %int2_10359 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %6797 = torch.aten.detach %6796 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_10360 = torch.constant.int -1
    %int17_10361 = torch.constant.int 17
    %6798 = torch.prim.ListConstruct %int-1_10360, %int17_10361 : (!torch.int, !torch.int) -> !torch.list<int>
    %6799 = torch.aten.view %6797, %6798 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_10362 = torch.constant.int 256
    %int-1_10363 = torch.constant.int -1
    %int17_10364 = torch.constant.int 17
    %6800 = torch.prim.ListConstruct %int256_10362, %int-1_10363, %int17_10364 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6801 = torch.aten.view %6799, %6800 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_10365 = torch.constant.int 2
    %int0_10366 = torch.constant.int 0
    %int1_10367 = torch.constant.int 1
    %int1_10368 = torch.constant.int 1
    %6802 = torch.aten.slice.Tensor %6801, %int2_10365, %int0_10366, %int1_10367, %int1_10368 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_10369 = torch.constant.int 5
    %6803 = torch.aten.view.dtype %6802, %int5_10369 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %6804 = torch.aten.detach %6803 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_10370 = torch.constant.int 2
    %int1_10371 = torch.constant.int 1
    %int9223372036854775807_10372 = torch.constant.int 9223372036854775807
    %int1_10373 = torch.constant.int 1
    %6805 = torch.aten.slice.Tensor %6801, %int2_10370, %int1_10371, %int9223372036854775807_10372, %int1_10373 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_10374 = torch.constant.int 1
    %6806 = torch.aten.view.dtype %6805, %int1_10374 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %6807 = torch.aten.detach %6806 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %6808 = torch_c.to_builtin_tensor %6778 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_10375 = tensor.cast %6808 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %6809 = torch_c.to_builtin_tensor %6804 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %6810 = torch_c.to_builtin_tensor %6807 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %6811 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_10375, %6809, %6810) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_10376 = tensor.cast %6811 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %6812 = torch_c.from_builtin_tensor %cast_10376 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_10377 = torch.constant.int 2
    %6813 = torch.aten.view.dtype %335, %int2_10377 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %6814 = torch.aten.detach %6813 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_10378 = torch.constant.int -1
    %int17_10379 = torch.constant.int 17
    %6815 = torch.prim.ListConstruct %int-1_10378, %int17_10379 : (!torch.int, !torch.int) -> !torch.list<int>
    %6816 = torch.aten.view %6814, %6815 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_10380 = torch.constant.int 256
    %int-1_10381 = torch.constant.int -1
    %int17_10382 = torch.constant.int 17
    %6817 = torch.prim.ListConstruct %int256_10380, %int-1_10381, %int17_10382 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6818 = torch.aten.view %6816, %6817 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_10383 = torch.constant.int 2
    %int0_10384 = torch.constant.int 0
    %int1_10385 = torch.constant.int 1
    %int1_10386 = torch.constant.int 1
    %6819 = torch.aten.slice.Tensor %6818, %int2_10383, %int0_10384, %int1_10385, %int1_10386 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_10387 = torch.constant.int 5
    %6820 = torch.aten.view.dtype %6819, %int5_10387 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %6821 = torch.aten.detach %6820 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_10388 = torch.constant.int 2
    %int1_10389 = torch.constant.int 1
    %int9223372036854775807_10390 = torch.constant.int 9223372036854775807
    %int1_10391 = torch.constant.int 1
    %6822 = torch.aten.slice.Tensor %6818, %int2_10388, %int1_10389, %int9223372036854775807_10390, %int1_10391 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_10392 = torch.constant.int 1
    %6823 = torch.aten.view.dtype %6822, %int1_10392 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %6824 = torch.aten.detach %6823 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %6825 = torch_c.to_builtin_tensor %6778 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_10393 = tensor.cast %6825 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %6826 = torch_c.to_builtin_tensor %6821 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %6827 = torch_c.to_builtin_tensor %6824 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %6828 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_10393, %6826, %6827) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_10394 = tensor.cast %6828 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %6829 = torch_c.from_builtin_tensor %cast_10394 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_10395 = torch.constant.int 4
    %int1_10396 = torch.constant.int 1
    %int32_10397 = torch.constant.int 32
    %int64_10398 = torch.constant.int 64
    %6830 = torch.prim.ListConstruct %int4_10395, %int1_10396, %int32_10397, %int64_10398 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6831 = torch.aten.view %6795, %6830 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_10399 = torch.constant.int 4
    %int1_10400 = torch.constant.int 1
    %int4_10401 = torch.constant.int 4
    %int64_10402 = torch.constant.int 64
    %6832 = torch.prim.ListConstruct %int4_10399, %int1_10400, %int4_10401, %int64_10402 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6833 = torch.aten.view %6812, %6832 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_10403 = torch.constant.int 4
    %int1_10404 = torch.constant.int 1
    %int4_10405 = torch.constant.int 4
    %int64_10406 = torch.constant.int 64
    %6834 = torch.prim.ListConstruct %int4_10403, %int1_10404, %int4_10405, %int64_10406 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6835 = torch.aten.view %6829, %6834 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_10407 = torch.constant.int 0
    %int1_10408 = torch.constant.int 1
    %none_10409 = torch.constant.none
    %none_10410 = torch.constant.none
    %cpu_10411 = torch.constant.device "cpu"
    %false_10412 = torch.constant.bool false
    %6836 = torch.aten.arange.start %int0_10407, %int1_10408, %none_10409, %none_10410, %cpu_10411, %false_10412 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_10413 = torch.constant.int 0
    %6837 = torch.aten.unsqueeze %6836, %int0_10413 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_10414 = torch.constant.int 1
    %6838 = torch.aten.unsqueeze %arg2, %int1_10414 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_10415 = torch.constant.int 1
    %6839 = torch.aten.add.Tensor %6837, %6838, %int1_10415 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_10416 = torch.constant.int 0
    %int64_10417 = torch.constant.int 64
    %int2_10418 = torch.constant.int 2
    %none_10419 = torch.constant.none
    %none_10420 = torch.constant.none
    %cpu_10421 = torch.constant.device "cpu"
    %false_10422 = torch.constant.bool false
    %6840 = torch.aten.arange.start_step %int0_10416, %int64_10417, %int2_10418, %none_10419, %none_10420, %cpu_10421, %false_10422 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_10423 = torch.constant.none
    %none_10424 = torch.constant.none
    %int4_10425 = torch.constant.int 4
    %cpu_10426 = torch.constant.device "cpu"
    %int0_10427 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6840, %none_10423, %none_10424, %int4_10425, %cpu_10426, %int0_10427 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10428 = torch.constant.int 6
    %6841 = torch.prims.convert_element_type %6840, %int6_10428 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_10429 = torch.constant.int 64
    %6842 = torch.aten.div.Scalar %6841, %int64_10429 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_10430 = torch.constant.float 1.000000e+04
    %6843 = torch.aten.pow.Scalar %float1.000000e04_10430, %6842 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %6844 = torch.aten.reciprocal %6843 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_10431 = torch.constant.float 1.000000e+00
    %6845 = torch.aten.mul.Scalar %6844, %float1.000000e00_10431 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_10432 = torch.constant.none
    %6846 = torch.aten.clone %324, %none_10432 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_10433 = torch.constant.int 0
    %6847 = torch.aten.unsqueeze %6845, %int0_10433 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_10434 = torch.constant.int 2
    %6848 = torch.aten.unsqueeze %6847, %int2_10434 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_10435 = torch.constant.none
    %none_10436 = torch.constant.none
    %int6_10437 = torch.constant.int 6
    %cpu_10438 = torch.constant.device "cpu"
    %int0_10439 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6848, %none_10435, %none_10436, %int6_10437, %cpu_10438, %int0_10439 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_10440 = torch.constant.int 4
    %int-1_10441 = torch.constant.int -1
    %int1_10442 = torch.constant.int 1
    %6849 = torch.prim.ListConstruct %int4_10440, %int-1_10441, %int1_10442 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_10443 = torch.constant.bool false
    %6850 = torch.aten.expand %6848, %6849, %false_10443 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_10444 = torch.constant.int 1
    %6851 = torch.aten.unsqueeze %6839, %int1_10444 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_10445 = torch.constant.none
    %none_10446 = torch.constant.none
    %int4_10447 = torch.constant.int 4
    %cpu_10448 = torch.constant.device "cpu"
    %int0_10449 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6851, %none_10445, %none_10446, %int4_10447, %cpu_10448, %int0_10449 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10450 = torch.constant.int 6
    %6852 = torch.prims.convert_element_type %6851, %int6_10450 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6853 = torch.aten.matmul %6850, %6852 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_10451 = torch.constant.int 1
    %int2_10452 = torch.constant.int 2
    %6854 = torch.aten.transpose.int %6853, %int1_10451, %int2_10452 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %6855 = torch.aten.cos %6854 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %6856 = torch.aten.mul.Tensor %6855, %6846 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_10453 = torch.constant.none
    %none_10454 = torch.constant.none
    %int6_10455 = torch.constant.int 6
    %cpu_10456 = torch.constant.device "cpu"
    %int0_10457 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6856, %none_10453, %none_10454, %int6_10455, %cpu_10456, %int0_10457 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10458 = torch.constant.int 5
    %6857 = torch.prims.convert_element_type %6856, %int5_10458 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %6858 = torch.aten.sin %6854 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %6859 = torch.aten.mul.Tensor %6858, %6846 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_10459 = torch.constant.none
    %none_10460 = torch.constant.none
    %int6_10461 = torch.constant.int 6
    %cpu_10462 = torch.constant.device "cpu"
    %int0_10463 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6859, %none_10459, %none_10460, %int6_10461, %cpu_10462, %int0_10463 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10464 = torch.constant.int 5
    %6860 = torch.prims.convert_element_type %6859, %int5_10464 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_10465 = torch.constant.int 2
    %6861 = torch.aten.unsqueeze %6857, %int2_10465 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_10466 = torch.constant.int 2
    %6862 = torch.aten.unsqueeze %6860, %int2_10466 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_10467 = torch.constant.none
    %none_10468 = torch.constant.none
    %int5_10469 = torch.constant.int 5
    %cpu_10470 = torch.constant.device "cpu"
    %int0_10471 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6861, %none_10467, %none_10468, %int5_10469, %cpu_10470, %int0_10471 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_10472 = torch.constant.none
    %none_10473 = torch.constant.none
    %int5_10474 = torch.constant.int 5
    %cpu_10475 = torch.constant.device "cpu"
    %int0_10476 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6862, %none_10472, %none_10473, %int5_10474, %cpu_10475, %int0_10476 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_10477 = torch.constant.none
    %none_10478 = torch.constant.none
    %int5_10479 = torch.constant.int 5
    %cpu_10480 = torch.constant.device "cpu"
    %int0_10481 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6831, %none_10477, %none_10478, %int5_10479, %cpu_10480, %int0_10481 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_10482 = torch.constant.int 3
    %int0_10483 = torch.constant.int 0
    %int64_10484 = torch.constant.int 64
    %int2_10485 = torch.constant.int 2
    %6863 = torch.aten.slice.Tensor %6831, %int3_10482, %int0_10483, %int64_10484, %int2_10485 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_10486 = torch.constant.int 3
    %int1_10487 = torch.constant.int 1
    %int64_10488 = torch.constant.int 64
    %int2_10489 = torch.constant.int 2
    %6864 = torch.aten.slice.Tensor %6831, %int3_10486, %int1_10487, %int64_10488, %int2_10489 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %6865 = torch.aten.mul.Tensor %6863, %6861 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %6866 = torch.aten.mul.Tensor %6864, %6862 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_10490 = torch.constant.int 1
    %6867 = torch.aten.sub.Tensor %6865, %6866, %int1_10490 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %6868 = torch.aten.mul.Tensor %6864, %6861 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %6869 = torch.aten.mul.Tensor %6863, %6862 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_10491 = torch.constant.int 1
    %6870 = torch.aten.add.Tensor %6868, %6869, %int1_10491 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %6871 = torch_c.to_builtin_tensor %6867 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_10492 = tensor.cast %6871 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %6872 = torch_c.to_builtin_tensor %6870 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_10493 = tensor.cast %6872 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %6873 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_10492, %cast_10493) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_10494 = tensor.cast %6873 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %6874 = torch_c.from_builtin_tensor %cast_10494 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_10495 = torch.constant.int 4
    %int1_10496 = torch.constant.int 1
    %int32_10497 = torch.constant.int 32
    %int64_10498 = torch.constant.int 64
    %6875 = torch.prim.ListConstruct %int4_10495, %int1_10496, %int32_10497, %int64_10498 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6876 = torch.aten.view %6874, %6875 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_10499 = torch.constant.none
    %none_10500 = torch.constant.none
    %int5_10501 = torch.constant.int 5
    %cpu_10502 = torch.constant.device "cpu"
    %int0_10503 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6876, %none_10499, %none_10500, %int5_10501, %cpu_10502, %int0_10503 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_10504 = torch.constant.int 0
    %int1_10505 = torch.constant.int 1
    %none_10506 = torch.constant.none
    %none_10507 = torch.constant.none
    %cpu_10508 = torch.constant.device "cpu"
    %false_10509 = torch.constant.bool false
    %6877 = torch.aten.arange.start %int0_10504, %int1_10505, %none_10506, %none_10507, %cpu_10508, %false_10509 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_10510 = torch.constant.int 0
    %6878 = torch.aten.unsqueeze %6877, %int0_10510 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_10511 = torch.constant.int 1
    %6879 = torch.aten.unsqueeze %arg2, %int1_10511 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_10512 = torch.constant.int 1
    %6880 = torch.aten.add.Tensor %6878, %6879, %int1_10512 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_10513 = torch.constant.int 0
    %int64_10514 = torch.constant.int 64
    %int2_10515 = torch.constant.int 2
    %none_10516 = torch.constant.none
    %none_10517 = torch.constant.none
    %cpu_10518 = torch.constant.device "cpu"
    %false_10519 = torch.constant.bool false
    %6881 = torch.aten.arange.start_step %int0_10513, %int64_10514, %int2_10515, %none_10516, %none_10517, %cpu_10518, %false_10519 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_10520 = torch.constant.none
    %none_10521 = torch.constant.none
    %int4_10522 = torch.constant.int 4
    %cpu_10523 = torch.constant.device "cpu"
    %int0_10524 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6881, %none_10520, %none_10521, %int4_10522, %cpu_10523, %int0_10524 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10525 = torch.constant.int 6
    %6882 = torch.prims.convert_element_type %6881, %int6_10525 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_10526 = torch.constant.int 64
    %6883 = torch.aten.div.Scalar %6882, %int64_10526 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_10527 = torch.constant.float 1.000000e+04
    %6884 = torch.aten.pow.Scalar %float1.000000e04_10527, %6883 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %6885 = torch.aten.reciprocal %6884 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_10528 = torch.constant.float 1.000000e+00
    %6886 = torch.aten.mul.Scalar %6885, %float1.000000e00_10528 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_10529 = torch.constant.none
    %6887 = torch.aten.clone %325, %none_10529 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_10530 = torch.constant.int 0
    %6888 = torch.aten.unsqueeze %6886, %int0_10530 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_10531 = torch.constant.int 2
    %6889 = torch.aten.unsqueeze %6888, %int2_10531 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_10532 = torch.constant.none
    %none_10533 = torch.constant.none
    %int6_10534 = torch.constant.int 6
    %cpu_10535 = torch.constant.device "cpu"
    %int0_10536 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6889, %none_10532, %none_10533, %int6_10534, %cpu_10535, %int0_10536 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_10537 = torch.constant.int 4
    %int-1_10538 = torch.constant.int -1
    %int1_10539 = torch.constant.int 1
    %6890 = torch.prim.ListConstruct %int4_10537, %int-1_10538, %int1_10539 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_10540 = torch.constant.bool false
    %6891 = torch.aten.expand %6889, %6890, %false_10540 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_10541 = torch.constant.int 1
    %6892 = torch.aten.unsqueeze %6880, %int1_10541 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_10542 = torch.constant.none
    %none_10543 = torch.constant.none
    %int4_10544 = torch.constant.int 4
    %cpu_10545 = torch.constant.device "cpu"
    %int0_10546 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6892, %none_10542, %none_10543, %int4_10544, %cpu_10545, %int0_10546 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10547 = torch.constant.int 6
    %6893 = torch.prims.convert_element_type %6892, %int6_10547 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6894 = torch.aten.matmul %6891, %6893 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_10548 = torch.constant.int 1
    %int2_10549 = torch.constant.int 2
    %6895 = torch.aten.transpose.int %6894, %int1_10548, %int2_10549 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %6896 = torch.aten.cos %6895 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %6897 = torch.aten.mul.Tensor %6896, %6887 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_10550 = torch.constant.none
    %none_10551 = torch.constant.none
    %int6_10552 = torch.constant.int 6
    %cpu_10553 = torch.constant.device "cpu"
    %int0_10554 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6897, %none_10550, %none_10551, %int6_10552, %cpu_10553, %int0_10554 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10555 = torch.constant.int 5
    %6898 = torch.prims.convert_element_type %6897, %int5_10555 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %6899 = torch.aten.sin %6895 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %6900 = torch.aten.mul.Tensor %6899, %6887 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_10556 = torch.constant.none
    %none_10557 = torch.constant.none
    %int6_10558 = torch.constant.int 6
    %cpu_10559 = torch.constant.device "cpu"
    %int0_10560 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6900, %none_10556, %none_10557, %int6_10558, %cpu_10559, %int0_10560 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10561 = torch.constant.int 5
    %6901 = torch.prims.convert_element_type %6900, %int5_10561 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_10562 = torch.constant.int 2
    %6902 = torch.aten.unsqueeze %6898, %int2_10562 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_10563 = torch.constant.int 2
    %6903 = torch.aten.unsqueeze %6901, %int2_10563 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_10564 = torch.constant.none
    %none_10565 = torch.constant.none
    %int5_10566 = torch.constant.int 5
    %cpu_10567 = torch.constant.device "cpu"
    %int0_10568 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6902, %none_10564, %none_10565, %int5_10566, %cpu_10567, %int0_10568 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_10569 = torch.constant.none
    %none_10570 = torch.constant.none
    %int5_10571 = torch.constant.int 5
    %cpu_10572 = torch.constant.device "cpu"
    %int0_10573 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6903, %none_10569, %none_10570, %int5_10571, %cpu_10572, %int0_10573 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_10574 = torch.constant.none
    %none_10575 = torch.constant.none
    %int5_10576 = torch.constant.int 5
    %cpu_10577 = torch.constant.device "cpu"
    %int0_10578 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6833, %none_10574, %none_10575, %int5_10576, %cpu_10577, %int0_10578 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_10579 = torch.constant.int 3
    %int0_10580 = torch.constant.int 0
    %int64_10581 = torch.constant.int 64
    %int2_10582 = torch.constant.int 2
    %6904 = torch.aten.slice.Tensor %6833, %int3_10579, %int0_10580, %int64_10581, %int2_10582 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_10583 = torch.constant.int 3
    %int1_10584 = torch.constant.int 1
    %int64_10585 = torch.constant.int 64
    %int2_10586 = torch.constant.int 2
    %6905 = torch.aten.slice.Tensor %6833, %int3_10583, %int1_10584, %int64_10585, %int2_10586 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %6906 = torch.aten.mul.Tensor %6904, %6902 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %6907 = torch.aten.mul.Tensor %6905, %6903 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_10587 = torch.constant.int 1
    %6908 = torch.aten.sub.Tensor %6906, %6907, %int1_10587 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %6909 = torch.aten.mul.Tensor %6905, %6902 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %6910 = torch.aten.mul.Tensor %6904, %6903 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_10588 = torch.constant.int 1
    %6911 = torch.aten.add.Tensor %6909, %6910, %int1_10588 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %6912 = torch_c.to_builtin_tensor %6908 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_10589 = tensor.cast %6912 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %6913 = torch_c.to_builtin_tensor %6911 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_10590 = tensor.cast %6913 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %6914 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_10589, %cast_10590) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_10591 = tensor.cast %6914 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %6915 = torch_c.from_builtin_tensor %cast_10591 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_10592 = torch.constant.int 4
    %int1_10593 = torch.constant.int 1
    %int4_10594 = torch.constant.int 4
    %int64_10595 = torch.constant.int 64
    %6916 = torch.prim.ListConstruct %int4_10592, %int1_10593, %int4_10594, %int64_10595 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6917 = torch.aten.view %6915, %6916 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_10596 = torch.constant.none
    %none_10597 = torch.constant.none
    %int5_10598 = torch.constant.int 5
    %cpu_10599 = torch.constant.device "cpu"
    %int0_10600 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6917, %none_10596, %none_10597, %int5_10598, %cpu_10599, %int0_10600 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_10601 = torch.constant.int 32
    %6918 = torch.aten.floor_divide.Scalar %arg2, %int32_10601 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_10602 = torch.constant.int 1
    %6919 = torch.aten.unsqueeze %6918, %int1_10602 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_10603 = torch.constant.int 1
    %false_10604 = torch.constant.bool false
    %6920 = torch.aten.gather %arg3, %int1_10603, %6919, %false_10604 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_10605 = torch.constant.int 4
    %int1_10606 = torch.constant.int 1
    %int1_10607 = torch.constant.int 1
    %6921 = torch.prim.ListConstruct %int4_10605, %int1_10606, %int1_10607 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6922 = torch.aten.view %6920, %6921 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_10608 = torch.constant.int 32
    %6923 = torch.aten.remainder.Scalar %arg2, %int32_10608 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_10609 = torch.constant.int 4
    %int1_10610 = torch.constant.int 1
    %int1_10611 = torch.constant.int 1
    %6924 = torch.prim.ListConstruct %int4_10609, %int1_10610, %int1_10611 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6925 = torch.aten.view %6923, %6924 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_10612 = torch.constant.int 4
    %none_10613 = torch.constant.none
    %none_10614 = torch.constant.none
    %cpu_10615 = torch.constant.device "cpu"
    %false_10616 = torch.constant.bool false
    %6926 = torch.aten.arange %int4_10612, %none_10613, %none_10614, %cpu_10615, %false_10616 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_10617 = torch.constant.int 1
    %int1_10618 = torch.constant.int 1
    %int4_10619 = torch.constant.int 4
    %6927 = torch.prim.ListConstruct %int1_10617, %int1_10618, %int4_10619 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6928 = torch.aten.view %6926, %6927 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_10620 = torch.constant.none
    %6929 = torch.aten.clone %326, %none_10620 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_10621 = torch.constant.int 1
    %int1_10622 = torch.constant.int 1
    %int1_10623 = torch.constant.int 1
    %6930 = torch.prim.ListConstruct %int1_10621, %int1_10622, %int1_10623 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6931 = torch.aten.view %6929, %6930 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_10624 = torch.constant.int 22
    %6932 = torch.aten.mul.Scalar %6922, %int22_10624 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int19 = torch.constant.int 19
    %int1_10625 = torch.constant.int 1
    %6933 = torch.aten.add.Scalar %6932, %int19, %int1_10625 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_10626 = torch.constant.int 2
    %6934 = torch.aten.mul.Scalar %6933, %int2_10626 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_10627 = torch.constant.int 1
    %6935 = torch.aten.add.Tensor %6934, %6931, %int1_10627 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_10628 = torch.constant.int 4
    %6936 = torch.aten.mul.Scalar %6935, %int4_10628 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_10629 = torch.constant.int 1
    %6937 = torch.aten.add.Tensor %6936, %6928, %int1_10629 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_10630 = torch.constant.int 32
    %6938 = torch.aten.mul.Scalar %6937, %int32_10630 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_10631 = torch.constant.int 1
    %6939 = torch.aten.add.Tensor %6938, %6925, %int1_10631 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_10632 = torch.constant.none
    %none_10633 = torch.constant.none
    %int5_10634 = torch.constant.int 5
    %cpu_10635 = torch.constant.device "cpu"
    %int0_10636 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6917, %none_10632, %none_10633, %int5_10634, %cpu_10635, %int0_10636 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_10637 = torch.constant.int 22
    %int2_10638 = torch.constant.int 2
    %int4_10639 = torch.constant.int 4
    %int32_10640 = torch.constant.int 32
    %int64_10641 = torch.constant.int 64
    %6940 = torch.prim.ListConstruct %381, %int22_10637, %int2_10638, %int4_10639, %int32_10640, %int64_10641 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6941 = torch.aten.view %6635, %6940 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6941, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_10642 = torch.constant.int 64
    %6942 = torch.prim.ListConstruct %553, %int64_10642 : (!torch.int, !torch.int) -> !torch.list<int>
    %6943 = torch.aten.view %6941, %6942 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %6943, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %6944 = torch.prim.ListConstruct %6939 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_10643 = torch.constant.bool false
    %6945 = torch.aten.index_put %6943, %6944, %6917, %false_10643 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %6945, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_10644 = torch.constant.int 22
    %int2_10645 = torch.constant.int 2
    %int4_10646 = torch.constant.int 4
    %int32_10647 = torch.constant.int 32
    %int64_10648 = torch.constant.int 64
    %6946 = torch.prim.ListConstruct %381, %int22_10644, %int2_10645, %int4_10646, %int32_10647, %int64_10648 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6947 = torch.aten.view %6945, %6946 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6947, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_10649 = torch.constant.int 360448
    %6948 = torch.prim.ListConstruct %381, %int360448_10649 : (!torch.int, !torch.int) -> !torch.list<int>
    %6949 = torch.aten.view %6947, %6948 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %6949, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_10650 = torch.constant.int 22
    %int2_10651 = torch.constant.int 2
    %int4_10652 = torch.constant.int 4
    %int32_10653 = torch.constant.int 32
    %int64_10654 = torch.constant.int 64
    %6950 = torch.prim.ListConstruct %381, %int22_10650, %int2_10651, %int4_10652, %int32_10653, %int64_10654 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6951 = torch.aten.view %6949, %6950 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6951, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_10655 = torch.constant.int 64
    %6952 = torch.prim.ListConstruct %553, %int64_10655 : (!torch.int, !torch.int) -> !torch.list<int>
    %6953 = torch.aten.view %6951, %6952 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %6953, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_10656 = torch.constant.none
    %6954 = torch.aten.clone %327, %none_10656 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_10657 = torch.constant.int 1
    %int1_10658 = torch.constant.int 1
    %int1_10659 = torch.constant.int 1
    %6955 = torch.prim.ListConstruct %int1_10657, %int1_10658, %int1_10659 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6956 = torch.aten.view %6954, %6955 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_10660 = torch.constant.int 22
    %6957 = torch.aten.mul.Scalar %6922, %int22_10660 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int19_10661 = torch.constant.int 19
    %int1_10662 = torch.constant.int 1
    %6958 = torch.aten.add.Scalar %6957, %int19_10661, %int1_10662 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_10663 = torch.constant.int 2
    %6959 = torch.aten.mul.Scalar %6958, %int2_10663 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_10664 = torch.constant.int 1
    %6960 = torch.aten.add.Tensor %6959, %6956, %int1_10664 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_10665 = torch.constant.int 4
    %6961 = torch.aten.mul.Scalar %6960, %int4_10665 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_10666 = torch.constant.int 1
    %6962 = torch.aten.add.Tensor %6961, %6928, %int1_10666 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_10667 = torch.constant.int 32
    %6963 = torch.aten.mul.Scalar %6962, %int32_10667 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_10668 = torch.constant.int 1
    %6964 = torch.aten.add.Tensor %6963, %6925, %int1_10668 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_10669 = torch.constant.none
    %none_10670 = torch.constant.none
    %int5_10671 = torch.constant.int 5
    %cpu_10672 = torch.constant.device "cpu"
    %int0_10673 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %6835, %none_10669, %none_10670, %int5_10671, %cpu_10672, %int0_10673 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %6965 = torch.prim.ListConstruct %6964 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_10674 = torch.constant.bool false
    %6966 = torch.aten.index_put %6953, %6965, %6835, %false_10674 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %6966, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_10675 = torch.constant.int 22
    %int2_10676 = torch.constant.int 2
    %int4_10677 = torch.constant.int 4
    %int32_10678 = torch.constant.int 32
    %int64_10679 = torch.constant.int 64
    %6967 = torch.prim.ListConstruct %381, %int22_10675, %int2_10676, %int4_10677, %int32_10678, %int64_10679 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6968 = torch.aten.view %6966, %6967 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6968, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_10680 = torch.constant.int 360448
    %6969 = torch.prim.ListConstruct %381, %int360448_10680 : (!torch.int, !torch.int) -> !torch.list<int>
    %6970 = torch.aten.view %6968, %6969 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %6970, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_10681 = torch.constant.none
    %6971 = torch.aten.clone %328, %none_10681 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_10682 = torch.constant.none
    %6972 = torch.aten.clone %329, %none_10682 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_10683 = torch.constant.none
    %6973 = torch.aten.clone %330, %none_10683 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_10684 = torch.constant.int 22
    %int2_10685 = torch.constant.int 2
    %int4_10686 = torch.constant.int 4
    %int32_10687 = torch.constant.int 32
    %int64_10688 = torch.constant.int 64
    %6974 = torch.prim.ListConstruct %381, %int22_10684, %int2_10685, %int4_10686, %int32_10687, %int64_10688 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6975 = torch.aten.view %6970, %6974 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %6975, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %6976 = torch_c.to_builtin_tensor %6975 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %6977 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_10689 = tensor.cast %6977 : tensor<4x?xi64> to tensor<?x?xi64>
    %6978 = torch_c.to_builtin_tensor %6971 : !torch.vtensor<[],si64> -> tensor<i64>
    %6979 = torch_c.to_builtin_tensor %6972 : !torch.vtensor<[],si64> -> tensor<i64>
    %6980 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%6976, %cast_10689, %6978, %6979) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_10690 = tensor.cast %6980 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %6981 = torch_c.from_builtin_tensor %cast_10690 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %6981, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %6982 = torch_c.to_builtin_tensor %6975 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %6983 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_10691 = tensor.cast %6983 : tensor<4x?xi64> to tensor<?x?xi64>
    %6984 = torch_c.to_builtin_tensor %6971 : !torch.vtensor<[],si64> -> tensor<i64>
    %6985 = torch_c.to_builtin_tensor %6973 : !torch.vtensor<[],si64> -> tensor<i64>
    %6986 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%6982, %cast_10691, %6984, %6985) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_10692 = tensor.cast %6986 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %6987 = torch_c.from_builtin_tensor %cast_10692 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %6987, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_10693 = torch.constant.int 2
    %int3_10694 = torch.constant.int 3
    %6988 = torch.aten.transpose.int %6981, %int2_10693, %int3_10694 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %6988, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_10695 = torch.constant.int 0
    %6989 = torch.aten.clone %6988, %int0_10695 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %6989, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_10696 = torch.constant.int 4
    %int4_10697 = torch.constant.int 4
    %int64_10698 = torch.constant.int 64
    %6990 = torch.prim.ListConstruct %int4_10696, %623, %int4_10697, %int64_10698 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6991 = torch.aten._unsafe_view %6989, %6990 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %6991, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_10699 = torch.constant.int 2
    %int3_10700 = torch.constant.int 3
    %6992 = torch.aten.transpose.int %6987, %int2_10699, %int3_10700 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %6992, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_10701 = torch.constant.int 0
    %6993 = torch.aten.clone %6992, %int0_10701 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %6993, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_10702 = torch.constant.int 4
    %int4_10703 = torch.constant.int 4
    %int64_10704 = torch.constant.int 64
    %6994 = torch.prim.ListConstruct %int4_10702, %623, %int4_10703, %int64_10704 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6995 = torch.aten._unsafe_view %6993, %6994 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %6995, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_10705 = torch.constant.int 0
    %int1_10706 = torch.constant.int 1
    %none_10707 = torch.constant.none
    %none_10708 = torch.constant.none
    %cpu_10709 = torch.constant.device "cpu"
    %false_10710 = torch.constant.bool false
    %6996 = torch.aten.arange.start_step %int0_10705, %623, %int1_10706, %none_10707, %none_10708, %cpu_10709, %false_10710 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6996, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_10711 = torch.constant.int -1
    %6997 = torch.aten.unsqueeze %arg1, %int-1_10711 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %6998 = torch.aten.ge.Tensor %6996, %6997 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %6998, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_10712 = torch.constant.none
    %6999 = torch.aten.clone %331, %none_10712 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_10713 = torch.constant.int 0
    %7000 = torch.aten.where.ScalarOther %6998, %6999, %int0_10713 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %7000, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_10714 = torch.constant.none
    %none_10715 = torch.constant.none
    %int5_10716 = torch.constant.int 5
    %cpu_10717 = torch.constant.device "cpu"
    %int0_10718 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7000, %none_10714, %none_10715, %int5_10716, %cpu_10717, %int0_10718 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_10719 = torch.constant.int 1
    %7001 = torch.aten.unsqueeze %7000, %int1_10719 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %7001, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_10720 = torch.constant.int 1
    %7002 = torch.aten.unsqueeze %7001, %int1_10720 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %7002, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_10721 = torch.constant.int -2
    %7003 = torch.aten.unsqueeze %6991, %int-2_10721 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %7003, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_10722 = torch.constant.int 4
    %int4_10723 = torch.constant.int 4
    %int8_10724 = torch.constant.int 8
    %int64_10725 = torch.constant.int 64
    %7004 = torch.prim.ListConstruct %int4_10722, %623, %int4_10723, %int8_10724, %int64_10725 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_10726 = torch.constant.bool false
    %7005 = torch.aten.expand %7003, %7004, %false_10726 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %7005, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_10727 = torch.constant.int 0
    %7006 = torch.aten.clone %7005, %int0_10727 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %7006, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_10728 = torch.constant.int 4
    %int32_10729 = torch.constant.int 32
    %int64_10730 = torch.constant.int 64
    %7007 = torch.prim.ListConstruct %int4_10728, %623, %int32_10729, %int64_10730 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7008 = torch.aten._unsafe_view %7006, %7007 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %7008, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_10731 = torch.constant.int -2
    %7009 = torch.aten.unsqueeze %6995, %int-2_10731 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %7009, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_10732 = torch.constant.int 4
    %int4_10733 = torch.constant.int 4
    %int8_10734 = torch.constant.int 8
    %int64_10735 = torch.constant.int 64
    %7010 = torch.prim.ListConstruct %int4_10732, %623, %int4_10733, %int8_10734, %int64_10735 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_10736 = torch.constant.bool false
    %7011 = torch.aten.expand %7009, %7010, %false_10736 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %7011, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_10737 = torch.constant.int 0
    %7012 = torch.aten.clone %7011, %int0_10737 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %7012, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_10738 = torch.constant.int 4
    %int32_10739 = torch.constant.int 32
    %int64_10740 = torch.constant.int 64
    %7013 = torch.prim.ListConstruct %int4_10738, %623, %int32_10739, %int64_10740 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7014 = torch.aten._unsafe_view %7012, %7013 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %7014, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_10741 = torch.constant.int 1
    %int2_10742 = torch.constant.int 2
    %7015 = torch.aten.transpose.int %6876, %int1_10741, %int2_10742 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_10743 = torch.constant.int 1
    %int2_10744 = torch.constant.int 2
    %7016 = torch.aten.transpose.int %7008, %int1_10743, %int2_10744 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %7016, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_10745 = torch.constant.int 1
    %int2_10746 = torch.constant.int 2
    %7017 = torch.aten.transpose.int %7014, %int1_10745, %int2_10746 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %7017, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_10747 = torch.constant.float 0.000000e+00
    %false_10748 = torch.constant.bool false
    %none_10749 = torch.constant.none
    %false_10750 = torch.constant.bool false
    %7018 = torch.aten.scaled_dot_product_attention %7015, %7016, %7017, %7002, %float0.000000e00_10747, %false_10748, %none_10749, %false_10750 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_10751 = torch.constant.int 1
    %int2_10752 = torch.constant.int 2
    %7019 = torch.aten.transpose.int %7018, %int1_10751, %int2_10752 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_10753 = torch.constant.int 4
    %int1_10754 = torch.constant.int 1
    %int2048_10755 = torch.constant.int 2048
    %7020 = torch.prim.ListConstruct %int4_10753, %int1_10754, %int2048_10755 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7021 = torch.aten.view %7019, %7020 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_10756 = torch.constant.int 2
    %7022 = torch.aten.view.dtype %336, %int2_10756 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %7023 = torch.aten.detach %7022 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_10757 = torch.constant.int -1
    %int17_10758 = torch.constant.int 17
    %7024 = torch.prim.ListConstruct %int-1_10757, %int17_10758 : (!torch.int, !torch.int) -> !torch.list<int>
    %7025 = torch.aten.view %7023, %7024 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_10759 = torch.constant.int 2048
    %int-1_10760 = torch.constant.int -1
    %int17_10761 = torch.constant.int 17
    %7026 = torch.prim.ListConstruct %int2048_10759, %int-1_10760, %int17_10761 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7027 = torch.aten.view %7025, %7026 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_10762 = torch.constant.int 2
    %int0_10763 = torch.constant.int 0
    %int1_10764 = torch.constant.int 1
    %int1_10765 = torch.constant.int 1
    %7028 = torch.aten.slice.Tensor %7027, %int2_10762, %int0_10763, %int1_10764, %int1_10765 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_10766 = torch.constant.int 5
    %7029 = torch.aten.view.dtype %7028, %int5_10766 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %7030 = torch.aten.detach %7029 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_10767 = torch.constant.int 2
    %int1_10768 = torch.constant.int 1
    %int9223372036854775807_10769 = torch.constant.int 9223372036854775807
    %int1_10770 = torch.constant.int 1
    %7031 = torch.aten.slice.Tensor %7027, %int2_10767, %int1_10768, %int9223372036854775807_10769, %int1_10770 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_10771 = torch.constant.int 1
    %7032 = torch.aten.view.dtype %7031, %int1_10771 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %7033 = torch.aten.detach %7032 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %7034 = torch_c.to_builtin_tensor %7021 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_10772 = tensor.cast %7034 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %7035 = torch_c.to_builtin_tensor %7030 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %7036 = torch_c.to_builtin_tensor %7033 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %7037 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_10772, %7035, %7036) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_10773 = tensor.cast %7037 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %7038 = torch_c.from_builtin_tensor %cast_10773 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_10774 = torch.constant.none
    %none_10775 = torch.constant.none
    %int5_10776 = torch.constant.int 5
    %cpu_10777 = torch.constant.device "cpu"
    %int0_10778 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7038, %none_10774, %none_10775, %int5_10776, %cpu_10777, %int0_10778 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_10779 = torch.constant.int 1
    %7039 = torch.aten.add.Tensor %6768, %7038, %int1_10779 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_10780 = torch.constant.none
    %none_10781 = torch.constant.none
    %int5_10782 = torch.constant.int 5
    %cpu_10783 = torch.constant.device "cpu"
    %int0_10784 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7039, %none_10780, %none_10781, %int5_10782, %cpu_10783, %int0_10784 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10785 = torch.constant.int 6
    %7040 = torch.prims.convert_element_type %7039, %int6_10785 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_10786 = torch.constant.int 2
    %7041 = torch.aten.pow.Tensor_Scalar %7040, %int2_10786 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_10787 = torch.constant.int -1
    %7042 = torch.prim.ListConstruct %int-1_10787 : (!torch.int) -> !torch.list<int>
    %true_10788 = torch.constant.bool true
    %none_10789 = torch.constant.none
    %7043 = torch.aten.mean.dim %7041, %7042, %true_10788, %none_10789 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_10790 = torch.constant.float 9.9999997473787516E-6
    %int1_10791 = torch.constant.int 1
    %7044 = torch.aten.add.Scalar %7043, %float9.999990e-06_10790, %int1_10791 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %7045 = torch.aten.rsqrt %7044 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %7046 = torch.aten.mul.Tensor %7040, %7045 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_10792 = torch.constant.none
    %none_10793 = torch.constant.none
    %int6_10794 = torch.constant.int 6
    %cpu_10795 = torch.constant.device "cpu"
    %int0_10796 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7046, %none_10792, %none_10793, %int6_10794, %cpu_10795, %int0_10796 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10797 = torch.constant.int 5
    %7047 = torch.prims.convert_element_type %7046, %int5_10797 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %7048 = torch.aten.mul.Tensor %337, %7047 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_10798 = torch.constant.none
    %none_10799 = torch.constant.none
    %int6_10800 = torch.constant.int 6
    %cpu_10801 = torch.constant.device "cpu"
    %int0_10802 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7048, %none_10798, %none_10799, %int6_10800, %cpu_10801, %int0_10802 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10803 = torch.constant.int 5
    %7049 = torch.prims.convert_element_type %7048, %int5_10803 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_10804 = torch.constant.int 2
    %7050 = torch.aten.view.dtype %338, %int2_10804 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %7051 = torch.aten.detach %7050 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_10805 = torch.constant.int -1
    %int17_10806 = torch.constant.int 17
    %7052 = torch.prim.ListConstruct %int-1_10805, %int17_10806 : (!torch.int, !torch.int) -> !torch.list<int>
    %7053 = torch.aten.view %7051, %7052 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_10807 = torch.constant.int 5632
    %int-1_10808 = torch.constant.int -1
    %int17_10809 = torch.constant.int 17
    %7054 = torch.prim.ListConstruct %int5632_10807, %int-1_10808, %int17_10809 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7055 = torch.aten.view %7053, %7054 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_10810 = torch.constant.int 2
    %int0_10811 = torch.constant.int 0
    %int1_10812 = torch.constant.int 1
    %int1_10813 = torch.constant.int 1
    %7056 = torch.aten.slice.Tensor %7055, %int2_10810, %int0_10811, %int1_10812, %int1_10813 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_10814 = torch.constant.int 5
    %7057 = torch.aten.view.dtype %7056, %int5_10814 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %7058 = torch.aten.detach %7057 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_10815 = torch.constant.int 2
    %int1_10816 = torch.constant.int 1
    %int9223372036854775807_10817 = torch.constant.int 9223372036854775807
    %int1_10818 = torch.constant.int 1
    %7059 = torch.aten.slice.Tensor %7055, %int2_10815, %int1_10816, %int9223372036854775807_10817, %int1_10818 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_10819 = torch.constant.int 1
    %7060 = torch.aten.view.dtype %7059, %int1_10819 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %7061 = torch.aten.detach %7060 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %7062 = torch_c.to_builtin_tensor %7049 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_10820 = tensor.cast %7062 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %7063 = torch_c.to_builtin_tensor %7058 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %7064 = torch_c.to_builtin_tensor %7061 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %7065 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_10820, %7063, %7064) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_10821 = tensor.cast %7065 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %7066 = torch_c.from_builtin_tensor %cast_10821 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %7067 = torch.aten.silu %7066 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_10822 = torch.constant.int 2
    %7068 = torch.aten.view.dtype %339, %int2_10822 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %7069 = torch.aten.detach %7068 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_10823 = torch.constant.int -1
    %int17_10824 = torch.constant.int 17
    %7070 = torch.prim.ListConstruct %int-1_10823, %int17_10824 : (!torch.int, !torch.int) -> !torch.list<int>
    %7071 = torch.aten.view %7069, %7070 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_10825 = torch.constant.int 5632
    %int-1_10826 = torch.constant.int -1
    %int17_10827 = torch.constant.int 17
    %7072 = torch.prim.ListConstruct %int5632_10825, %int-1_10826, %int17_10827 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7073 = torch.aten.view %7071, %7072 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_10828 = torch.constant.int 2
    %int0_10829 = torch.constant.int 0
    %int1_10830 = torch.constant.int 1
    %int1_10831 = torch.constant.int 1
    %7074 = torch.aten.slice.Tensor %7073, %int2_10828, %int0_10829, %int1_10830, %int1_10831 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_10832 = torch.constant.int 5
    %7075 = torch.aten.view.dtype %7074, %int5_10832 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %7076 = torch.aten.detach %7075 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_10833 = torch.constant.int 2
    %int1_10834 = torch.constant.int 1
    %int9223372036854775807_10835 = torch.constant.int 9223372036854775807
    %int1_10836 = torch.constant.int 1
    %7077 = torch.aten.slice.Tensor %7073, %int2_10833, %int1_10834, %int9223372036854775807_10835, %int1_10836 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_10837 = torch.constant.int 1
    %7078 = torch.aten.view.dtype %7077, %int1_10837 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %7079 = torch.aten.detach %7078 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %7080 = torch_c.to_builtin_tensor %7049 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_10838 = tensor.cast %7080 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %7081 = torch_c.to_builtin_tensor %7076 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %7082 = torch_c.to_builtin_tensor %7079 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %7083 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_10838, %7081, %7082) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_10839 = tensor.cast %7083 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %7084 = torch_c.from_builtin_tensor %cast_10839 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %7085 = torch.aten.mul.Tensor %7067, %7084 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_10840 = torch.constant.int 2
    %7086 = torch.aten.view.dtype %340, %int2_10840 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %7087 = torch.aten.detach %7086 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_10841 = torch.constant.int -1
    %int17_10842 = torch.constant.int 17
    %7088 = torch.prim.ListConstruct %int-1_10841, %int17_10842 : (!torch.int, !torch.int) -> !torch.list<int>
    %7089 = torch.aten.view %7087, %7088 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_10843 = torch.constant.int 2048
    %int-1_10844 = torch.constant.int -1
    %int17_10845 = torch.constant.int 17
    %7090 = torch.prim.ListConstruct %int2048_10843, %int-1_10844, %int17_10845 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7091 = torch.aten.view %7089, %7090 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_10846 = torch.constant.int 2
    %int0_10847 = torch.constant.int 0
    %int1_10848 = torch.constant.int 1
    %int1_10849 = torch.constant.int 1
    %7092 = torch.aten.slice.Tensor %7091, %int2_10846, %int0_10847, %int1_10848, %int1_10849 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_10850 = torch.constant.int 5
    %7093 = torch.aten.view.dtype %7092, %int5_10850 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %7094 = torch.aten.detach %7093 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_10851 = torch.constant.int 2
    %int1_10852 = torch.constant.int 1
    %int9223372036854775807_10853 = torch.constant.int 9223372036854775807
    %int1_10854 = torch.constant.int 1
    %7095 = torch.aten.slice.Tensor %7091, %int2_10851, %int1_10852, %int9223372036854775807_10853, %int1_10854 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_10855 = torch.constant.int 1
    %7096 = torch.aten.view.dtype %7095, %int1_10855 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %7097 = torch.aten.detach %7096 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %7098 = torch_c.to_builtin_tensor %7085 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_10856 = tensor.cast %7098 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %7099 = torch_c.to_builtin_tensor %7094 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %7100 = torch_c.to_builtin_tensor %7097 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %7101 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_10856, %7099, %7100) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_10857 = tensor.cast %7101 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %7102 = torch_c.from_builtin_tensor %cast_10857 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_10858 = torch.constant.int 1
    %7103 = torch.aten.add.Tensor %7039, %7102, %int1_10858 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_10859 = torch.constant.none
    %none_10860 = torch.constant.none
    %int5_10861 = torch.constant.int 5
    %cpu_10862 = torch.constant.device "cpu"
    %int0_10863 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7103, %none_10859, %none_10860, %int5_10861, %cpu_10862, %int0_10863 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10864 = torch.constant.int 6
    %7104 = torch.prims.convert_element_type %7103, %int6_10864 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_10865 = torch.constant.int 2
    %7105 = torch.aten.pow.Tensor_Scalar %7104, %int2_10865 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_10866 = torch.constant.int -1
    %7106 = torch.prim.ListConstruct %int-1_10866 : (!torch.int) -> !torch.list<int>
    %true_10867 = torch.constant.bool true
    %none_10868 = torch.constant.none
    %7107 = torch.aten.mean.dim %7105, %7106, %true_10867, %none_10868 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_10869 = torch.constant.float 9.9999997473787516E-6
    %int1_10870 = torch.constant.int 1
    %7108 = torch.aten.add.Scalar %7107, %float9.999990e-06_10869, %int1_10870 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %7109 = torch.aten.rsqrt %7108 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %7110 = torch.aten.mul.Tensor %7104, %7109 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_10871 = torch.constant.none
    %none_10872 = torch.constant.none
    %int6_10873 = torch.constant.int 6
    %cpu_10874 = torch.constant.device "cpu"
    %int0_10875 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7110, %none_10871, %none_10872, %int6_10873, %cpu_10874, %int0_10875 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10876 = torch.constant.int 5
    %7111 = torch.prims.convert_element_type %7110, %int5_10876 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %7112 = torch.aten.mul.Tensor %349, %7111 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_10877 = torch.constant.none
    %none_10878 = torch.constant.none
    %int6_10879 = torch.constant.int 6
    %cpu_10880 = torch.constant.device "cpu"
    %int0_10881 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7112, %none_10877, %none_10878, %int6_10879, %cpu_10880, %int0_10881 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_10882 = torch.constant.int 5
    %7113 = torch.prims.convert_element_type %7112, %int5_10882 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_10883 = torch.constant.int 2
    %7114 = torch.aten.view.dtype %350, %int2_10883 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %7115 = torch.aten.detach %7114 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_10884 = torch.constant.int -1
    %int17_10885 = torch.constant.int 17
    %7116 = torch.prim.ListConstruct %int-1_10884, %int17_10885 : (!torch.int, !torch.int) -> !torch.list<int>
    %7117 = torch.aten.view %7115, %7116 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_10886 = torch.constant.int 2048
    %int-1_10887 = torch.constant.int -1
    %int17_10888 = torch.constant.int 17
    %7118 = torch.prim.ListConstruct %int2048_10886, %int-1_10887, %int17_10888 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7119 = torch.aten.view %7117, %7118 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_10889 = torch.constant.int 2
    %int0_10890 = torch.constant.int 0
    %int1_10891 = torch.constant.int 1
    %int1_10892 = torch.constant.int 1
    %7120 = torch.aten.slice.Tensor %7119, %int2_10889, %int0_10890, %int1_10891, %int1_10892 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_10893 = torch.constant.int 5
    %7121 = torch.aten.view.dtype %7120, %int5_10893 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %7122 = torch.aten.detach %7121 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_10894 = torch.constant.int 2
    %int1_10895 = torch.constant.int 1
    %int9223372036854775807_10896 = torch.constant.int 9223372036854775807
    %int1_10897 = torch.constant.int 1
    %7123 = torch.aten.slice.Tensor %7119, %int2_10894, %int1_10895, %int9223372036854775807_10896, %int1_10897 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_10898 = torch.constant.int 1
    %7124 = torch.aten.view.dtype %7123, %int1_10898 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %7125 = torch.aten.detach %7124 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %7126 = torch_c.to_builtin_tensor %7113 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_10899 = tensor.cast %7126 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %7127 = torch_c.to_builtin_tensor %7122 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %7128 = torch_c.to_builtin_tensor %7125 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %7129 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_10899, %7127, %7128) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_10900 = tensor.cast %7129 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %7130 = torch_c.from_builtin_tensor %cast_10900 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_10901 = torch.constant.int 2
    %7131 = torch.aten.view.dtype %351, %int2_10901 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %7132 = torch.aten.detach %7131 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_10902 = torch.constant.int -1
    %int17_10903 = torch.constant.int 17
    %7133 = torch.prim.ListConstruct %int-1_10902, %int17_10903 : (!torch.int, !torch.int) -> !torch.list<int>
    %7134 = torch.aten.view %7132, %7133 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_10904 = torch.constant.int 256
    %int-1_10905 = torch.constant.int -1
    %int17_10906 = torch.constant.int 17
    %7135 = torch.prim.ListConstruct %int256_10904, %int-1_10905, %int17_10906 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7136 = torch.aten.view %7134, %7135 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_10907 = torch.constant.int 2
    %int0_10908 = torch.constant.int 0
    %int1_10909 = torch.constant.int 1
    %int1_10910 = torch.constant.int 1
    %7137 = torch.aten.slice.Tensor %7136, %int2_10907, %int0_10908, %int1_10909, %int1_10910 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_10911 = torch.constant.int 5
    %7138 = torch.aten.view.dtype %7137, %int5_10911 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %7139 = torch.aten.detach %7138 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_10912 = torch.constant.int 2
    %int1_10913 = torch.constant.int 1
    %int9223372036854775807_10914 = torch.constant.int 9223372036854775807
    %int1_10915 = torch.constant.int 1
    %7140 = torch.aten.slice.Tensor %7136, %int2_10912, %int1_10913, %int9223372036854775807_10914, %int1_10915 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_10916 = torch.constant.int 1
    %7141 = torch.aten.view.dtype %7140, %int1_10916 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %7142 = torch.aten.detach %7141 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %7143 = torch_c.to_builtin_tensor %7113 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_10917 = tensor.cast %7143 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %7144 = torch_c.to_builtin_tensor %7139 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %7145 = torch_c.to_builtin_tensor %7142 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %7146 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_10917, %7144, %7145) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_10918 = tensor.cast %7146 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %7147 = torch_c.from_builtin_tensor %cast_10918 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_10919 = torch.constant.int 2
    %7148 = torch.aten.view.dtype %352, %int2_10919 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %7149 = torch.aten.detach %7148 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_10920 = torch.constant.int -1
    %int17_10921 = torch.constant.int 17
    %7150 = torch.prim.ListConstruct %int-1_10920, %int17_10921 : (!torch.int, !torch.int) -> !torch.list<int>
    %7151 = torch.aten.view %7149, %7150 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_10922 = torch.constant.int 256
    %int-1_10923 = torch.constant.int -1
    %int17_10924 = torch.constant.int 17
    %7152 = torch.prim.ListConstruct %int256_10922, %int-1_10923, %int17_10924 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7153 = torch.aten.view %7151, %7152 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_10925 = torch.constant.int 2
    %int0_10926 = torch.constant.int 0
    %int1_10927 = torch.constant.int 1
    %int1_10928 = torch.constant.int 1
    %7154 = torch.aten.slice.Tensor %7153, %int2_10925, %int0_10926, %int1_10927, %int1_10928 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_10929 = torch.constant.int 5
    %7155 = torch.aten.view.dtype %7154, %int5_10929 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %7156 = torch.aten.detach %7155 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_10930 = torch.constant.int 2
    %int1_10931 = torch.constant.int 1
    %int9223372036854775807_10932 = torch.constant.int 9223372036854775807
    %int1_10933 = torch.constant.int 1
    %7157 = torch.aten.slice.Tensor %7153, %int2_10930, %int1_10931, %int9223372036854775807_10932, %int1_10933 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_10934 = torch.constant.int 1
    %7158 = torch.aten.view.dtype %7157, %int1_10934 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %7159 = torch.aten.detach %7158 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %7160 = torch_c.to_builtin_tensor %7113 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_10935 = tensor.cast %7160 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %7161 = torch_c.to_builtin_tensor %7156 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %7162 = torch_c.to_builtin_tensor %7159 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %7163 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_10935, %7161, %7162) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_10936 = tensor.cast %7163 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %7164 = torch_c.from_builtin_tensor %cast_10936 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_10937 = torch.constant.int 4
    %int1_10938 = torch.constant.int 1
    %int32_10939 = torch.constant.int 32
    %int64_10940 = torch.constant.int 64
    %7165 = torch.prim.ListConstruct %int4_10937, %int1_10938, %int32_10939, %int64_10940 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7166 = torch.aten.view %7130, %7165 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_10941 = torch.constant.int 4
    %int1_10942 = torch.constant.int 1
    %int4_10943 = torch.constant.int 4
    %int64_10944 = torch.constant.int 64
    %7167 = torch.prim.ListConstruct %int4_10941, %int1_10942, %int4_10943, %int64_10944 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7168 = torch.aten.view %7147, %7167 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_10945 = torch.constant.int 4
    %int1_10946 = torch.constant.int 1
    %int4_10947 = torch.constant.int 4
    %int64_10948 = torch.constant.int 64
    %7169 = torch.prim.ListConstruct %int4_10945, %int1_10946, %int4_10947, %int64_10948 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7170 = torch.aten.view %7164, %7169 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_10949 = torch.constant.int 0
    %int1_10950 = torch.constant.int 1
    %none_10951 = torch.constant.none
    %none_10952 = torch.constant.none
    %cpu_10953 = torch.constant.device "cpu"
    %false_10954 = torch.constant.bool false
    %7171 = torch.aten.arange.start %int0_10949, %int1_10950, %none_10951, %none_10952, %cpu_10953, %false_10954 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_10955 = torch.constant.int 0
    %7172 = torch.aten.unsqueeze %7171, %int0_10955 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_10956 = torch.constant.int 1
    %7173 = torch.aten.unsqueeze %arg2, %int1_10956 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_10957 = torch.constant.int 1
    %7174 = torch.aten.add.Tensor %7172, %7173, %int1_10957 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_10958 = torch.constant.int 0
    %int64_10959 = torch.constant.int 64
    %int2_10960 = torch.constant.int 2
    %none_10961 = torch.constant.none
    %none_10962 = torch.constant.none
    %cpu_10963 = torch.constant.device "cpu"
    %false_10964 = torch.constant.bool false
    %7175 = torch.aten.arange.start_step %int0_10958, %int64_10959, %int2_10960, %none_10961, %none_10962, %cpu_10963, %false_10964 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_10965 = torch.constant.none
    %none_10966 = torch.constant.none
    %int4_10967 = torch.constant.int 4
    %cpu_10968 = torch.constant.device "cpu"
    %int0_10969 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7175, %none_10965, %none_10966, %int4_10967, %cpu_10968, %int0_10969 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10970 = torch.constant.int 6
    %7176 = torch.prims.convert_element_type %7175, %int6_10970 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_10971 = torch.constant.int 64
    %7177 = torch.aten.div.Scalar %7176, %int64_10971 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_10972 = torch.constant.float 1.000000e+04
    %7178 = torch.aten.pow.Scalar %float1.000000e04_10972, %7177 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %7179 = torch.aten.reciprocal %7178 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_10973 = torch.constant.float 1.000000e+00
    %7180 = torch.aten.mul.Scalar %7179, %float1.000000e00_10973 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_10974 = torch.constant.none
    %7181 = torch.aten.clone %341, %none_10974 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_10975 = torch.constant.int 0
    %7182 = torch.aten.unsqueeze %7180, %int0_10975 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_10976 = torch.constant.int 2
    %7183 = torch.aten.unsqueeze %7182, %int2_10976 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_10977 = torch.constant.none
    %none_10978 = torch.constant.none
    %int6_10979 = torch.constant.int 6
    %cpu_10980 = torch.constant.device "cpu"
    %int0_10981 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7183, %none_10977, %none_10978, %int6_10979, %cpu_10980, %int0_10981 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_10982 = torch.constant.int 4
    %int-1_10983 = torch.constant.int -1
    %int1_10984 = torch.constant.int 1
    %7184 = torch.prim.ListConstruct %int4_10982, %int-1_10983, %int1_10984 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_10985 = torch.constant.bool false
    %7185 = torch.aten.expand %7183, %7184, %false_10985 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_10986 = torch.constant.int 1
    %7186 = torch.aten.unsqueeze %7174, %int1_10986 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_10987 = torch.constant.none
    %none_10988 = torch.constant.none
    %int4_10989 = torch.constant.int 4
    %cpu_10990 = torch.constant.device "cpu"
    %int0_10991 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7186, %none_10987, %none_10988, %int4_10989, %cpu_10990, %int0_10991 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_10992 = torch.constant.int 6
    %7187 = torch.prims.convert_element_type %7186, %int6_10992 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %7188 = torch.aten.matmul %7185, %7187 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_10993 = torch.constant.int 1
    %int2_10994 = torch.constant.int 2
    %7189 = torch.aten.transpose.int %7188, %int1_10993, %int2_10994 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %7190 = torch.aten.cos %7189 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %7191 = torch.aten.mul.Tensor %7190, %7181 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_10995 = torch.constant.none
    %none_10996 = torch.constant.none
    %int6_10997 = torch.constant.int 6
    %cpu_10998 = torch.constant.device "cpu"
    %int0_10999 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7191, %none_10995, %none_10996, %int6_10997, %cpu_10998, %int0_10999 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11000 = torch.constant.int 5
    %7192 = torch.prims.convert_element_type %7191, %int5_11000 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %7193 = torch.aten.sin %7189 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %7194 = torch.aten.mul.Tensor %7193, %7181 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_11001 = torch.constant.none
    %none_11002 = torch.constant.none
    %int6_11003 = torch.constant.int 6
    %cpu_11004 = torch.constant.device "cpu"
    %int0_11005 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7194, %none_11001, %none_11002, %int6_11003, %cpu_11004, %int0_11005 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11006 = torch.constant.int 5
    %7195 = torch.prims.convert_element_type %7194, %int5_11006 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_11007 = torch.constant.int 2
    %7196 = torch.aten.unsqueeze %7192, %int2_11007 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_11008 = torch.constant.int 2
    %7197 = torch.aten.unsqueeze %7195, %int2_11008 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_11009 = torch.constant.none
    %none_11010 = torch.constant.none
    %int5_11011 = torch.constant.int 5
    %cpu_11012 = torch.constant.device "cpu"
    %int0_11013 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7196, %none_11009, %none_11010, %int5_11011, %cpu_11012, %int0_11013 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_11014 = torch.constant.none
    %none_11015 = torch.constant.none
    %int5_11016 = torch.constant.int 5
    %cpu_11017 = torch.constant.device "cpu"
    %int0_11018 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7197, %none_11014, %none_11015, %int5_11016, %cpu_11017, %int0_11018 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_11019 = torch.constant.none
    %none_11020 = torch.constant.none
    %int5_11021 = torch.constant.int 5
    %cpu_11022 = torch.constant.device "cpu"
    %int0_11023 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7166, %none_11019, %none_11020, %int5_11021, %cpu_11022, %int0_11023 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_11024 = torch.constant.int 3
    %int0_11025 = torch.constant.int 0
    %int64_11026 = torch.constant.int 64
    %int2_11027 = torch.constant.int 2
    %7198 = torch.aten.slice.Tensor %7166, %int3_11024, %int0_11025, %int64_11026, %int2_11027 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_11028 = torch.constant.int 3
    %int1_11029 = torch.constant.int 1
    %int64_11030 = torch.constant.int 64
    %int2_11031 = torch.constant.int 2
    %7199 = torch.aten.slice.Tensor %7166, %int3_11028, %int1_11029, %int64_11030, %int2_11031 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %7200 = torch.aten.mul.Tensor %7198, %7196 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %7201 = torch.aten.mul.Tensor %7199, %7197 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_11032 = torch.constant.int 1
    %7202 = torch.aten.sub.Tensor %7200, %7201, %int1_11032 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %7203 = torch.aten.mul.Tensor %7199, %7196 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %7204 = torch.aten.mul.Tensor %7198, %7197 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_11033 = torch.constant.int 1
    %7205 = torch.aten.add.Tensor %7203, %7204, %int1_11033 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %7206 = torch_c.to_builtin_tensor %7202 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_11034 = tensor.cast %7206 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %7207 = torch_c.to_builtin_tensor %7205 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_11035 = tensor.cast %7207 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %7208 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_11034, %cast_11035) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_11036 = tensor.cast %7208 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %7209 = torch_c.from_builtin_tensor %cast_11036 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_11037 = torch.constant.int 4
    %int1_11038 = torch.constant.int 1
    %int32_11039 = torch.constant.int 32
    %int64_11040 = torch.constant.int 64
    %7210 = torch.prim.ListConstruct %int4_11037, %int1_11038, %int32_11039, %int64_11040 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7211 = torch.aten.view %7209, %7210 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_11041 = torch.constant.none
    %none_11042 = torch.constant.none
    %int5_11043 = torch.constant.int 5
    %cpu_11044 = torch.constant.device "cpu"
    %int0_11045 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7211, %none_11041, %none_11042, %int5_11043, %cpu_11044, %int0_11045 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_11046 = torch.constant.int 0
    %int1_11047 = torch.constant.int 1
    %none_11048 = torch.constant.none
    %none_11049 = torch.constant.none
    %cpu_11050 = torch.constant.device "cpu"
    %false_11051 = torch.constant.bool false
    %7212 = torch.aten.arange.start %int0_11046, %int1_11047, %none_11048, %none_11049, %cpu_11050, %false_11051 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_11052 = torch.constant.int 0
    %7213 = torch.aten.unsqueeze %7212, %int0_11052 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_11053 = torch.constant.int 1
    %7214 = torch.aten.unsqueeze %arg2, %int1_11053 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_11054 = torch.constant.int 1
    %7215 = torch.aten.add.Tensor %7213, %7214, %int1_11054 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_11055 = torch.constant.int 0
    %int64_11056 = torch.constant.int 64
    %int2_11057 = torch.constant.int 2
    %none_11058 = torch.constant.none
    %none_11059 = torch.constant.none
    %cpu_11060 = torch.constant.device "cpu"
    %false_11061 = torch.constant.bool false
    %7216 = torch.aten.arange.start_step %int0_11055, %int64_11056, %int2_11057, %none_11058, %none_11059, %cpu_11060, %false_11061 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_11062 = torch.constant.none
    %none_11063 = torch.constant.none
    %int4_11064 = torch.constant.int 4
    %cpu_11065 = torch.constant.device "cpu"
    %int0_11066 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7216, %none_11062, %none_11063, %int4_11064, %cpu_11065, %int0_11066 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_11067 = torch.constant.int 6
    %7217 = torch.prims.convert_element_type %7216, %int6_11067 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_11068 = torch.constant.int 64
    %7218 = torch.aten.div.Scalar %7217, %int64_11068 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_11069 = torch.constant.float 1.000000e+04
    %7219 = torch.aten.pow.Scalar %float1.000000e04_11069, %7218 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %7220 = torch.aten.reciprocal %7219 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_11070 = torch.constant.float 1.000000e+00
    %7221 = torch.aten.mul.Scalar %7220, %float1.000000e00_11070 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_11071 = torch.constant.none
    %7222 = torch.aten.clone %342, %none_11071 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_11072 = torch.constant.int 0
    %7223 = torch.aten.unsqueeze %7221, %int0_11072 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_11073 = torch.constant.int 2
    %7224 = torch.aten.unsqueeze %7223, %int2_11073 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_11074 = torch.constant.none
    %none_11075 = torch.constant.none
    %int6_11076 = torch.constant.int 6
    %cpu_11077 = torch.constant.device "cpu"
    %int0_11078 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7224, %none_11074, %none_11075, %int6_11076, %cpu_11077, %int0_11078 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_11079 = torch.constant.int 4
    %int-1_11080 = torch.constant.int -1
    %int1_11081 = torch.constant.int 1
    %7225 = torch.prim.ListConstruct %int4_11079, %int-1_11080, %int1_11081 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_11082 = torch.constant.bool false
    %7226 = torch.aten.expand %7224, %7225, %false_11082 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_11083 = torch.constant.int 1
    %7227 = torch.aten.unsqueeze %7215, %int1_11083 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_11084 = torch.constant.none
    %none_11085 = torch.constant.none
    %int4_11086 = torch.constant.int 4
    %cpu_11087 = torch.constant.device "cpu"
    %int0_11088 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7227, %none_11084, %none_11085, %int4_11086, %cpu_11087, %int0_11088 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_11089 = torch.constant.int 6
    %7228 = torch.prims.convert_element_type %7227, %int6_11089 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %7229 = torch.aten.matmul %7226, %7228 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_11090 = torch.constant.int 1
    %int2_11091 = torch.constant.int 2
    %7230 = torch.aten.transpose.int %7229, %int1_11090, %int2_11091 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %7231 = torch.aten.cos %7230 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %7232 = torch.aten.mul.Tensor %7231, %7222 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_11092 = torch.constant.none
    %none_11093 = torch.constant.none
    %int6_11094 = torch.constant.int 6
    %cpu_11095 = torch.constant.device "cpu"
    %int0_11096 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7232, %none_11092, %none_11093, %int6_11094, %cpu_11095, %int0_11096 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11097 = torch.constant.int 5
    %7233 = torch.prims.convert_element_type %7232, %int5_11097 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %7234 = torch.aten.sin %7230 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %7235 = torch.aten.mul.Tensor %7234, %7222 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_11098 = torch.constant.none
    %none_11099 = torch.constant.none
    %int6_11100 = torch.constant.int 6
    %cpu_11101 = torch.constant.device "cpu"
    %int0_11102 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7235, %none_11098, %none_11099, %int6_11100, %cpu_11101, %int0_11102 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11103 = torch.constant.int 5
    %7236 = torch.prims.convert_element_type %7235, %int5_11103 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_11104 = torch.constant.int 2
    %7237 = torch.aten.unsqueeze %7233, %int2_11104 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_11105 = torch.constant.int 2
    %7238 = torch.aten.unsqueeze %7236, %int2_11105 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_11106 = torch.constant.none
    %none_11107 = torch.constant.none
    %int5_11108 = torch.constant.int 5
    %cpu_11109 = torch.constant.device "cpu"
    %int0_11110 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7237, %none_11106, %none_11107, %int5_11108, %cpu_11109, %int0_11110 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_11111 = torch.constant.none
    %none_11112 = torch.constant.none
    %int5_11113 = torch.constant.int 5
    %cpu_11114 = torch.constant.device "cpu"
    %int0_11115 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7238, %none_11111, %none_11112, %int5_11113, %cpu_11114, %int0_11115 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_11116 = torch.constant.none
    %none_11117 = torch.constant.none
    %int5_11118 = torch.constant.int 5
    %cpu_11119 = torch.constant.device "cpu"
    %int0_11120 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7168, %none_11116, %none_11117, %int5_11118, %cpu_11119, %int0_11120 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_11121 = torch.constant.int 3
    %int0_11122 = torch.constant.int 0
    %int64_11123 = torch.constant.int 64
    %int2_11124 = torch.constant.int 2
    %7239 = torch.aten.slice.Tensor %7168, %int3_11121, %int0_11122, %int64_11123, %int2_11124 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_11125 = torch.constant.int 3
    %int1_11126 = torch.constant.int 1
    %int64_11127 = torch.constant.int 64
    %int2_11128 = torch.constant.int 2
    %7240 = torch.aten.slice.Tensor %7168, %int3_11125, %int1_11126, %int64_11127, %int2_11128 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %7241 = torch.aten.mul.Tensor %7239, %7237 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %7242 = torch.aten.mul.Tensor %7240, %7238 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_11129 = torch.constant.int 1
    %7243 = torch.aten.sub.Tensor %7241, %7242, %int1_11129 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %7244 = torch.aten.mul.Tensor %7240, %7237 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %7245 = torch.aten.mul.Tensor %7239, %7238 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_11130 = torch.constant.int 1
    %7246 = torch.aten.add.Tensor %7244, %7245, %int1_11130 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %7247 = torch_c.to_builtin_tensor %7243 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_11131 = tensor.cast %7247 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %7248 = torch_c.to_builtin_tensor %7246 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_11132 = tensor.cast %7248 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %7249 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_11131, %cast_11132) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_11133 = tensor.cast %7249 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %7250 = torch_c.from_builtin_tensor %cast_11133 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_11134 = torch.constant.int 4
    %int1_11135 = torch.constant.int 1
    %int4_11136 = torch.constant.int 4
    %int64_11137 = torch.constant.int 64
    %7251 = torch.prim.ListConstruct %int4_11134, %int1_11135, %int4_11136, %int64_11137 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7252 = torch.aten.view %7250, %7251 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_11138 = torch.constant.none
    %none_11139 = torch.constant.none
    %int5_11140 = torch.constant.int 5
    %cpu_11141 = torch.constant.device "cpu"
    %int0_11142 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7252, %none_11138, %none_11139, %int5_11140, %cpu_11141, %int0_11142 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_11143 = torch.constant.int 32
    %7253 = torch.aten.floor_divide.Scalar %arg2, %int32_11143 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_11144 = torch.constant.int 1
    %7254 = torch.aten.unsqueeze %7253, %int1_11144 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_11145 = torch.constant.int 1
    %false_11146 = torch.constant.bool false
    %7255 = torch.aten.gather %arg3, %int1_11145, %7254, %false_11146 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_11147 = torch.constant.int 4
    %int1_11148 = torch.constant.int 1
    %int1_11149 = torch.constant.int 1
    %7256 = torch.prim.ListConstruct %int4_11147, %int1_11148, %int1_11149 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7257 = torch.aten.view %7255, %7256 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_11150 = torch.constant.int 32
    %7258 = torch.aten.remainder.Scalar %arg2, %int32_11150 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_11151 = torch.constant.int 4
    %int1_11152 = torch.constant.int 1
    %int1_11153 = torch.constant.int 1
    %7259 = torch.prim.ListConstruct %int4_11151, %int1_11152, %int1_11153 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7260 = torch.aten.view %7258, %7259 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_11154 = torch.constant.int 4
    %none_11155 = torch.constant.none
    %none_11156 = torch.constant.none
    %cpu_11157 = torch.constant.device "cpu"
    %false_11158 = torch.constant.bool false
    %7261 = torch.aten.arange %int4_11154, %none_11155, %none_11156, %cpu_11157, %false_11158 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_11159 = torch.constant.int 1
    %int1_11160 = torch.constant.int 1
    %int4_11161 = torch.constant.int 4
    %7262 = torch.prim.ListConstruct %int1_11159, %int1_11160, %int4_11161 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7263 = torch.aten.view %7261, %7262 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_11162 = torch.constant.none
    %7264 = torch.aten.clone %343, %none_11162 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_11163 = torch.constant.int 1
    %int1_11164 = torch.constant.int 1
    %int1_11165 = torch.constant.int 1
    %7265 = torch.prim.ListConstruct %int1_11163, %int1_11164, %int1_11165 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7266 = torch.aten.view %7264, %7265 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_11166 = torch.constant.int 22
    %7267 = torch.aten.mul.Scalar %7257, %int22_11166 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int20 = torch.constant.int 20
    %int1_11167 = torch.constant.int 1
    %7268 = torch.aten.add.Scalar %7267, %int20, %int1_11167 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_11168 = torch.constant.int 2
    %7269 = torch.aten.mul.Scalar %7268, %int2_11168 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_11169 = torch.constant.int 1
    %7270 = torch.aten.add.Tensor %7269, %7266, %int1_11169 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_11170 = torch.constant.int 4
    %7271 = torch.aten.mul.Scalar %7270, %int4_11170 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_11171 = torch.constant.int 1
    %7272 = torch.aten.add.Tensor %7271, %7263, %int1_11171 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_11172 = torch.constant.int 32
    %7273 = torch.aten.mul.Scalar %7272, %int32_11172 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_11173 = torch.constant.int 1
    %7274 = torch.aten.add.Tensor %7273, %7260, %int1_11173 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_11174 = torch.constant.none
    %none_11175 = torch.constant.none
    %int5_11176 = torch.constant.int 5
    %cpu_11177 = torch.constant.device "cpu"
    %int0_11178 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7252, %none_11174, %none_11175, %int5_11176, %cpu_11177, %int0_11178 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_11179 = torch.constant.int 22
    %int2_11180 = torch.constant.int 2
    %int4_11181 = torch.constant.int 4
    %int32_11182 = torch.constant.int 32
    %int64_11183 = torch.constant.int 64
    %7275 = torch.prim.ListConstruct %381, %int22_11179, %int2_11180, %int4_11181, %int32_11182, %int64_11183 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7276 = torch.aten.view %6970, %7275 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %7276, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_11184 = torch.constant.int 64
    %7277 = torch.prim.ListConstruct %553, %int64_11184 : (!torch.int, !torch.int) -> !torch.list<int>
    %7278 = torch.aten.view %7276, %7277 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %7278, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %7279 = torch.prim.ListConstruct %7274 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_11185 = torch.constant.bool false
    %7280 = torch.aten.index_put %7278, %7279, %7252, %false_11185 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %7280, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_11186 = torch.constant.int 22
    %int2_11187 = torch.constant.int 2
    %int4_11188 = torch.constant.int 4
    %int32_11189 = torch.constant.int 32
    %int64_11190 = torch.constant.int 64
    %7281 = torch.prim.ListConstruct %381, %int22_11186, %int2_11187, %int4_11188, %int32_11189, %int64_11190 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7282 = torch.aten.view %7280, %7281 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %7282, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_11191 = torch.constant.int 360448
    %7283 = torch.prim.ListConstruct %381, %int360448_11191 : (!torch.int, !torch.int) -> !torch.list<int>
    %7284 = torch.aten.view %7282, %7283 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %7284, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_11192 = torch.constant.int 22
    %int2_11193 = torch.constant.int 2
    %int4_11194 = torch.constant.int 4
    %int32_11195 = torch.constant.int 32
    %int64_11196 = torch.constant.int 64
    %7285 = torch.prim.ListConstruct %381, %int22_11192, %int2_11193, %int4_11194, %int32_11195, %int64_11196 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7286 = torch.aten.view %7284, %7285 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %7286, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_11197 = torch.constant.int 64
    %7287 = torch.prim.ListConstruct %553, %int64_11197 : (!torch.int, !torch.int) -> !torch.list<int>
    %7288 = torch.aten.view %7286, %7287 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %7288, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_11198 = torch.constant.none
    %7289 = torch.aten.clone %344, %none_11198 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_11199 = torch.constant.int 1
    %int1_11200 = torch.constant.int 1
    %int1_11201 = torch.constant.int 1
    %7290 = torch.prim.ListConstruct %int1_11199, %int1_11200, %int1_11201 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7291 = torch.aten.view %7289, %7290 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_11202 = torch.constant.int 22
    %7292 = torch.aten.mul.Scalar %7257, %int22_11202 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int20_11203 = torch.constant.int 20
    %int1_11204 = torch.constant.int 1
    %7293 = torch.aten.add.Scalar %7292, %int20_11203, %int1_11204 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_11205 = torch.constant.int 2
    %7294 = torch.aten.mul.Scalar %7293, %int2_11205 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_11206 = torch.constant.int 1
    %7295 = torch.aten.add.Tensor %7294, %7291, %int1_11206 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_11207 = torch.constant.int 4
    %7296 = torch.aten.mul.Scalar %7295, %int4_11207 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_11208 = torch.constant.int 1
    %7297 = torch.aten.add.Tensor %7296, %7263, %int1_11208 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_11209 = torch.constant.int 32
    %7298 = torch.aten.mul.Scalar %7297, %int32_11209 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_11210 = torch.constant.int 1
    %7299 = torch.aten.add.Tensor %7298, %7260, %int1_11210 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_11211 = torch.constant.none
    %none_11212 = torch.constant.none
    %int5_11213 = torch.constant.int 5
    %cpu_11214 = torch.constant.device "cpu"
    %int0_11215 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7170, %none_11211, %none_11212, %int5_11213, %cpu_11214, %int0_11215 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %7300 = torch.prim.ListConstruct %7299 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_11216 = torch.constant.bool false
    %7301 = torch.aten.index_put %7288, %7300, %7170, %false_11216 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %7301, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_11217 = torch.constant.int 22
    %int2_11218 = torch.constant.int 2
    %int4_11219 = torch.constant.int 4
    %int32_11220 = torch.constant.int 32
    %int64_11221 = torch.constant.int 64
    %7302 = torch.prim.ListConstruct %381, %int22_11217, %int2_11218, %int4_11219, %int32_11220, %int64_11221 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7303 = torch.aten.view %7301, %7302 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %7303, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_11222 = torch.constant.int 360448
    %7304 = torch.prim.ListConstruct %381, %int360448_11222 : (!torch.int, !torch.int) -> !torch.list<int>
    %7305 = torch.aten.view %7303, %7304 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %7305, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_11223 = torch.constant.none
    %7306 = torch.aten.clone %345, %none_11223 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_11224 = torch.constant.none
    %7307 = torch.aten.clone %346, %none_11224 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_11225 = torch.constant.none
    %7308 = torch.aten.clone %347, %none_11225 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_11226 = torch.constant.int 22
    %int2_11227 = torch.constant.int 2
    %int4_11228 = torch.constant.int 4
    %int32_11229 = torch.constant.int 32
    %int64_11230 = torch.constant.int 64
    %7309 = torch.prim.ListConstruct %381, %int22_11226, %int2_11227, %int4_11228, %int32_11229, %int64_11230 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7310 = torch.aten.view %7305, %7309 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %7310, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %7311 = torch_c.to_builtin_tensor %7310 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %7312 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_11231 = tensor.cast %7312 : tensor<4x?xi64> to tensor<?x?xi64>
    %7313 = torch_c.to_builtin_tensor %7306 : !torch.vtensor<[],si64> -> tensor<i64>
    %7314 = torch_c.to_builtin_tensor %7307 : !torch.vtensor<[],si64> -> tensor<i64>
    %7315 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%7311, %cast_11231, %7313, %7314) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_11232 = tensor.cast %7315 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %7316 = torch_c.from_builtin_tensor %cast_11232 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %7316, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %7317 = torch_c.to_builtin_tensor %7310 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %7318 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_11233 = tensor.cast %7318 : tensor<4x?xi64> to tensor<?x?xi64>
    %7319 = torch_c.to_builtin_tensor %7306 : !torch.vtensor<[],si64> -> tensor<i64>
    %7320 = torch_c.to_builtin_tensor %7308 : !torch.vtensor<[],si64> -> tensor<i64>
    %7321 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%7317, %cast_11233, %7319, %7320) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_11234 = tensor.cast %7321 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %7322 = torch_c.from_builtin_tensor %cast_11234 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %7322, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_11235 = torch.constant.int 2
    %int3_11236 = torch.constant.int 3
    %7323 = torch.aten.transpose.int %7316, %int2_11235, %int3_11236 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %7323, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_11237 = torch.constant.int 0
    %7324 = torch.aten.clone %7323, %int0_11237 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %7324, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_11238 = torch.constant.int 4
    %int4_11239 = torch.constant.int 4
    %int64_11240 = torch.constant.int 64
    %7325 = torch.prim.ListConstruct %int4_11238, %623, %int4_11239, %int64_11240 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7326 = torch.aten._unsafe_view %7324, %7325 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %7326, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_11241 = torch.constant.int 2
    %int3_11242 = torch.constant.int 3
    %7327 = torch.aten.transpose.int %7322, %int2_11241, %int3_11242 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %7327, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_11243 = torch.constant.int 0
    %7328 = torch.aten.clone %7327, %int0_11243 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %7328, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_11244 = torch.constant.int 4
    %int4_11245 = torch.constant.int 4
    %int64_11246 = torch.constant.int 64
    %7329 = torch.prim.ListConstruct %int4_11244, %623, %int4_11245, %int64_11246 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7330 = torch.aten._unsafe_view %7328, %7329 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %7330, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_11247 = torch.constant.int 0
    %int1_11248 = torch.constant.int 1
    %none_11249 = torch.constant.none
    %none_11250 = torch.constant.none
    %cpu_11251 = torch.constant.device "cpu"
    %false_11252 = torch.constant.bool false
    %7331 = torch.aten.arange.start_step %int0_11247, %623, %int1_11248, %none_11249, %none_11250, %cpu_11251, %false_11252 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %7331, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_11253 = torch.constant.int -1
    %7332 = torch.aten.unsqueeze %arg1, %int-1_11253 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %7333 = torch.aten.ge.Tensor %7331, %7332 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %7333, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_11254 = torch.constant.none
    %7334 = torch.aten.clone %348, %none_11254 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_11255 = torch.constant.int 0
    %7335 = torch.aten.where.ScalarOther %7333, %7334, %int0_11255 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %7335, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_11256 = torch.constant.none
    %none_11257 = torch.constant.none
    %int5_11258 = torch.constant.int 5
    %cpu_11259 = torch.constant.device "cpu"
    %int0_11260 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7335, %none_11256, %none_11257, %int5_11258, %cpu_11259, %int0_11260 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_11261 = torch.constant.int 1
    %7336 = torch.aten.unsqueeze %7335, %int1_11261 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %7336, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_11262 = torch.constant.int 1
    %7337 = torch.aten.unsqueeze %7336, %int1_11262 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %7337, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_11263 = torch.constant.int -2
    %7338 = torch.aten.unsqueeze %7326, %int-2_11263 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %7338, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_11264 = torch.constant.int 4
    %int4_11265 = torch.constant.int 4
    %int8_11266 = torch.constant.int 8
    %int64_11267 = torch.constant.int 64
    %7339 = torch.prim.ListConstruct %int4_11264, %623, %int4_11265, %int8_11266, %int64_11267 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_11268 = torch.constant.bool false
    %7340 = torch.aten.expand %7338, %7339, %false_11268 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %7340, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_11269 = torch.constant.int 0
    %7341 = torch.aten.clone %7340, %int0_11269 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %7341, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_11270 = torch.constant.int 4
    %int32_11271 = torch.constant.int 32
    %int64_11272 = torch.constant.int 64
    %7342 = torch.prim.ListConstruct %int4_11270, %623, %int32_11271, %int64_11272 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7343 = torch.aten._unsafe_view %7341, %7342 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %7343, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_11273 = torch.constant.int -2
    %7344 = torch.aten.unsqueeze %7330, %int-2_11273 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %7344, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_11274 = torch.constant.int 4
    %int4_11275 = torch.constant.int 4
    %int8_11276 = torch.constant.int 8
    %int64_11277 = torch.constant.int 64
    %7345 = torch.prim.ListConstruct %int4_11274, %623, %int4_11275, %int8_11276, %int64_11277 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_11278 = torch.constant.bool false
    %7346 = torch.aten.expand %7344, %7345, %false_11278 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %7346, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_11279 = torch.constant.int 0
    %7347 = torch.aten.clone %7346, %int0_11279 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %7347, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_11280 = torch.constant.int 4
    %int32_11281 = torch.constant.int 32
    %int64_11282 = torch.constant.int 64
    %7348 = torch.prim.ListConstruct %int4_11280, %623, %int32_11281, %int64_11282 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7349 = torch.aten._unsafe_view %7347, %7348 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %7349, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_11283 = torch.constant.int 1
    %int2_11284 = torch.constant.int 2
    %7350 = torch.aten.transpose.int %7211, %int1_11283, %int2_11284 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_11285 = torch.constant.int 1
    %int2_11286 = torch.constant.int 2
    %7351 = torch.aten.transpose.int %7343, %int1_11285, %int2_11286 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %7351, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_11287 = torch.constant.int 1
    %int2_11288 = torch.constant.int 2
    %7352 = torch.aten.transpose.int %7349, %int1_11287, %int2_11288 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %7352, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_11289 = torch.constant.float 0.000000e+00
    %false_11290 = torch.constant.bool false
    %none_11291 = torch.constant.none
    %false_11292 = torch.constant.bool false
    %7353 = torch.aten.scaled_dot_product_attention %7350, %7351, %7352, %7337, %float0.000000e00_11289, %false_11290, %none_11291, %false_11292 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_11293 = torch.constant.int 1
    %int2_11294 = torch.constant.int 2
    %7354 = torch.aten.transpose.int %7353, %int1_11293, %int2_11294 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_11295 = torch.constant.int 4
    %int1_11296 = torch.constant.int 1
    %int2048_11297 = torch.constant.int 2048
    %7355 = torch.prim.ListConstruct %int4_11295, %int1_11296, %int2048_11297 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7356 = torch.aten.view %7354, %7355 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_11298 = torch.constant.int 2
    %7357 = torch.aten.view.dtype %353, %int2_11298 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %7358 = torch.aten.detach %7357 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_11299 = torch.constant.int -1
    %int17_11300 = torch.constant.int 17
    %7359 = torch.prim.ListConstruct %int-1_11299, %int17_11300 : (!torch.int, !torch.int) -> !torch.list<int>
    %7360 = torch.aten.view %7358, %7359 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_11301 = torch.constant.int 2048
    %int-1_11302 = torch.constant.int -1
    %int17_11303 = torch.constant.int 17
    %7361 = torch.prim.ListConstruct %int2048_11301, %int-1_11302, %int17_11303 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7362 = torch.aten.view %7360, %7361 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_11304 = torch.constant.int 2
    %int0_11305 = torch.constant.int 0
    %int1_11306 = torch.constant.int 1
    %int1_11307 = torch.constant.int 1
    %7363 = torch.aten.slice.Tensor %7362, %int2_11304, %int0_11305, %int1_11306, %int1_11307 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_11308 = torch.constant.int 5
    %7364 = torch.aten.view.dtype %7363, %int5_11308 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %7365 = torch.aten.detach %7364 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_11309 = torch.constant.int 2
    %int1_11310 = torch.constant.int 1
    %int9223372036854775807_11311 = torch.constant.int 9223372036854775807
    %int1_11312 = torch.constant.int 1
    %7366 = torch.aten.slice.Tensor %7362, %int2_11309, %int1_11310, %int9223372036854775807_11311, %int1_11312 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_11313 = torch.constant.int 1
    %7367 = torch.aten.view.dtype %7366, %int1_11313 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %7368 = torch.aten.detach %7367 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %7369 = torch_c.to_builtin_tensor %7356 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_11314 = tensor.cast %7369 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %7370 = torch_c.to_builtin_tensor %7365 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %7371 = torch_c.to_builtin_tensor %7368 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %7372 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_11314, %7370, %7371) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_11315 = tensor.cast %7372 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %7373 = torch_c.from_builtin_tensor %cast_11315 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_11316 = torch.constant.none
    %none_11317 = torch.constant.none
    %int5_11318 = torch.constant.int 5
    %cpu_11319 = torch.constant.device "cpu"
    %int0_11320 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7373, %none_11316, %none_11317, %int5_11318, %cpu_11319, %int0_11320 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_11321 = torch.constant.int 1
    %7374 = torch.aten.add.Tensor %7103, %7373, %int1_11321 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_11322 = torch.constant.none
    %none_11323 = torch.constant.none
    %int5_11324 = torch.constant.int 5
    %cpu_11325 = torch.constant.device "cpu"
    %int0_11326 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7374, %none_11322, %none_11323, %int5_11324, %cpu_11325, %int0_11326 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_11327 = torch.constant.int 6
    %7375 = torch.prims.convert_element_type %7374, %int6_11327 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_11328 = torch.constant.int 2
    %7376 = torch.aten.pow.Tensor_Scalar %7375, %int2_11328 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_11329 = torch.constant.int -1
    %7377 = torch.prim.ListConstruct %int-1_11329 : (!torch.int) -> !torch.list<int>
    %true_11330 = torch.constant.bool true
    %none_11331 = torch.constant.none
    %7378 = torch.aten.mean.dim %7376, %7377, %true_11330, %none_11331 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_11332 = torch.constant.float 9.9999997473787516E-6
    %int1_11333 = torch.constant.int 1
    %7379 = torch.aten.add.Scalar %7378, %float9.999990e-06_11332, %int1_11333 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %7380 = torch.aten.rsqrt %7379 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %7381 = torch.aten.mul.Tensor %7375, %7380 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_11334 = torch.constant.none
    %none_11335 = torch.constant.none
    %int6_11336 = torch.constant.int 6
    %cpu_11337 = torch.constant.device "cpu"
    %int0_11338 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7381, %none_11334, %none_11335, %int6_11336, %cpu_11337, %int0_11338 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11339 = torch.constant.int 5
    %7382 = torch.prims.convert_element_type %7381, %int5_11339 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %7383 = torch.aten.mul.Tensor %354, %7382 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_11340 = torch.constant.none
    %none_11341 = torch.constant.none
    %int6_11342 = torch.constant.int 6
    %cpu_11343 = torch.constant.device "cpu"
    %int0_11344 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7383, %none_11340, %none_11341, %int6_11342, %cpu_11343, %int0_11344 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11345 = torch.constant.int 5
    %7384 = torch.prims.convert_element_type %7383, %int5_11345 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_11346 = torch.constant.int 2
    %7385 = torch.aten.view.dtype %355, %int2_11346 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %7386 = torch.aten.detach %7385 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_11347 = torch.constant.int -1
    %int17_11348 = torch.constant.int 17
    %7387 = torch.prim.ListConstruct %int-1_11347, %int17_11348 : (!torch.int, !torch.int) -> !torch.list<int>
    %7388 = torch.aten.view %7386, %7387 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_11349 = torch.constant.int 5632
    %int-1_11350 = torch.constant.int -1
    %int17_11351 = torch.constant.int 17
    %7389 = torch.prim.ListConstruct %int5632_11349, %int-1_11350, %int17_11351 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7390 = torch.aten.view %7388, %7389 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_11352 = torch.constant.int 2
    %int0_11353 = torch.constant.int 0
    %int1_11354 = torch.constant.int 1
    %int1_11355 = torch.constant.int 1
    %7391 = torch.aten.slice.Tensor %7390, %int2_11352, %int0_11353, %int1_11354, %int1_11355 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_11356 = torch.constant.int 5
    %7392 = torch.aten.view.dtype %7391, %int5_11356 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %7393 = torch.aten.detach %7392 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_11357 = torch.constant.int 2
    %int1_11358 = torch.constant.int 1
    %int9223372036854775807_11359 = torch.constant.int 9223372036854775807
    %int1_11360 = torch.constant.int 1
    %7394 = torch.aten.slice.Tensor %7390, %int2_11357, %int1_11358, %int9223372036854775807_11359, %int1_11360 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_11361 = torch.constant.int 1
    %7395 = torch.aten.view.dtype %7394, %int1_11361 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %7396 = torch.aten.detach %7395 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %7397 = torch_c.to_builtin_tensor %7384 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_11362 = tensor.cast %7397 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %7398 = torch_c.to_builtin_tensor %7393 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %7399 = torch_c.to_builtin_tensor %7396 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %7400 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_11362, %7398, %7399) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_11363 = tensor.cast %7400 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %7401 = torch_c.from_builtin_tensor %cast_11363 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %7402 = torch.aten.silu %7401 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_11364 = torch.constant.int 2
    %7403 = torch.aten.view.dtype %356, %int2_11364 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %7404 = torch.aten.detach %7403 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_11365 = torch.constant.int -1
    %int17_11366 = torch.constant.int 17
    %7405 = torch.prim.ListConstruct %int-1_11365, %int17_11366 : (!torch.int, !torch.int) -> !torch.list<int>
    %7406 = torch.aten.view %7404, %7405 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_11367 = torch.constant.int 5632
    %int-1_11368 = torch.constant.int -1
    %int17_11369 = torch.constant.int 17
    %7407 = torch.prim.ListConstruct %int5632_11367, %int-1_11368, %int17_11369 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7408 = torch.aten.view %7406, %7407 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_11370 = torch.constant.int 2
    %int0_11371 = torch.constant.int 0
    %int1_11372 = torch.constant.int 1
    %int1_11373 = torch.constant.int 1
    %7409 = torch.aten.slice.Tensor %7408, %int2_11370, %int0_11371, %int1_11372, %int1_11373 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_11374 = torch.constant.int 5
    %7410 = torch.aten.view.dtype %7409, %int5_11374 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %7411 = torch.aten.detach %7410 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_11375 = torch.constant.int 2
    %int1_11376 = torch.constant.int 1
    %int9223372036854775807_11377 = torch.constant.int 9223372036854775807
    %int1_11378 = torch.constant.int 1
    %7412 = torch.aten.slice.Tensor %7408, %int2_11375, %int1_11376, %int9223372036854775807_11377, %int1_11378 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_11379 = torch.constant.int 1
    %7413 = torch.aten.view.dtype %7412, %int1_11379 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %7414 = torch.aten.detach %7413 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %7415 = torch_c.to_builtin_tensor %7384 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_11380 = tensor.cast %7415 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %7416 = torch_c.to_builtin_tensor %7411 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %7417 = torch_c.to_builtin_tensor %7414 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %7418 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_11380, %7416, %7417) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_11381 = tensor.cast %7418 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %7419 = torch_c.from_builtin_tensor %cast_11381 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %7420 = torch.aten.mul.Tensor %7402, %7419 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_11382 = torch.constant.int 2
    %7421 = torch.aten.view.dtype %357, %int2_11382 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %7422 = torch.aten.detach %7421 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_11383 = torch.constant.int -1
    %int17_11384 = torch.constant.int 17
    %7423 = torch.prim.ListConstruct %int-1_11383, %int17_11384 : (!torch.int, !torch.int) -> !torch.list<int>
    %7424 = torch.aten.view %7422, %7423 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_11385 = torch.constant.int 2048
    %int-1_11386 = torch.constant.int -1
    %int17_11387 = torch.constant.int 17
    %7425 = torch.prim.ListConstruct %int2048_11385, %int-1_11386, %int17_11387 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7426 = torch.aten.view %7424, %7425 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_11388 = torch.constant.int 2
    %int0_11389 = torch.constant.int 0
    %int1_11390 = torch.constant.int 1
    %int1_11391 = torch.constant.int 1
    %7427 = torch.aten.slice.Tensor %7426, %int2_11388, %int0_11389, %int1_11390, %int1_11391 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_11392 = torch.constant.int 5
    %7428 = torch.aten.view.dtype %7427, %int5_11392 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %7429 = torch.aten.detach %7428 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_11393 = torch.constant.int 2
    %int1_11394 = torch.constant.int 1
    %int9223372036854775807_11395 = torch.constant.int 9223372036854775807
    %int1_11396 = torch.constant.int 1
    %7430 = torch.aten.slice.Tensor %7426, %int2_11393, %int1_11394, %int9223372036854775807_11395, %int1_11396 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_11397 = torch.constant.int 1
    %7431 = torch.aten.view.dtype %7430, %int1_11397 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %7432 = torch.aten.detach %7431 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %7433 = torch_c.to_builtin_tensor %7420 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_11398 = tensor.cast %7433 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %7434 = torch_c.to_builtin_tensor %7429 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %7435 = torch_c.to_builtin_tensor %7432 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %7436 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_11398, %7434, %7435) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_11399 = tensor.cast %7436 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %7437 = torch_c.from_builtin_tensor %cast_11399 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_11400 = torch.constant.int 1
    %7438 = torch.aten.add.Tensor %7374, %7437, %int1_11400 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_11401 = torch.constant.none
    %none_11402 = torch.constant.none
    %int5_11403 = torch.constant.int 5
    %cpu_11404 = torch.constant.device "cpu"
    %int0_11405 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7438, %none_11401, %none_11402, %int5_11403, %cpu_11404, %int0_11405 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_11406 = torch.constant.int 6
    %7439 = torch.prims.convert_element_type %7438, %int6_11406 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_11407 = torch.constant.int 2
    %7440 = torch.aten.pow.Tensor_Scalar %7439, %int2_11407 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_11408 = torch.constant.int -1
    %7441 = torch.prim.ListConstruct %int-1_11408 : (!torch.int) -> !torch.list<int>
    %true_11409 = torch.constant.bool true
    %none_11410 = torch.constant.none
    %7442 = torch.aten.mean.dim %7440, %7441, %true_11409, %none_11410 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_11411 = torch.constant.float 9.9999997473787516E-6
    %int1_11412 = torch.constant.int 1
    %7443 = torch.aten.add.Scalar %7442, %float9.999990e-06_11411, %int1_11412 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %7444 = torch.aten.rsqrt %7443 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %7445 = torch.aten.mul.Tensor %7439, %7444 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_11413 = torch.constant.none
    %none_11414 = torch.constant.none
    %int6_11415 = torch.constant.int 6
    %cpu_11416 = torch.constant.device "cpu"
    %int0_11417 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7445, %none_11413, %none_11414, %int6_11415, %cpu_11416, %int0_11417 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11418 = torch.constant.int 5
    %7446 = torch.prims.convert_element_type %7445, %int5_11418 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %7447 = torch.aten.mul.Tensor %366, %7446 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_11419 = torch.constant.none
    %none_11420 = torch.constant.none
    %int6_11421 = torch.constant.int 6
    %cpu_11422 = torch.constant.device "cpu"
    %int0_11423 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7447, %none_11419, %none_11420, %int6_11421, %cpu_11422, %int0_11423 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11424 = torch.constant.int 5
    %7448 = torch.prims.convert_element_type %7447, %int5_11424 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_11425 = torch.constant.int 2
    %7449 = torch.aten.view.dtype %367, %int2_11425 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %7450 = torch.aten.detach %7449 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_11426 = torch.constant.int -1
    %int17_11427 = torch.constant.int 17
    %7451 = torch.prim.ListConstruct %int-1_11426, %int17_11427 : (!torch.int, !torch.int) -> !torch.list<int>
    %7452 = torch.aten.view %7450, %7451 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_11428 = torch.constant.int 2048
    %int-1_11429 = torch.constant.int -1
    %int17_11430 = torch.constant.int 17
    %7453 = torch.prim.ListConstruct %int2048_11428, %int-1_11429, %int17_11430 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7454 = torch.aten.view %7452, %7453 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_11431 = torch.constant.int 2
    %int0_11432 = torch.constant.int 0
    %int1_11433 = torch.constant.int 1
    %int1_11434 = torch.constant.int 1
    %7455 = torch.aten.slice.Tensor %7454, %int2_11431, %int0_11432, %int1_11433, %int1_11434 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_11435 = torch.constant.int 5
    %7456 = torch.aten.view.dtype %7455, %int5_11435 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %7457 = torch.aten.detach %7456 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_11436 = torch.constant.int 2
    %int1_11437 = torch.constant.int 1
    %int9223372036854775807_11438 = torch.constant.int 9223372036854775807
    %int1_11439 = torch.constant.int 1
    %7458 = torch.aten.slice.Tensor %7454, %int2_11436, %int1_11437, %int9223372036854775807_11438, %int1_11439 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_11440 = torch.constant.int 1
    %7459 = torch.aten.view.dtype %7458, %int1_11440 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %7460 = torch.aten.detach %7459 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %7461 = torch_c.to_builtin_tensor %7448 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_11441 = tensor.cast %7461 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %7462 = torch_c.to_builtin_tensor %7457 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %7463 = torch_c.to_builtin_tensor %7460 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %7464 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_11441, %7462, %7463) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_11442 = tensor.cast %7464 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %7465 = torch_c.from_builtin_tensor %cast_11442 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int2_11443 = torch.constant.int 2
    %7466 = torch.aten.view.dtype %368, %int2_11443 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %7467 = torch.aten.detach %7466 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_11444 = torch.constant.int -1
    %int17_11445 = torch.constant.int 17
    %7468 = torch.prim.ListConstruct %int-1_11444, %int17_11445 : (!torch.int, !torch.int) -> !torch.list<int>
    %7469 = torch.aten.view %7467, %7468 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_11446 = torch.constant.int 256
    %int-1_11447 = torch.constant.int -1
    %int17_11448 = torch.constant.int 17
    %7470 = torch.prim.ListConstruct %int256_11446, %int-1_11447, %int17_11448 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7471 = torch.aten.view %7469, %7470 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_11449 = torch.constant.int 2
    %int0_11450 = torch.constant.int 0
    %int1_11451 = torch.constant.int 1
    %int1_11452 = torch.constant.int 1
    %7472 = torch.aten.slice.Tensor %7471, %int2_11449, %int0_11450, %int1_11451, %int1_11452 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_11453 = torch.constant.int 5
    %7473 = torch.aten.view.dtype %7472, %int5_11453 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %7474 = torch.aten.detach %7473 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_11454 = torch.constant.int 2
    %int1_11455 = torch.constant.int 1
    %int9223372036854775807_11456 = torch.constant.int 9223372036854775807
    %int1_11457 = torch.constant.int 1
    %7475 = torch.aten.slice.Tensor %7471, %int2_11454, %int1_11455, %int9223372036854775807_11456, %int1_11457 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_11458 = torch.constant.int 1
    %7476 = torch.aten.view.dtype %7475, %int1_11458 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %7477 = torch.aten.detach %7476 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %7478 = torch_c.to_builtin_tensor %7448 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_11459 = tensor.cast %7478 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %7479 = torch_c.to_builtin_tensor %7474 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %7480 = torch_c.to_builtin_tensor %7477 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %7481 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_11459, %7479, %7480) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_11460 = tensor.cast %7481 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %7482 = torch_c.from_builtin_tensor %cast_11460 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int2_11461 = torch.constant.int 2
    %7483 = torch.aten.view.dtype %369, %int2_11461 : !torch.vtensor<[256,2176],ui8>, !torch.int -> !torch.vtensor<[256,1088],si16>
    %7484 = torch.aten.detach %7483 : !torch.vtensor<[256,1088],si16> -> !torch.vtensor<[256,1088],si16>
    %int-1_11462 = torch.constant.int -1
    %int17_11463 = torch.constant.int 17
    %7485 = torch.prim.ListConstruct %int-1_11462, %int17_11463 : (!torch.int, !torch.int) -> !torch.list<int>
    %7486 = torch.aten.view %7484, %7485 : !torch.vtensor<[256,1088],si16>, !torch.list<int> -> !torch.vtensor<[16384,17],si16>
    %int256_11464 = torch.constant.int 256
    %int-1_11465 = torch.constant.int -1
    %int17_11466 = torch.constant.int 17
    %7487 = torch.prim.ListConstruct %int256_11464, %int-1_11465, %int17_11466 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7488 = torch.aten.view %7486, %7487 : !torch.vtensor<[16384,17],si16>, !torch.list<int> -> !torch.vtensor<[256,64,17],si16>
    %int2_11467 = torch.constant.int 2
    %int0_11468 = torch.constant.int 0
    %int1_11469 = torch.constant.int 1
    %int1_11470 = torch.constant.int 1
    %7489 = torch.aten.slice.Tensor %7488, %int2_11467, %int0_11468, %int1_11469, %int1_11470 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,1],si16>
    %int5_11471 = torch.constant.int 5
    %7490 = torch.aten.view.dtype %7489, %int5_11471 : !torch.vtensor<[256,64,1],si16>, !torch.int -> !torch.vtensor<[256,64,1],f16>
    %7491 = torch.aten.detach %7490 : !torch.vtensor<[256,64,1],f16> -> !torch.vtensor<[256,64,1],f16>
    %int2_11472 = torch.constant.int 2
    %int1_11473 = torch.constant.int 1
    %int9223372036854775807_11474 = torch.constant.int 9223372036854775807
    %int1_11475 = torch.constant.int 1
    %7492 = torch.aten.slice.Tensor %7488, %int2_11472, %int1_11473, %int9223372036854775807_11474, %int1_11475 : !torch.vtensor<[256,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[256,64,16],si16>
    %int1_11476 = torch.constant.int 1
    %7493 = torch.aten.view.dtype %7492, %int1_11476 : !torch.vtensor<[256,64,16],si16>, !torch.int -> !torch.vtensor<[256,64,32],si8>
    %7494 = torch.aten.detach %7493 : !torch.vtensor<[256,64,32],si8> -> !torch.vtensor<[256,64,32],si8>
    %7495 = torch_c.to_builtin_tensor %7448 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_11477 = tensor.cast %7495 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %7496 = torch_c.to_builtin_tensor %7491 : !torch.vtensor<[256,64,1],f16> -> tensor<256x64x1xf16>
    %7497 = torch_c.to_builtin_tensor %7494 : !torch.vtensor<[256,64,32],si8> -> tensor<256x64x32xi8>
    %7498 = util.call @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%cast_11477, %7496, %7497) : (tensor<?x?x2048xf16>, tensor<256x64x1xf16>, tensor<256x64x32xi8>) -> tensor<?x?x256xf16>
    %cast_11478 = tensor.cast %7498 : tensor<?x?x256xf16> to tensor<4x1x256xf16>
    %7499 = torch_c.from_builtin_tensor %cast_11478 : tensor<4x1x256xf16> -> !torch.vtensor<[4,1,256],f16>
    %int4_11479 = torch.constant.int 4
    %int1_11480 = torch.constant.int 1
    %int32_11481 = torch.constant.int 32
    %int64_11482 = torch.constant.int 64
    %7500 = torch.prim.ListConstruct %int4_11479, %int1_11480, %int32_11481, %int64_11482 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7501 = torch.aten.view %7465, %7500 : !torch.vtensor<[4,1,2048],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %int4_11483 = torch.constant.int 4
    %int1_11484 = torch.constant.int 1
    %int4_11485 = torch.constant.int 4
    %int64_11486 = torch.constant.int 64
    %7502 = torch.prim.ListConstruct %int4_11483, %int1_11484, %int4_11485, %int64_11486 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7503 = torch.aten.view %7482, %7502 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int4_11487 = torch.constant.int 4
    %int1_11488 = torch.constant.int 1
    %int4_11489 = torch.constant.int 4
    %int64_11490 = torch.constant.int 64
    %7504 = torch.prim.ListConstruct %int4_11487, %int1_11488, %int4_11489, %int64_11490 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7505 = torch.aten.view %7499, %7504 : !torch.vtensor<[4,1,256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %int0_11491 = torch.constant.int 0
    %int1_11492 = torch.constant.int 1
    %none_11493 = torch.constant.none
    %none_11494 = torch.constant.none
    %cpu_11495 = torch.constant.device "cpu"
    %false_11496 = torch.constant.bool false
    %7506 = torch.aten.arange.start %int0_11491, %int1_11492, %none_11493, %none_11494, %cpu_11495, %false_11496 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_11497 = torch.constant.int 0
    %7507 = torch.aten.unsqueeze %7506, %int0_11497 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_11498 = torch.constant.int 1
    %7508 = torch.aten.unsqueeze %arg2, %int1_11498 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_11499 = torch.constant.int 1
    %7509 = torch.aten.add.Tensor %7507, %7508, %int1_11499 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_11500 = torch.constant.int 0
    %int64_11501 = torch.constant.int 64
    %int2_11502 = torch.constant.int 2
    %none_11503 = torch.constant.none
    %none_11504 = torch.constant.none
    %cpu_11505 = torch.constant.device "cpu"
    %false_11506 = torch.constant.bool false
    %7510 = torch.aten.arange.start_step %int0_11500, %int64_11501, %int2_11502, %none_11503, %none_11504, %cpu_11505, %false_11506 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_11507 = torch.constant.none
    %none_11508 = torch.constant.none
    %int4_11509 = torch.constant.int 4
    %cpu_11510 = torch.constant.device "cpu"
    %int0_11511 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7510, %none_11507, %none_11508, %int4_11509, %cpu_11510, %int0_11511 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_11512 = torch.constant.int 6
    %7511 = torch.prims.convert_element_type %7510, %int6_11512 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_11513 = torch.constant.int 64
    %7512 = torch.aten.div.Scalar %7511, %int64_11513 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_11514 = torch.constant.float 1.000000e+04
    %7513 = torch.aten.pow.Scalar %float1.000000e04_11514, %7512 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %7514 = torch.aten.reciprocal %7513 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_11515 = torch.constant.float 1.000000e+00
    %7515 = torch.aten.mul.Scalar %7514, %float1.000000e00_11515 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_11516 = torch.constant.none
    %7516 = torch.aten.clone %358, %none_11516 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_11517 = torch.constant.int 0
    %7517 = torch.aten.unsqueeze %7515, %int0_11517 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_11518 = torch.constant.int 2
    %7518 = torch.aten.unsqueeze %7517, %int2_11518 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_11519 = torch.constant.none
    %none_11520 = torch.constant.none
    %int6_11521 = torch.constant.int 6
    %cpu_11522 = torch.constant.device "cpu"
    %int0_11523 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7518, %none_11519, %none_11520, %int6_11521, %cpu_11522, %int0_11523 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_11524 = torch.constant.int 4
    %int-1_11525 = torch.constant.int -1
    %int1_11526 = torch.constant.int 1
    %7519 = torch.prim.ListConstruct %int4_11524, %int-1_11525, %int1_11526 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_11527 = torch.constant.bool false
    %7520 = torch.aten.expand %7518, %7519, %false_11527 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_11528 = torch.constant.int 1
    %7521 = torch.aten.unsqueeze %7509, %int1_11528 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_11529 = torch.constant.none
    %none_11530 = torch.constant.none
    %int4_11531 = torch.constant.int 4
    %cpu_11532 = torch.constant.device "cpu"
    %int0_11533 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7521, %none_11529, %none_11530, %int4_11531, %cpu_11532, %int0_11533 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_11534 = torch.constant.int 6
    %7522 = torch.prims.convert_element_type %7521, %int6_11534 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %7523 = torch.aten.matmul %7520, %7522 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_11535 = torch.constant.int 1
    %int2_11536 = torch.constant.int 2
    %7524 = torch.aten.transpose.int %7523, %int1_11535, %int2_11536 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %7525 = torch.aten.cos %7524 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %7526 = torch.aten.mul.Tensor %7525, %7516 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_11537 = torch.constant.none
    %none_11538 = torch.constant.none
    %int6_11539 = torch.constant.int 6
    %cpu_11540 = torch.constant.device "cpu"
    %int0_11541 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7526, %none_11537, %none_11538, %int6_11539, %cpu_11540, %int0_11541 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11542 = torch.constant.int 5
    %7527 = torch.prims.convert_element_type %7526, %int5_11542 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %7528 = torch.aten.sin %7524 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %7529 = torch.aten.mul.Tensor %7528, %7516 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_11543 = torch.constant.none
    %none_11544 = torch.constant.none
    %int6_11545 = torch.constant.int 6
    %cpu_11546 = torch.constant.device "cpu"
    %int0_11547 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7529, %none_11543, %none_11544, %int6_11545, %cpu_11546, %int0_11547 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11548 = torch.constant.int 5
    %7530 = torch.prims.convert_element_type %7529, %int5_11548 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_11549 = torch.constant.int 2
    %7531 = torch.aten.unsqueeze %7527, %int2_11549 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_11550 = torch.constant.int 2
    %7532 = torch.aten.unsqueeze %7530, %int2_11550 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_11551 = torch.constant.none
    %none_11552 = torch.constant.none
    %int5_11553 = torch.constant.int 5
    %cpu_11554 = torch.constant.device "cpu"
    %int0_11555 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7531, %none_11551, %none_11552, %int5_11553, %cpu_11554, %int0_11555 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_11556 = torch.constant.none
    %none_11557 = torch.constant.none
    %int5_11558 = torch.constant.int 5
    %cpu_11559 = torch.constant.device "cpu"
    %int0_11560 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7532, %none_11556, %none_11557, %int5_11558, %cpu_11559, %int0_11560 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_11561 = torch.constant.none
    %none_11562 = torch.constant.none
    %int5_11563 = torch.constant.int 5
    %cpu_11564 = torch.constant.device "cpu"
    %int0_11565 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7501, %none_11561, %none_11562, %int5_11563, %cpu_11564, %int0_11565 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_11566 = torch.constant.int 3
    %int0_11567 = torch.constant.int 0
    %int64_11568 = torch.constant.int 64
    %int2_11569 = torch.constant.int 2
    %7533 = torch.aten.slice.Tensor %7501, %int3_11566, %int0_11567, %int64_11568, %int2_11569 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %int3_11570 = torch.constant.int 3
    %int1_11571 = torch.constant.int 1
    %int64_11572 = torch.constant.int 64
    %int2_11573 = torch.constant.int 2
    %7534 = torch.aten.slice.Tensor %7501, %int3_11570, %int1_11571, %int64_11572, %int2_11573 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %7535 = torch.aten.mul.Tensor %7533, %7531 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %7536 = torch.aten.mul.Tensor %7534, %7532 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_11574 = torch.constant.int 1
    %7537 = torch.aten.sub.Tensor %7535, %7536, %int1_11574 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %7538 = torch.aten.mul.Tensor %7534, %7531 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %7539 = torch.aten.mul.Tensor %7533, %7532 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,32,32],f16>
    %int1_11575 = torch.constant.int 1
    %7540 = torch.aten.add.Tensor %7538, %7539, %int1_11575 : !torch.vtensor<[4,1,32,32],f16>, !torch.vtensor<[4,1,32,32],f16>, !torch.int -> !torch.vtensor<[4,1,32,32],f16>
    %7541 = torch_c.to_builtin_tensor %7537 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_11576 = tensor.cast %7541 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %7542 = torch_c.to_builtin_tensor %7540 : !torch.vtensor<[4,1,32,32],f16> -> tensor<4x1x32x32xf16>
    %cast_11577 = tensor.cast %7542 : tensor<4x1x32x32xf16> to tensor<?x?x?x?xf16>
    %7543 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_11576, %cast_11577) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_11578 = tensor.cast %7543 : tensor<?x?x?x2x?xf16> to tensor<4x1x32x2x32xf16>
    %7544 = torch_c.from_builtin_tensor %cast_11578 : tensor<4x1x32x2x32xf16> -> !torch.vtensor<[4,1,32,2,32],f16>
    %int4_11579 = torch.constant.int 4
    %int1_11580 = torch.constant.int 1
    %int32_11581 = torch.constant.int 32
    %int64_11582 = torch.constant.int 64
    %7545 = torch.prim.ListConstruct %int4_11579, %int1_11580, %int32_11581, %int64_11582 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7546 = torch.aten.view %7544, %7545 : !torch.vtensor<[4,1,32,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,64],f16>
    %none_11583 = torch.constant.none
    %none_11584 = torch.constant.none
    %int5_11585 = torch.constant.int 5
    %cpu_11586 = torch.constant.device "cpu"
    %int0_11587 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7546, %none_11583, %none_11584, %int5_11585, %cpu_11586, %int0_11587 : !torch.vtensor<[4,1,32,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int0_11588 = torch.constant.int 0
    %int1_11589 = torch.constant.int 1
    %none_11590 = torch.constant.none
    %none_11591 = torch.constant.none
    %cpu_11592 = torch.constant.device "cpu"
    %false_11593 = torch.constant.bool false
    %7547 = torch.aten.arange.start %int0_11588, %int1_11589, %none_11590, %none_11591, %cpu_11592, %false_11593 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_11594 = torch.constant.int 0
    %7548 = torch.aten.unsqueeze %7547, %int0_11594 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_11595 = torch.constant.int 1
    %7549 = torch.aten.unsqueeze %arg2, %int1_11595 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_11596 = torch.constant.int 1
    %7550 = torch.aten.add.Tensor %7548, %7549, %int1_11596 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int0_11597 = torch.constant.int 0
    %int64_11598 = torch.constant.int 64
    %int2_11599 = torch.constant.int 2
    %none_11600 = torch.constant.none
    %none_11601 = torch.constant.none
    %cpu_11602 = torch.constant.device "cpu"
    %false_11603 = torch.constant.bool false
    %7551 = torch.aten.arange.start_step %int0_11597, %int64_11598, %int2_11599, %none_11600, %none_11601, %cpu_11602, %false_11603 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[32],si64>
    %none_11604 = torch.constant.none
    %none_11605 = torch.constant.none
    %int4_11606 = torch.constant.int 4
    %cpu_11607 = torch.constant.device "cpu"
    %int0_11608 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7551, %none_11604, %none_11605, %int4_11606, %cpu_11607, %int0_11608 : !torch.vtensor<[32],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_11609 = torch.constant.int 6
    %7552 = torch.prims.convert_element_type %7551, %int6_11609 : !torch.vtensor<[32],si64>, !torch.int -> !torch.vtensor<[32],f32>
    %int64_11610 = torch.constant.int 64
    %7553 = torch.aten.div.Scalar %7552, %int64_11610 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[32],f32>
    %float1.000000e04_11611 = torch.constant.float 1.000000e+04
    %7554 = torch.aten.pow.Scalar %float1.000000e04_11611, %7553 : !torch.float, !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %7555 = torch.aten.reciprocal %7554 : !torch.vtensor<[32],f32> -> !torch.vtensor<[32],f32>
    %float1.000000e00_11612 = torch.constant.float 1.000000e+00
    %7556 = torch.aten.mul.Scalar %7555, %float1.000000e00_11612 : !torch.vtensor<[32],f32>, !torch.float -> !torch.vtensor<[32],f32>
    %none_11613 = torch.constant.none
    %7557 = torch.aten.clone %359, %none_11613 : !torch.vtensor<[],f32>, !torch.none -> !torch.vtensor<[],f32>
    %int0_11614 = torch.constant.int 0
    %7558 = torch.aten.unsqueeze %7556, %int0_11614 : !torch.vtensor<[32],f32>, !torch.int -> !torch.vtensor<[1,32],f32>
    %int2_11615 = torch.constant.int 2
    %7559 = torch.aten.unsqueeze %7558, %int2_11615 : !torch.vtensor<[1,32],f32>, !torch.int -> !torch.vtensor<[1,32,1],f32>
    %none_11616 = torch.constant.none
    %none_11617 = torch.constant.none
    %int6_11618 = torch.constant.int 6
    %cpu_11619 = torch.constant.device "cpu"
    %int0_11620 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7559, %none_11616, %none_11617, %int6_11618, %cpu_11619, %int0_11620 : !torch.vtensor<[1,32,1],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int4_11621 = torch.constant.int 4
    %int-1_11622 = torch.constant.int -1
    %int1_11623 = torch.constant.int 1
    %7560 = torch.prim.ListConstruct %int4_11621, %int-1_11622, %int1_11623 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_11624 = torch.constant.bool false
    %7561 = torch.aten.expand %7559, %7560, %false_11624 : !torch.vtensor<[1,32,1],f32>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,32,1],f32>
    %int1_11625 = torch.constant.int 1
    %7562 = torch.aten.unsqueeze %7550, %int1_11625 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %none_11626 = torch.constant.none
    %none_11627 = torch.constant.none
    %int4_11628 = torch.constant.int 4
    %cpu_11629 = torch.constant.device "cpu"
    %int0_11630 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7562, %none_11626, %none_11627, %int4_11628, %cpu_11629, %int0_11630 : !torch.vtensor<[4,1,1],si64>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_11631 = torch.constant.int 6
    %7563 = torch.prims.convert_element_type %7562, %int6_11631 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %7564 = torch.aten.matmul %7561, %7563 : !torch.vtensor<[4,32,1],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,32,1],f32>
    %int1_11632 = torch.constant.int 1
    %int2_11633 = torch.constant.int 2
    %7565 = torch.aten.transpose.int %7564, %int1_11632, %int2_11633 : !torch.vtensor<[4,32,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32],f32>
    %7566 = torch.aten.cos %7565 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %7567 = torch.aten.mul.Tensor %7566, %7557 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_11634 = torch.constant.none
    %none_11635 = torch.constant.none
    %int6_11636 = torch.constant.int 6
    %cpu_11637 = torch.constant.device "cpu"
    %int0_11638 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7567, %none_11634, %none_11635, %int6_11636, %cpu_11637, %int0_11638 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11639 = torch.constant.int 5
    %7568 = torch.prims.convert_element_type %7567, %int5_11639 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %7569 = torch.aten.sin %7565 : !torch.vtensor<[4,1,32],f32> -> !torch.vtensor<[4,1,32],f32>
    %7570 = torch.aten.mul.Tensor %7569, %7557 : !torch.vtensor<[4,1,32],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,1,32],f32>
    %none_11640 = torch.constant.none
    %none_11641 = torch.constant.none
    %int6_11642 = torch.constant.int 6
    %cpu_11643 = torch.constant.device "cpu"
    %int0_11644 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7570, %none_11640, %none_11641, %int6_11642, %cpu_11643, %int0_11644 : !torch.vtensor<[4,1,32],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11645 = torch.constant.int 5
    %7571 = torch.prims.convert_element_type %7570, %int5_11645 : !torch.vtensor<[4,1,32],f32>, !torch.int -> !torch.vtensor<[4,1,32],f16>
    %int2_11646 = torch.constant.int 2
    %7572 = torch.aten.unsqueeze %7568, %int2_11646 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %int2_11647 = torch.constant.int 2
    %7573 = torch.aten.unsqueeze %7571, %int2_11647 : !torch.vtensor<[4,1,32],f16>, !torch.int -> !torch.vtensor<[4,1,1,32],f16>
    %none_11648 = torch.constant.none
    %none_11649 = torch.constant.none
    %int5_11650 = torch.constant.int 5
    %cpu_11651 = torch.constant.device "cpu"
    %int0_11652 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7572, %none_11648, %none_11649, %int5_11650, %cpu_11651, %int0_11652 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_11653 = torch.constant.none
    %none_11654 = torch.constant.none
    %int5_11655 = torch.constant.int 5
    %cpu_11656 = torch.constant.device "cpu"
    %int0_11657 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7573, %none_11653, %none_11654, %int5_11655, %cpu_11656, %int0_11657 : !torch.vtensor<[4,1,1,32],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_11658 = torch.constant.none
    %none_11659 = torch.constant.none
    %int5_11660 = torch.constant.int 5
    %cpu_11661 = torch.constant.device "cpu"
    %int0_11662 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7503, %none_11658, %none_11659, %int5_11660, %cpu_11661, %int0_11662 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int3_11663 = torch.constant.int 3
    %int0_11664 = torch.constant.int 0
    %int64_11665 = torch.constant.int 64
    %int2_11666 = torch.constant.int 2
    %7574 = torch.aten.slice.Tensor %7503, %int3_11663, %int0_11664, %int64_11665, %int2_11666 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %int3_11667 = torch.constant.int 3
    %int1_11668 = torch.constant.int 1
    %int64_11669 = torch.constant.int 64
    %int2_11670 = torch.constant.int 2
    %7575 = torch.aten.slice.Tensor %7503, %int3_11667, %int1_11668, %int64_11669, %int2_11670 : !torch.vtensor<[4,1,4,64],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %7576 = torch.aten.mul.Tensor %7574, %7572 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %7577 = torch.aten.mul.Tensor %7575, %7573 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_11671 = torch.constant.int 1
    %7578 = torch.aten.sub.Tensor %7576, %7577, %int1_11671 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %7579 = torch.aten.mul.Tensor %7575, %7572 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %7580 = torch.aten.mul.Tensor %7574, %7573 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,1,32],f16> -> !torch.vtensor<[4,1,4,32],f16>
    %int1_11672 = torch.constant.int 1
    %7581 = torch.aten.add.Tensor %7579, %7580, %int1_11672 : !torch.vtensor<[4,1,4,32],f16>, !torch.vtensor<[4,1,4,32],f16>, !torch.int -> !torch.vtensor<[4,1,4,32],f16>
    %7582 = torch_c.to_builtin_tensor %7578 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_11673 = tensor.cast %7582 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %7583 = torch_c.to_builtin_tensor %7581 : !torch.vtensor<[4,1,4,32],f16> -> tensor<4x1x4x32xf16>
    %cast_11674 = tensor.cast %7583 : tensor<4x1x4x32xf16> to tensor<?x?x?x?xf16>
    %7584 = util.call @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%cast_11673, %cast_11674) : (tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16>
    %cast_11675 = tensor.cast %7584 : tensor<?x?x?x2x?xf16> to tensor<4x1x4x2x32xf16>
    %7585 = torch_c.from_builtin_tensor %cast_11675 : tensor<4x1x4x2x32xf16> -> !torch.vtensor<[4,1,4,2,32],f16>
    %int4_11676 = torch.constant.int 4
    %int1_11677 = torch.constant.int 1
    %int4_11678 = torch.constant.int 4
    %int64_11679 = torch.constant.int 64
    %7586 = torch.prim.ListConstruct %int4_11676, %int1_11677, %int4_11678, %int64_11679 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7587 = torch.aten.view %7585, %7586 : !torch.vtensor<[4,1,4,2,32],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4,64],f16>
    %none_11680 = torch.constant.none
    %none_11681 = torch.constant.none
    %int5_11682 = torch.constant.int 5
    %cpu_11683 = torch.constant.device "cpu"
    %int0_11684 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7587, %none_11680, %none_11681, %int5_11682, %cpu_11683, %int0_11684 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int32_11685 = torch.constant.int 32
    %7588 = torch.aten.floor_divide.Scalar %arg2, %int32_11685 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_11686 = torch.constant.int 1
    %7589 = torch.aten.unsqueeze %7588, %int1_11686 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_11687 = torch.constant.int 1
    %false_11688 = torch.constant.bool false
    %7590 = torch.aten.gather %arg3, %int1_11687, %7589, %false_11688 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_11689 = torch.constant.int 4
    %int1_11690 = torch.constant.int 1
    %int1_11691 = torch.constant.int 1
    %7591 = torch.prim.ListConstruct %int4_11689, %int1_11690, %int1_11691 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7592 = torch.aten.view %7590, %7591 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int32_11692 = torch.constant.int 32
    %7593 = torch.aten.remainder.Scalar %arg2, %int32_11692 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int4_11693 = torch.constant.int 4
    %int1_11694 = torch.constant.int 1
    %int1_11695 = torch.constant.int 1
    %7594 = torch.prim.ListConstruct %int4_11693, %int1_11694, %int1_11695 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7595 = torch.aten.view %7593, %7594 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[4,1,1],si64>
    %int4_11696 = torch.constant.int 4
    %none_11697 = torch.constant.none
    %none_11698 = torch.constant.none
    %cpu_11699 = torch.constant.device "cpu"
    %false_11700 = torch.constant.bool false
    %7596 = torch.aten.arange %int4_11696, %none_11697, %none_11698, %cpu_11699, %false_11700 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[4],si64>
    %int1_11701 = torch.constant.int 1
    %int1_11702 = torch.constant.int 1
    %int4_11703 = torch.constant.int 4
    %7597 = torch.prim.ListConstruct %int1_11701, %int1_11702, %int4_11703 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7598 = torch.aten.view %7596, %7597 : !torch.vtensor<[4],si64>, !torch.list<int> -> !torch.vtensor<[1,1,4],si64>
    %none_11704 = torch.constant.none
    %7599 = torch.aten.clone %360, %none_11704 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_11705 = torch.constant.int 1
    %int1_11706 = torch.constant.int 1
    %int1_11707 = torch.constant.int 1
    %7600 = torch.prim.ListConstruct %int1_11705, %int1_11706, %int1_11707 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7601 = torch.aten.view %7599, %7600 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_11708 = torch.constant.int 22
    %7602 = torch.aten.mul.Scalar %7592, %int22_11708 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int21 = torch.constant.int 21
    %int1_11709 = torch.constant.int 1
    %7603 = torch.aten.add.Scalar %7602, %int21, %int1_11709 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_11710 = torch.constant.int 2
    %7604 = torch.aten.mul.Scalar %7603, %int2_11710 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_11711 = torch.constant.int 1
    %7605 = torch.aten.add.Tensor %7604, %7601, %int1_11711 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_11712 = torch.constant.int 4
    %7606 = torch.aten.mul.Scalar %7605, %int4_11712 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_11713 = torch.constant.int 1
    %7607 = torch.aten.add.Tensor %7606, %7598, %int1_11713 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_11714 = torch.constant.int 32
    %7608 = torch.aten.mul.Scalar %7607, %int32_11714 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_11715 = torch.constant.int 1
    %7609 = torch.aten.add.Tensor %7608, %7595, %int1_11715 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_11716 = torch.constant.none
    %none_11717 = torch.constant.none
    %int5_11718 = torch.constant.int 5
    %cpu_11719 = torch.constant.device "cpu"
    %int0_11720 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7587, %none_11716, %none_11717, %int5_11718, %cpu_11719, %int0_11720 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int22_11721 = torch.constant.int 22
    %int2_11722 = torch.constant.int 2
    %int4_11723 = torch.constant.int 4
    %int32_11724 = torch.constant.int 32
    %int64_11725 = torch.constant.int 64
    %7610 = torch.prim.ListConstruct %381, %int22_11721, %int2_11722, %int4_11723, %int32_11724, %int64_11725 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7611 = torch.aten.view %7305, %7610 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %7611, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_11726 = torch.constant.int 64
    %7612 = torch.prim.ListConstruct %553, %int64_11726 : (!torch.int, !torch.int) -> !torch.list<int>
    %7613 = torch.aten.view %7611, %7612 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %7613, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %7614 = torch.prim.ListConstruct %7609 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_11727 = torch.constant.bool false
    %7615 = torch.aten.index_put %7613, %7614, %7587, %false_11727 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %7615, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_11728 = torch.constant.int 22
    %int2_11729 = torch.constant.int 2
    %int4_11730 = torch.constant.int 4
    %int32_11731 = torch.constant.int 32
    %int64_11732 = torch.constant.int 64
    %7616 = torch.prim.ListConstruct %381, %int22_11728, %int2_11729, %int4_11730, %int32_11731, %int64_11732 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7617 = torch.aten.view %7615, %7616 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %7617, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_11733 = torch.constant.int 360448
    %7618 = torch.prim.ListConstruct %381, %int360448_11733 : (!torch.int, !torch.int) -> !torch.list<int>
    %7619 = torch.aten.view %7617, %7618 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.bind_symbolic_shape %7619, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %int22_11734 = torch.constant.int 22
    %int2_11735 = torch.constant.int 2
    %int4_11736 = torch.constant.int 4
    %int32_11737 = torch.constant.int 32
    %int64_11738 = torch.constant.int 64
    %7620 = torch.prim.ListConstruct %381, %int22_11734, %int2_11735, %int4_11736, %int32_11737, %int64_11738 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7621 = torch.aten.view %7619, %7620 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %7621, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int64_11739 = torch.constant.int 64
    %7622 = torch.prim.ListConstruct %553, %int64_11739 : (!torch.int, !torch.int) -> !torch.list<int>
    %7623 = torch.aten.view %7621, %7622 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %7623, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %none_11740 = torch.constant.none
    %7624 = torch.aten.clone %361, %none_11740 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int1_11741 = torch.constant.int 1
    %int1_11742 = torch.constant.int 1
    %int1_11743 = torch.constant.int 1
    %7625 = torch.prim.ListConstruct %int1_11741, %int1_11742, %int1_11743 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7626 = torch.aten.view %7624, %7625 : !torch.vtensor<[],si64>, !torch.list<int> -> !torch.vtensor<[1,1,1],si64>
    %int22_11744 = torch.constant.int 22
    %7627 = torch.aten.mul.Scalar %7592, %int22_11744 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int21_11745 = torch.constant.int 21
    %int1_11746 = torch.constant.int 1
    %7628 = torch.aten.add.Scalar %7627, %int21_11745, %int1_11746 : !torch.vtensor<[4,1,1],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int2_11747 = torch.constant.int 2
    %7629 = torch.aten.mul.Scalar %7628, %int2_11747 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_11748 = torch.constant.int 1
    %7630 = torch.aten.add.Tensor %7629, %7626, %int1_11748 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int4_11749 = torch.constant.int 4
    %7631 = torch.aten.mul.Scalar %7630, %int4_11749 : !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,1],si64>
    %int1_11750 = torch.constant.int 1
    %7632 = torch.aten.add.Tensor %7631, %7598, %int1_11750 : !torch.vtensor<[4,1,1],si64>, !torch.vtensor<[1,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int32_11751 = torch.constant.int 32
    %7633 = torch.aten.mul.Scalar %7632, %int32_11751 : !torch.vtensor<[4,1,4],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %int1_11752 = torch.constant.int 1
    %7634 = torch.aten.add.Tensor %7633, %7595, %int1_11752 : !torch.vtensor<[4,1,4],si64>, !torch.vtensor<[4,1,1],si64>, !torch.int -> !torch.vtensor<[4,1,4],si64>
    %none_11753 = torch.constant.none
    %none_11754 = torch.constant.none
    %int5_11755 = torch.constant.int 5
    %cpu_11756 = torch.constant.device "cpu"
    %int0_11757 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7505, %none_11753, %none_11754, %int5_11755, %cpu_11756, %int0_11757 : !torch.vtensor<[4,1,4,64],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %7635 = torch.prim.ListConstruct %7634 : (!torch.vtensor<[4,1,4],si64>) -> !torch.list<optional<vtensor>>
    %false_11758 = torch.constant.bool false
    %7636 = torch.aten.index_put %7623, %7635, %7505, %false_11758 : !torch.vtensor<[?,64],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,4,64],f16>, !torch.bool -> !torch.vtensor<[?,64],f16>
    torch.bind_symbolic_shape %7636, [%379], affine_map<()[s0] -> (s0 * 5632, 64)> : !torch.vtensor<[?,64],f16>
    %int22_11759 = torch.constant.int 22
    %int2_11760 = torch.constant.int 2
    %int4_11761 = torch.constant.int 4
    %int32_11762 = torch.constant.int 32
    %int64_11763 = torch.constant.int 64
    %7637 = torch.prim.ListConstruct %381, %int22_11759, %int2_11760, %int4_11761, %int32_11762, %int64_11763 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7638 = torch.aten.view %7636, %7637 : !torch.vtensor<[?,64],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %7638, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %int360448_11764 = torch.constant.int 360448
    %7639 = torch.prim.ListConstruct %381, %int360448_11764 : (!torch.int, !torch.int) -> !torch.list<int>
    %7640 = torch.aten.view %7638, %7639 : !torch.vtensor<[?,22,2,4,32,64],f16>, !torch.list<int> -> !torch.vtensor<[?,360448],f16>
    torch.overwrite.tensor.contents %7640 overwrites %arg4 : !torch.vtensor<[?,360448],f16>, !torch.tensor<[?,360448],f16>
    torch.bind_symbolic_shape %7640, [%379], affine_map<()[s0] -> (s0, 360448)> : !torch.vtensor<[?,360448],f16>
    %none_11765 = torch.constant.none
    %7641 = torch.aten.clone %362, %none_11765 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_11766 = torch.constant.none
    %7642 = torch.aten.clone %363, %none_11766 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %none_11767 = torch.constant.none
    %7643 = torch.aten.clone %364, %none_11767 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int22_11768 = torch.constant.int 22
    %int2_11769 = torch.constant.int 2
    %int4_11770 = torch.constant.int 4
    %int32_11771 = torch.constant.int 32
    %int64_11772 = torch.constant.int 64
    %7644 = torch.prim.ListConstruct %381, %int22_11768, %int2_11769, %int4_11770, %int32_11771, %int64_11772 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7645 = torch.aten.view %7640, %7644 : !torch.vtensor<[?,360448],f16>, !torch.list<int> -> !torch.vtensor<[?,22,2,4,32,64],f16>
    torch.bind_symbolic_shape %7645, [%379], affine_map<()[s0] -> (s0, 22, 2, 4, 32, 64)> : !torch.vtensor<[?,22,2,4,32,64],f16>
    %7646 = torch_c.to_builtin_tensor %7645 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %7647 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_11773 = tensor.cast %7647 : tensor<4x?xi64> to tensor<?x?xi64>
    %7648 = torch_c.to_builtin_tensor %7641 : !torch.vtensor<[],si64> -> tensor<i64>
    %7649 = torch_c.to_builtin_tensor %7642 : !torch.vtensor<[],si64> -> tensor<i64>
    %7650 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%7646, %cast_11773, %7648, %7649) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_11774 = tensor.cast %7650 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %7651 = torch_c.from_builtin_tensor %cast_11774 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %7651, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %7652 = torch_c.to_builtin_tensor %7645 : !torch.vtensor<[?,22,2,4,32,64],f16> -> tensor<?x22x2x4x32x64xf16>
    %7653 = torch_c.to_builtin_tensor %arg3 : !torch.vtensor<[4,?],si64> -> tensor<4x?xi64>
    %cast_11775 = tensor.cast %7653 : tensor<4x?xi64> to tensor<?x?xi64>
    %7654 = torch_c.to_builtin_tensor %7641 : !torch.vtensor<[],si64> -> tensor<i64>
    %7655 = torch_c.to_builtin_tensor %7643 : !torch.vtensor<[],si64> -> tensor<i64>
    %7656 = util.call @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%7652, %cast_11775, %7654, %7655) : (tensor<?x22x2x4x32x64xf16>, tensor<?x?xi64>, tensor<i64>, tensor<i64>) -> tensor<?x?x4x32x64xf16>
    %cast_11776 = tensor.cast %7656 : tensor<?x?x4x32x64xf16> to tensor<4x?x4x32x64xf16>
    %7657 = torch_c.from_builtin_tensor %cast_11776 : tensor<4x?x4x32x64xf16> -> !torch.vtensor<[4,?,4,32,64],f16>
    torch.bind_symbolic_shape %7657, [%378], affine_map<()[s0] -> (4, s0, 4, 32, 64)> : !torch.vtensor<[4,?,4,32,64],f16>
    %int2_11777 = torch.constant.int 2
    %int3_11778 = torch.constant.int 3
    %7658 = torch.aten.transpose.int %7651, %int2_11777, %int3_11778 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %7658, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_11779 = torch.constant.int 0
    %7659 = torch.aten.clone %7658, %int0_11779 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %7659, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_11780 = torch.constant.int 4
    %int4_11781 = torch.constant.int 4
    %int64_11782 = torch.constant.int 64
    %7660 = torch.prim.ListConstruct %int4_11780, %623, %int4_11781, %int64_11782 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7661 = torch.aten._unsafe_view %7659, %7660 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %7661, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int2_11783 = torch.constant.int 2
    %int3_11784 = torch.constant.int 3
    %7662 = torch.aten.transpose.int %7657, %int2_11783, %int3_11784 : !torch.vtensor<[4,?,4,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %7662, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int0_11785 = torch.constant.int 0
    %7663 = torch.aten.clone %7662, %int0_11785 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,32,4,64],f16>
    torch.bind_symbolic_shape %7663, [%378], affine_map<()[s0] -> (4, s0, 32, 4, 64)> : !torch.vtensor<[4,?,32,4,64],f16>
    %int4_11786 = torch.constant.int 4
    %int4_11787 = torch.constant.int 4
    %int64_11788 = torch.constant.int 64
    %7664 = torch.prim.ListConstruct %int4_11786, %623, %int4_11787, %int64_11788 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7665 = torch.aten._unsafe_view %7663, %7664 : !torch.vtensor<[4,?,32,4,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4,64],f16>
    torch.bind_symbolic_shape %7665, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 64)> : !torch.vtensor<[4,?,4,64],f16>
    %int0_11789 = torch.constant.int 0
    %int1_11790 = torch.constant.int 1
    %none_11791 = torch.constant.none
    %none_11792 = torch.constant.none
    %cpu_11793 = torch.constant.device "cpu"
    %false_11794 = torch.constant.bool false
    %7666 = torch.aten.arange.start_step %int0_11789, %623, %int1_11790, %none_11791, %none_11792, %cpu_11793, %false_11794 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %7666, [%378], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1_11795 = torch.constant.int -1
    %7667 = torch.aten.unsqueeze %arg1, %int-1_11795 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %7668 = torch.aten.ge.Tensor %7666, %7667 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %7668, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %none_11796 = torch.constant.none
    %7669 = torch.aten.clone %365, %none_11796 : !torch.vtensor<[],f16>, !torch.none -> !torch.vtensor<[],f16>
    %int0_11797 = torch.constant.int 0
    %7670 = torch.aten.where.ScalarOther %7668, %7669, %int0_11797 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f16>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %7670, [%378], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %none_11798 = torch.constant.none
    %none_11799 = torch.constant.none
    %int5_11800 = torch.constant.int 5
    %cpu_11801 = torch.constant.device "cpu"
    %int0_11802 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7670, %none_11798, %none_11799, %int5_11800, %cpu_11801, %int0_11802 : !torch.vtensor<[4,?],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_11803 = torch.constant.int 1
    %7671 = torch.aten.unsqueeze %7670, %int1_11803 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %7671, [%378], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_11804 = torch.constant.int 1
    %7672 = torch.aten.unsqueeze %7671, %int1_11804 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %7672, [%378], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int-2_11805 = torch.constant.int -2
    %7673 = torch.aten.unsqueeze %7661, %int-2_11805 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %7673, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_11806 = torch.constant.int 4
    %int4_11807 = torch.constant.int 4
    %int8_11808 = torch.constant.int 8
    %int64_11809 = torch.constant.int 64
    %7674 = torch.prim.ListConstruct %int4_11806, %623, %int4_11807, %int8_11808, %int64_11809 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_11810 = torch.constant.bool false
    %7675 = torch.aten.expand %7673, %7674, %false_11810 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %7675, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_11811 = torch.constant.int 0
    %7676 = torch.aten.clone %7675, %int0_11811 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %7676, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_11812 = torch.constant.int 4
    %int32_11813 = torch.constant.int 32
    %int64_11814 = torch.constant.int 64
    %7677 = torch.prim.ListConstruct %int4_11812, %623, %int32_11813, %int64_11814 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7678 = torch.aten._unsafe_view %7676, %7677 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %7678, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int-2_11815 = torch.constant.int -2
    %7679 = torch.aten.unsqueeze %7665, %int-2_11815 : !torch.vtensor<[4,?,4,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,1,64],f16>
    torch.bind_symbolic_shape %7679, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 1, 64)> : !torch.vtensor<[4,?,4,1,64],f16>
    %int4_11816 = torch.constant.int 4
    %int4_11817 = torch.constant.int 4
    %int8_11818 = torch.constant.int 8
    %int64_11819 = torch.constant.int 64
    %7680 = torch.prim.ListConstruct %int4_11816, %623, %int4_11817, %int8_11818, %int64_11819 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_11820 = torch.constant.bool false
    %7681 = torch.aten.expand %7679, %7680, %false_11820 : !torch.vtensor<[4,?,4,1,64],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %7681, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int0_11821 = torch.constant.int 0
    %7682 = torch.aten.clone %7681, %int0_11821 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.int -> !torch.vtensor<[4,?,4,8,64],f16>
    torch.bind_symbolic_shape %7682, [%378], affine_map<()[s0] -> (4, s0 * 32, 4, 8, 64)> : !torch.vtensor<[4,?,4,8,64],f16>
    %int4_11822 = torch.constant.int 4
    %int32_11823 = torch.constant.int 32
    %int64_11824 = torch.constant.int 64
    %7683 = torch.prim.ListConstruct %int4_11822, %623, %int32_11823, %int64_11824 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7684 = torch.aten._unsafe_view %7682, %7683 : !torch.vtensor<[4,?,4,8,64],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,64],f16>
    torch.bind_symbolic_shape %7684, [%378], affine_map<()[s0] -> (4, s0 * 32, 32, 64)> : !torch.vtensor<[4,?,32,64],f16>
    %int1_11825 = torch.constant.int 1
    %int2_11826 = torch.constant.int 2
    %7685 = torch.aten.transpose.int %7546, %int1_11825, %int2_11826 : !torch.vtensor<[4,1,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,64],f16>
    %int1_11827 = torch.constant.int 1
    %int2_11828 = torch.constant.int 2
    %7686 = torch.aten.transpose.int %7678, %int1_11827, %int2_11828 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %7686, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %int1_11829 = torch.constant.int 1
    %int2_11830 = torch.constant.int 2
    %7687 = torch.aten.transpose.int %7684, %int1_11829, %int2_11830 : !torch.vtensor<[4,?,32,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,64],f16>
    torch.bind_symbolic_shape %7687, [%378], affine_map<()[s0] -> (4, 32, s0 * 32, 64)> : !torch.vtensor<[4,32,?,64],f16>
    %float0.000000e00_11831 = torch.constant.float 0.000000e+00
    %false_11832 = torch.constant.bool false
    %none_11833 = torch.constant.none
    %false_11834 = torch.constant.bool false
    %7688 = torch.aten.scaled_dot_product_attention %7685, %7686, %7687, %7672, %float0.000000e00_11831, %false_11832, %none_11833, %false_11834 : !torch.vtensor<[4,32,1,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,32,?,64],f16>, !torch.vtensor<[4,1,1,?],f16>, !torch.float, !torch.bool, !torch.none, !torch.bool -> !torch.vtensor<[4,32,1,64],f16>
    %int1_11835 = torch.constant.int 1
    %int2_11836 = torch.constant.int 2
    %7689 = torch.aten.transpose.int %7688, %int1_11835, %int2_11836 : !torch.vtensor<[4,32,1,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,64],f16>
    %int4_11837 = torch.constant.int 4
    %int1_11838 = torch.constant.int 1
    %int2048_11839 = torch.constant.int 2048
    %7690 = torch.prim.ListConstruct %int4_11837, %int1_11838, %int2048_11839 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7691 = torch.aten.view %7689, %7690 : !torch.vtensor<[4,1,32,64],f16>, !torch.list<int> -> !torch.vtensor<[4,1,2048],f16>
    %int2_11840 = torch.constant.int 2
    %7692 = torch.aten.view.dtype %370, %int2_11840 : !torch.vtensor<[2048,2176],ui8>, !torch.int -> !torch.vtensor<[2048,1088],si16>
    %7693 = torch.aten.detach %7692 : !torch.vtensor<[2048,1088],si16> -> !torch.vtensor<[2048,1088],si16>
    %int-1_11841 = torch.constant.int -1
    %int17_11842 = torch.constant.int 17
    %7694 = torch.prim.ListConstruct %int-1_11841, %int17_11842 : (!torch.int, !torch.int) -> !torch.list<int>
    %7695 = torch.aten.view %7693, %7694 : !torch.vtensor<[2048,1088],si16>, !torch.list<int> -> !torch.vtensor<[131072,17],si16>
    %int2048_11843 = torch.constant.int 2048
    %int-1_11844 = torch.constant.int -1
    %int17_11845 = torch.constant.int 17
    %7696 = torch.prim.ListConstruct %int2048_11843, %int-1_11844, %int17_11845 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7697 = torch.aten.view %7695, %7696 : !torch.vtensor<[131072,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,64,17],si16>
    %int2_11846 = torch.constant.int 2
    %int0_11847 = torch.constant.int 0
    %int1_11848 = torch.constant.int 1
    %int1_11849 = torch.constant.int 1
    %7698 = torch.aten.slice.Tensor %7697, %int2_11846, %int0_11847, %int1_11848, %int1_11849 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,1],si16>
    %int5_11850 = torch.constant.int 5
    %7699 = torch.aten.view.dtype %7698, %int5_11850 : !torch.vtensor<[2048,64,1],si16>, !torch.int -> !torch.vtensor<[2048,64,1],f16>
    %7700 = torch.aten.detach %7699 : !torch.vtensor<[2048,64,1],f16> -> !torch.vtensor<[2048,64,1],f16>
    %int2_11851 = torch.constant.int 2
    %int1_11852 = torch.constant.int 1
    %int9223372036854775807_11853 = torch.constant.int 9223372036854775807
    %int1_11854 = torch.constant.int 1
    %7701 = torch.aten.slice.Tensor %7697, %int2_11851, %int1_11852, %int9223372036854775807_11853, %int1_11854 : !torch.vtensor<[2048,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,64,16],si16>
    %int1_11855 = torch.constant.int 1
    %7702 = torch.aten.view.dtype %7701, %int1_11855 : !torch.vtensor<[2048,64,16],si16>, !torch.int -> !torch.vtensor<[2048,64,32],si8>
    %7703 = torch.aten.detach %7702 : !torch.vtensor<[2048,64,32],si8> -> !torch.vtensor<[2048,64,32],si8>
    %7704 = torch_c.to_builtin_tensor %7691 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_11856 = tensor.cast %7704 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %7705 = torch_c.to_builtin_tensor %7700 : !torch.vtensor<[2048,64,1],f16> -> tensor<2048x64x1xf16>
    %7706 = torch_c.to_builtin_tensor %7703 : !torch.vtensor<[2048,64,32],si8> -> tensor<2048x64x32xi8>
    %7707 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%cast_11856, %7705, %7706) : (tensor<?x?x2048xf16>, tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16>
    %cast_11857 = tensor.cast %7707 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %7708 = torch_c.from_builtin_tensor %cast_11857 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %none_11858 = torch.constant.none
    %none_11859 = torch.constant.none
    %int5_11860 = torch.constant.int 5
    %cpu_11861 = torch.constant.device "cpu"
    %int0_11862 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7708, %none_11858, %none_11859, %int5_11860, %cpu_11861, %int0_11862 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int1_11863 = torch.constant.int 1
    %7709 = torch.aten.add.Tensor %7438, %7708, %int1_11863 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_11864 = torch.constant.none
    %none_11865 = torch.constant.none
    %int5_11866 = torch.constant.int 5
    %cpu_11867 = torch.constant.device "cpu"
    %int0_11868 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7709, %none_11864, %none_11865, %int5_11866, %cpu_11867, %int0_11868 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_11869 = torch.constant.int 6
    %7710 = torch.prims.convert_element_type %7709, %int6_11869 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_11870 = torch.constant.int 2
    %7711 = torch.aten.pow.Tensor_Scalar %7710, %int2_11870 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_11871 = torch.constant.int -1
    %7712 = torch.prim.ListConstruct %int-1_11871 : (!torch.int) -> !torch.list<int>
    %true_11872 = torch.constant.bool true
    %none_11873 = torch.constant.none
    %7713 = torch.aten.mean.dim %7711, %7712, %true_11872, %none_11873 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_11874 = torch.constant.float 9.9999997473787516E-6
    %int1_11875 = torch.constant.int 1
    %7714 = torch.aten.add.Scalar %7713, %float9.999990e-06_11874, %int1_11875 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %7715 = torch.aten.rsqrt %7714 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %7716 = torch.aten.mul.Tensor %7710, %7715 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_11876 = torch.constant.none
    %none_11877 = torch.constant.none
    %int6_11878 = torch.constant.int 6
    %cpu_11879 = torch.constant.device "cpu"
    %int0_11880 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7716, %none_11876, %none_11877, %int6_11878, %cpu_11879, %int0_11880 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11881 = torch.constant.int 5
    %7717 = torch.prims.convert_element_type %7716, %int5_11881 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %7718 = torch.aten.mul.Tensor %371, %7717 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_11882 = torch.constant.none
    %none_11883 = torch.constant.none
    %int6_11884 = torch.constant.int 6
    %cpu_11885 = torch.constant.device "cpu"
    %int0_11886 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7718, %none_11882, %none_11883, %int6_11884, %cpu_11885, %int0_11886 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11887 = torch.constant.int 5
    %7719 = torch.prims.convert_element_type %7718, %int5_11887 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_11888 = torch.constant.int 2
    %7720 = torch.aten.view.dtype %372, %int2_11888 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %7721 = torch.aten.detach %7720 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_11889 = torch.constant.int -1
    %int17_11890 = torch.constant.int 17
    %7722 = torch.prim.ListConstruct %int-1_11889, %int17_11890 : (!torch.int, !torch.int) -> !torch.list<int>
    %7723 = torch.aten.view %7721, %7722 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_11891 = torch.constant.int 5632
    %int-1_11892 = torch.constant.int -1
    %int17_11893 = torch.constant.int 17
    %7724 = torch.prim.ListConstruct %int5632_11891, %int-1_11892, %int17_11893 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7725 = torch.aten.view %7723, %7724 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_11894 = torch.constant.int 2
    %int0_11895 = torch.constant.int 0
    %int1_11896 = torch.constant.int 1
    %int1_11897 = torch.constant.int 1
    %7726 = torch.aten.slice.Tensor %7725, %int2_11894, %int0_11895, %int1_11896, %int1_11897 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_11898 = torch.constant.int 5
    %7727 = torch.aten.view.dtype %7726, %int5_11898 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %7728 = torch.aten.detach %7727 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_11899 = torch.constant.int 2
    %int1_11900 = torch.constant.int 1
    %int9223372036854775807_11901 = torch.constant.int 9223372036854775807
    %int1_11902 = torch.constant.int 1
    %7729 = torch.aten.slice.Tensor %7725, %int2_11899, %int1_11900, %int9223372036854775807_11901, %int1_11902 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_11903 = torch.constant.int 1
    %7730 = torch.aten.view.dtype %7729, %int1_11903 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %7731 = torch.aten.detach %7730 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %7732 = torch_c.to_builtin_tensor %7719 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_11904 = tensor.cast %7732 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %7733 = torch_c.to_builtin_tensor %7728 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %7734 = torch_c.to_builtin_tensor %7731 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %7735 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_11904, %7733, %7734) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_11905 = tensor.cast %7735 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %7736 = torch_c.from_builtin_tensor %cast_11905 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %7737 = torch.aten.silu %7736 : !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_11906 = torch.constant.int 2
    %7738 = torch.aten.view.dtype %373, %int2_11906 : !torch.vtensor<[5632,2176],ui8>, !torch.int -> !torch.vtensor<[5632,1088],si16>
    %7739 = torch.aten.detach %7738 : !torch.vtensor<[5632,1088],si16> -> !torch.vtensor<[5632,1088],si16>
    %int-1_11907 = torch.constant.int -1
    %int17_11908 = torch.constant.int 17
    %7740 = torch.prim.ListConstruct %int-1_11907, %int17_11908 : (!torch.int, !torch.int) -> !torch.list<int>
    %7741 = torch.aten.view %7739, %7740 : !torch.vtensor<[5632,1088],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int5632_11909 = torch.constant.int 5632
    %int-1_11910 = torch.constant.int -1
    %int17_11911 = torch.constant.int 17
    %7742 = torch.prim.ListConstruct %int5632_11909, %int-1_11910, %int17_11911 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7743 = torch.aten.view %7741, %7742 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[5632,64,17],si16>
    %int2_11912 = torch.constant.int 2
    %int0_11913 = torch.constant.int 0
    %int1_11914 = torch.constant.int 1
    %int1_11915 = torch.constant.int 1
    %7744 = torch.aten.slice.Tensor %7743, %int2_11912, %int0_11913, %int1_11914, %int1_11915 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,1],si16>
    %int5_11916 = torch.constant.int 5
    %7745 = torch.aten.view.dtype %7744, %int5_11916 : !torch.vtensor<[5632,64,1],si16>, !torch.int -> !torch.vtensor<[5632,64,1],f16>
    %7746 = torch.aten.detach %7745 : !torch.vtensor<[5632,64,1],f16> -> !torch.vtensor<[5632,64,1],f16>
    %int2_11917 = torch.constant.int 2
    %int1_11918 = torch.constant.int 1
    %int9223372036854775807_11919 = torch.constant.int 9223372036854775807
    %int1_11920 = torch.constant.int 1
    %7747 = torch.aten.slice.Tensor %7743, %int2_11917, %int1_11918, %int9223372036854775807_11919, %int1_11920 : !torch.vtensor<[5632,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[5632,64,16],si16>
    %int1_11921 = torch.constant.int 1
    %7748 = torch.aten.view.dtype %7747, %int1_11921 : !torch.vtensor<[5632,64,16],si16>, !torch.int -> !torch.vtensor<[5632,64,32],si8>
    %7749 = torch.aten.detach %7748 : !torch.vtensor<[5632,64,32],si8> -> !torch.vtensor<[5632,64,32],si8>
    %7750 = torch_c.to_builtin_tensor %7719 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_11922 = tensor.cast %7750 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %7751 = torch_c.to_builtin_tensor %7746 : !torch.vtensor<[5632,64,1],f16> -> tensor<5632x64x1xf16>
    %7752 = torch_c.to_builtin_tensor %7749 : !torch.vtensor<[5632,64,32],si8> -> tensor<5632x64x32xi8>
    %7753 = util.call @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%cast_11922, %7751, %7752) : (tensor<?x?x2048xf16>, tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16>
    %cast_11923 = tensor.cast %7753 : tensor<?x?x5632xf16> to tensor<4x1x5632xf16>
    %7754 = torch_c.from_builtin_tensor %cast_11923 : tensor<4x1x5632xf16> -> !torch.vtensor<[4,1,5632],f16>
    %7755 = torch.aten.mul.Tensor %7737, %7754 : !torch.vtensor<[4,1,5632],f16>, !torch.vtensor<[4,1,5632],f16> -> !torch.vtensor<[4,1,5632],f16>
    %int2_11924 = torch.constant.int 2
    %7756 = torch.aten.view.dtype %374, %int2_11924 : !torch.vtensor<[2048,5984],ui8>, !torch.int -> !torch.vtensor<[2048,2992],si16>
    %7757 = torch.aten.detach %7756 : !torch.vtensor<[2048,2992],si16> -> !torch.vtensor<[2048,2992],si16>
    %int-1_11925 = torch.constant.int -1
    %int17_11926 = torch.constant.int 17
    %7758 = torch.prim.ListConstruct %int-1_11925, %int17_11926 : (!torch.int, !torch.int) -> !torch.list<int>
    %7759 = torch.aten.view %7757, %7758 : !torch.vtensor<[2048,2992],si16>, !torch.list<int> -> !torch.vtensor<[360448,17],si16>
    %int2048_11927 = torch.constant.int 2048
    %int-1_11928 = torch.constant.int -1
    %int17_11929 = torch.constant.int 17
    %7760 = torch.prim.ListConstruct %int2048_11927, %int-1_11928, %int17_11929 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7761 = torch.aten.view %7759, %7760 : !torch.vtensor<[360448,17],si16>, !torch.list<int> -> !torch.vtensor<[2048,176,17],si16>
    %int2_11930 = torch.constant.int 2
    %int0_11931 = torch.constant.int 0
    %int1_11932 = torch.constant.int 1
    %int1_11933 = torch.constant.int 1
    %7762 = torch.aten.slice.Tensor %7761, %int2_11930, %int0_11931, %int1_11932, %int1_11933 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,1],si16>
    %int5_11934 = torch.constant.int 5
    %7763 = torch.aten.view.dtype %7762, %int5_11934 : !torch.vtensor<[2048,176,1],si16>, !torch.int -> !torch.vtensor<[2048,176,1],f16>
    %7764 = torch.aten.detach %7763 : !torch.vtensor<[2048,176,1],f16> -> !torch.vtensor<[2048,176,1],f16>
    %int2_11935 = torch.constant.int 2
    %int1_11936 = torch.constant.int 1
    %int9223372036854775807_11937 = torch.constant.int 9223372036854775807
    %int1_11938 = torch.constant.int 1
    %7765 = torch.aten.slice.Tensor %7761, %int2_11935, %int1_11936, %int9223372036854775807_11937, %int1_11938 : !torch.vtensor<[2048,176,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2048,176,16],si16>
    %int1_11939 = torch.constant.int 1
    %7766 = torch.aten.view.dtype %7765, %int1_11939 : !torch.vtensor<[2048,176,16],si16>, !torch.int -> !torch.vtensor<[2048,176,32],si8>
    %7767 = torch.aten.detach %7766 : !torch.vtensor<[2048,176,32],si8> -> !torch.vtensor<[2048,176,32],si8>
    %7768 = torch_c.to_builtin_tensor %7755 : !torch.vtensor<[4,1,5632],f16> -> tensor<4x1x5632xf16>
    %cast_11940 = tensor.cast %7768 : tensor<4x1x5632xf16> to tensor<?x?x5632xf16>
    %7769 = torch_c.to_builtin_tensor %7764 : !torch.vtensor<[2048,176,1],f16> -> tensor<2048x176x1xf16>
    %7770 = torch_c.to_builtin_tensor %7767 : !torch.vtensor<[2048,176,32],si8> -> tensor<2048x176x32xi8>
    %7771 = util.call @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%cast_11940, %7769, %7770) : (tensor<?x?x5632xf16>, tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16>
    %cast_11941 = tensor.cast %7771 : tensor<?x?x2048xf16> to tensor<4x1x2048xf16>
    %7772 = torch_c.from_builtin_tensor %cast_11941 : tensor<4x1x2048xf16> -> !torch.vtensor<[4,1,2048],f16>
    %int1_11942 = torch.constant.int 1
    %7773 = torch.aten.add.Tensor %7709, %7772, %int1_11942 : !torch.vtensor<[4,1,2048],f16>, !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %none_11943 = torch.constant.none
    %none_11944 = torch.constant.none
    %int5_11945 = torch.constant.int 5
    %cpu_11946 = torch.constant.device "cpu"
    %int0_11947 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7773, %none_11943, %none_11944, %int5_11945, %cpu_11946, %int0_11947 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %none_11948 = torch.constant.none
    %none_11949 = torch.constant.none
    %int5_11950 = torch.constant.int 5
    %cpu_11951 = torch.constant.device "cpu"
    %int0_11952 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7773, %none_11948, %none_11949, %int5_11950, %cpu_11951, %int0_11952 : !torch.vtensor<[4,1,2048],f16>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int6_11953 = torch.constant.int 6
    %7774 = torch.prims.convert_element_type %7773, %int6_11953 : !torch.vtensor<[4,1,2048],f16>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int2_11954 = torch.constant.int 2
    %7775 = torch.aten.pow.Tensor_Scalar %7774, %int2_11954 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f32>
    %int-1_11955 = torch.constant.int -1
    %7776 = torch.prim.ListConstruct %int-1_11955 : (!torch.int) -> !torch.list<int>
    %true_11956 = torch.constant.bool true
    %none_11957 = torch.constant.none
    %7777 = torch.aten.mean.dim %7775, %7776, %true_11956, %none_11957 : !torch.vtensor<[4,1,2048],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_11958 = torch.constant.float 9.9999997473787516E-6
    %int1_11959 = torch.constant.int 1
    %7778 = torch.aten.add.Scalar %7777, %float9.999990e-06_11958, %int1_11959 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %7779 = torch.aten.rsqrt %7778 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %7780 = torch.aten.mul.Tensor %7774, %7779 : !torch.vtensor<[4,1,2048],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,2048],f32>
    %none_11960 = torch.constant.none
    %none_11961 = torch.constant.none
    %int6_11962 = torch.constant.int 6
    %cpu_11963 = torch.constant.device "cpu"
    %int0_11964 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7780, %none_11960, %none_11961, %int6_11962, %cpu_11963, %int0_11964 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11965 = torch.constant.int 5
    %7781 = torch.prims.convert_element_type %7780, %int5_11965 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %7782 = torch.aten.mul.Tensor %375, %7781 : !torch.vtensor<[2048],f32>, !torch.vtensor<[4,1,2048],f16> -> !torch.vtensor<[4,1,2048],f32>
    %none_11966 = torch.constant.none
    %none_11967 = torch.constant.none
    %int6_11968 = torch.constant.int 6
    %cpu_11969 = torch.constant.device "cpu"
    %int0_11970 = torch.constant.int 0
    torch.aten._assert_tensor_metadata %7782, %none_11966, %none_11967, %int6_11968, %cpu_11969, %int0_11970 : !torch.vtensor<[4,1,2048],f32>, !torch.none, !torch.none, !torch.int, !torch.Device, !torch.int
    %int5_11971 = torch.constant.int 5
    %7783 = torch.prims.convert_element_type %7782, %int5_11971 : !torch.vtensor<[4,1,2048],f32>, !torch.int -> !torch.vtensor<[4,1,2048],f16>
    %int2_11972 = torch.constant.int 2
    %7784 = torch.aten.view.dtype %376, %int2_11972 : !torch.vtensor<[32000,2176],ui8>, !torch.int -> !torch.vtensor<[32000,1088],si16>
    %7785 = torch.aten.detach %7784 : !torch.vtensor<[32000,1088],si16> -> !torch.vtensor<[32000,1088],si16>
    %int-1_11973 = torch.constant.int -1
    %int17_11974 = torch.constant.int 17
    %7786 = torch.prim.ListConstruct %int-1_11973, %int17_11974 : (!torch.int, !torch.int) -> !torch.list<int>
    %7787 = torch.aten.view %7785, %7786 : !torch.vtensor<[32000,1088],si16>, !torch.list<int> -> !torch.vtensor<[2048000,17],si16>
    %int32000_11975 = torch.constant.int 32000
    %int-1_11976 = torch.constant.int -1
    %int17_11977 = torch.constant.int 17
    %7788 = torch.prim.ListConstruct %int32000_11975, %int-1_11976, %int17_11977 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7789 = torch.aten.view %7787, %7788 : !torch.vtensor<[2048000,17],si16>, !torch.list<int> -> !torch.vtensor<[32000,64,17],si16>
    %int2_11978 = torch.constant.int 2
    %int0_11979 = torch.constant.int 0
    %int1_11980 = torch.constant.int 1
    %int1_11981 = torch.constant.int 1
    %7790 = torch.aten.slice.Tensor %7789, %int2_11978, %int0_11979, %int1_11980, %int1_11981 : !torch.vtensor<[32000,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[32000,64,1],si16>
    %int5_11982 = torch.constant.int 5
    %7791 = torch.aten.view.dtype %7790, %int5_11982 : !torch.vtensor<[32000,64,1],si16>, !torch.int -> !torch.vtensor<[32000,64,1],f16>
    %7792 = torch.aten.detach %7791 : !torch.vtensor<[32000,64,1],f16> -> !torch.vtensor<[32000,64,1],f16>
    %int2_11983 = torch.constant.int 2
    %int1_11984 = torch.constant.int 1
    %int9223372036854775807_11985 = torch.constant.int 9223372036854775807
    %int1_11986 = torch.constant.int 1
    %7793 = torch.aten.slice.Tensor %7789, %int2_11983, %int1_11984, %int9223372036854775807_11985, %int1_11986 : !torch.vtensor<[32000,64,17],si16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[32000,64,16],si16>
    %int1_11987 = torch.constant.int 1
    %7794 = torch.aten.view.dtype %7793, %int1_11987 : !torch.vtensor<[32000,64,16],si16>, !torch.int -> !torch.vtensor<[32000,64,32],si8>
    %7795 = torch.aten.detach %7794 : !torch.vtensor<[32000,64,32],si8> -> !torch.vtensor<[32000,64,32],si8>
    %7796 = torch_c.to_builtin_tensor %7783 : !torch.vtensor<[4,1,2048],f16> -> tensor<4x1x2048xf16>
    %cast_11988 = tensor.cast %7796 : tensor<4x1x2048xf16> to tensor<?x?x2048xf16>
    %7797 = torch_c.to_builtin_tensor %7792 : !torch.vtensor<[32000,64,1],f16> -> tensor<32000x64x1xf16>
    %7798 = torch_c.to_builtin_tensor %7795 : !torch.vtensor<[32000,64,32],si8> -> tensor<32000x64x32xi8>
    %7799 = util.call @sharktank_mmt_block_scaled_q8_3d_32000_2048_32_f16(%cast_11988, %7797, %7798) : (tensor<?x?x2048xf16>, tensor<32000x64x1xf16>, tensor<32000x64x32xi8>) -> tensor<?x?x32000xf16>
    %cast_11989 = tensor.cast %7799 : tensor<?x?x32000xf16> to tensor<4x1x32000xf16>
    %7800 = torch_c.from_builtin_tensor %cast_11989 : tensor<4x1x32000xf16> -> !torch.vtensor<[4,1,32000],f16>
    return %7800 : !torch.vtensor<[4,1,32000],f16>
  }
  util.func private @sharktank_mmt_block_scaled_q8_3d_2048_2048_32_f16(%arg0: tensor<?x?x2048xf16>, %arg1: tensor<2048x64x1xf16>, %arg2: tensor<2048x64x32xi8>) -> tensor<?x?x2048xf16> {
    %cst = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x2048xf16>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x2048xf16>
    %0 = tensor.empty() : tensor<2048x64x32xf16>
    %1 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel", "parallel"]} ins(%arg1, %arg2 : tensor<2048x64x1xf16>, tensor<2048x64x32xi8>) outs(%0 : tensor<2048x64x32xf16>) {
    ^bb0(%in: f16, %in_1: i8, %out: f16):
      %7 = arith.extsi %in_1 : i8 to i32
      %8 = arith.sitofp %7 : i32 to f16
      %9 = arith.mulf %8, %in : f16
      linalg.yield %9 : f16
    } -> tensor<2048x64x32xf16>
    %expanded = tensor.expand_shape %arg0 [[0], [1], [2, 3]] output_shape [%dim, %dim_0, 64, 32] : tensor<?x?x2048xf16> into tensor<?x?x64x32xf16>
    %2 = tensor.empty(%dim, %dim_0) : tensor<?x?x2048xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<?x?x2048xf32>) -> tensor<?x?x2048xf32>
    %4 = linalg.generic {indexing_maps = [#map2, #map3, #map4], iterator_types = ["parallel", "parallel", "parallel", "reduction", "reduction"]} ins(%expanded, %1 : tensor<?x?x64x32xf16>, tensor<2048x64x32xf16>) outs(%3 : tensor<?x?x2048xf32>) {
    ^bb0(%in: f16, %in_1: f16, %out: f32):
      %7 = arith.mulf %in, %in_1 : f16
      %8 = arith.extf %7 : f16 to f32
      %9 = arith.addf %8, %out : f32
      linalg.yield %9 : f32
    } -> tensor<?x?x2048xf32>
    %5 = tensor.empty(%dim, %dim_0) : tensor<?x?x2048xf16>
    %6 = linalg.copy ins(%4 : tensor<?x?x2048xf32>) outs(%5 : tensor<?x?x2048xf16>) -> tensor<?x?x2048xf16>
    util.return %6 : tensor<?x?x2048xf16>
  }
  util.func private @sharktank_mmt_block_scaled_q8_3d_256_2048_32_f16(%arg0: tensor<?x?x2048xf16>, %arg1: tensor<256x64x1xf16>, %arg2: tensor<256x64x32xi8>) -> tensor<?x?x256xf16> {
    %cst = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x2048xf16>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x2048xf16>
    %0 = tensor.empty() : tensor<256x64x32xf16>
    %1 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel", "parallel"]} ins(%arg1, %arg2 : tensor<256x64x1xf16>, tensor<256x64x32xi8>) outs(%0 : tensor<256x64x32xf16>) {
    ^bb0(%in: f16, %in_1: i8, %out: f16):
      %7 = arith.extsi %in_1 : i8 to i32
      %8 = arith.sitofp %7 : i32 to f16
      %9 = arith.mulf %8, %in : f16
      linalg.yield %9 : f16
    } -> tensor<256x64x32xf16>
    %expanded = tensor.expand_shape %arg0 [[0], [1], [2, 3]] output_shape [%dim, %dim_0, 64, 32] : tensor<?x?x2048xf16> into tensor<?x?x64x32xf16>
    %2 = tensor.empty(%dim, %dim_0) : tensor<?x?x256xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<?x?x256xf32>) -> tensor<?x?x256xf32>
    %4 = linalg.generic {indexing_maps = [#map2, #map3, #map4], iterator_types = ["parallel", "parallel", "parallel", "reduction", "reduction"]} ins(%expanded, %1 : tensor<?x?x64x32xf16>, tensor<256x64x32xf16>) outs(%3 : tensor<?x?x256xf32>) {
    ^bb0(%in: f16, %in_1: f16, %out: f32):
      %7 = arith.mulf %in, %in_1 : f16
      %8 = arith.extf %7 : f16 to f32
      %9 = arith.addf %8, %out : f32
      linalg.yield %9 : f32
    } -> tensor<?x?x256xf32>
    %5 = tensor.empty(%dim, %dim_0) : tensor<?x?x256xf16>
    %6 = linalg.copy ins(%4 : tensor<?x?x256xf32>) outs(%5 : tensor<?x?x256xf16>) -> tensor<?x?x256xf16>
    util.return %6 : tensor<?x?x256xf16>
  }
  util.func private @rope_select_concat_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_HALFDIM_f16_BS_SL_HEADS_TWO_2_HALFDIM_f16(%arg0: tensor<?x?x?x?xf16>, %arg1: tensor<?x?x?x?xf16>) -> tensor<?x?x?x2x?xf16> {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?x?xf16>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?x?xf16>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?x?xf16>
    %dim_2 = tensor.dim %arg0, %c3 : tensor<?x?x?x?xf16>
    %0 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x2x?xf16>
    %1 = linalg.generic {indexing_maps = [#map5, #map5, #map6], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%arg0, %arg1 : tensor<?x?x?x?xf16>, tensor<?x?x?x?xf16>) outs(%0 : tensor<?x?x?x2x?xf16>) {
    ^bb0(%in: f16, %in_3: f16, %out: f16):
      %2 = linalg.index 3 : index
      %3 = arith.cmpi eq, %2, %c0 : index
      %4 = arith.select %3, %in, %in_3 : f16
      linalg.yield %4 : f16
    } -> tensor<?x?x?x2x?xf16>
    util.return %1 : tensor<?x?x?x2x?xf16>
  }
  util.func private @sharktank_mmt_block_scaled_q8_3d_5632_2048_32_f16(%arg0: tensor<?x?x2048xf16>, %arg1: tensor<5632x64x1xf16>, %arg2: tensor<5632x64x32xi8>) -> tensor<?x?x5632xf16> {
    %cst = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x2048xf16>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x2048xf16>
    %0 = tensor.empty() : tensor<5632x64x32xf16>
    %1 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel", "parallel"]} ins(%arg1, %arg2 : tensor<5632x64x1xf16>, tensor<5632x64x32xi8>) outs(%0 : tensor<5632x64x32xf16>) {
    ^bb0(%in: f16, %in_1: i8, %out: f16):
      %7 = arith.extsi %in_1 : i8 to i32
      %8 = arith.sitofp %7 : i32 to f16
      %9 = arith.mulf %8, %in : f16
      linalg.yield %9 : f16
    } -> tensor<5632x64x32xf16>
    %expanded = tensor.expand_shape %arg0 [[0], [1], [2, 3]] output_shape [%dim, %dim_0, 64, 32] : tensor<?x?x2048xf16> into tensor<?x?x64x32xf16>
    %2 = tensor.empty(%dim, %dim_0) : tensor<?x?x5632xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<?x?x5632xf32>) -> tensor<?x?x5632xf32>
    %4 = linalg.generic {indexing_maps = [#map2, #map3, #map4], iterator_types = ["parallel", "parallel", "parallel", "reduction", "reduction"]} ins(%expanded, %1 : tensor<?x?x64x32xf16>, tensor<5632x64x32xf16>) outs(%3 : tensor<?x?x5632xf32>) {
    ^bb0(%in: f16, %in_1: f16, %out: f32):
      %7 = arith.mulf %in, %in_1 : f16
      %8 = arith.extf %7 : f16 to f32
      %9 = arith.addf %8, %out : f32
      linalg.yield %9 : f32
    } -> tensor<?x?x5632xf32>
    %5 = tensor.empty(%dim, %dim_0) : tensor<?x?x5632xf16>
    %6 = linalg.copy ins(%4 : tensor<?x?x5632xf32>) outs(%5 : tensor<?x?x5632xf16>) -> tensor<?x?x5632xf16>
    util.return %6 : tensor<?x?x5632xf16>
  }
  util.func private @sharktank_mmt_block_scaled_q8_3d_2048_5632_32_f16(%arg0: tensor<?x?x5632xf16>, %arg1: tensor<2048x176x1xf16>, %arg2: tensor<2048x176x32xi8>) -> tensor<?x?x2048xf16> {
    %cst = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x5632xf16>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x5632xf16>
    %0 = tensor.empty() : tensor<2048x176x32xf16>
    %1 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel", "parallel"]} ins(%arg1, %arg2 : tensor<2048x176x1xf16>, tensor<2048x176x32xi8>) outs(%0 : tensor<2048x176x32xf16>) {
    ^bb0(%in: f16, %in_1: i8, %out: f16):
      %7 = arith.extsi %in_1 : i8 to i32
      %8 = arith.sitofp %7 : i32 to f16
      %9 = arith.mulf %8, %in : f16
      linalg.yield %9 : f16
    } -> tensor<2048x176x32xf16>
    %expanded = tensor.expand_shape %arg0 [[0], [1], [2, 3]] output_shape [%dim, %dim_0, 176, 32] : tensor<?x?x5632xf16> into tensor<?x?x176x32xf16>
    %2 = tensor.empty(%dim, %dim_0) : tensor<?x?x2048xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<?x?x2048xf32>) -> tensor<?x?x2048xf32>
    %4 = linalg.generic {indexing_maps = [#map2, #map3, #map4], iterator_types = ["parallel", "parallel", "parallel", "reduction", "reduction"]} ins(%expanded, %1 : tensor<?x?x176x32xf16>, tensor<2048x176x32xf16>) outs(%3 : tensor<?x?x2048xf32>) {
    ^bb0(%in: f16, %in_1: f16, %out: f32):
      %7 = arith.mulf %in, %in_1 : f16
      %8 = arith.extf %7 : f16 to f32
      %9 = arith.addf %8, %out : f32
      linalg.yield %9 : f32
    } -> tensor<?x?x2048xf32>
    %5 = tensor.empty(%dim, %dim_0) : tensor<?x?x2048xf16>
    %6 = linalg.copy ins(%4 : tensor<?x?x2048xf32>) outs(%5 : tensor<?x?x2048xf16>) -> tensor<?x?x2048xf16>
    util.return %6 : tensor<?x?x2048xf16>
  }
  util.func private @sharktank_mmt_block_scaled_q8_3d_32000_2048_32_f16(%arg0: tensor<?x?x2048xf16>, %arg1: tensor<32000x64x1xf16>, %arg2: tensor<32000x64x32xi8>) -> tensor<?x?x32000xf16> {
    %cst = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x2048xf16>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x2048xf16>
    %0 = tensor.empty() : tensor<32000x64x32xf16>
    %1 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel", "parallel"]} ins(%arg1, %arg2 : tensor<32000x64x1xf16>, tensor<32000x64x32xi8>) outs(%0 : tensor<32000x64x32xf16>) {
    ^bb0(%in: f16, %in_1: i8, %out: f16):
      %7 = arith.extsi %in_1 : i8 to i32
      %8 = arith.sitofp %7 : i32 to f16
      %9 = arith.mulf %8, %in : f16
      linalg.yield %9 : f16
    } -> tensor<32000x64x32xf16>
    %expanded = tensor.expand_shape %arg0 [[0], [1], [2, 3]] output_shape [%dim, %dim_0, 64, 32] : tensor<?x?x2048xf16> into tensor<?x?x64x32xf16>
    %2 = tensor.empty(%dim, %dim_0) : tensor<?x?x32000xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<?x?x32000xf32>) -> tensor<?x?x32000xf32>
    %4 = linalg.generic {indexing_maps = [#map2, #map3, #map4], iterator_types = ["parallel", "parallel", "parallel", "reduction", "reduction"]} ins(%expanded, %1 : tensor<?x?x64x32xf16>, tensor<32000x64x32xf16>) outs(%3 : tensor<?x?x32000xf32>) {
    ^bb0(%in: f16, %in_1: f16, %out: f32):
      %7 = arith.mulf %in, %in_1 : f16
      %8 = arith.extf %7 : f16 to f32
      %9 = arith.addf %8, %out : f32
      linalg.yield %9 : f32
    } -> tensor<?x?x32000xf32>
    %5 = tensor.empty(%dim, %dim_0) : tensor<?x?x32000xf16>
    %6 = linalg.copy ins(%4 : tensor<?x?x32000xf32>) outs(%5 : tensor<?x?x32000xf16>) -> tensor<?x?x32000xf16>
    util.return %6 : tensor<?x?x32000xf16>
  }
  util.func private @paged_attention_kv_cache_gather_CACHE_SIZE_T_BLOCK_22_PART_2_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16_BATCH_PAGES_i64__i64__i64_BATCH_PAGES_HEAD_COUNT_KV_4_BLOCK_SEQ_STRIDE_32_ATTN_HEAD_DIM_64_f16(%arg0: tensor<?x22x2x4x32x64xf16>, %arg1: tensor<?x?xi64>, %arg2: tensor<i64>, %arg3: tensor<i64>) -> tensor<?x?x4x32x64xf16> {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %extracted = tensor.extract %arg2[] : tensor<i64>
    %extracted_0 = tensor.extract %arg3[] : tensor<i64>
    %0 = arith.index_cast %extracted : i64 to index
    %1 = arith.index_cast %extracted_0 : i64 to index
    %dim = tensor.dim %arg0, %c0 : tensor<?x22x2x4x32x64xf16>
    %dim_1 = tensor.dim %arg1, %c0 : tensor<?x?xi64>
    %dim_2 = tensor.dim %arg1, %c1 : tensor<?x?xi64>
    %extracted_slice = tensor.extract_slice %arg0[0, %0, %1, 0, 0, 0] [%dim, 1, 1, 4, 32, 64] [1, 1, 1, 1, 1, 1] : tensor<?x22x2x4x32x64xf16> to tensor<?x4x32x64xf16>
    %2 = tensor.empty(%dim_1, %dim_2) : tensor<?x?x4x32x64xf16>
    %3 = iree_linalg_ext.gather dimension_map = [0] ins(%extracted_slice, %arg1 : tensor<?x4x32x64xf16>, tensor<?x?xi64>) outs(%2 : tensor<?x?x4x32x64xf16>) -> tensor<?x?x4x32x64xf16>
    util.return %3 : tensor<?x?x4x32x64xf16>
  }
}
