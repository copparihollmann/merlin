#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "znver3", cpu_features = "+prfchw,-cldemote,+avx,+aes,+sahf,+pclmul,-xop,+crc32,-amx-fp8,+xsaves,-avx512fp16,-usermsr,-sm4,-egpr,+sse4.1,-avx512ifma,+xsave,+sse4.2,-tsxldtrk,-sm3,-ptwrite,-widekl,-movrs,+invpcid,+64bit,+xsavec,-avx10.1-512,-avx512vpopcntdq,+cmov,-avx512vp2intersect,-avx512cd,+movbe,-avxvnniint8,-ccmp,-amx-int8,-kl,-avx10.1-256,-sha512,-avxvnni,-rtm,+adx,+avx2,-hreset,-movdiri,-serialize,+vpclmulqdq,-avx512vl,-uintr,-cf,+clflushopt,-raoint,-cmpccxadd,+bmi,-amx-tile,+sse,-avx10.2-256,-gfni,-avxvnniint16,-amx-fp16,-zu,-ndd,+xsaveopt,+rdrnd,-avx512f,-amx-bf16,-avx512bf16,-avx512vnni,-push2pop2,+cx8,-avx512bw,+sse3,+pku,-nf,-amx-tf32,-amx-avx512,+fsgsbase,+clzero,+mwaitx,-lwp,+lzcnt,+sha,-movdir64b,-ppx,+wbnoinvd,-enqcmd,-amx-transpose,-avx10.2-512,-avxneconvert,-tbm,-pconfig,-amx-complex,+ssse3,+cx16,+bmi2,+fma,+popcnt,-avxifma,+f16c,-avx512bitalg,+rdpru,+clwb,+mmx,+sse2,+rdseed,-avx512vbmi2,-prefetchi,-amx-movrs,+rdpid,-fma4,-avx512vbmi,+shstk,+vaes,-waitpkg,-sgx,+fxsr,-avx512dq,+sse4a", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 32 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<() -> ()>
#map1 = affine_map<(d0, d1, d2) -> (d0)>
#map2 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d3, d1 * 4 + d4, d2 * 4 + d5)>
#map4 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d3, d4, d5)>
#map5 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2)>
#map6 = affine_map<(d0, d1, d2, d3, d4) -> (d0, d1 * 2 + d3, d2 * 2 + d4)>
#map7 = affine_map<(d0, d1, d2, d3, d4) -> (d3, d4)>
#map8 = affine_map<(d0, d1, d2, d3, d4) -> (d0, d1, d2)>
#map9 = affine_map<(d0) -> (d0)>
#map10 = affine_map<(d0) -> ()>
#map11 = affine_map<(d0, d1) -> (d0, d1)>
#map12 = affine_map<(d0, d1) -> ()>
#map13 = affine_map<(d0, d1) -> (d1)>
#map14 = affine_map<(d0, d1) -> (d0)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module @integrated_robot_application attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  util.func public @main$async(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view, %arg3: !hal.buffer_view, %arg4: !hal.fence, %arg5: !hal.fence, %arg6: !hal.fence, %arg7: !hal.fence) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "async func @main$async(%input0: !hal.buffer_view, %input1: !hal.buffer_view, %input2: !hal.buffer_view, %input3: !hal.buffer_view, %input4: !hal.fence, %input5: !hal.fence) -> (%output0: !hal.buffer_view, %output1: !hal.buffer_view)", iree.abi.model = "coarse-fences"}} {
    %0:2 = util.call @_main$async(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5) : (!hal.buffer_view, !hal.buffer_view, !hal.buffer_view, !hal.buffer_view, !hal.fence, !hal.fence) -> (!hal.buffer_view, !hal.buffer_view)
    hal.fence.signal<%arg7 : !hal.fence>
    util.return %0#0, %0#1 : !hal.buffer_view, !hal.buffer_view
  }
  util.global private @__hoisted_tensor_1x256x8x1xf32 {stream.affinity.default = #hal.device.affinity<@__device_0>} = dense<"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tensor<1x256x8x1xf32>
  util.func private @_main$async(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view, %arg3: !hal.buffer_view, %arg4: !hal.fence, %arg5: !hal.fence) -> (!hal.buffer_view, !hal.buffer_view) attributes {inlining_policy = #util.inline.never} {
    %cst = arith.constant dense<[0.000000e+00, 0.111111112, 0.222222224, 0.333333343, 0.444444448, 0.555555582, 0.666666686, 0.777777791, 0.888888895, 1.000000e+00]> : tensor<10xf32>
    %cst_0 = arith.constant dense<[1.000000e+00, 0.888888895, 0.777777791, 0.666666627, 0.555555582, 0.444444418, 0.333333313, 0.222222209, 0.111111104, 0.000000e+00]> : tensor<10xf32>
    %cst_1 = arith.constant dense<2> : tensor<i64>
    %cst_2 = arith.constant dense<1> : tensor<i64>
    %cst_3 = arith.constant dense<0> : tensor<i64>
    %cst_4 = arith.constant dense<[-2.000000e-01, 0.000000e+00]> : tensor<2xf32>
    %cst_5 = arith.constant dense<[0.000000e+00, -1.000000e-01]> : tensor<2xf32>
    %cst_6 = arith.constant 1.000000e+00 : f32
    %cst_7 = arith.constant dense<1.000000e+00> : tensor<f32>
    %cst_8 = arith.constant dense<-1.000000e+00> : tensor<f32>
    %cst_9 = arith.constant dense<9.99999997E-7> : tensor<f32>
    %cst_10 = arith.constant dense<1.000000e-01> : tensor<f32>
    %cst_11 = arith.constant 0xFF800000 : f32
    %cst_12 = arith.constant dense<[0.036251314, -0.0380887836]> : tensor<2xf32>
    %cst_13 = arith.constant dense<[0.0602946617, 0.0679099783, 0.0262253601, 2.52176687E-4]> : tensor<4xf32>
    %cst_14 = arith.constant dense<"0xC33C46BD233AA03D8760E43DF310A3BD92EA023DDFFFFA3C3FB59CBDD67822BD657B11BE2E4CEEBD428D22BD46CC993D3076D8BD25E617BD5F6A8EBB21A71F3C9F7F80BD92A3FE3C723613BE77408B3D9251B13DCF91BFBDF9DA743DD64527BD639EB5BD68CB173D33409E3C1DDFA7BD3723DD3DF05AB9BCF96FE7BD4299C63D030B3A3D238ABBBCFBF0BC3D746E953DA3624D3C3065213D6DF8993CBF1588BD5115883C8A42E53AF62D0EBEE6A0DFBB684E123D953A313DE3220C3C419B17BDECF9CBBD0A518DBD93F5E6BDD33064BD1122F63D7F91023E7F8CE83D0153A63C624B9C3D96D3FABDCCB00DBE49488CBD147795BD5FDA12BE5B0F4BBD569FE0BD372A103E5ACA8A3CA12B9A3CC949703DE500CEBD67CBA9BDD8E8703B36DE92BDBCD80DBD0133053E6302033D8483A13B5FC4D5BDE3670CBECF09723DB12606BEC171D23DC993023C82E1B9BD8AA9AA3C39B0ABBDABC471BD29BC033EB5F06E3D80E30E3E8DA7D73DC70D5B3DD7F9F23BC14F93BD3EA8E33DF6F293BD558AE63C0F46CCBDBE2712BED055373D7EF5A73DF061B93DA792ACBD4CF702BECD2E18BC8B5EF4BDC4FABB3DC56C0A3EBC5FDA3DF4B408BDCBD6E43C765E0C3E42B85CBDC5C6FBBCBA5EF5BDE549E6BD347B0DBC83BF0EBD746DA4BC84A2E2BD36A4BEBDFC0CDDBDB05E7EBDCBB8383DD8B091BD4EDBB23C5A70F3BDE7B5D83D37E6AD3C68940EBD3AF579BDDB8E083EC064CC3CD7CAA43C15972B3DC6F16FBD35E99B3D4526683DD97395BA82FFF43C85E6C3BC93B88D3DFC2BA3BD30A6CE3D36411FBD80D71BBD33695B3DA7BAAC3DFC17E03D9B5C13BDFCF6D1BC3B988A3C185DE0BDC3ED03BC0B99993D9E1E613DA0DB2D3D3889FE3D9BB8AF3CF375AE3D68C98FBDDF570FBE3CB1063ED6D8693D89BDD93D5C95C73DFD9CAA3D3CA5033C1DCC9BBCD18D9C3CD8AC16BCF22914BD162811BDD87B67BD87DFBFBDD74EDBBCB6346FBD6B688BBD566703BE03ECE53D18578FBD7EA4D9BDF9FF05BE8B083CBD2786BCBD64FEF6BC8114093DB4D0F23BBD513DBC3D56E13C81CF82BC2F18FB3DD6AC58BD"> : tensor<4x3x4x4xf32>
    %cst_15 = arith.constant dense<9.99999974E-5> : tensor<f32>
    %cst_16 = arith.constant dense<2.000000e+00> : tensor<f32>
    %cst_17 = arith.constant dense<5.000000e-01> : tensor<f32>
    %cst_18 = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %cst_19 = arith.constant 9.99999993E-9 : f32
    %0 = hal.tensor.import wait(%arg4) => %arg0 : !hal.buffer_view -> tensor<1x3x64x64xf32>
    %1 = hal.tensor.import wait(%arg4) => %arg1 : !hal.buffer_view -> tensor<3xf32>
    %2 = hal.tensor.import wait(%arg4) => %arg2 : !hal.buffer_view -> tensor<2xf32>
    %3 = hal.tensor.import wait(%arg4) => %arg3 : !hal.buffer_view -> tensor<2xf32>
    %4 = tensor.empty() : tensor<f32>
    %5 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst_3 : tensor<i64>) outs(%4 : tensor<f32>) {
    ^bb0(%in: i64, %out: f32):
      %49 = arith.index_cast %in : i64 to index
      %extracted_27 = tensor.extract %3[%49] : tensor<2xf32>
      linalg.yield %extracted_27 : f32
    } -> tensor<f32>
    %6 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%5, %cst_15 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
    ^bb0(%in: f32, %in_27: f32, %out: f32):
      %49 = arith.mulf %in, %in_27 : f32
      linalg.yield %49 : f32
    } -> tensor<f32>
    %7 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst_2 : tensor<i64>) outs(%4 : tensor<f32>) {
    ^bb0(%in: i64, %out: f32):
      %49 = arith.index_cast %in : i64 to index
      %extracted_27 = tensor.extract %3[%49] : tensor<2xf32>
      linalg.yield %extracted_27 : f32
    } -> tensor<f32>
    %8 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%7, %cst_15 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
    ^bb0(%in: f32, %in_27: f32, %out: f32):
      %49 = arith.mulf %in, %in_27 : f32
      linalg.yield %49 : f32
    } -> tensor<f32>
    %9 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%6, %8 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
    ^bb0(%in: f32, %in_27: f32, %out: f32):
      %49 = arith.addf %in, %in_27 : f32
      linalg.yield %49 : f32
    } -> tensor<f32>
    %10 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%9, %cst_16 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
    ^bb0(%in: f32, %in_27: f32, %out: f32):
      %49 = arith.divf %in, %in_27 : f32
      linalg.yield %49 : f32
    } -> tensor<f32>
    %11 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%8, %6 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
    ^bb0(%in: f32, %in_27: f32, %out: f32):
      %49 = arith.subf %in, %in_27 : f32
      linalg.yield %49 : f32
    } -> tensor<f32>
    %12 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%11, %cst_17 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
    ^bb0(%in: f32, %in_27: f32, %out: f32):
      %49 = arith.divf %in, %in_27 : f32
      linalg.yield %49 : f32
    } -> tensor<f32>
    %13 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst_1 : tensor<i64>) outs(%4 : tensor<f32>) {
    ^bb0(%in: i64, %out: f32):
      %49 = arith.index_cast %in : i64 to index
      %extracted_27 = tensor.extract %1[%49] : tensor<3xf32>
      linalg.yield %extracted_27 : f32
    } -> tensor<f32>
    %14 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%12, %cst_16 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
    ^bb0(%in: f32, %in_27: f32, %out: f32):
      %49 = arith.divf %in, %in_27 : f32
      linalg.yield %49 : f32
    } -> tensor<f32>
    %15 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%13, %14 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
    ^bb0(%in: f32, %in_27: f32, %out: f32):
      %49 = arith.addf %in, %in_27 : f32
      linalg.yield %49 : f32
    } -> tensor<f32>
    %16 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst_3 : tensor<i64>) outs(%4 : tensor<f32>) {
    ^bb0(%in: i64, %out: f32):
      %49 = arith.index_cast %in : i64 to index
      %extracted_27 = tensor.extract %1[%49] : tensor<3xf32>
      linalg.yield %extracted_27 : f32
    } -> tensor<f32>
    %17 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%15 : tensor<f32>) outs(%4 : tensor<f32>) {
    ^bb0(%in: f32, %out: f32):
      %49 = math.cos %in : f32
      linalg.yield %49 : f32
    } -> tensor<f32>
    %18 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%10, %17 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
    ^bb0(%in: f32, %in_27: f32, %out: f32):
      %49 = arith.mulf %in, %in_27 : f32
      linalg.yield %49 : f32
    } -> tensor<f32>
    %19 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%16, %18 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
    ^bb0(%in: f32, %in_27: f32, %out: f32):
      %49 = arith.addf %in, %in_27 : f32
      linalg.yield %49 : f32
    } -> tensor<f32>
    %20 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst_2 : tensor<i64>) outs(%4 : tensor<f32>) {
    ^bb0(%in: i64, %out: f32):
      %49 = arith.index_cast %in : i64 to index
      %extracted_27 = tensor.extract %1[%49] : tensor<3xf32>
      linalg.yield %extracted_27 : f32
    } -> tensor<f32>
    %21 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%15 : tensor<f32>) outs(%4 : tensor<f32>) {
    ^bb0(%in: f32, %out: f32):
      %49 = math.sin %in : f32
      linalg.yield %49 : f32
    } -> tensor<f32>
    %22 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%10, %21 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
    ^bb0(%in: f32, %in_27: f32, %out: f32):
      %49 = arith.mulf %in, %in_27 : f32
      linalg.yield %49 : f32
    } -> tensor<f32>
    %23 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%20, %22 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
    ^bb0(%in: f32, %in_27: f32, %out: f32):
      %49 = arith.addf %in, %in_27 : f32
      linalg.yield %49 : f32
    } -> tensor<f32>
    %24 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%13, %12 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
    ^bb0(%in: f32, %in_27: f32, %out: f32):
      %49 = arith.addf %in, %in_27 : f32
      linalg.yield %49 : f32
    } -> tensor<f32>
    %expanded = tensor.expand_shape %19 [] output_shape [1] : tensor<f32> into tensor<1xf32>
    %expanded_20 = tensor.expand_shape %23 [] output_shape [1] : tensor<f32> into tensor<1xf32>
    %expanded_21 = tensor.expand_shape %24 [] output_shape [1] : tensor<f32> into tensor<1xf32>
    %concat = tensor.concat dim(0) %expanded, %expanded_20, %expanded_21 : (tensor<1xf32>, tensor<1xf32>, tensor<1xf32>) -> tensor<3xf32>
    %25 = tensor.empty() : tensor<1x3x66x66xf32>
    %26 = linalg.fill ins(%cst_18 : f32) outs(%25 : tensor<1x3x66x66xf32>) -> tensor<1x3x66x66xf32>
    %collapsed = tensor.collapse_shape %0 [[0, 1], [2], [3]] : tensor<1x3x64x64xf32> into tensor<3x64x64xf32>
    %inserted_slice = tensor.insert_slice %collapsed into %26[0, 0, 1, 1] [1, 3, 64, 64] [1, 1, 1, 1] : tensor<3x64x64xf32> into tensor<1x3x66x66xf32>
    %27 = tensor.empty() : tensor<4x16x16xf32>
    %28 = linalg.generic {indexing_maps = [#map1, #map2], iterator_types = ["parallel", "parallel", "parallel"]} ins(%cst_13 : tensor<4xf32>) outs(%27 : tensor<4x16x16xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<4x16x16xf32>
    %collapsed_22 = tensor.collapse_shape %inserted_slice [[0, 1], [2], [3]] : tensor<1x3x66x66xf32> into tensor<3x66x66xf32>
    %29 = linalg.fill ins(%cst_18 : f32) outs(%27 : tensor<4x16x16xf32>) -> tensor<4x16x16xf32>
    %30 = linalg.generic {indexing_maps = [#map3, #map4, #map5], iterator_types = ["parallel", "parallel", "parallel", "reduction", "reduction", "reduction"]} ins(%collapsed_22, %cst_14 : tensor<3x66x66xf32>, tensor<4x3x4x4xf32>) outs(%29 : tensor<4x16x16xf32>) {
    ^bb0(%in: f32, %in_27: f32, %out: f32):
      %49 = arith.mulf %in, %in_27 : f32
      %50 = arith.addf %out, %49 : f32
      linalg.yield %50 : f32
    } -> tensor<4x16x16xf32>
    %31 = linalg.generic {indexing_maps = [#map2, #map2, #map2], iterator_types = ["parallel", "parallel", "parallel"]} ins(%30, %28 : tensor<4x16x16xf32>, tensor<4x16x16xf32>) outs(%29 : tensor<4x16x16xf32>) {
    ^bb0(%in: f32, %in_27: f32, %out: f32):
      %49 = arith.addf %in, %in_27 : f32
      linalg.yield %49 : f32
    } -> tensor<4x16x16xf32>
    %32 = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = ["parallel", "parallel", "parallel"]} ins(%31 : tensor<4x16x16xf32>) outs(%27 : tensor<4x16x16xf32>) {
    ^bb0(%in: f32, %out: f32):
      %49 = arith.cmpf ugt, %in, %cst_18 : f32
      %50 = arith.select %49, %in, %cst_18 : f32
      linalg.yield %50 : f32
    } -> tensor<4x16x16xf32>
    %33 = tensor.empty() : tensor<2x2xf32>
    %34 = tensor.empty() : tensor<4x8x8xf32>
    %35 = linalg.fill ins(%cst_11 : f32) outs(%34 : tensor<4x8x8xf32>) -> tensor<4x8x8xf32>
    %36 = linalg.generic {indexing_maps = [#map6, #map7, #map8], iterator_types = ["parallel", "parallel", "parallel", "reduction", "reduction"]} ins(%32, %33 : tensor<4x16x16xf32>, tensor<2x2xf32>) outs(%35 : tensor<4x8x8xf32>) {
    ^bb0(%in: f32, %in_27: f32, %out: f32):
      %49 = arith.maximumf %out, %in : f32
      linalg.yield %49 : f32
    } -> tensor<4x8x8xf32>
    %expanded_23 = tensor.expand_shape %36 [[0, 1], [2], [3]] output_shape [1, 4, 8, 8] : tensor<4x8x8xf32> into tensor<1x4x8x8xf32>
    %collapsed_24 = tensor.collapse_shape %expanded_23 [[0], [1, 2, 3]] : tensor<1x4x8x8xf32> into tensor<1x256xf32>
    %expanded_25 = tensor.expand_shape %collapsed_24 [[0], [1, 2, 3]] output_shape [1, 256, 1, 1] : tensor<1x256xf32> into tensor<1x256x1x1xf32>
    %__hoisted_tensor_1x256x8x1xf32 = util.global.load immutable @__hoisted_tensor_1x256x8x1xf32 : tensor<1x256x8x1xf32>
    %37 = tensor.empty() : tensor<1x1x1x8xf32>
    %38 = linalg.fill ins(%cst_18 : f32) outs(%37 : tensor<1x1x1x8xf32>) -> tensor<1x1x1x8xf32>
    %39 = linalg.mmt4d ins(%expanded_25, %__hoisted_tensor_1x256x8x1xf32 : tensor<1x256x1x1xf32>, tensor<1x256x8x1xf32>) outs(%38 : tensor<1x1x1x8xf32>) -> tensor<1x1x1x8xf32>
    %40 = tensor.empty() : tensor<1x2xf32>
    %unpack = linalg.unpack %39 outer_dims_perm = [0, 1] inner_dims_pos = [0, 1] inner_tiles = [1, 8] into %40 : tensor<1x1x1x8xf32> -> tensor<1x2xf32>
    %collapsed_26 = tensor.collapse_shape %unpack [[0, 1]] : tensor<1x2xf32> into tensor<2xf32>
    %41 = tensor.empty() : tensor<2xf32>
    %42 = linalg.generic {indexing_maps = [#map9, #map9, #map9], iterator_types = ["parallel"]} ins(%collapsed_26, %cst_12 : tensor<2xf32>, tensor<2xf32>) outs(%41 : tensor<2xf32>) {
    ^bb0(%in: f32, %in_27: f32, %out: f32):
      %49 = arith.addf %in, %in_27 : f32
      linalg.yield %49 : f32
    } -> tensor<2xf32>
    %43 = tensor.empty() : tensor<1x2x8x1xf32>
    %44 = scf.while (%arg6 = %2) : (tensor<2xf32>) -> tensor<2xf32> {
      %49 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst_3 : tensor<i64>) outs(%4 : tensor<f32>) {
      ^bb0(%in: i64, %out: f32):
        %89 = arith.index_cast %in : i64 to index
        %extracted_51 = tensor.extract %arg6[%89] : tensor<2xf32>
        linalg.yield %extracted_51 : f32
      } -> tensor<f32>
      %50 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst_2 : tensor<i64>) outs(%4 : tensor<f32>) {
      ^bb0(%in: i64, %out: f32):
        %89 = arith.index_cast %in : i64 to index
        %extracted_51 = tensor.extract %arg6[%89] : tensor<2xf32>
        linalg.yield %extracted_51 : f32
      } -> tensor<f32>
      %51 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%49 : tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %out: f32):
        %89 = math.cos %in : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %52 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%51, %cst_7 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.mulf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %53 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%49, %50 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.addf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %54 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%53 : tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %out: f32):
        %89 = math.cos %in : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %55 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%54, %cst_7 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.mulf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %56 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%52, %55 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.addf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %57 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%49 : tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %out: f32):
        %89 = math.sin %in : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %58 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%57, %cst_7 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.mulf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %59 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%53 : tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %out: f32):
        %89 = math.sin %in : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %60 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%59, %cst_7 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.mulf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %61 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%58, %60 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.addf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %expanded_27 = tensor.expand_shape %56 [] output_shape [1] : tensor<f32> into tensor<1xf32>
      %expanded_28 = tensor.expand_shape %61 [] output_shape [1] : tensor<f32> into tensor<1xf32>
      %concat_29 = tensor.concat dim(0) %expanded_27, %expanded_28 : (tensor<1xf32>, tensor<1xf32>) -> tensor<2xf32>
      %62 = linalg.generic {indexing_maps = [#map9, #map9, #map9], iterator_types = ["parallel"]} ins(%42, %concat_29 : tensor<2xf32>, tensor<2xf32>) outs(%41 : tensor<2xf32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.subf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<2xf32>
      %63 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%57, %cst_8 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.mulf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %64 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%63, %60 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.subf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %65 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%59, %cst_8 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.mulf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %expanded_30 = tensor.expand_shape %64 [] output_shape [1] : tensor<f32> into tensor<1xf32>
      %expanded_31 = tensor.expand_shape %65 [] output_shape [1] : tensor<f32> into tensor<1xf32>
      %concat_32 = tensor.concat dim(0) %expanded_30, %expanded_31 : (tensor<1xf32>, tensor<1xf32>) -> tensor<2xf32>
      %expanded_33 = tensor.expand_shape %55 [] output_shape [1] : tensor<f32> into tensor<1xf32>
      %concat_34 = tensor.concat dim(0) %expanded_27, %expanded_33 : (tensor<1xf32>, tensor<1xf32>) -> tensor<2xf32>
      %expanded_35 = tensor.expand_shape %concat_32 [[0, 1]] output_shape [1, 2] : tensor<2xf32> into tensor<1x2xf32>
      %expanded_36 = tensor.expand_shape %concat_34 [[0, 1]] output_shape [1, 2] : tensor<2xf32> into tensor<1x2xf32>
      %concat_37 = tensor.concat dim(0) %expanded_35, %expanded_36 : (tensor<1x2xf32>, tensor<1x2xf32>) -> tensor<2x2xf32>
      %66 = linalg.generic {indexing_maps = [#map10, #map9], iterator_types = ["parallel"]} ins(%cst_3 : tensor<i64>) outs(%41 : tensor<2xf32>) {
      ^bb0(%in: i64, %out: f32):
        %89 = arith.index_cast %in : i64 to index
        %90 = linalg.index 0 : index
        %extracted_51 = tensor.extract %concat_37[%89, %90] : tensor<2x2xf32>
        linalg.yield %extracted_51 : f32
      } -> tensor<2xf32>
      %67 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst_3 : tensor<i64>) outs(%4 : tensor<f32>) {
      ^bb0(%in: i64, %out: f32):
        %89 = arith.index_cast %in : i64 to index
        %extracted_51 = tensor.extract %66[%89] : tensor<2xf32>
        linalg.yield %extracted_51 : f32
      } -> tensor<f32>
      %expanded_38 = tensor.expand_shape %67 [] output_shape [1] : tensor<f32> into tensor<1xf32>
      %68 = linalg.generic {indexing_maps = [#map10, #map9], iterator_types = ["parallel"]} ins(%cst_2 : tensor<i64>) outs(%41 : tensor<2xf32>) {
      ^bb0(%in: i64, %out: f32):
        %89 = arith.index_cast %in : i64 to index
        %90 = linalg.index 0 : index
        %extracted_51 = tensor.extract %concat_37[%89, %90] : tensor<2x2xf32>
        linalg.yield %extracted_51 : f32
      } -> tensor<2xf32>
      %69 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst_2 : tensor<i64>) outs(%4 : tensor<f32>) {
      ^bb0(%in: i64, %out: f32):
        %89 = arith.index_cast %in : i64 to index
        %extracted_51 = tensor.extract %68[%89] : tensor<2xf32>
        linalg.yield %extracted_51 : f32
      } -> tensor<f32>
      %expanded_39 = tensor.expand_shape %69 [] output_shape [1] : tensor<f32> into tensor<1xf32>
      %70 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%67, %69 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.mulf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %71 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst_2 : tensor<i64>) outs(%4 : tensor<f32>) {
      ^bb0(%in: i64, %out: f32):
        %89 = arith.index_cast %in : i64 to index
        %extracted_51 = tensor.extract %66[%89] : tensor<2xf32>
        linalg.yield %extracted_51 : f32
      } -> tensor<f32>
      %72 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst_3 : tensor<i64>) outs(%4 : tensor<f32>) {
      ^bb0(%in: i64, %out: f32):
        %89 = arith.index_cast %in : i64 to index
        %extracted_51 = tensor.extract %68[%89] : tensor<2xf32>
        linalg.yield %extracted_51 : f32
      } -> tensor<f32>
      %73 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%71, %72 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.mulf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %74 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%70, %73 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.subf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %75 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%74, %cst_9 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.addf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %76 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%75 : tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %out: f32):
        %89 = arith.cmpf one, %in, %cst_18 : f32
        cf.assert %89, "unimplemented: tensor with zero element"
        %90 = arith.divf %cst_6, %in : f32
        linalg.yield %90 : f32
      } -> tensor<f32>
      %77 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%76, %cst_7 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.mulf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %78 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%71 : tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %out: f32):
        %89 = arith.negf %in : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %expanded_40 = tensor.expand_shape %78 [] output_shape [1] : tensor<f32> into tensor<1xf32>
      %concat_41 = tensor.concat dim(0) %expanded_39, %expanded_40 : (tensor<1xf32>, tensor<1xf32>) -> tensor<2xf32>
      %79 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%72 : tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %out: f32):
        %89 = arith.negf %in : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %expanded_42 = tensor.expand_shape %79 [] output_shape [1] : tensor<f32> into tensor<1xf32>
      %concat_43 = tensor.concat dim(0) %expanded_42, %expanded_38 : (tensor<1xf32>, tensor<1xf32>) -> tensor<2xf32>
      %expanded_44 = tensor.expand_shape %concat_41 [[0, 1]] output_shape [1, 2] : tensor<2xf32> into tensor<1x2xf32>
      %expanded_45 = tensor.expand_shape %concat_43 [[0, 1]] output_shape [1, 2] : tensor<2xf32> into tensor<1x2xf32>
      %concat_46 = tensor.concat dim(0) %expanded_44, %expanded_45 : (tensor<1x2xf32>, tensor<1x2xf32>) -> tensor<2x2xf32>
      %80 = linalg.generic {indexing_maps = [#map11, #map12, #map11], iterator_types = ["parallel", "parallel"]} ins(%concat_46, %77 : tensor<2x2xf32>, tensor<f32>) outs(%33 : tensor<2x2xf32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.mulf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<2x2xf32>
      %pack = linalg.pack %80 padding_value(%cst_18 : f32) outer_dims_perm = [0, 1] inner_dims_pos = [0, 1] inner_tiles = [8, 1] into %43 : tensor<2x2xf32> -> tensor<1x2x8x1xf32>
      %expanded_47 = tensor.expand_shape %62 [[0, 1, 2, 3]] output_shape [1, 2, 1, 1] : tensor<2xf32> into tensor<1x2x1x1xf32>
      %81 = linalg.mmt4d ins(%expanded_47, %pack : tensor<1x2x1x1xf32>, tensor<1x2x8x1xf32>) outs(%38 : tensor<1x1x1x8xf32>) -> tensor<1x1x1x8xf32>
      %collapsed_48 = tensor.collapse_shape %81 [[0, 1], [2, 3]] : tensor<1x1x1x8xf32> into tensor<1x8xf32>
      %unpack_49 = linalg.unpack %collapsed_48 outer_dims_perm = [0] inner_dims_pos = [0] inner_tiles = [8] into %41 : tensor<1x8xf32> -> tensor<2xf32>
      %82 = linalg.generic {indexing_maps = [#map9, #map10, #map9], iterator_types = ["parallel"]} ins(%unpack_49, %cst_10 : tensor<2xf32>, tensor<f32>) outs(%41 : tensor<2xf32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.mulf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<2xf32>
      %83 = linalg.generic {indexing_maps = [#map9, #map9, #map9], iterator_types = ["parallel"]} ins(%arg6, %82 : tensor<2xf32>, tensor<2xf32>) outs(%41 : tensor<2xf32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.addf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<2xf32>
      %84 = linalg.generic {indexing_maps = [#map9, #map9, #map9], iterator_types = ["parallel"]} ins(%83, %arg6 : tensor<2xf32>, tensor<2xf32>) outs(%41 : tensor<2xf32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.subf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<2xf32>
      %85 = linalg.generic {indexing_maps = [#map9, #map9, #map9], iterator_types = ["parallel"]} ins(%84, %84 : tensor<2xf32>, tensor<2xf32>) outs(%41 : tensor<2xf32>) {
      ^bb0(%in: f32, %in_51: f32, %out: f32):
        %89 = arith.mulf %in, %in_51 : f32
        linalg.yield %89 : f32
      } -> tensor<2xf32>
      %86 = linalg.fill ins(%cst_18 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
      %87 = linalg.generic {indexing_maps = [#map9, #map10], iterator_types = ["reduction"]} ins(%85 : tensor<2xf32>) outs(%86 : tensor<f32>) {
      ^bb0(%in: f32, %out: f32):
        %89 = arith.addf %in, %out : f32
        linalg.yield %89 : f32
      } -> tensor<f32>
      %extracted_50 = tensor.extract %87[] : tensor<f32>
      %88 = arith.cmpf ogt, %extracted_50, %cst_19 : f32
      scf.condition(%88) %83 : tensor<2xf32>
    } do {
    ^bb0(%arg6: tensor<2xf32>):
      scf.yield %arg6 : tensor<2xf32>
    }
    %extracted = tensor.extract %44[%c1] : tensor<2xf32>
    %45 = arith.cmpf ogt, %extracted, %cst_18 : f32
    %46 = scf.if %45 -> (tensor<19x2xf32>) {
      %49 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst_3 : tensor<i64>) outs(%4 : tensor<f32>) {
      ^bb0(%in: i64, %out: f32):
        %67 = arith.index_cast %in : i64 to index
        %extracted_30 = tensor.extract %44[%67] : tensor<2xf32>
        linalg.yield %extracted_30 : f32
      } -> tensor<f32>
      %50 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst_2 : tensor<i64>) outs(%4 : tensor<f32>) {
      ^bb0(%in: i64, %out: f32):
        %67 = arith.index_cast %in : i64 to index
        %extracted_30 = tensor.extract %44[%67] : tensor<2xf32>
        linalg.yield %extracted_30 : f32
      } -> tensor<f32>
      %51 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%49 : tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %out: f32):
        %67 = math.cos %in : f32
        linalg.yield %67 : f32
      } -> tensor<f32>
      %52 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%51, %cst_7 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_30: f32, %out: f32):
        %67 = arith.mulf %in, %in_30 : f32
        linalg.yield %67 : f32
      } -> tensor<f32>
      %53 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%49, %50 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_30: f32, %out: f32):
        %67 = arith.addf %in, %in_30 : f32
        linalg.yield %67 : f32
      } -> tensor<f32>
      %54 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%53 : tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %out: f32):
        %67 = math.cos %in : f32
        linalg.yield %67 : f32
      } -> tensor<f32>
      %55 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%54, %cst_7 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_30: f32, %out: f32):
        %67 = arith.mulf %in, %in_30 : f32
        linalg.yield %67 : f32
      } -> tensor<f32>
      %56 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%52, %55 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_30: f32, %out: f32):
        %67 = arith.addf %in, %in_30 : f32
        linalg.yield %67 : f32
      } -> tensor<f32>
      %57 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%49 : tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %out: f32):
        %67 = math.sin %in : f32
        linalg.yield %67 : f32
      } -> tensor<f32>
      %58 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%57, %cst_7 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_30: f32, %out: f32):
        %67 = arith.mulf %in, %in_30 : f32
        linalg.yield %67 : f32
      } -> tensor<f32>
      %59 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%53 : tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %out: f32):
        %67 = math.sin %in : f32
        linalg.yield %67 : f32
      } -> tensor<f32>
      %60 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%59, %cst_7 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_30: f32, %out: f32):
        %67 = arith.mulf %in, %in_30 : f32
        linalg.yield %67 : f32
      } -> tensor<f32>
      %61 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%58, %60 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_30: f32, %out: f32):
        %67 = arith.addf %in, %in_30 : f32
        linalg.yield %67 : f32
      } -> tensor<f32>
      %expanded_27 = tensor.expand_shape %56 [] output_shape [1] : tensor<f32> into tensor<1xf32>
      %expanded_28 = tensor.expand_shape %61 [] output_shape [1] : tensor<f32> into tensor<1xf32>
      %concat_29 = tensor.concat dim(0) %expanded_27, %expanded_28 : (tensor<1xf32>, tensor<1xf32>) -> tensor<2xf32>
      %62 = linalg.generic {indexing_maps = [#map9, #map9, #map9], iterator_types = ["parallel"]} ins(%concat_29, %cst_5 : tensor<2xf32>, tensor<2xf32>) outs(%41 : tensor<2xf32>) {
      ^bb0(%in: f32, %in_30: f32, %out: f32):
        %67 = arith.addf %in, %in_30 : f32
        linalg.yield %67 : f32
      } -> tensor<2xf32>
      %63 = tensor.empty() : tensor<10x2xf32>
      %64 = linalg.generic {indexing_maps = [#map13, #map14, #map11], iterator_types = ["parallel", "parallel"]} ins(%concat_29, %cst_0 : tensor<2xf32>, tensor<10xf32>) outs(%63 : tensor<10x2xf32>) {
      ^bb0(%in: f32, %in_30: f32, %out: f32):
        %67 = arith.mulf %in, %in_30 : f32
        linalg.yield %67 : f32
      } -> tensor<10x2xf32>
      %65 = linalg.generic {indexing_maps = [#map13, #map14, #map11], iterator_types = ["parallel", "parallel"]} ins(%62, %cst : tensor<2xf32>, tensor<10xf32>) outs(%63 : tensor<10x2xf32>) {
      ^bb0(%in: f32, %in_30: f32, %out: f32):
        %67 = arith.mulf %in, %in_30 : f32
        linalg.yield %67 : f32
      } -> tensor<10x2xf32>
      %66 = linalg.generic {indexing_maps = [#map11, #map11, #map11], iterator_types = ["parallel", "parallel"]} ins(%64, %65 : tensor<10x2xf32>, tensor<10x2xf32>) outs(%63 : tensor<10x2xf32>) {
      ^bb0(%in: f32, %in_30: f32, %out: f32):
        %67 = arith.addf %in, %in_30 : f32
        linalg.yield %67 : f32
      } -> tensor<10x2xf32>
      %padded = tensor.pad %66 low[0, 0] high[9, 0] {
      ^bb0(%arg6: index, %arg7: index):
        tensor.yield %cst_18 : f32
      } : tensor<10x2xf32> to tensor<19x2xf32>
      scf.yield %padded : tensor<19x2xf32>
    } else {
      %49 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst_3 : tensor<i64>) outs(%4 : tensor<f32>) {
      ^bb0(%in: i64, %out: f32):
        %71 = arith.index_cast %in : i64 to index
        %extracted_31 = tensor.extract %44[%71] : tensor<2xf32>
        linalg.yield %extracted_31 : f32
      } -> tensor<f32>
      %50 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%cst_2 : tensor<i64>) outs(%4 : tensor<f32>) {
      ^bb0(%in: i64, %out: f32):
        %71 = arith.index_cast %in : i64 to index
        %extracted_31 = tensor.extract %44[%71] : tensor<2xf32>
        linalg.yield %extracted_31 : f32
      } -> tensor<f32>
      %51 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%49 : tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %out: f32):
        %71 = math.cos %in : f32
        linalg.yield %71 : f32
      } -> tensor<f32>
      %52 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%51, %cst_7 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_31: f32, %out: f32):
        %71 = arith.mulf %in, %in_31 : f32
        linalg.yield %71 : f32
      } -> tensor<f32>
      %53 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%49, %50 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_31: f32, %out: f32):
        %71 = arith.addf %in, %in_31 : f32
        linalg.yield %71 : f32
      } -> tensor<f32>
      %54 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%53 : tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %out: f32):
        %71 = math.cos %in : f32
        linalg.yield %71 : f32
      } -> tensor<f32>
      %55 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%54, %cst_7 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_31: f32, %out: f32):
        %71 = arith.mulf %in, %in_31 : f32
        linalg.yield %71 : f32
      } -> tensor<f32>
      %56 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%52, %55 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_31: f32, %out: f32):
        %71 = arith.addf %in, %in_31 : f32
        linalg.yield %71 : f32
      } -> tensor<f32>
      %57 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%49 : tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %out: f32):
        %71 = math.sin %in : f32
        linalg.yield %71 : f32
      } -> tensor<f32>
      %58 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%57, %cst_7 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_31: f32, %out: f32):
        %71 = arith.mulf %in, %in_31 : f32
        linalg.yield %71 : f32
      } -> tensor<f32>
      %59 = linalg.generic {indexing_maps = [#map, #map], iterator_types = []} ins(%53 : tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %out: f32):
        %71 = math.sin %in : f32
        linalg.yield %71 : f32
      } -> tensor<f32>
      %60 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%59, %cst_7 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_31: f32, %out: f32):
        %71 = arith.mulf %in, %in_31 : f32
        linalg.yield %71 : f32
      } -> tensor<f32>
      %61 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = []} ins(%58, %60 : tensor<f32>, tensor<f32>) outs(%4 : tensor<f32>) {
      ^bb0(%in: f32, %in_31: f32, %out: f32):
        %71 = arith.addf %in, %in_31 : f32
        linalg.yield %71 : f32
      } -> tensor<f32>
      %expanded_27 = tensor.expand_shape %56 [] output_shape [1] : tensor<f32> into tensor<1xf32>
      %expanded_28 = tensor.expand_shape %61 [] output_shape [1] : tensor<f32> into tensor<1xf32>
      %concat_29 = tensor.concat dim(0) %expanded_27, %expanded_28 : (tensor<1xf32>, tensor<1xf32>) -> tensor<2xf32>
      %62 = linalg.generic {indexing_maps = [#map9, #map9, #map9], iterator_types = ["parallel"]} ins(%concat_29, %cst_4 : tensor<2xf32>, tensor<2xf32>) outs(%41 : tensor<2xf32>) {
      ^bb0(%in: f32, %in_31: f32, %out: f32):
        %71 = arith.addf %in, %in_31 : f32
        linalg.yield %71 : f32
      } -> tensor<2xf32>
      %63 = linalg.generic {indexing_maps = [#map9, #map9, #map9], iterator_types = ["parallel"]} ins(%62, %cst_5 : tensor<2xf32>, tensor<2xf32>) outs(%41 : tensor<2xf32>) {
      ^bb0(%in: f32, %in_31: f32, %out: f32):
        %71 = arith.addf %in, %in_31 : f32
        linalg.yield %71 : f32
      } -> tensor<2xf32>
      %64 = tensor.empty() : tensor<10x2xf32>
      %65 = linalg.generic {indexing_maps = [#map13, #map14, #map11], iterator_types = ["parallel", "parallel"]} ins(%concat_29, %cst_0 : tensor<2xf32>, tensor<10xf32>) outs(%64 : tensor<10x2xf32>) {
      ^bb0(%in: f32, %in_31: f32, %out: f32):
        %71 = arith.mulf %in, %in_31 : f32
        linalg.yield %71 : f32
      } -> tensor<10x2xf32>
      %66 = linalg.generic {indexing_maps = [#map13, #map14, #map11], iterator_types = ["parallel", "parallel"]} ins(%62, %cst : tensor<2xf32>, tensor<10xf32>) outs(%64 : tensor<10x2xf32>) {
      ^bb0(%in: f32, %in_31: f32, %out: f32):
        %71 = arith.mulf %in, %in_31 : f32
        linalg.yield %71 : f32
      } -> tensor<10x2xf32>
      %67 = linalg.generic {indexing_maps = [#map11, #map11, #map11], iterator_types = ["parallel", "parallel"]} ins(%65, %66 : tensor<10x2xf32>, tensor<10x2xf32>) outs(%64 : tensor<10x2xf32>) {
      ^bb0(%in: f32, %in_31: f32, %out: f32):
        %71 = arith.addf %in, %in_31 : f32
        linalg.yield %71 : f32
      } -> tensor<10x2xf32>
      %68 = linalg.generic {indexing_maps = [#map13, #map14, #map11], iterator_types = ["parallel", "parallel"]} ins(%62, %cst_0 : tensor<2xf32>, tensor<10xf32>) outs(%64 : tensor<10x2xf32>) {
      ^bb0(%in: f32, %in_31: f32, %out: f32):
        %71 = arith.mulf %in, %in_31 : f32
        linalg.yield %71 : f32
      } -> tensor<10x2xf32>
      %69 = linalg.generic {indexing_maps = [#map13, #map14, #map11], iterator_types = ["parallel", "parallel"]} ins(%63, %cst : tensor<2xf32>, tensor<10xf32>) outs(%64 : tensor<10x2xf32>) {
      ^bb0(%in: f32, %in_31: f32, %out: f32):
        %71 = arith.mulf %in, %in_31 : f32
        linalg.yield %71 : f32
      } -> tensor<10x2xf32>
      %70 = linalg.generic {indexing_maps = [#map11, #map11, #map11], iterator_types = ["parallel", "parallel"]} ins(%68, %69 : tensor<10x2xf32>, tensor<10x2xf32>) outs(%64 : tensor<10x2xf32>) {
      ^bb0(%in: f32, %in_31: f32, %out: f32):
        %71 = arith.addf %in, %in_31 : f32
        linalg.yield %71 : f32
      } -> tensor<10x2xf32>
      %extracted_slice = tensor.extract_slice %70[1, 0] [9, 2] [1, 1] : tensor<10x2xf32> to tensor<9x2xf32>
      %concat_30 = tensor.concat dim(0) %67, %extracted_slice : (tensor<10x2xf32>, tensor<9x2xf32>) -> tensor<19x2xf32>
      scf.yield %concat_30 : tensor<19x2xf32>
    }
    %47 = hal.tensor.export %46 : tensor<19x2xf32> -> !hal.buffer_view
    %48 = hal.tensor.export %concat : tensor<3xf32> -> !hal.buffer_view
    hal.fence.signal<%arg5 : !hal.fence>
    util.return %47, %48 : !hal.buffer_view, !hal.buffer_view
  }
  util.func public @main(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view, %arg3: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %1:2 = util.call @_main$async(%arg0, %arg1, %arg2, %arg3, %0, %fence) : (!hal.buffer_view, !hal.buffer_view, !hal.buffer_view, !hal.buffer_view, !hal.fence, !hal.fence) -> (!hal.buffer_view, !hal.buffer_view)
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) flags("None") : i32
    util.return %1#0, %1#1 : !hal.buffer_view, !hal.buffer_view
  }
}
