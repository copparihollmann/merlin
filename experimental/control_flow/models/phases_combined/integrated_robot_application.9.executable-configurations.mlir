#composite_of_8960b = #util.composite<8960xi8, [
    dense<"0xC33C46BD233AA03D8760E43DF310A3BD92EA023DDFFFFA3C3FB59CBDD67822BD657B11BE2E4CEEBD428D22BD46CC993D3076D8BD25E617BD5F6A8EBB21A71F3C9F7F80BD92A3FE3C723613BE77408B3D9251B13DCF91BFBDF9DA743DD64527BD639EB5BD68CB173D33409E3C1DDFA7BD3723DD3DF05AB9BCF96FE7BD4299C63D030B3A3D238ABBBCFBF0BC3D746E953DA3624D3C3065213D6DF8993CBF1588BD5115883C8A42E53AF62D0EBEE6A0DFBB684E123D953A313DE3220C3C419B17BDECF9CBBD0A518DBD93F5E6BDD33064BD1122F63D7F91023E7F8CE83D0153A63C624B9C3D96D3FABDCCB00DBE49488CBD147795BD5FDA12BE5B0F4BBD569FE0BD372A103E5ACA8A3CA12B9A3CC949703DE500CEBD67CBA9BDD8E8703B36DE92BDBCD80DBD0133053E6302033D8483A13B5FC4D5BDE3670CBECF09723DB12606BEC171D23DC993023C82E1B9BD8AA9AA3C39B0ABBDABC471BD29BC033EB5F06E3D80E30E3E8DA7D73DC70D5B3DD7F9F23BC14F93BD3EA8E33DF6F293BD558AE63C0F46CCBDBE2712BED055373D7EF5A73DF061B93DA792ACBD4CF702BECD2E18BC8B5EF4BDC4FABB3DC56C0A3EBC5FDA3DF4B408BDCBD6E43C765E0C3E42B85CBDC5C6FBBCBA5EF5BDE549E6BD347B0DBC83BF0EBD746DA4BC84A2E2BD36A4BEBDFC0CDDBDB05E7EBDCBB8383DD8B091BD4EDBB23C5A70F3BDE7B5D83D37E6AD3C68940EBD3AF579BDDB8E083EC064CC3CD7CAA43C15972B3DC6F16FBD35E99B3D4526683DD97395BA82FFF43C85E6C3BC93B88D3DFC2BA3BD30A6CE3D36411FBD80D71BBD33695B3DA7BAAC3DFC17E03D9B5C13BDFCF6D1BC3B988A3C185DE0BDC3ED03BC0B99993D9E1E613DA0DB2D3D3889FE3D9BB8AF3CF375AE3D68C98FBDDF570FBE3CB1063ED6D8693D89BDD93D5C95C73DFD9CAA3D3CA5033C1DCC9BBCD18D9C3CD8AC16BCF22914BD162811BDD87B67BD87DFBFBDD74EDBBCB6346FBD6B688BBD566703BE03ECE53D18578FBD7EA4D9BDF9FF05BE8B083CBD2786BCBD64FEF6BC8114093DB4D0F23BBD513DBC3D56E13C81CF82BC2F18FB3DD6AC58BD"> : tensor<4x3x4x4xf32>,
    dense<"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tensor<1x256x8x1xf32>,
]>
#config = #iree_cpu.lowering_config<>
#config1 = #iree_cpu.lowering_config<distribution = []>
#config2 = #iree_cpu.lowering_config<distribution = [3, 32, 64], vector_common_parallel = [1, 1, 8]>
#config3 = #iree_cpu.lowering_config<vector_common_parallel = [1, 1, 8]>
#config4 = #iree_cpu.lowering_config<distribution = [1, 4, 16, 0, 0, 0], vector_common_parallel = [1, 1, 8, 0, 0, 0], vector_reduction = [0, 0, 0, 1, 1, 8]>
#config5 = #iree_cpu.lowering_config<distribution = [1, 2, 8, 0, 0], vector_common_parallel = [1, 1, 8, 0, 0], vector_reduction = [0, 0, 0, 1, 8]>
#config6 = #iree_cpu.lowering_config<vector_common_parallel = [1, 1, 0, 1]>
#config7 = #iree_cpu.lowering_config<distribution = [1, 1, 0, 0, 0, 0], vector_common_parallel = [1, 1, 0, 1, 8, 0], vector_reduction = [0, 0, 1, 0, 0, 1]>
#config8 = #iree_cpu.lowering_config<distribution = [1, 8], vector_common_parallel = [1, 8]>
#config9 = #iree_cpu.lowering_config<distribution = [0, 2], vector_common_parallel = [1, 8]>
#config10 = #iree_cpu.lowering_config<distribution = [2], vector_common_parallel = [8]>
#config11 = #iree_cpu.lowering_config<distribution = [8, 2], vector_common_parallel = [8, 1]>
#config12 = #iree_cpu.lowering_config<vector_common_parallel = [1, 1]>
#config13 = #iree_cpu.lowering_config<vector_common_parallel = [8]>
#config14 = #iree_cpu.lowering_config<distribution = [8], vector_common_parallel = [8]>
#config15 = #iree_cpu.lowering_config<distribution = [0], vector_reduction = [8]>
#config16 = #iree_cpu.lowering_config<distribution = [10, 2], vector_common_parallel = [1, 8]>
#config17 = #iree_cpu.lowering_config<distribution = [9, 2], vector_common_parallel = [1, 8]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "znver3", cpu_features = "+prfchw,-cldemote,+avx,+aes,+sahf,+pclmul,-xop,+crc32,-amx-fp8,+xsaves,-avx512fp16,-usermsr,-sm4,-egpr,+sse4.1,-avx512ifma,+xsave,+sse4.2,-tsxldtrk,-sm3,-ptwrite,-widekl,-movrs,+invpcid,+64bit,+xsavec,-avx10.1-512,-avx512vpopcntdq,+cmov,-avx512vp2intersect,-avx512cd,+movbe,-avxvnniint8,-ccmp,-amx-int8,-kl,-avx10.1-256,-sha512,-avxvnni,-rtm,+adx,+avx2,-hreset,-movdiri,-serialize,+vpclmulqdq,-avx512vl,-uintr,-cf,+clflushopt,-raoint,-cmpccxadd,+bmi,-amx-tile,+sse,-avx10.2-256,-gfni,-avxvnniint16,-amx-fp16,-zu,-ndd,+xsaveopt,+rdrnd,-avx512f,-amx-bf16,-avx512bf16,-avx512vnni,-push2pop2,+cx8,-avx512bw,+sse3,+pku,-nf,-amx-tf32,-amx-avx512,+fsgsbase,+clzero,+mwaitx,-lwp,+lzcnt,+sha,-movdir64b,-ppx,+wbnoinvd,-enqcmd,-amx-transpose,-avx10.2-512,-avxneconvert,-tbm,-pconfig,-amx-complex,+ssse3,+cx16,+bmi2,+fma,+popcnt,-avxifma,+f16c,-avx512bitalg,+rdpru,+clwb,+mmx,+sse2,+rdseed,-avx512vbmi2,-prefetchi,-amx-movrs,+rdpid,-fma4,-avx512vbmi,+shstk,+vaes,-waitpkg,-sgx,+fxsr,-avx512dq,+sse4a", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 32 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<() -> ()>
#map1 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d3, d1 * 4 + d4, d2 * 4 + d5)>
#map3 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d3, d4, d5)>
#map4 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d2)>
#map5 = affine_map<(d0, d1, d2) -> (d0)>
#map6 = affine_map<(d0, d1, d2, d3, d4) -> (d0, d1 * 2 + d3, d2 * 2 + d4)>
#map7 = affine_map<(d0, d1, d2, d3, d4) -> (d3, d4)>
#map8 = affine_map<(d0, d1, d2, d3, d4) -> (d0, d1, d2)>
#map9 = affine_map<(d0, d1) -> (d0, d1)>
#map10 = affine_map<(d0) -> (d0)>
#map11 = affine_map<(d0) -> ()>
#map12 = affine_map<(d0, d1) -> (d1)>
#map13 = affine_map<(d0, d1) -> (d0)>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#pipeline_layout1 = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#pipeline_layout2 = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, ReadOnly>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#pipeline_layout3 = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#translation = #iree_codegen.translation_info<pipeline = CPUDefault>
#translation1 = #iree_codegen.translation_info<pipeline = CPUBufferOpsTileAndVectorize>
#translation2 = #iree_codegen.translation_info<pipeline = CPUDoubleTilingExpert>
#translation3 = #iree_codegen.translation_info<pipeline = Mmt4dTilingExpert>
#translation4 = #iree_codegen.translation_info<pipeline = CPUDataTiling>
#translation5 = #iree_codegen.translation_info<pipeline = CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module @integrated_robot_application attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  util.func public @main$async(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view, %arg3: !hal.buffer_view, %arg4: !hal.fence, %arg5: !hal.fence, %arg6: !hal.fence, %arg7: !hal.fence) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "async func @main$async(%input0: !hal.buffer_view, %input1: !hal.buffer_view, %input2: !hal.buffer_view, %input3: !hal.buffer_view, %input4: !hal.fence, %input5: !hal.fence) -> (%output0: !hal.buffer_view, %output1: !hal.buffer_view)", iree.abi.model = "coarse-fences"}} {
    %0:2 = util.call @_main$async(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5) : (!hal.buffer_view, !hal.buffer_view, !hal.buffer_view, !hal.buffer_view, !hal.fence, !hal.fence) -> (!hal.buffer_view, !hal.buffer_view)
    hal.fence.signal<%arg7 : !hal.fence>
    util.return %0#0, %0#1 : !hal.buffer_view, !hal.buffer_view
  }
  hal.executable private @_main$async_dispatch_0 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_0_elementwise_broadcast ordinal(0) layout(#pipeline_layout) count(%arg0: !hal.device) -> (index, index, index) {
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_0_elementwise_broadcast() attributes {translation_info = #translation} {
          %c0 = arith.constant 0 : index
          %cst = arith.constant 9.99999974E-5 : f32
          %cst_0 = arith.constant 2.000000e+00 : f32
          %cst_1 = arith.constant 5.000000e-01 : f32
          %c1 = arith.constant 1 : index
          %c2 = arith.constant 2 : index
          %c8 = arith.constant 8 : index
          %c4 = arith.constant 4 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<3xf32>>
          %2 = hal.interface.binding.subspan layout(#pipeline_layout) binding(2) alignment(64) offset(%c8) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          %3 = hal.interface.binding.subspan layout(#pipeline_layout) binding(3) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          %4 = hal.interface.binding.subspan layout(#pipeline_layout) binding(4) alignment(64) offset(%c4) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          %5 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %6 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [3], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<3xf32>> -> tensor<3xf32>
          %7 = tensor.empty() : tensor<f32>
          %8 = linalg.generic {indexing_maps = [#map], iterator_types = []} outs(%7 : tensor<f32>) attrs =  {lowering_config = #config} {
          ^bb0(%out: f32):
            %extracted = tensor.extract %5[%c0] : tensor<2xf32>
            %11 = arith.mulf %extracted, %cst : f32
            %extracted_2 = tensor.extract %5[%c1] : tensor<2xf32>
            %12 = arith.mulf %extracted_2, %cst : f32
            %13 = arith.addf %11, %12 : f32
            %14 = arith.divf %13, %cst_0 : f32
            %15 = arith.subf %12, %11 : f32
            %16 = arith.divf %15, %cst_1 : f32
            %extracted_3 = tensor.extract %6[%c2] : tensor<3xf32>
            %17 = arith.divf %16, %cst_0 : f32
            %18 = arith.addf %extracted_3, %17 : f32
            %19 = math.sin %18 : f32
            %20 = arith.mulf %14, %19 : f32
            %extracted_4 = tensor.extract %6[%c1] : tensor<3xf32>
            %21 = arith.addf %extracted_4, %20 : f32
            linalg.yield %21 : f32
          } -> tensor<f32>
          %9 = linalg.generic {indexing_maps = [#map], iterator_types = []} outs(%7 : tensor<f32>) attrs =  {lowering_config = #config} {
          ^bb0(%out: f32):
            %extracted = tensor.extract %5[%c0] : tensor<2xf32>
            %11 = arith.mulf %extracted, %cst : f32
            %extracted_2 = tensor.extract %5[%c1] : tensor<2xf32>
            %12 = arith.mulf %extracted_2, %cst : f32
            %13 = arith.addf %11, %12 : f32
            %14 = arith.divf %13, %cst_0 : f32
            %15 = arith.subf %12, %11 : f32
            %16 = arith.divf %15, %cst_1 : f32
            %extracted_3 = tensor.extract %6[%c2] : tensor<3xf32>
            %17 = arith.divf %16, %cst_0 : f32
            %18 = arith.addf %extracted_3, %17 : f32
            %19 = math.cos %18 : f32
            %20 = arith.mulf %14, %19 : f32
            %extracted_4 = tensor.extract %6[%c0] : tensor<3xf32>
            %21 = arith.addf %extracted_4, %20 : f32
            linalg.yield %21 : f32
          } -> tensor<f32>
          %10 = linalg.generic {indexing_maps = [#map], iterator_types = []} outs(%7 : tensor<f32>) attrs =  {lowering_config = #config1} {
          ^bb0(%out: f32):
            %extracted = tensor.extract %5[%c0] : tensor<2xf32>
            %11 = arith.mulf %extracted, %cst : f32
            %extracted_2 = tensor.extract %5[%c1] : tensor<2xf32>
            %12 = arith.mulf %extracted_2, %cst : f32
            %13 = arith.subf %12, %11 : f32
            %14 = arith.divf %13, %cst_1 : f32
            %extracted_3 = tensor.extract %6[%c2] : tensor<3xf32>
            %15 = arith.addf %extracted_3, %14 : f32
            linalg.yield %15 : f32
          } -> tensor<f32>
          iree_tensor_ext.dispatch.tensor.store %10, %2, offsets = [], sizes = [], strides = [] : tensor<f32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          iree_tensor_ext.dispatch.tensor.store %9, %3, offsets = [], sizes = [], strides = [] : tensor<f32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          iree_tensor_ext.dispatch.tensor.store %8, %4, offsets = [], sizes = [], strides = [] : tensor<f32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_1 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_1_slow_memcpy ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_1_slow_memcpy() attributes {translation_info = #translation1} {
          %c0 = arith.constant 0 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<3x64x64xf32>
          %assume_align = memref.assume_alignment %0, 64 : memref<3x64x64xf32>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<1x3x66x66xf32>
          %assume_align_0 = memref.assume_alignment %1, 64 : memref<1x3x66x66xf32>
          %subview = memref.subview %assume_align_0[0, 0, 1, 1] [1, 3, 64, 64] [1, 1, 1, 1] : memref<1x3x66x66xf32> to memref<3x64x64xf32, strided<[4356, 66, 1], offset: 67>>
          linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel", "parallel"]} ins(%assume_align : memref<3x64x64xf32>) outs(%subview : memref<3x64x64xf32, strided<[4356, 66, 1], offset: 67>>) attrs =  {lowering_config = #config2} {
          ^bb0(%in: f32, %out: f32):
            linalg.yield %in : f32
          }
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_2 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_2_conv_4x16x16x3x4x4_f32 ordinal(0) layout(#pipeline_layout2) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_2_conv_4x16x16x3x4x4_f32() attributes {translation_info = #translation2} {
          %cst = arith.constant 0.000000e+00 : f32
          %cst_0 = arith.constant dense<[0.0602946617, 0.0679099783, 0.0262253601, 2.52176687E-4]> : tensor<4xf32>
          %c0 = arith.constant 0 : index
          %c52288 = arith.constant 52288 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout2) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<3x66x66xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout2) binding(1) alignment(64) offset(%c0) flags(ReadOnly) : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x3x4x4xf32>>
          %2 = hal.interface.binding.subspan layout(#pipeline_layout2) binding(2) alignment(64) offset(%c52288) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x16xf32>>
          %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [3, 66, 66], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<3x66x66xf32>> -> tensor<3x66x66xf32>
          %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0, 0], sizes = [4, 3, 4, 4], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x3x4x4xf32>> -> tensor<4x3x4x4xf32>
          %5 = tensor.empty() : tensor<4x16x16xf32>
          %6 = linalg.fill {lowering_config = #config3} ins(%cst : f32) outs(%5 : tensor<4x16x16xf32>) -> tensor<4x16x16xf32>
          %7 = linalg.generic {indexing_maps = [#map2, #map3, #map4], iterator_types = ["parallel", "parallel", "parallel", "reduction", "reduction", "reduction"]} ins(%3, %4 : tensor<3x66x66xf32>, tensor<4x3x4x4xf32>) outs(%6 : tensor<4x16x16xf32>) attrs =  {lowering_config = #config4} {
          ^bb0(%in: f32, %in_1: f32, %out: f32):
            %9 = arith.mulf %in, %in_1 : f32
            %10 = arith.addf %out, %9 : f32
            linalg.yield %10 : f32
          } -> tensor<4x16x16xf32>
          %8 = linalg.generic {indexing_maps = [#map1, #map5, #map1], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7, %cst_0 : tensor<4x16x16xf32>, tensor<4xf32>) outs(%5 : tensor<4x16x16xf32>) attrs =  {lowering_config = #config3} {
          ^bb0(%in: f32, %in_1: f32, %out: f32):
            %9 = arith.addf %in, %in_1 : f32
            %10 = arith.cmpf ugt, %9, %cst : f32
            %11 = arith.select %10, %9, %cst : f32
            linalg.yield %11 : f32
          } -> tensor<4x16x16xf32>
          iree_tensor_ext.dispatch.tensor.store %8, %2, offsets = [0, 0, 0], sizes = [4, 16, 16], strides = [1, 1, 1] : tensor<4x16x16xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x16xf32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_3 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_3_conv_4x8x8x2x2_f32 ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_3_conv_4x8x8x2x2_f32() attributes {translation_info = #translation2} {
          %cst = arith.constant 0xFF800000 : f32
          %c52288 = arith.constant 52288 : index
          %c0 = arith.constant 0 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c52288) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x16xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x8x8xf32>>
          %2 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 16], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x16xf32>> -> tensor<4x16x16xf32>
          %3 = tensor.empty() : tensor<4x8x8xf32>
          %4 = tensor.empty() : tensor<2x2xf32>
          %5 = linalg.fill {lowering_config = #config3} ins(%cst : f32) outs(%3 : tensor<4x8x8xf32>) -> tensor<4x8x8xf32>
          %6 = linalg.generic {indexing_maps = [#map6, #map7, #map8], iterator_types = ["parallel", "parallel", "parallel", "reduction", "reduction"]} ins(%2, %4 : tensor<4x16x16xf32>, tensor<2x2xf32>) outs(%5 : tensor<4x8x8xf32>) attrs =  {lowering_config = #config5} {
          ^bb0(%in: f32, %in_0: f32, %out: f32):
            %7 = arith.maximumf %out, %in : f32
            linalg.yield %7 : f32
          } -> tensor<4x8x8xf32>
          iree_tensor_ext.dispatch.tensor.store %6, %1, offsets = [0, 0, 0], sizes = [4, 8, 8], strides = [1, 1, 1] : tensor<4x8x8xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x8x8xf32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_4 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_4_mmt4d_1x1x256x1x8x1_f32 ordinal(0) layout(#pipeline_layout2) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_4_mmt4d_1x1x256x1x8x1_f32() attributes {translation_info = #translation3} {
          %cst = arith.constant 0.000000e+00 : f32
          %c0 = arith.constant 0 : index
          %c768 = arith.constant 768 : index
          %c1024 = arith.constant 1024 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout2) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x256x1x1xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout2) binding(1) alignment(64) offset(%c768) flags(ReadOnly) : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x256x8x1xf32>>
          %2 = hal.interface.binding.subspan layout(#pipeline_layout2) binding(2) alignment(64) offset(%c1024) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<1x1x1x8xf32>>
          %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0, 0], sizes = [1, 256, 1, 1], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x256x1x1xf32>> -> tensor<1x256x1x1xf32>
          %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0, 0], sizes = [1, 256, 8, 1], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x256x8x1xf32>> -> tensor<1x256x8x1xf32>
          %5 = tensor.empty() : tensor<1x1x1x8xf32>
          %6 = linalg.fill {lowering_config = #config6} ins(%cst : f32) outs(%5 : tensor<1x1x1x8xf32>) -> tensor<1x1x1x8xf32>
          %7 = linalg.mmt4d {lowering_config = #config7} ins(%3, %4 : tensor<1x256x1x1xf32>, tensor<1x256x8x1xf32>) outs(%6 : tensor<1x1x1x8xf32>) -> tensor<1x1x1x8xf32>
          iree_tensor_ext.dispatch.tensor.store %7, %2, offsets = [0, 0, 0, 0], sizes = [1, 1, 1, 8], strides = [1, 1, 1, 1] : tensor<1x1x1x8xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<1x1x1x8xf32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_5 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_5_unpack_f32 ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_5_unpack_f32() attributes {translation_info = #translation4} {
          %c1024 = arith.constant 1024 : index
          %c0 = arith.constant 0 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c1024) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x1x1x8xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<1x2xf32>>
          %2 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0, 0], sizes = [1, 1, 1, 8], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x1x1x8xf32>> -> tensor<1x1x1x8xf32>
          %3 = tensor.empty() : tensor<1x2xf32>
          %unpack = linalg.unpack %2 outer_dims_perm = [0, 1] inner_dims_pos = [0, 1] inner_tiles = [1, 8] into %3 {lowering_config = #config8} : tensor<1x1x1x8xf32> -> tensor<1x2xf32>
          iree_tensor_ext.dispatch.tensor.store %unpack, %1, offsets = [0, 0], sizes = [1, 2], strides = [1, 1] : tensor<1x2xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<1x2xf32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_6 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_6_elementwise_broadcast ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_6_elementwise_broadcast() attributes {translation_info = #translation} {
          %c0 = arith.constant 0 : index
          %c1 = arith.constant 1 : index
          %c68 = arith.constant 68 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c68) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          %2 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %3 = tensor.empty() : tensor<f32>
          %4 = linalg.generic {indexing_maps = [#map], iterator_types = []} outs(%3 : tensor<f32>) attrs =  {lowering_config = #config1} {
          ^bb0(%out: f32):
            %extracted = tensor.extract %2[%c0] : tensor<2xf32>
            %extracted_0 = tensor.extract %2[%c1] : tensor<2xf32>
            %5 = arith.addf %extracted, %extracted_0 : f32
            %6 = math.cos %5 : f32
            linalg.yield %6 : f32
          } -> tensor<f32>
          iree_tensor_ext.dispatch.tensor.store %4, %1, offsets = [], sizes = [], strides = [] : tensor<f32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_7 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_7_elementwise_broadcast ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_7_elementwise_broadcast() attributes {translation_info = #translation} {
          %c0 = arith.constant 0 : index
          %c1 = arith.constant 1 : index
          %c64 = arith.constant 64 : index
          %c128 = arith.constant 128 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c64) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c128) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          %2 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %3 = tensor.empty() : tensor<f32>
          %4 = linalg.generic {indexing_maps = [#map], iterator_types = []} outs(%3 : tensor<f32>) attrs =  {lowering_config = #config1} {
          ^bb0(%out: f32):
            %extracted = tensor.extract %2[%c0] : tensor<2xf32>
            %extracted_0 = tensor.extract %2[%c1] : tensor<2xf32>
            %5 = arith.addf %extracted, %extracted_0 : f32
            %6 = math.cos %5 : f32
            %7 = math.cos %extracted : f32
            %8 = arith.addf %7, %6 : f32
            linalg.yield %8 : f32
          } -> tensor<f32>
          iree_tensor_ext.dispatch.tensor.store %4, %1, offsets = [], sizes = [], strides = [] : tensor<f32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_8 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_8_elementwise_broadcast ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_8_elementwise_broadcast() attributes {translation_info = #translation} {
          %c0 = arith.constant 0 : index
          %c1 = arith.constant 1 : index
          %c132 = arith.constant 132 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c132) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          %2 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %3 = tensor.empty() : tensor<f32>
          %4 = linalg.generic {indexing_maps = [#map], iterator_types = []} outs(%3 : tensor<f32>) attrs =  {lowering_config = #config1} {
          ^bb0(%out: f32):
            %extracted = tensor.extract %2[%c0] : tensor<2xf32>
            %extracted_0 = tensor.extract %2[%c1] : tensor<2xf32>
            %5 = arith.addf %extracted, %extracted_0 : f32
            %6 = math.sin %5 : f32
            %7 = math.sin %extracted : f32
            %8 = arith.addf %7, %6 : f32
            linalg.yield %8 : f32
          } -> tensor<f32>
          iree_tensor_ext.dispatch.tensor.store %4, %1, offsets = [], sizes = [], strides = [] : tensor<f32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_9 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_9_elementwise_1x2_f32 ordinal(0) layout(#pipeline_layout3) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_9_elementwise_1x2_f32() attributes {translation_info = #translation2} {
          %cst = arith.constant dense<[[0.036251314, -0.0380887836]]> : tensor<1x2xf32>
          %c0 = arith.constant 0 : index
          %c128 = arith.constant 128 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout3) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout3) binding(1) alignment(64) offset(%c128) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x2xf32>>
          %2 = hal.interface.binding.subspan layout(#pipeline_layout3) binding(2) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<1x2xf32>>
          %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 2], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x2xf32>> -> tensor<1x2xf32>
          %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0], sizes = [1, 2], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x2xf32>> -> tensor<1x2xf32>
          %5 = tensor.empty() : tensor<1x2xf32>
          %6 = linalg.generic {indexing_maps = [#map9, #map9, #map9, #map9], iterator_types = ["parallel", "parallel"]} ins(%3, %cst, %4 : tensor<1x2xf32>, tensor<1x2xf32>, tensor<1x2xf32>) outs(%5 : tensor<1x2xf32>) attrs =  {lowering_config = #config9} {
          ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
            %7 = arith.addf %in, %in_0 : f32
            %8 = arith.subf %7, %in_1 : f32
            linalg.yield %8 : f32
          } -> tensor<1x2xf32>
          iree_tensor_ext.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [1, 2], strides = [1, 1] : tensor<1x2xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<1x2xf32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_10 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_10_elementwise_broadcast ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_10_elementwise_broadcast() attributes {translation_info = #translation} {
          %c0 = arith.constant 0 : index
          %cst = arith.constant -1.000000e+00 : f32
          %c1 = arith.constant 1 : index
          %c128 = arith.constant 128 : index
          %c192 = arith.constant 192 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c128) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c192) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          %2 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %3 = tensor.empty() : tensor<f32>
          %4 = linalg.generic {indexing_maps = [#map], iterator_types = []} outs(%3 : tensor<f32>) attrs =  {lowering_config = #config1} {
          ^bb0(%out: f32):
            %extracted = tensor.extract %2[%c0] : tensor<2xf32>
            %extracted_0 = tensor.extract %2[%c1] : tensor<2xf32>
            %5 = arith.addf %extracted, %extracted_0 : f32
            %6 = math.sin %5 : f32
            %7 = math.sin %extracted : f32
            %8 = arith.mulf %7, %cst : f32
            %9 = arith.subf %8, %6 : f32
            linalg.yield %9 : f32
          } -> tensor<f32>
          iree_tensor_ext.dispatch.tensor.store %4, %1, offsets = [], sizes = [], strides = [] : tensor<f32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_11 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_11_elementwise_broadcast ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_11_elementwise_broadcast() attributes {translation_info = #translation} {
          %c0 = arith.constant 0 : index
          %cst = arith.constant -1.000000e+00 : f32
          %c1 = arith.constant 1 : index
          %c128 = arith.constant 128 : index
          %c196 = arith.constant 196 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c128) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c196) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          %2 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %3 = tensor.empty() : tensor<f32>
          %4 = linalg.generic {indexing_maps = [#map], iterator_types = []} outs(%3 : tensor<f32>) attrs =  {lowering_config = #config1} {
          ^bb0(%out: f32):
            %extracted = tensor.extract %2[%c0] : tensor<2xf32>
            %extracted_0 = tensor.extract %2[%c1] : tensor<2xf32>
            %5 = arith.addf %extracted, %extracted_0 : f32
            %6 = math.sin %5 : f32
            %7 = arith.mulf %6, %cst : f32
            linalg.yield %7 : f32
          } -> tensor<f32>
          iree_tensor_ext.dispatch.tensor.store %4, %1, offsets = [], sizes = [], strides = [] : tensor<f32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_12 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_12_elementwise_broadcast_2_f32 ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_12_elementwise_broadcast_2_f32() attributes {translation_info = #translation2} {
          %c0 = arith.constant 0 : index
          %c128 = arith.constant 128 : index
          %c64 = arith.constant 64 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c128) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2x2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c64) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<2xf32>>
          %2 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0], sizes = [2, 2], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2x2xf32>> -> tensor<2x2xf32>
          %3 = tensor.empty() : tensor<2xf32>
          %4 = linalg.generic {indexing_maps = [#map10], iterator_types = ["parallel"]} outs(%3 : tensor<2xf32>) attrs =  {lowering_config = #config10} {
          ^bb0(%out: f32):
            %5 = linalg.index 0 : index
            %extracted = tensor.extract %2[%c0, %5] : tensor<2x2xf32>
            linalg.yield %extracted : f32
          } -> tensor<2xf32>
          iree_tensor_ext.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<2xf32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_13 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_13_elementwise_broadcast ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_13_elementwise_broadcast() attributes {translation_info = #translation} {
          %c0 = arith.constant 0 : index
          %c64 = arith.constant 64 : index
          %c260 = arith.constant 260 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c64) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c260) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          %2 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %3 = tensor.empty() : tensor<f32>
          %4 = linalg.generic {indexing_maps = [#map], iterator_types = []} outs(%3 : tensor<f32>) attrs =  {lowering_config = #config1} {
          ^bb0(%out: f32):
            %extracted = tensor.extract %2[%c0] : tensor<2xf32>
            linalg.yield %extracted : f32
          } -> tensor<f32>
          iree_tensor_ext.dispatch.tensor.store %4, %1, offsets = [], sizes = [], strides = [] : tensor<f32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_14 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_14_elementwise_broadcast_2_f32 ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_14_elementwise_broadcast_2_f32() attributes {translation_info = #translation2} {
          %c1 = arith.constant 1 : index
          %c128 = arith.constant 128 : index
          %c192 = arith.constant 192 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c128) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2x2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c192) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<2xf32>>
          %2 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0], sizes = [2, 2], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2x2xf32>> -> tensor<2x2xf32>
          %3 = tensor.empty() : tensor<2xf32>
          %4 = linalg.generic {indexing_maps = [#map10], iterator_types = ["parallel"]} outs(%3 : tensor<2xf32>) attrs =  {lowering_config = #config10} {
          ^bb0(%out: f32):
            %5 = linalg.index 0 : index
            %extracted = tensor.extract %2[%c1, %5] : tensor<2x2xf32>
            linalg.yield %extracted : f32
          } -> tensor<2xf32>
          iree_tensor_ext.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<2xf32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_15 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_15_elementwise_broadcast ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_15_elementwise_broadcast() attributes {translation_info = #translation} {
          %c1 = arith.constant 1 : index
          %c192 = arith.constant 192 : index
          %c128 = arith.constant 128 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c192) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c128) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          %2 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %3 = tensor.empty() : tensor<f32>
          %4 = linalg.generic {indexing_maps = [#map], iterator_types = []} outs(%3 : tensor<f32>) attrs =  {lowering_config = #config1} {
          ^bb0(%out: f32):
            %extracted = tensor.extract %2[%c1] : tensor<2xf32>
            linalg.yield %extracted : f32
          } -> tensor<f32>
          iree_tensor_ext.dispatch.tensor.store %4, %1, offsets = [], sizes = [], strides = [] : tensor<f32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_16 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_16_elementwise_broadcast ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_16_elementwise_broadcast() attributes {translation_info = #translation} {
          %c1 = arith.constant 1 : index
          %c64 = arith.constant 64 : index
          %c132 = arith.constant 132 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c64) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c132) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          %2 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %3 = tensor.empty() : tensor<f32>
          %4 = linalg.generic {indexing_maps = [#map], iterator_types = []} outs(%3 : tensor<f32>) attrs =  {lowering_config = #config1} {
          ^bb0(%out: f32):
            %extracted = tensor.extract %2[%c1] : tensor<2xf32>
            %5 = arith.negf %extracted : f32
            linalg.yield %5 : f32
          } -> tensor<f32>
          iree_tensor_ext.dispatch.tensor.store %4, %1, offsets = [], sizes = [], strides = [] : tensor<f32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_17 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_17_elementwise_broadcast ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_17_elementwise_broadcast() attributes {translation_info = #translation} {
          %c0 = arith.constant 0 : index
          %c192 = arith.constant 192 : index
          %c128 = arith.constant 128 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c192) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c128) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          %2 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %3 = tensor.empty() : tensor<f32>
          %4 = linalg.generic {indexing_maps = [#map], iterator_types = []} outs(%3 : tensor<f32>) attrs =  {lowering_config = #config1} {
          ^bb0(%out: f32):
            %extracted = tensor.extract %2[%c0] : tensor<2xf32>
            %5 = arith.negf %extracted : f32
            linalg.yield %5 : f32
          } -> tensor<f32>
          iree_tensor_ext.dispatch.tensor.store %4, %1, offsets = [], sizes = [], strides = [] : tensor<f32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_18 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_18_elementwise_2x2_f32_pack ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_18_elementwise_2x2_f32_pack() attributes {translation_info = #translation5} {
          %c0 = arith.constant 0 : index
          %cst = arith.constant 9.99999997E-7 : f32
          %cst_0 = arith.constant 0.000000e+00 : f32
          %cst_1 = arith.constant 1.000000e+00 : f32
          %c1 = arith.constant 1 : index
          %c64 = arith.constant 64 : index
          %c192 = arith.constant 192 : index
          %c320 = arith.constant 320 : index
          %c128 = arith.constant 128 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c64) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c192) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %2 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c320) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2x2xf32>>
          %3 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c128) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<1x2x8x1xf32>>
          %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %6 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 0], sizes = [2, 2], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2x2xf32>> -> tensor<2x2xf32>
          %7 = tensor.empty() : tensor<1x2x8x1xf32>
          %8 = tensor.empty() : tensor<2x2xf32>
          %9 = linalg.generic {indexing_maps = [#map9, #map9], iterator_types = ["parallel", "parallel"]} ins(%6 : tensor<2x2xf32>) outs(%8 : tensor<2x2xf32>) attrs =  {lowering_config = #config11} {
          ^bb0(%in: f32, %out: f32):
            %extracted = tensor.extract %4[%c0] : tensor<2xf32>
            %extracted_2 = tensor.extract %5[%c1] : tensor<2xf32>
            %extracted_3 = tensor.extract %4[%c1] : tensor<2xf32>
            %extracted_4 = tensor.extract %5[%c0] : tensor<2xf32>
            %10 = arith.mulf %extracted_3, %extracted_4 : f32
            %11 = arith.mulf %extracted, %extracted_2 : f32
            %12 = arith.subf %11, %10 : f32
            %13 = arith.addf %12, %cst : f32
            %14 = arith.divf %cst_1, %13 : f32
            %15 = arith.mulf %in, %14 : f32
            linalg.yield %15 : f32
          } -> tensor<2x2xf32>
          %pack = linalg.pack %9 padding_value(%cst_0 : f32) outer_dims_perm = [0, 1] inner_dims_pos = [0, 1] inner_tiles = [8, 1] into %7 {lowering_config = #config12} : tensor<2x2xf32> -> tensor<1x2x8x1xf32>
          iree_tensor_ext.dispatch.tensor.store %pack, %3, offsets = [0, 0, 0, 0], sizes = [1, 2, 8, 1], strides = [1, 1, 1, 1] : tensor<1x2x8x1xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<1x2x8x1xf32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_19 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_19_mmt4d_1x1x2x1x8x1_f32 ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_19_mmt4d_1x1x2x1x8x1_f32() attributes {translation_info = #translation3} {
          %cst = arith.constant 0.000000e+00 : f32
          %c0 = arith.constant 0 : index
          %c128 = arith.constant 128 : index
          %c64 = arith.constant 64 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x2x1x1xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c128) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x2x8x1xf32>>
          %2 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c64) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<1x1x1x8xf32>>
          %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0, 0], sizes = [1, 2, 1, 1], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x2x1x1xf32>> -> tensor<1x2x1x1xf32>
          %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0, 0], sizes = [1, 2, 8, 1], strides = [1, 1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x2x8x1xf32>> -> tensor<1x2x8x1xf32>
          %5 = tensor.empty() : tensor<1x1x1x8xf32>
          %6 = linalg.fill {lowering_config = #config6} ins(%cst : f32) outs(%5 : tensor<1x1x1x8xf32>) -> tensor<1x1x1x8xf32>
          %7 = linalg.mmt4d {lowering_config = #config7} ins(%3, %4 : tensor<1x2x1x1xf32>, tensor<1x2x8x1xf32>) outs(%6 : tensor<1x1x1x8xf32>) -> tensor<1x1x1x8xf32>
          iree_tensor_ext.dispatch.tensor.store %7, %2, offsets = [0, 0, 0, 0], sizes = [1, 1, 1, 8], strides = [1, 1, 1, 1] : tensor<1x1x1x8xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<1x1x1x8xf32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_20 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_20_unpack_elementwise_2_f32 ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_20_unpack_elementwise_2_f32() attributes {translation_info = #translation2} {
          %cst = arith.constant 1.000000e-01 : f32
          %c64 = arith.constant 64 : index
          %c0 = arith.constant 0 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c64) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x8xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %2 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<2xf32>>
          %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 8], strides = [1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<1x8xf32>> -> tensor<1x8xf32>
          %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %5 = tensor.empty() : tensor<2xf32>
          %unpack = linalg.unpack %3 outer_dims_perm = [0] inner_dims_pos = [0] inner_tiles = [8] into %5 {lowering_config = #config13} : tensor<1x8xf32> -> tensor<2xf32>
          %6 = linalg.generic {indexing_maps = [#map10, #map10, #map10], iterator_types = ["parallel"]} ins(%4, %unpack : tensor<2xf32>, tensor<2xf32>) outs(%5 : tensor<2xf32>) attrs =  {lowering_config = #config14} {
          ^bb0(%in: f32, %in_0: f32, %out: f32):
            %7 = arith.mulf %in_0, %cst : f32
            %8 = arith.addf %in, %7 : f32
            linalg.yield %8 : f32
          } -> tensor<2xf32>
          iree_tensor_ext.dispatch.tensor.store %6, %2, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<2xf32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_21 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_21_reduction_2_f32 ordinal(0) layout(#pipeline_layout3) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_21_reduction_2_f32() attributes {translation_info = #translation2} {
          %cst = arith.constant 0.000000e+00 : f32
          %c0 = arith.constant 0 : index
          %c64 = arith.constant 64 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout3) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout3) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %2 = hal.interface.binding.subspan layout(#pipeline_layout3) binding(2) alignment(64) offset(%c64) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %5 = tensor.empty() : tensor<f32>
          %6 = linalg.fill {lowering_config = #config} ins(%cst : f32) outs(%5 : tensor<f32>) -> tensor<f32>
          %7 = linalg.generic {indexing_maps = [#map10, #map10, #map11], iterator_types = ["reduction"]} ins(%3, %4 : tensor<2xf32>, tensor<2xf32>) outs(%6 : tensor<f32>) attrs =  {lowering_config = #config15} {
          ^bb0(%in: f32, %in_0: f32, %out: f32):
            %8 = arith.subf %in, %in_0 : f32
            %9 = arith.mulf %8, %8 : f32
            %10 = arith.addf %9, %out : f32
            linalg.yield %10 : f32
          } -> tensor<f32>
          iree_tensor_ext.dispatch.tensor.store %7, %2, offsets = [], sizes = [], strides = [] : tensor<f32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_22 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_22_elementwise_broadcast ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_22_elementwise_broadcast() attributes {translation_info = #translation} {
          %c0 = arith.constant 0 : index
          %c1 = arith.constant 1 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          %2 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %3 = tensor.empty() : tensor<f32>
          %extracted = tensor.extract %2[%c1] : tensor<2xf32>
          %4 = linalg.generic {indexing_maps = [#map], iterator_types = []} outs(%3 : tensor<f32>) attrs =  {lowering_config = #config1} {
          ^bb0(%out: f32):
            %extracted_0 = tensor.extract %2[%c0] : tensor<2xf32>
            %5 = arith.addf %extracted_0, %extracted : f32
            %6 = math.cos %5 : f32
            %7 = math.cos %extracted_0 : f32
            %8 = arith.addf %7, %6 : f32
            linalg.yield %8 : f32
          } -> tensor<f32>
          iree_tensor_ext.dispatch.tensor.store %4, %1, offsets = [], sizes = [], strides = [] : tensor<f32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_23 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_23_elementwise_broadcast ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_23_elementwise_broadcast() attributes {translation_info = #translation} {
          %c0 = arith.constant 0 : index
          %c1 = arith.constant 1 : index
          %c4 = arith.constant 4 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c4) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          %2 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %3 = tensor.empty() : tensor<f32>
          %extracted = tensor.extract %2[%c1] : tensor<2xf32>
          %4 = linalg.generic {indexing_maps = [#map], iterator_types = []} outs(%3 : tensor<f32>) attrs =  {lowering_config = #config1} {
          ^bb0(%out: f32):
            %extracted_0 = tensor.extract %2[%c0] : tensor<2xf32>
            %5 = arith.addf %extracted_0, %extracted : f32
            %6 = math.sin %5 : f32
            %7 = math.sin %extracted_0 : f32
            %8 = arith.addf %7, %6 : f32
            linalg.yield %8 : f32
          } -> tensor<f32>
          iree_tensor_ext.dispatch.tensor.store %4, %1, offsets = [], sizes = [], strides = [] : tensor<f32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<f32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_24 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_24_elementwise_broadcast_10x2_f32 ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_24_elementwise_broadcast_10x2_f32() attributes {translation_info = #translation2} {
          %cst = arith.constant dense<[1.000000e+00, 0.888888895, 0.777777791, 0.666666627, 0.555555582, 0.444444418, 0.333333313, 0.222222209, 0.111111104, 0.000000e+00]> : tensor<10xf32>
          %cst_0 = arith.constant dense<[0.000000e+00, -1.000000e-01]> : tensor<2xf32>
          %cst_1 = arith.constant dense<[0.000000e+00, 0.111111112, 0.222222224, 0.333333343, 0.444444448, 0.555555582, 0.666666686, 0.777777791, 0.888888895, 1.000000e+00]> : tensor<10xf32>
          %c0 = arith.constant 0 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<readwrite:tensor<19x2xf32>>
          %2 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %3 = tensor.empty() : tensor<10x2xf32>
          %4 = linalg.generic {indexing_maps = [#map12, #map13, #map12, #map13, #map9], iterator_types = ["parallel", "parallel"]} ins(%2, %cst, %cst_0, %cst_1 : tensor<2xf32>, tensor<10xf32>, tensor<2xf32>, tensor<10xf32>) outs(%3 : tensor<10x2xf32>) attrs =  {lowering_config = #config16} {
          ^bb0(%in: f32, %in_2: f32, %in_3: f32, %in_4: f32, %out: f32):
            %5 = arith.addf %in, %in_3 : f32
            %6 = arith.mulf %5, %in_4 : f32
            %7 = arith.mulf %in, %in_2 : f32
            %8 = arith.addf %7, %6 : f32
            linalg.yield %8 : f32
          } -> tensor<10x2xf32>
          iree_tensor_ext.dispatch.tensor.store %4, %1, offsets = [0, 0], sizes = [10, 2], strides = [1, 1] : tensor<10x2xf32> -> !iree_tensor_ext.dispatch.tensor<readwrite:tensor<19x2xf32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_27 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_27_elementwise_2_f32 ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_27_elementwise_2_f32() attributes {translation_info = #translation2} {
          %cst = arith.constant dense<[-2.000000e-01, 0.000000e+00]> : tensor<2xf32>
          %c0 = arith.constant 0 : index
          %c64 = arith.constant 64 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c64) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<2xf32>>
          %2 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %3 = tensor.empty() : tensor<2xf32>
          %4 = linalg.generic {indexing_maps = [#map10, #map10, #map10], iterator_types = ["parallel"]} ins(%2, %cst : tensor<2xf32>, tensor<2xf32>) outs(%3 : tensor<2xf32>) attrs =  {lowering_config = #config10} {
          ^bb0(%in: f32, %in_0: f32, %out: f32):
            %5 = arith.addf %in, %in_0 : f32
            linalg.yield %5 : f32
          } -> tensor<2xf32>
          iree_tensor_ext.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<2xf32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_28 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_28_elementwise_broadcast_10x2_f32 ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_28_elementwise_broadcast_10x2_f32() attributes {translation_info = #translation2} {
          %cst = arith.constant dense<[1.000000e+00, 0.888888895, 0.777777791, 0.666666627, 0.555555582, 0.444444418, 0.333333313, 0.222222209, 0.111111104, 0.000000e+00]> : tensor<10xf32>
          %cst_0 = arith.constant dense<[0.000000e+00, 0.111111112, 0.222222224, 0.333333343, 0.444444448, 0.555555582, 0.666666686, 0.777777791, 0.888888895, 1.000000e+00]> : tensor<10xf32>
          %c0 = arith.constant 0 : index
          %c64 = arith.constant 64 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c64) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %2 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<10x2xf32>>
          %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %5 = tensor.empty() : tensor<10x2xf32>
          %6 = linalg.generic {indexing_maps = [#map12, #map13, #map12, #map13, #map9], iterator_types = ["parallel", "parallel"]} ins(%3, %cst, %4, %cst_0 : tensor<2xf32>, tensor<10xf32>, tensor<2xf32>, tensor<10xf32>) outs(%5 : tensor<10x2xf32>) attrs =  {lowering_config = #config16} {
          ^bb0(%in: f32, %in_1: f32, %in_2: f32, %in_3: f32, %out: f32):
            %7 = arith.mulf %in_2, %in_3 : f32
            %8 = arith.mulf %in, %in_1 : f32
            %9 = arith.addf %8, %7 : f32
            linalg.yield %9 : f32
          } -> tensor<10x2xf32>
          iree_tensor_ext.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [10, 2], strides = [1, 1] : tensor<10x2xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<10x2xf32>>
          return
        }
      }
    }
  }
  hal.executable private @_main$async_dispatch_29 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @_main$async_dispatch_29_elementwise_broadcast_9x2_f32 ordinal(0) layout(#pipeline_layout1) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main$async_dispatch_29_elementwise_broadcast_9x2_f32() attributes {translation_info = #translation2} {
          %cst = arith.constant dense<[1.000000e+00, 0.888888895, 0.777777791, 0.666666627, 0.555555582, 0.444444418, 0.333333313, 0.222222209, 0.111111104, 0.000000e+00]> : tensor<10xf32>
          %cst_0 = arith.constant dense<[0.000000e+00, -1.000000e-01]> : tensor<2xf32>
          %cst_1 = arith.constant dense<[0.000000e+00, 0.111111112, 0.222222224, 0.333333343, 0.444444448, 0.555555582, 0.666666686, 0.777777791, 0.888888895, 1.000000e+00]> : tensor<10xf32>
          %c64 = arith.constant 64 : index
          %c80 = arith.constant 80 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(0) alignment(64) offset(%c64) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout1) binding(1) alignment(64) offset(%c80) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<9x2xf32>>
          %2 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %3 = tensor.empty() : tensor<9x2xf32>
          %extracted_slice = tensor.extract_slice %cst[1] [9] [1] : tensor<10xf32> to tensor<9xf32>
          %extracted_slice_2 = tensor.extract_slice %cst_1[1] [9] [1] : tensor<10xf32> to tensor<9xf32>
          %4 = linalg.generic {indexing_maps = [#map12, #map13, #map12, #map13, #map9], iterator_types = ["parallel", "parallel"]} ins(%2, %extracted_slice, %cst_0, %extracted_slice_2 : tensor<2xf32>, tensor<9xf32>, tensor<2xf32>, tensor<9xf32>) outs(%3 : tensor<9x2xf32>) attrs =  {lowering_config = #config17} {
          ^bb0(%in: f32, %in_3: f32, %in_4: f32, %in_5: f32, %out: f32):
            %5 = arith.addf %in, %in_4 : f32
            %6 = arith.mulf %5, %in_5 : f32
            %7 = arith.mulf %in, %in_3 : f32
            %8 = arith.addf %7, %6 : f32
            linalg.yield %8 : f32
          } -> tensor<9x2xf32>
          iree_tensor_ext.dispatch.tensor.store %4, %1, offsets = [0, 0], sizes = [9, 2], strides = [1, 1] : tensor<9x2xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<9x2xf32>>
          return
        }
      }
    }
  }
  util.global private @__constant_tensor_4x3x4x4xf32 : !stream.resource<constant>
  util.initializer {
    %c0 = arith.constant 0 : index
    %c0_i64 = arith.constant 0 : i64
    %c8960 = arith.constant 8960 : index
    %0 = stream.timepoint.immediate => !stream.timepoint
    %buffer_cst = util.buffer.constant {alignment = 64 : index} : !util.buffer = #composite_of_8960b
    %did_map, %result = stream.resource.try_map on(#hal.device.affinity<@__device_0>) %buffer_cst[%c0] : !util.buffer -> i1, !stream.resource<constant>{%c8960}
    cf.cond_br %did_map, ^bb2(%0, %result : !stream.timepoint, !stream.resource<constant>), ^bb1
  ^bb1:  // pred: ^bb0
    %1 = stream.resource.alloc uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<constant>{%c8960}
    %file = stream.file.constant on(#hal.device.affinity<@__device_0>) %buffer_cst[%c0 for %c8960] : !util.buffer{%c8960} -> !stream.file
    %2 = stream.file.read on(#hal.device.affinity<@__device_0>) await(%0) => %file[%c0_i64], %1[%c0], %c8960 : !stream.file -> !stream.resource<constant>{%c8960} => !stream.timepoint
    cf.br ^bb2(%2, %1 : !stream.timepoint, !stream.resource<constant>)
  ^bb2(%3: !stream.timepoint, %4: !stream.resource<constant>):  // 2 preds: ^bb0, ^bb1
    %5 = stream.timepoint.await sync %3 => %4 : !stream.resource<constant>{%c8960}
    util.global.store %5, @__constant_tensor_4x3x4x4xf32 : !stream.resource<constant>
    util.return
  }
  util.func private @_main$async(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view, %arg3: !hal.buffer_view, %arg4: !hal.fence, %arg5: !hal.fence) -> (!hal.buffer_view, !hal.buffer_view) attributes {inlining_policy = #util.inline.never} {
    %c152 = arith.constant 152 : index
    %c52272 = arith.constant 52272 : index
    %c4 = arith.constant 4 : index
    %c8 = arith.constant 8 : index
    %c12 = arith.constant 12 : index
    %c49152 = arith.constant 49152 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c1 = arith.constant 1 : index
    %cst_0 = arith.constant 9.99999993E-9 : f32
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c64 = arith.constant 64 : index
    %c0_i8 = arith.constant 0 : i8
    %c0 = arith.constant 0 : index
    %c56384 = arith.constant 56384 : index
    %c128 = arith.constant 128 : index
    %c192 = arith.constant 192 : index
    %c256 = arith.constant 256 : index
    %c320 = arith.constant 320 : index
    %c384 = arith.constant 384 : index
    %c328 = arith.constant 328 : index
    %c136 = arith.constant 136 : index
    %c8960 = arith.constant 8960 : index
    %__constant_tensor_4x3x4x4xf32 = util.global.load immutable @__constant_tensor_4x3x4x4xf32 : !stream.resource<constant>
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c1, %c3, %c64, %c64]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<1x3x64x64xf32> in !stream.resource<external>{%c49152}
    %1 = stream.timepoint.import on(#hal.device.affinity<@__device_0>) %arg4 : (!hal.fence) => !stream.timepoint
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c3]) type(%element_type_f32) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<3xf32> in !stream.resource<external>{%c12}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c2]) type(%element_type_f32) encoding(%dense_row_major)
    %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg2 : !hal.buffer_view -> tensor<2xf32> in !stream.resource<external>{%c8}
    hal.buffer_view.assert<%arg3 : !hal.buffer_view> message("tensor") shape([%c2]) type(%element_type_f32) encoding(%dense_row_major)
    %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg3 : !hal.buffer_view -> tensor<2xf32> in !stream.resource<external>{%c8}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) await(%1) => !stream.resource<external>{%c12} => !stream.timepoint
    %result_1, %result_timepoint_2 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) await(%1) => !stream.resource<transient>{%c8} => !stream.timepoint
    %result_3, %result_timepoint_4 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) await(%1) => !stream.resource<transient>{%c56384} => !stream.timepoint
    %5 = stream.timepoint.join max(%result_timepoint, %result_timepoint_2, %result_timepoint_4) => !stream.timepoint
    %6 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%5) => with(%4 as %arg6: !stream.resource<external>{%c8}, %2 as %arg7: !stream.resource<external>{%c12}, %0 as %arg8: !stream.resource<external>{%c49152}, %__constant_tensor_4x3x4x4xf32 as %arg9: !stream.resource<constant>{%c8960}, %result as %arg10: !stream.resource<external>{%c12}, %result_1 as %arg11: !stream.resource<transient>{%c8}, %result_3 as %arg12: !stream.resource<transient>{%c56384}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main$async_dispatch_0::@embedded_elf_x86_64::@_main$async_dispatch_0_elementwise_broadcast {
          ro %arg6[%c0 for %c8] : !stream.resource<external>{%c8},
          ro %arg7[%c0 for %c12] : !stream.resource<external>{%c12},
          rw %arg10[%c0 for %c12] : !stream.resource<external>{%c12},
          rw %arg10[%c0 for %c12] : !stream.resource<external>{%c12},
          rw %arg10[%c0 for %c12] : !stream.resource<external>{%c12}
        }
        stream.cmd.fill %c0_i8, %arg12[%c0 for %c52272] : i8 -> !stream.resource<transient>{%c56384}
      }
      stream.cmd.dispatch @_main$async_dispatch_1::@embedded_elf_x86_64::@_main$async_dispatch_1_slow_memcpy {
        ro %arg8[%c0 for %c49152] : !stream.resource<external>{%c49152},
        rw %arg12[%c0 for %c56384] : !stream.resource<transient>{%c56384}
      }
      stream.cmd.dispatch @_main$async_dispatch_2::@embedded_elf_x86_64::@_main$async_dispatch_2_conv_4x16x16x3x4x4_f32 {
        ro %arg12[%c0 for %c56384] : !stream.resource<transient>{%c56384},
        ro %arg9[%c0 for %c8960] : !stream.resource<constant>{%c8960},
        wo %arg12[%c0 for %c56384] : !stream.resource<transient>{%c56384}
      }
      stream.cmd.dispatch @_main$async_dispatch_3::@embedded_elf_x86_64::@_main$async_dispatch_3_conv_4x8x8x2x2_f32 {
        ro %arg12[%c0 for %c56384] : !stream.resource<transient>{%c56384},
        wo %arg12[%c0 for %c56384] : !stream.resource<transient>{%c56384}
      }
      stream.cmd.dispatch @_main$async_dispatch_4::@embedded_elf_x86_64::@_main$async_dispatch_4_mmt4d_1x1x256x1x8x1_f32 {
        ro %arg12[%c0 for %c56384] : !stream.resource<transient>{%c56384},
        ro %arg9[%c0 for %c8960] : !stream.resource<constant>{%c8960},
        wo %arg12[%c0 for %c56384] : !stream.resource<transient>{%c56384}
      }
      stream.cmd.dispatch @_main$async_dispatch_5::@embedded_elf_x86_64::@_main$async_dispatch_5_unpack_f32 {
        ro %arg12[%c0 for %c56384] : !stream.resource<transient>{%c56384},
        wo %arg11[%c0 for %c8] : !stream.resource<transient>{%c8}
      }
    } => !stream.timepoint
    %7 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%6) => %result_3 : !stream.resource<transient>{%c56384} => !stream.timepoint
    %8:4 = stream.timepoint.await %1 => %4, %2, %0, %3 : !stream.resource<external>{%c8}, !stream.resource<external>{%c12}, !stream.resource<external>{%c49152}, !stream.resource<external>{%c8}
    cf.br ^bb1(%8#3 : !stream.resource<external>)
  ^bb1(%9: !stream.resource<external>):  // 2 preds: ^bb0, ^bb1
    %result_5, %result_timepoint_6 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) await(%7) => !stream.resource<external>{%c8} => !stream.timepoint
    %result_7, %result_timepoint_8 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) await(%7) => !stream.resource<staging>{%c4} => !stream.timepoint
    %result_9, %result_timepoint_10 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) await(%7) => !stream.resource<transient>{%c384} => !stream.timepoint
    %10 = stream.timepoint.join max(%result_timepoint_6, %result_timepoint_8, %result_timepoint_10) => !stream.timepoint
    %11 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%10) => with(%9 as %arg6: !stream.resource<external>{%c8}, %result_1 as %arg7: !stream.resource<transient>{%c8}, %result_5 as %arg8: !stream.resource<external>{%c8}, %result_7 as %arg9: !stream.resource<staging>{%c4}, %result_9 as %arg10: !stream.resource<transient>{%c384}) {
      stream.cmd.copy %arg6[%c0], %arg10[%c0], %c8 : !stream.resource<external>{%c8} -> !stream.resource<transient>{%c384}
      stream.cmd.copy %arg6[%c0], %arg10[%c64], %c8 : !stream.resource<external>{%c8} -> !stream.resource<transient>{%c384}
      stream.cmd.dispatch @_main$async_dispatch_7::@embedded_elf_x86_64::@_main$async_dispatch_7_elementwise_broadcast {
        ro %arg10[%c0 for %c384] : !stream.resource<transient>{%c384},
        wo %arg10[%c0 for %c384] : !stream.resource<transient>{%c384}
      }
      stream.cmd.copy %arg10[%c128], %arg10[%c64], %c4 : !stream.resource<transient>{%c384} -> !stream.resource<transient>{%c384}
      stream.cmd.dispatch @_main$async_dispatch_6::@embedded_elf_x86_64::@_main$async_dispatch_6_elementwise_broadcast {
        ro %arg10[%c0 for %c384] : !stream.resource<transient>{%c384},
        rw %arg10[%c0 for %c384] : !stream.resource<transient>{%c384}
      }
      stream.cmd.copy %arg6[%c0], %arg10[%c0], %c8 : !stream.resource<external>{%c8} -> !stream.resource<transient>{%c384}
      stream.cmd.copy %arg10[%c64], %arg10[%c128], %c8 : !stream.resource<transient>{%c384} -> !stream.resource<transient>{%c384}
      stream.cmd.dispatch @_main$async_dispatch_8::@embedded_elf_x86_64::@_main$async_dispatch_8_elementwise_broadcast {
        ro %arg10[%c0 for %c384] : !stream.resource<transient>{%c384},
        rw %arg10[%c0 for %c384] : !stream.resource<transient>{%c384}
      }
      stream.cmd.dispatch @_main$async_dispatch_9::@embedded_elf_x86_64::@_main$async_dispatch_9_elementwise_1x2_f32 {
        ro %arg7[%c0 for %c8] : !stream.resource<transient>{%c8},
        ro %arg10[%c0 for %c384] : !stream.resource<transient>{%c384},
        wo %arg10[%c0 for %c384] : !stream.resource<transient>{%c384}
      }
      stream.cmd.copy %arg6[%c0], %arg10[%c128], %c8 : !stream.resource<external>{%c8} -> !stream.resource<transient>{%c384}
      stream.cmd.dispatch @_main$async_dispatch_10::@embedded_elf_x86_64::@_main$async_dispatch_10_elementwise_broadcast {
        ro %arg10[%c0 for %c384] : !stream.resource<transient>{%c384},
        rw %arg10[%c0 for %c384] : !stream.resource<transient>{%c384}
      }
      stream.cmd.copy %arg6[%c0], %arg10[%c128], %c8 : !stream.resource<external>{%c8} -> !stream.resource<transient>{%c384}
      stream.cmd.dispatch @_main$async_dispatch_11::@embedded_elf_x86_64::@_main$async_dispatch_11_elementwise_broadcast {
        ro %arg10[%c0 for %c384] : !stream.resource<transient>{%c384},
        rw %arg10[%c0 for %c384] : !stream.resource<transient>{%c384}
      }
      stream.cmd.copy %arg10[%c192], %arg10[%c128], %c8 : !stream.resource<transient>{%c384} -> !stream.resource<transient>{%c384}
      stream.cmd.copy %arg10[%c64], %arg10[%c136], %c8 : !stream.resource<transient>{%c384} -> !stream.resource<transient>{%c384}
      stream.cmd.dispatch @_main$async_dispatch_12::@embedded_elf_x86_64::@_main$async_dispatch_12_elementwise_broadcast_2_f32 {
        ro %arg10[%c0 for %c384] : !stream.resource<transient>{%c384},
        wo %arg10[%c0 for %c384] : !stream.resource<transient>{%c384}
      }
      stream.cmd.dispatch @_main$async_dispatch_14::@embedded_elf_x86_64::@_main$async_dispatch_14_elementwise_broadcast_2_f32 {
        ro %arg10[%c0 for %c384] : !stream.resource<transient>{%c384},
        wo %arg10[%c0 for %c384] : !stream.resource<transient>{%c384}
      }
      stream.cmd.dispatch @_main$async_dispatch_17::@embedded_elf_x86_64::@_main$async_dispatch_17_elementwise_broadcast {
        ro %arg10[%c0 for %c384] : !stream.resource<transient>{%c384},
        wo %arg10[%c0 for %c384] : !stream.resource<transient>{%c384}
      }
      stream.cmd.copy %arg10[%c128], %arg10[%c256], %c4 : !stream.resource<transient>{%c384} -> !stream.resource<transient>{%c384}
      stream.cmd.dispatch @_main$async_dispatch_13::@embedded_elf_x86_64::@_main$async_dispatch_13_elementwise_broadcast {
        ro %arg10[%c0 for %c384] : !stream.resource<transient>{%c384},
        rw %arg10[%c0 for %c384] : !stream.resource<transient>{%c384}
      }
      stream.cmd.dispatch @_main$async_dispatch_15::@embedded_elf_x86_64::@_main$async_dispatch_15_elementwise_broadcast {
        ro %arg10[%c0 for %c384] : !stream.resource<transient>{%c384},
        rw %arg10[%c0 for %c384] : !stream.resource<transient>{%c384}
      }
      stream.cmd.dispatch @_main$async_dispatch_16::@embedded_elf_x86_64::@_main$async_dispatch_16_elementwise_broadcast {
        ro %arg10[%c0 for %c384] : !stream.resource<transient>{%c384},
        rw %arg10[%c0 for %c384] : !stream.resource<transient>{%c384}
      }
      stream.cmd.copy %arg10[%c128], %arg10[%c320], %c8 : !stream.resource<transient>{%c384} -> !stream.resource<transient>{%c384}
      stream.cmd.copy %arg10[%c256], %arg10[%c328], %c8 : !stream.resource<transient>{%c384} -> !stream.resource<transient>{%c384}
      stream.cmd.dispatch @_main$async_dispatch_18::@embedded_elf_x86_64::@_main$async_dispatch_18_elementwise_2x2_f32_pack {
        ro %arg10[%c0 for %c384] : !stream.resource<transient>{%c384},
        wo %arg10[%c0 for %c384] : !stream.resource<transient>{%c384}
      }
      stream.cmd.dispatch @_main$async_dispatch_19::@embedded_elf_x86_64::@_main$async_dispatch_19_mmt4d_1x1x2x1x8x1_f32 {
        ro %arg10[%c0 for %c384] : !stream.resource<transient>{%c384},
        wo %arg10[%c0 for %c384] : !stream.resource<transient>{%c384}
      }
      stream.cmd.copy %arg6[%c0], %arg10[%c0], %c8 : !stream.resource<external>{%c8} -> !stream.resource<transient>{%c384}
      stream.cmd.dispatch @_main$async_dispatch_20::@embedded_elf_x86_64::@_main$async_dispatch_20_unpack_elementwise_2_f32 {
        ro %arg10[%c0 for %c384] : !stream.resource<transient>{%c384},
        wo %arg8[%c0 for %c8] : !stream.resource<external>{%c8}
      }
      stream.cmd.copy %arg6[%c0], %arg10[%c0], %c8 : !stream.resource<external>{%c8} -> !stream.resource<transient>{%c384}
      stream.cmd.dispatch @_main$async_dispatch_21::@embedded_elf_x86_64::@_main$async_dispatch_21_reduction_2_f32 {
        ro %arg8[%c0 for %c8] : !stream.resource<external>{%c8},
        ro %arg10[%c0 for %c384] : !stream.resource<transient>{%c384},
        wo %arg10[%c0 for %c384] : !stream.resource<transient>{%c384}
      }
      stream.cmd.copy %arg10[%c64], %arg9[%c0], %c4 : !stream.resource<transient>{%c384} -> !stream.resource<staging>{%c4}
      stream.cmd.flush %arg9[%c0 for %c4] : !stream.resource<staging>{%c4}
    } => !stream.timepoint
    %12 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%11) => %result_9 : !stream.resource<transient>{%c384} => !stream.timepoint
    %13:2 = stream.timepoint.await %12 => %result_7, %result_5 : !stream.resource<staging>{%c4}, !stream.resource<external>{%c8}
    %14 = stream.resource.load %13#0[%c0] : !stream.resource<staging>{%c4} -> f32
    %15 = arith.cmpf ogt, %14, %cst_0 : f32
    cf.cond_br %15, ^bb1(%13#1 : !stream.resource<external>), ^bb2
  ^bb2:  // pred: ^bb1
    %result_11, %result_timepoint_12 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<staging>{%c4} => !stream.timepoint
    %result_13, %result_timepoint_14 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c64} => !stream.timepoint
    %16 = stream.timepoint.join max(%result_timepoint_12, %result_timepoint_14, %12) => !stream.timepoint
    %17 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%16) => with(%result_5 as %arg6: !stream.resource<external>{%c8}, %result_11 as %arg7: !stream.resource<staging>{%c4}, %result_13 as %arg8: !stream.resource<transient>{%c64}) {
      stream.cmd.copy %arg6[%c4], %arg8[%c0], %c4 : !stream.resource<external>{%c8} -> !stream.resource<transient>{%c64}
      stream.cmd.copy %arg8[%c0], %arg7[%c0], %c4 : !stream.resource<transient>{%c64} -> !stream.resource<staging>{%c4}
      stream.cmd.flush %arg7[%c0 for %c4] : !stream.resource<staging>{%c4}
    } => !stream.timepoint
    %18 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%17) => %result_13 : !stream.resource<transient>{%c64} => !stream.timepoint
    %19 = stream.timepoint.await %18 => %result_11 : !stream.resource<staging>{%c4}
    %20 = stream.resource.load %19[%c0] : !stream.resource<staging>{%c4} -> f32
    %21 = arith.cmpf ogt, %20, %cst : f32
    cf.cond_br %21, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %result_15, %result_timepoint_16 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c152} => !stream.timepoint
    %result_17, %result_timepoint_18 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c64} => !stream.timepoint
    %22 = stream.timepoint.join max(%result_timepoint_16, %result_timepoint_18, %12) => !stream.timepoint
    %23 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%22) => with(%result_5 as %arg6: !stream.resource<external>{%c8}, %result_15 as %arg7: !stream.resource<external>{%c152}, %result_17 as %arg8: !stream.resource<transient>{%c64}) {
      stream.cmd.dispatch @_main$async_dispatch_22::@embedded_elf_x86_64::@_main$async_dispatch_22_elementwise_broadcast {
        ro %arg6[%c0 for %c8] : !stream.resource<external>{%c8},
        rw %arg8[%c0 for %c64] : !stream.resource<transient>{%c64}
      }
      stream.cmd.dispatch @_main$async_dispatch_23::@embedded_elf_x86_64::@_main$async_dispatch_23_elementwise_broadcast {
        ro %arg6[%c0 for %c8] : !stream.resource<external>{%c8},
        rw %arg8[%c0 for %c64] : !stream.resource<transient>{%c64}
      }
      stream.cmd.fill %c0_i8, %arg7[%c0 for %c152] : i8 -> !stream.resource<external>{%c152}
      stream.cmd.dispatch @_main$async_dispatch_24::@embedded_elf_x86_64::@_main$async_dispatch_24_elementwise_broadcast_10x2_f32 {
        ro %arg8[%c0 for %c64] : !stream.resource<transient>{%c64},
        rw %arg7[%c0 for %c152] : !stream.resource<external>{%c152}
      }
    } => !stream.timepoint
    %24 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%23) => %result_17 : !stream.resource<transient>{%c64} => !stream.timepoint
    %25 = stream.timepoint.await %24 => %result_15 : !stream.resource<external>{%c152}
    cf.br ^bb5(%25 : !stream.resource<external>)
  ^bb4:  // pred: ^bb2
    %result_19, %result_timepoint_20 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c152} => !stream.timepoint
    %result_21, %result_timepoint_22 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c128} => !stream.timepoint
    %26 = stream.timepoint.join max(%result_timepoint_20, %result_timepoint_22, %12) => !stream.timepoint
    %27 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%26) => with(%result_5 as %arg6: !stream.resource<external>{%c8}, %result_19 as %arg7: !stream.resource<external>{%c152}, %result_21 as %arg8: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @_main$async_dispatch_22::@embedded_elf_x86_64::@_main$async_dispatch_22_elementwise_broadcast {
        ro %arg6[%c0 for %c8] : !stream.resource<external>{%c8},
        rw %arg8[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @_main$async_dispatch_23::@embedded_elf_x86_64::@_main$async_dispatch_23_elementwise_broadcast {
        ro %arg6[%c0 for %c8] : !stream.resource<external>{%c8},
        rw %arg8[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @_main$async_dispatch_27::@embedded_elf_x86_64::@_main$async_dispatch_27_elementwise_2_f32 {
        ro %arg8[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg8[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @_main$async_dispatch_28::@embedded_elf_x86_64::@_main$async_dispatch_28_elementwise_broadcast_10x2_f32 {
        ro %arg8[%c0 for %c128] : !stream.resource<transient>{%c128},
        rw %arg7[%c0 for %c152] : !stream.resource<external>{%c152}
      }
      stream.cmd.dispatch @_main$async_dispatch_29::@embedded_elf_x86_64::@_main$async_dispatch_29_elementwise_broadcast_9x2_f32 {
        ro %arg8[%c0 for %c128] : !stream.resource<transient>{%c128},
        rw %arg7[%c0 for %c152] : !stream.resource<external>{%c152}
      }
    } => !stream.timepoint
    %28 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%27) => %result_21 : !stream.resource<transient>{%c128} => !stream.timepoint
    %29 = stream.timepoint.await %28 => %result_19 : !stream.resource<external>{%c152}
    cf.br ^bb5(%29 : !stream.resource<external>)
  ^bb5(%30: !stream.resource<external>):  // 2 preds: ^bb3, ^bb4
    %31 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %30 : tensor<19x2xf32> in !stream.resource<external>{%c152} -> !hal.buffer_view
    %32 = stream.timepoint.await %7 => %result : !stream.resource<external>{%c12}
    %33 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %32 : tensor<3xf32> in !stream.resource<external>{%c12} -> !hal.buffer_view
    hal.fence.signal<%arg5 : !hal.fence>
    util.return %31, %33 : !hal.buffer_view, !hal.buffer_view
  }
  util.func public @main(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view, %arg3: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %0 = util.null : !hal.fence
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %1:2 = util.call @_main$async(%arg0, %arg1, %arg2, %arg3, %0, %fence) : (!hal.buffer_view, !hal.buffer_view, !hal.buffer_view, !hal.buffer_view, !hal.fence, !hal.fence) -> (!hal.buffer_view, !hal.buffer_view)
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) flags("None") : i32
    util.return %1#0, %1#1 : !hal.buffer_view, !hal.buffer_view
  }
}
